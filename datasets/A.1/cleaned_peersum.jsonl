{"paper_id": "iclr_2018_ryQu7f-RZ", "review_text": "this paper analyzes a problem with the convergence of adam, and presents a solution. it identifies an error in the convergence proof of adam which also applies to related methods such as rmsprop and gives a simple example where it fails to converge. the paper then repairs the algorithm in a way that guarantees convergence without introducing much computational or memory overhead. there ought to be a lot of interest in this paper adam is a widely used algorithm, but sometimes underperforms sgd on certain problems, and this could be part of the explanation. the fix is both principled and practical. overall, this is a strong paper, and i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2018_HkL7n1-0b", "review_text": "this paper proposes a new generative model that has the stability of variational autoencoders vae while producing better samples. the authors clearly compare their work to previous efforts that combine vaes and generative adversarial networks with similar goals. authors show that the proposed algorithm is a generalization of adversarial autoencoder aae and minimizes wasserstein distance between model and target distribution. the paper is well written with convincing results. reviewers agree that the algorithm is novel and practical; and close connections of the algorithm to related approaches are clearly discussed with useful insights. overall, the paper is strong and i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2018_Hy6GHpkCW", "review_text": "this work presents a rnn tailored to generate sketch drawings. the model has novel elements and advances specific to the considered task, and allows for free generation as well as generation with partial input. the results are very satisfactory. importantly, as part of this work a large dataset of sketch drawings is released. the only negative aspect is the insufficient evaluation, as pointed out by r1 who points out the need for baselines and evaluation metrics. r1s concerns have been acknowledged by the authors but not really addressed in the revision. still, this is a very interesting contribution.", "accepted": 1}
{"paper_id": "iclr_2018_SJaP_-xAb", "review_text": "in this paper the authors show how to allow deep neural network training on logged contextual bandit feedback. the newly introduced framework comprises a new kind of output layer and an associated training procedure. this is a solid piece of work and a significant contribution to the literature, opening up the way for applications of deep neural networks when losses based on manual feedback and labels is not possible.", "accepted": 1}
{"paper_id": "iclr_2018_rk07ZXZRb", "review_text": "this is a paper introducing a hierarchical rl method which incorporates the learning of a latent space, which enables the sharing of learned skills. the reviewers unanimously rate this as a good paper. they suggest that it can be further improved by demonstrating the effectiveness through more experiments, especially since this is a rather generic framework. to some extent, the authors have addressed this concern in the rebuttal.", "accepted": null}
{"paper_id": "iclr_2018_S1HlA-ZAZ", "review_text": "this paper presents a distributed memory architecture based on a generative model with a vaelike training criterion. the claim is that this approach is easier to train than other memorybased architectures. the model seems sound, and it is described clearly. the experimental validation seems a bit limited most of the comparisons are against plain vaes, which arent a memorybased architecture. the discussion of oneshot generalization is confusing, since the task is modified without justification to have many categories and samples per category. the experiment of section 4.4 seems promising, but this needs to be expanded to more tasks and baselines since its the only experiment that really tests the kanerva machine as a memory architecture. despite these concerns, i think the idea is promising and this paper contributes usefully to the discussion, so i recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2018_S1ANxQW0b", "review_text": "the main idea of policyasinference is not new, but it seems to be the first application of this idea to deep rl, and is somewhat well motivated. the computational details get a bit hairy, but the good experimental results and the inclusion of ablation studies pushes this above the bar.", "accepted": null}
{"paper_id": "iclr_2018_SyX0IeWAW", "review_text": "this paper presents a fairly straightforward algorithm for learning a set of subcontrollers that can be reused between tasks. the development of these concepts in a relatively clear way is a nice contribution. however, the real problem is how niche the setup is. however, its over the bar in general.", "accepted": 0}
{"paper_id": "iclr_2018_rywDjg-RW", "review_text": "the pros and cons of this paper cited by the reviewers can be summarized below pros  the method proposed here is highly technically sophisticated and appropriate for the problem of program synthesis from examples  the results are convincing, demonstrating that the proposed method is able to greatly speed up search in an existing synthesis system cons  the contribution in terms of machine learning or representation learning is minimal mainly adding an lstm to an existing system  the overall system itself is quite complicated, which might raise the barrier of entry to other researchers who might want to follow the work, limiting impact in our decision, the fact that the paper significantly moves forward the state of the art in this area outweighs the concerns about lack of machine learning contribution or barrier of entry.", "accepted": 1}
{"paper_id": "iclr_2018_rkHVZWZAZ", "review_text": "this paper presents a nice set of results on a new rl algorithm. the main downside is the limitation to the atari domain, but otherwise the ablation studies are nice and the results are strong.", "accepted": null}
{"paper_id": "iclr_2018_rJGZq6g0-", "review_text": "an interesting paper, generally wellwritten. though it would be nice to see that the methods and observations generalize to other datasets, it is probably too much to ask as datasets with required properties do not seem to exist. there is a clear consensus to accept the paper.  an interesting extension of previous work on emergent communications e.g., referential games  well written paper", "accepted": null}
{"paper_id": "iclr_2018_HJIoJWZCZ", "review_text": "the general consensus is that this method provides a practical and interesting approach to unsupervised domain adaptation. one reviewer had concerns with comparing to state of the art baselines, but those have been addressed in the revision. there were also issues concerning correctness due to a typo. based on the responses, and on the pseudocode, it seems like there wasnt an issue with the results, just in the way the entropy objective was reported. you may want to consider reporting the example given by reviewer 2 as a negative example where you expect the method to fail. this will be helpful for researchers using and building on your paper.", "accepted": 1}
{"paper_id": "iclr_2018_H1kG7GZAW", "review_text": "thank you for submitting you paper to iclr. the reviewers are all in agreement that the paper is suitable for publication, each revising their score upwards in response to the revision that has made the paper stronger. the authors may want to consider adding a discussion about whether the simple standard gaussian prior, which is invariant under transformation by an orthogonal matrix, is a sensible one if the objective is to find disentangled representations. alternatives, such as sparse priors, might be more sensible if a modelbased solution to this problem is sought.", "accepted": null}
{"paper_id": "iclr_2018_BkwHObbRZ", "review_text": "i recommend acceptance based on the reviews. the paper makes novel contributions to learning onehidden layer neural networks and designing new objective function with no bad local optima. there is one point that the paper is missing. it only mentions janzamin et al in the passing. janzamin et al propose using score function framework for designing alternative objective function. for the case of gaussian input that this paper considers, the score function reduces to hermite polynomials. lack of discussion about this connection is weird. there should be proper acknowledgement of prior work. also missing are some of the key papers on tensor decomposition and its analysis i think there are enough contributions in the paper for acceptance irrespective of the above aspect.", "accepted": 1}
{"paper_id": "iclr_2018_SyZI0GWCZ", "review_text": "the reviewers all agree this is a well written and interesting paper describing a novel black box adversarial attack. there were missing relevant references in the original submission, but these have been added. i would suggest the authors follow the reviewer suggestions on claims of generality beyond cnn; although there may not be anything obvious stopping this method from working more generally, it hasnt been tested in this work. even if you keep the title you might be more careful to frame the body in the context of cnns.", "accepted": 1}
{"paper_id": "iclr_2018_rkPLzgZAZ", "review_text": "important problem modular continual rl and novel contributions. the initial submission was judged to be a little dense and hard to read, but the authors have been responsive in responding and updating the paper. i support accepting this paper.", "accepted": 1}
{"paper_id": "iclr_2018_rJYFzMZC-", "review_text": "this submission proposes a novel extension of existing recurrent networks that focus on capturing longterm dependencies via tracking entitiestheir statesand tested it on a new task. theres a concern that the proposed approach is heavily engineered toward the proposed task and may not be applicable to other tasks, which i fully agree with. i however find the proposed approach and the authors justification to be thorough enough, and for now, recommend it to be accepted.", "accepted": 1}
{"paper_id": "iclr_2018_BkeqO7x0-", "review_text": "this work adapts cycle gan to the problem of decipherment with some success. its still an early result, but all the reviewers have found it to be interesting and worthwhile for publication.", "accepted": 1}
{"paper_id": "iclr_2018_SyJS-OgR-", "review_text": "this submission proposes a learning algorithm for resnets based on their interpreration of them as a discrete approximation to a continuoustime dynamical system. all the reviewers have found the submission to be clearly written, well motivated and have proposed an interesting and effective learning algorithm for resnets.", "accepted": null}
{"paper_id": "iclr_2018_SJcKhk-Ab", "review_text": "all the reviews like the theoretical result presented in the paper which relates the gating mechanism of lstms and grus to time invariance  warping. the theoretical result is great and is used to propose a heuristic for setting biases when time invariance scales are known. the experiments are not mindboggling, but none of the reviewers seem to think thats a show stopper.", "accepted": 1}
{"paper_id": "iclr_2018_r1Dx7fbCW", "review_text": "well motivated and well received by all of the expert reviewers. the ac recommends that the paper be accepted.", "accepted": 1}
{"paper_id": "iclr_2018_SJ1Xmf-Rb", "review_text": "a novel dual memory system inspired by brain for the important incremental learning and very good results.", "accepted": null}
{"paper_id": "iclr_2018_BydjJte0-", "review_text": "novel way of analyzing neural networks to predict nn attributes such as architecture, training method, batch size etc. and the method works surprisingly good on the mnist and imagenet.", "accepted": null}
{"paper_id": "iclr_2018_SyyGPP0TZ", "review_text": "this paper presents a simple yet effective method for weight dropping for an lstm that requires no modification of an rnn cells formulation. experimental results shows good perplexity results on benchmarks compared to many baselines. all reviewers agree that the paper will bring good contribution to the conference.", "accepted": null}
{"paper_id": "iclr_2018_BJE-4xW0W", "review_text": "this paper proposes an interesting machinery around generative adversarial networks to enable sampling not only from conditional observational distributions but also from interventional distributions. this is an important contribution as this means that we can obtain samples with desired properties that may not be present in the training set; useful in applications such as ones involving fairness and also when data collection is expensive and biased. the main component called the causal controller models the label dependencies and drives the standard conditional gan. as reviewers point out, the causal controller assumes the knowledge of the causal graph which is a limitation as this is not known a priori in many applications. nevertheless, this is a strong paper that convincingly demonstrates a novel approach to incorporate causal structure into generative models. this should be of great interest to the community and may lead to interesting applications that exploit causality. i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2018_SkVqXOxCb", "review_text": "the paper provides an interesting take on gan training based on coulomb dynamics. the proposed formulation is theoretically well motivated and authors provide guarantees for convergence. reviewers agree that the theoretical analysis is interesting but are not completely impressed by the results. the method addresses mode collapse issue but still lacks in sample quality. nevertheless, reviewers agree that this is a good step towards the understanding of gan training.", "accepted": null}
{"paper_id": "iclr_2018_BJQRKzbA-", "review_text": "pros 1. overall, the paper is wellwritten, clear in its exposition and technically sound. 2. with some caveats, an independent team concluded that the results were largely reproducible 3. the key idea is a smart evolution scheme. it circumvents the traditional tradeoff between search space size and complexity of the found models. 4. the implementation seems technically sound. cons 1. the results were a bit overstated the authors promise to correct 2. could benefit from more comparison with other approaches e.g. rl", "accepted": 1}
{"paper_id": "iclr_2018_ByOfBggRZ", "review_text": "the paper proposes a way of detecting statistical interactions in a dataset based on the weights learned by a dnn. the idea is interesting and quite useful as is showcased in the experiments. the reviewers feel that the paper is also quite well written and easy to follow.", "accepted": null}
{"paper_id": "iclr_2018_SJLlmG-AZ", "review_text": "an interesting model, for an interesting problem but perhaps of limited applicability  doesnt achieve state of the art results on practical tasks. paper has other limitations, though the authors have addressed some in rebuttals.", "accepted": 0}
{"paper_id": "iclr_2018_r1HhRfWRZ", "review_text": "since this seems interesting, i suggest to accept this paper at the conference. however, there are still some serious issues with the paper, including missing references.", "accepted": 0}
{"paper_id": "iclr_2018_HyZoi-WRb", "review_text": "the authors analyze the iwae bound as an estimator of the marginal loglikelihood and show how to reduce its bias by using the jackknife. they then evaluate the effect of using the resulting estimator jvi for training and evaluating vaes on mnist. this is an interesting and well written paper. it could be improved by including a convincing explanation of the relatively poor performance of the jvitrained, jvievaluated models.", "accepted": null}
{"paper_id": "iclr_2018_S1cZsf-RW", "review_text": "the paper proposes a new approach for scalable training of deep topic models based on amortized inference for the local parameters and stochasticgradient mcmc for the global ones. the key aspect of the method involves using weibull distributions instead of gammas to model the variational posteriors over the local parameters, enabling the use of the reparameterization trick. the resulting methods perform slightly worse that the gibbssamplingbased approaches but are much faster at test time. amortized inference has already been applied to topic models, but the use of weibull posteriors proposed here appears novel. however, there seems to be no clear advantage to using stochasticgradient mcmc instead of vanilla sgd to infer the global parameters, so the value of this aspect of whai unclear.", "accepted": null}
{"paper_id": "iclr_2018_B1nZ1weCZ", "review_text": "the paper contains an interesting way to do online multi task learning, by borrowing ideas from active learning and comparing and contrasting a number of ways on the arcade learning environment. like the reviewers, i have some concerns about using the target scores and i think more analysis would be needed to see just how robust this method is to the choicedistribution of target scores the authors mention that things dont break down as long as the scores are reasonable, but thats not a particularly empirical nor precise statement. my inclination is to accept the paper, because of the earnest efforts made by the authors in understanding how dua4c works. however, i do agree that the paper should have a larger focus on that basically section 6 should be expanded, and the experiments should be rerun in such a way that the setup for dua4c is more favorable in terms of hyperparameter optimization. if theres any gap between any of the proposed methods and dua4c, then this would warrant further analysis of course since it would mean that theres an advantage to using target scores.", "accepted": null}
{"paper_id": "iclr_2018_H1dh6Ax0Z", "review_text": "this is a nicely written paper proposing a reasonably interesting extension to existing work e.g. vpn. while the atari results are not particular convincing, they do show promise. i encourage the authors to carefully take the reviewers comment into consideration and incorporate them to the final version.", "accepted": 1}
{"paper_id": "iclr_2018_B13njo1R-", "review_text": "the authors propose an architecture that uses a curriculum and multitask distillation to gain higher performance without forgetting. the paper is largely a smart composition of known methods, and it requires keeping data from all tasks to do the distillation, so it is not truly a scalable continual learning approach. there were a lot of concerns about clarity in the manuscript, but many of these have been assuaged by an update to the paper. this is a borderline paper, but the authors rebuttal and update probably tip it towards acceptance.", "accepted": null}
{"paper_id": "iclr_2018_SJJQVZW0b", "review_text": "this method has a lot of strong points, but the reviewers had concerns about baselines, comparisons, and handengineered aspects of the method. the authors gave a strong rebuttal and made substantial updates to the paper to address the concerns. i think that this has saved the submission and tipped the balance towards acceptance.", "accepted": null}
{"paper_id": "iclr_2018_SkFqf0lAZ", "review_text": "this paper provides a comparison of different types of a memory augmented models and extends some of them to beyond their simple form. reviewers found the paper to be clearly written, saying it nice introduction to the topic and noting that they enjoyed reading this paper. in general though there was a feeling that the substance of the work is limited. one reviewer complained that experiments were limited to small english datasets ptb and wikitext2 and asked why they didnt try machine translation or speech recognition. the authors note that they did try the linzen dataset, and while the reviewers found the experiments impressive, the task itself felt artificial . another felt that the multipop model alone was not too large a contribution. the actual experiments in the work are well done, although given the fact that the models are known there was expectation of more indepth analysis of the different models. overall this is a good empirical study, which shows the limited gains achieved by these models, a nevertheless useful piece of information for those working in this area.", "accepted": 1}
{"paper_id": "iclr_2018_HJjvxl-Cb", "review_text": "the reviewers agree that the results are promising and there are some interesting and novel aspect to the formulation. however, two of the reviews have raised concerns regarding the exposition and the discussion of previous work. the paper benefits from a detailed description of soft qlearning, pcl, and offpolicy actorcritic algorithms, and how sac is different from those. instead of differentiating against previous work by saying soft qlearning and pcl are not actorcritic algorithms, discuss the similarities and differences and present empirical evaluation.", "accepted": 0}
{"paper_id": "iclr_2018_rk6H0ZbRb", "review_text": "i am somewhat of two minds from the paper. the authors show empirically that adversarial perturbation error follows power law and looks for a possible explanation. the tie in with generalization is not clear to me and makes me wonder how to evaluate the significance of the finding of the power law distribution.. on the other hand, the authors present an interesting analysis, show that the finding holds in all the cases they explored and also found that architecture search can be used to find neural networks that are more resilient to adversarial search the last shouldnt be surprising if that was indeed the training criterion. all in all, i think that while the paper needs a further iteration prior to publication, it already contains interesting bits that could spur very interesting discussion at the workshop. side note theres a reference missing on page 4, first paragraph", "accepted": 1}
{"paper_id": "iclr_2018_r1hsJCe0Z", "review_text": "to summarize the pros and cons pro  interesting application  impressive results on a difficult task  nice discussion of results and informative examples  clear presentation, easy to read. con  the method appears to be highly specialized to the four bug types. it is not clear how generalizable it will be to more complex bugs, and to the real application scenarios where we are dealing with open world classification and there is not fixed set of possible bugs. there were additional reviewer complaints that comparison to the simple seqtoseq baseline may not be fair, but i believe that these have been addressed appropriately by the authors response noting that all other reasonable baselines require test cases, which is an extra data requirement that is not available in many realworld applications of interest. this paper is somewhat on the borderline, and given the competitive nature of a top conference like iclr i feel that it does not quite make the cut. it is definitely a good candidate for presentation at the workshop however.", "accepted": 0}
{"paper_id": "iclr_2018_Sk4w0A0Tb", "review_text": "although the authors argue that their experiments were selected from the earlier work from which major comparing approaches were taken, the reviewers found the empirical result to be weak. why not some real tasks i do not believe babi nor ptb could be considered real that could clearly reveal the superiority of the proposed unit against existing ones?", "accepted": 0}
{"paper_id": "iclr_2018_SJTB5GZCb", "review_text": "interesting novel extension of equilibrium propagation, as a biologically more plausible alternative to backpropagation, with encouraging initial experimental validation.  currently lacks theoretical guarantees regarding convergence of the algorithm to a meaningful result  experimental study should be more extensive to support the claims", "accepted": 0}
{"paper_id": "iclr_2018_BJInEZsTb", "review_text": "this paper compares autoencoder and ganbased methods for 3d point cloud representation and generation, as well as new and welcome metrics for quantitatively evaluating generative models. the experiments form a good but still a bit too incomplete exploration of this topic. more analysis is needed to calibrate the new metrics. qualitative analysis would be very helpful here to complement and calibrate the quantitative ones. the writing also needs improvement for clarity and verbosity. the author replies and revisions are very helpful, but there is still some way to go on the issues above. overall, the committee is intersting and recommends this paper for the workshop track.", "accepted": 1}
{"paper_id": "iclr_2018_H18WqugAb", "review_text": "reviewers were somewhat lukewarm about this paper, which seeks to present an analysis of the limitations of sequence models when it comes to understanding compositionality. somewhat synthetic experiments show that such models generalise poorly on patterns not attested during training, even if the information required to interpret such patterns is present in the training data when combined with knowledge of the compositional structure of the language. this conclusion seems as unsurprising to me as it does to some of the reviewers, so i would be inclined to agree with the moderate enthusiasm two out of three reviewers have for the paper, and suggest that it be redirected to the workshop track. other criticisms found in the review have to do with the lack of any discussion on the topic of how to address these limitations, or what message to take home from these empirical observations. it would be good for the authors to consider how to evaluate their claims against real data, to avoid the accusation that the conclusion is trivial from the task set up. therefore, while well written, it is not clear that the paper is ready for the main conference. it could potentially generate interesting discussion, so i am happy for it to be invited to the workshop track, or failing that, to suggest that further work on this topic be done before the paper is accepted somewhere.", "accepted": null}
{"paper_id": "iclr_2018_BJypUGZ0Z", "review_text": "the paper proposes to use simple regression models for predicting the accuracy of a neural network based on its initial training curve, architecture, and hyperparameters; this can be used for speeding up architecture search. while this is an interesting direction and the presented experiments look quite encouraging, the paper would benefit from more evaluation, as suggested by reviewers, especially within stateoftheart architecture search frameworks andor large datasets.", "accepted": 0}
{"paper_id": "iclr_2018_SkOb1Fl0Z", "review_text": "the paper presents a domainspecific language for rnn architecture search, which can be used in combination with learned ranking function or rlbased search. while the approach is interesting and novel, the paper would benefit from an improved evaluation, as pointed out by reviewers. for example, the paper currently evaluated coredslranking for language modelling and extendeddslrl for machine translation. the authors should use the same evaluation protocol on all tasks, and also compare with the stateoftheart mt approaches.", "accepted": 0}
{"paper_id": "iclr_2018_SJvu-GW0b", "review_text": "the reviewers agree that the problem being studied is important and relevant but express serious concerns. i recommend the authors to carefully go through the reviews and significantly scale up their experiments.", "accepted": 0}
{"paper_id": "iclr_2018_rJv4XWZA-", "review_text": "this paper presents an interesting idea employ gans in a manner that guarantees the generation of differentially private data. the reviewers liked the motivation but identified various issues. also, the authors themselves discovered some problems in their formulation; on behalf of the community, thanks for letting the readers know. the discovered issues will need to be reviewed in a future submission.", "accepted": 0}
{"paper_id": "iclr_2018_r1h2DllAW", "review_text": "this paper presents a somewhat new approach to training neural nets with ternary or lowprecision weights. however the bayesian motivation doesnt translate into an elegant and selftuning method, and ends up seeming kind of complicated and adhoc. the results also seem somewhat toy. the paper is fairly clearly written, however.", "accepted": null}
{"paper_id": "iclr_2018_S1Y7OOlRZ", "review_text": "this paper presents a simple tweak to hyperband to allow it to be run asynchonously on a large cluster, and contains reasonably largescale experiments. the paper is written clearly enough, and will be of interest to anyone running largescale ml experiments. however, it falls below the bar by 1 not exploring the space of related ideas more. 2 not providing novel insights. 3 not attempting to compare against modelbased parallel approaches.", "accepted": null}
{"paper_id": "iclr_2018_r1HNP0eCW", "review_text": "the pros and cons of this paper cited by the reviewers can be summarized below pros  the motivation of the problem is presented well  the architecture is simple and potentially applicable to realworld applications cons  the novel methodological contribution is limited to nonexistant  comparison against other relevant baselines is missing, and the baseline is not strong  the evaluation methodology does not follows standard practice in ir, and thus it is difficult to analyze and compare results  paper is hard to read and requires proofreading considering these pros and cons, my conclusion is that this paper is not up to the standards of iclr.", "accepted": null}
{"paper_id": "iclr_2018_H1Ww66x0-", "review_text": "the output kernel idea for lifelong learning is interesting, but insufficiently developed in the current draft.", "accepted": 0}
{"paper_id": "iclr_2018_rJa90ceAb", "review_text": "the paper proposes a method for learning convolutional networks with dynamic inputconditioned filters. there are several prior work along this idea, but there is no comparison agaist them. overall, experimental results are not convincing enough.", "accepted": 0}
{"paper_id": "iclr_2018_ry4S90l0b", "review_text": "the paper presents selftraining scheme for gans. the proposed idea is simple but reasonable, and the experimental results show promise for mnist and cifar10. however, the novelty of the proposed method seems relatively small and experimental results lack comparison against other stronger baselines e.g., stateoftheart semisupervised methods. presentation needs to be improved. more comprehensive experiments on other datasets would also strengthen the future version of the paper.", "accepted": 0}
{"paper_id": "iclr_2018_HJRV1ZZAW", "review_text": "the key motivation for the work is producing both an efficient parallelizable  fast and accurate reading comprehension model. at least two reviewers are not convinced that this goal is really achieved e.g., no comparison to hierarchical modeling, performance is not as strong. i also share concerns of r1 that, without proper ablation search and more careful architecture choice, the modeling decisions seem somewhat arbitrary.  the goal of achieving effective reading comprehesion models is important  alternative parallelization techniques e.g., hierarchical modeling are not considered  ablation studies  more systematic architecture search are missing  it is not clear that the drop in accuracy can be justified by the potential efficiency gains also see details in r3  no author response to them", "accepted": 0}
{"paper_id": "iclr_2018_rkhCSO4T-", "review_text": "thank you for submitting you paper to iclr. iclr. the consensus from the reviewers is that this is not quite ready for publication.", "accepted": 0}
{"paper_id": "iclr_2018_HJjePwx0-", "review_text": "there are two parts to this paper 1 an efficient procedure for solving trustregion subproblems in secondorder optimization of neural nets, and 2 evidence that the proposed trust region method leads to better generalization performance than sgd in the largebatch setting. in both cases, there are some promising leads here. but it feels like two separate papers here, and im not sure either individual contribution is well enough supported to merit publication in iclr. for 1, the contribution is novel and potentially useful, to the best of my knowledge. but as theres been a lot of work on trust region solvers and secondorder optimization of neural nets more generally, claims about computational efficiency would require comparisons against existing methods. the focus on efficiency also doesnt seem to fit with the experiments section, where the proposed method optimizes less efficiently than sgd and is instead meant to provide a regularization benefit. for 2, its an interesting empirical finding that the method improves generalization, but the explanation for this is very handwavy. if secondorder optimization in general turned out to help with sharp minima, this would be an interesting finding indeed, but it doesnt seem to be supported by other work in the area. the training curves in table 1 are interesting, but dont really distinguish the claims of section 4.5 from other possible hypotheses.", "accepted": null}
{"paper_id": "iclr_2018_BJInMmWC-", "review_text": "this paper presents a novel model for generating images and natural language descriptions simultaneously. the aim is to distangle representations learned for image generation by connecting them to the paired text. the reviews praise the problem setup and the mathematical formulation. however they point out significant issues with the clarity of the presentation in particular the diagrams, citations, and optimization procedure in general. they also point out issues with the experimental setup in terms of datasets used and lack of natural images for the tasks in question. reviews are impressively thorough and should be of use for a future submission.", "accepted": 0}
{"paper_id": "iclr_2018_r1CE9GWR-", "review_text": "while the reviewers agree that this is an important topic, there are numerous concerns novelty, correctness and limitations.", "accepted": 0}
{"paper_id": "iclr_2018_SJ1fQYlCZ", "review_text": "the authors give evidence that is certain cases, the ordering of sample inclusion in a curriculum is not important. however, the reviewers believe the experiments are inconclusive, both in the sense that as reported, they do not demonstrate the authors hypothesis, and that they may leave out many relevant factors of variation such as hyperparameter tuning.", "accepted": 0}
{"paper_id": "iclr_2018_rJaE2alRW", "review_text": "the reviewers feel that the novelties in the model are not significant. furthermore, they suggest that empirical results could be improved by 1 analyses showing how the significance network functions and directly measuring its impact 2 more reproducible experiments. in particular, this is really an applications paper, and the experiments on the main application are not reproducible because the data is proprietary. 3 baselines that make assumptions more in line with the authors problem setup", "accepted": 0}
{"paper_id": "iclr_2018_HkMCybx0-", "review_text": "the authors introduce a new activation function which is similar in shape to elu, but is faster to compute. the reviewers consider this to not be a significant innovation because the amount of time spent in computing the activation function is small compared to other neural network operations.", "accepted": 0}
{"paper_id": "iclr_2018_H139Q_gAW", "review_text": "this paper proposes to combine depthwise separable convolutions developed for 2d grids with recent graph convolutional architectures. the resulting architecture can be seen as learning both node and edge features, the latter encoding node similarities with learnt weights. reviewers agreed that this is an interesting line of work, but that further work is needed in both the presentation and the experimental front before publication. in particular, the paper should also compare against recent models such as the mpnn from gilmer et al that also propose edge feature learning. therefore, the ac recommends rejection at this time.", "accepted": 0}
{"paper_id": "iclr_2018_BJ78bJZCZ", "review_text": "rda improves on rwa, but even so, the model is inferior to the other standard rnn models. as a result r1 and r3 question the motivation for the use of this model  something the authors should motivate.", "accepted": 0}
{"paper_id": "iclr_2018_SkHkeixAW", "review_text": "the paper is a wellwritten review of regularization approaches in deep learning. it does not offer novel approaches or novel insight with empirically demonstrated usefulness  iclr is not the appropriate venue for it.", "accepted": 0}
{"paper_id": "iclr_2018_rkQu4Wb0Z", "review_text": "the paper received scores of 5,5,5, with the reviewers agreeing the paper was marginally below the acceptance threshold. the main issue, raised by both r2 and r3 was that connection between representation learning in deep nets and coding theory was not fully justifiedmade. with no reviewer advocating acceptance, it is not possible to accept the paper unfortunately.", "accepted": null}
{"paper_id": "iclr_2018_Syl3_2JCZ", "review_text": "this work extends druckmann and chklowskii, 2012 and demonstrates some interesting properties of the new model. this would be of interest to a neuroscience audience, but the focus is off for iclr.", "accepted": 0}
{"paper_id": "iclr_2018_B1NGT8xCZ", "review_text": "pros  nice way to formulate domain adaptation in a bayesian framework that explains why autoencoder and domain difference losses are useful. cons  model closely follows the framework, but the overall strategy is similar to previous models but with improved rationale.  experimental section can be improved. it would interesting to explore and develop the relationship between the proposed technique and tzeng et al. given the aforementioned cons, the ac is recommending that the paper be rejected.", "accepted": null}
{"paper_id": "iclr_2018_B1p461b0W", "review_text": "the paper studies the robustness of deep learning against label noise on mnist, cifar10 and imagenet. but the generalization of the claim deep learning is robust to massive label noise is still questionable due to the limited noise types investigated. the paper presents some tricks to improve learning with high label noise batch size and learning rate, which is not novel enough.", "accepted": 0}
{"paper_id": "iclr_2018_BJQPG5lR-", "review_text": "pros  interesting perspective on training deep networks cons  not a lot of practical significance why would one want to use this algorithm over standard methods like resnets or highway networks given that the proposed algorithm is more complex than established methods?", "accepted": null}
{"paper_id": "iclr_2018_HJSA_e1AW", "review_text": "the paper proposes a modification to adam which is intended to ensure that the direction of weight update lies in the span of the historical gradients and to ensure that the effective learning rate does not decrease as the magnitudes of the weights increase. the reviewers wanted a clearer justification of the changes made to adam and a more extensive evaluation, and held to this opinion after reading the authors rebuttal and revisions. pros  the basic idea of treating the direction and magnitude separately in the optimization is interesting. cons  insufficient evaluation of the new method.  more justification and analysis needed for the modifications. for example, are there circumstances under which they will fail?  the modification to adam and batchnormalized softmax idea are orthogonal to one another, making for a less coherent story.  proposed method does not have better generalization performance than sgd.  concern that constraining weight vectors to the unit sphere can harm generalization.", "accepted": 0}
{"paper_id": "iclr_2018_r1Kr3TyAb", "review_text": "two of the reviewers liked the intent of the paper  to analyze gradient flow in residual networks and understand the tradeoffs between width and depth in such networks. however, all reviewers flagged a number of problems in the paper, and the authors did not participate in the discussion period. pros  interesting analysis suggests wider, shallower resnets should outperform narrower, deeper resnets, and empirical results support the analysis. cons  independence assumption on weights is not valid after any weight updates.  the notation is not as clear as it should be.  empirical results would be more convincing if obtained on several tasks.  the architecture analyzed in the paper is not standard, so it isnt clear how relevant it is for other practitioners.  analysis and paper should take into account other work in this area, e.g. veit et al., 2016 and schoenholz et al., 2017.", "accepted": 0}
{"paper_id": "iclr_2018_r1AMITFaW", "review_text": "the reviewers of the paper are not very enthusiastic of the new model proposed, nor are they very happy with the experiments presented. it is unclear from both the pos tagging and dependency parsing results where they stand with respect to state of the art methods that do not use rnns. we understand that the idea is to compare various rnn architectures, but it is surprising that the authors do not show any comparisons with other methods in the literature. the idea of truncating sequences beyond a certain length is also a really strange choice. addressing the concerns of the reviewers will lead to a much stronger paper in the future.", "accepted": 0}
{"paper_id": "iclr_2018_SJvrXqvaZ", "review_text": "reviewers are unanimous in scoring this paper below threshold for acceptance. the authors did not submit any rebuttals of the reviews. pros paper is generally clear. hardware results are valuable. cons limited simulation results. proposed method is not really novel. insufficient empirical validation of the approach.", "accepted": 0}
{"paper_id": "iclr_2018_ryserbZR-", "review_text": "authors present a method for disease classification and localization in histopathology images. standard image processing techniques are used to extract and normalize tiles of tissue, after which features are extracted from pertained networks. a 1d convolutional filter is applied to the bag of features from the tiles along the tile dimension, kernel filter size equal to dimensionality of feature vector. the max r and min r values are kept as input into a neural network for classification, and thresholding of these values provides localization for disease  nondisease. pro  potential to reduce annotation complexity of datasets while producing predictions and localization con  results are not great. if anything, results reaffirm why strong annotations are necessary.  several reviewer concerns regarding novelty of proposed method. while authors have made clear the distinctions from prior art, the significance of those changes are debated. given the current proscons, the committee feels the paper is not ready for acceptance in its current form.", "accepted": null}
{"paper_id": "iclr_2018_ry4SNTe0-", "review_text": "the paper aims to combine wasserstein gan with improved gan framework for semisupervised learning. the reviewers unanimously agree that  the paper lacks novelty and such approaches have been tried before.  the approach does not make sufficient gains over the baselines and stronger baselines are missing.  the paper is not well written and experimental results are not satisfactory.", "accepted": 0}
{"paper_id": "iclr_2018_BJgd7m0xRZ", "review_text": "the reviewers have unanimously expressed concerns about clarity, novelty, sound theoretical justification and intuitive motivation of the proposed approach.", "accepted": 0}
{"paper_id": "iclr_2018_B13EC5u6W", "review_text": "the paper proposes a semisupervised method to make deep learning more interpretable and at the same time be accurate on small datasets. the main idea is to learn dense representations from unlabelled data and then use those for building classifiers on small datasets as well as generate visual explanations. the idea is interesting, however, as one reviewer points out the presentation is poor. for instance, table 2 is not understandable. given the high standards of iclr this cannot be ignored especially given the fact that the authors had the benefit of updating the paper which is a luxury for conference submissions.", "accepted": 1}
{"paper_id": "iclr_2018_HJUOHGWRb", "review_text": "the paper proposes a method to learn and explain simultaneously. the explanations are generated as part of the learning and in some sense come for free. it also goes the other way in that the explanations also help performance in simpler settings. reviewers found the paper easy to follow and the idea has some value, however, the related work is sparse and consequently comparison to existing stateoftheart explanation methods is also sparse. these are nontrivial concerns which should have been addressed in the main article not hidden away in the supplement.", "accepted": null}
{"paper_id": "iclr_2018_H1xJjlbAZ", "review_text": "the paper tries to show that many of the stateoftheart interpretability methods are brittle and do not provide consistent stable explanations. the authors show this by perturbing even randomly the inputs so that the differences are imperceptible to a human observer but the interpretability methods provide completely different explanations. although the output class is maintained before and after the perturbation it is not clear to me or the reviewers why one shouldnt have different explanations. the difference in explanations can be attributed to the fragility of the learned models highly nonsmooth decision boundaries rather than the explanation methods. this is a critical point and has to come out more clearly in the paper.", "accepted": 0}
{"paper_id": "iclr_2018_r1ayG7WRZ", "review_text": "the reviewers highlight a lack of technical content and poor writing. they all agree on rejection. there was no author rebuttal or pointer to a new version.", "accepted": 0}
{"paper_id": "iclr_2018_HkGcX--0-", "review_text": "to ensure that a vae with a powerful autoregressive decoder does not ignore its latent variables, the authors propose adding an extra term to the elbo, corresponding to a reconstruction with an auxiliary nonautoregressive decoder. this does indeed produce models that use latent variables and with some tuning of the weight on the kl term perform as well as the underlying autoregressive model alone. however, as the reviewers pointed out, the paper does not demonstrate the value of the resulting models. if the goal is learning meaningful latent representations, then the quality of the representations should be evaluated empirically. currently it is not clear whether that the proposed approach would yield better representations than a vae with a nonautoregressive decoder or a vae with an autoregressive decoder trained using the free bits trick of kingma et al. 2016. this is certainly an interesting idea, but without a proper evaluation it is impossible to judge its value.", "accepted": null}
{"paper_id": "iclr_2018_S16FPMgRZ", "review_text": "this paper proposes methods for replacing parts of neural networks with tensors, the values of which are efficiently estimated through factorisation methods. the paper is well written and clear, but the two main objections from reviewers surround the novelty and evaluation of the method proposed. i am conscious that the authors have responded to reviewers on the topic of novelty, but the case could be made more strongly in the paper, perhaps by showing significant improvements over alternatives. the evaluation was considered weak by reviewers, in particular due to the lack of comparable baselines. interesting work, but im afraid on the basis of the reviews, i must recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2018_SkqV-XZRZ", "review_text": "this paper proposes a method for performing stochastic variational inference for bidirectional lstms through introducing an additional latent variable that induces a dependence between the forward and backward directions. the authors demonstrate that their method achieves very strong empirical performance loglikelihood on test data on the benchmark timit and blizzard datasets. the paper is borderline in terms of scores with a 7, 6 and 4. unfortunately the highest rating also corresponds to the least thorough review and that review seems to indicate that the reviewer found the technical exposition confusing. anonreviewer2 also found the writing confusing and discovered mistakes in the technical aspects of the paper e.g. in eq 1. unfortunately, the reviewer who seemed to find the paper most easy to understand also gave the lowest score. a trend among the reviewers and anonymous comments was that the paper didnt do a good enough job of placing itself in the context of related work goyal et. al, zforcing in particular. the authors seem to have addressed this curiously in an anonymous link and not in an updated manuscript but the manuscript itself has not been updated. in general, this paper presents an interesting idea with strong empirical results. the paper itself is not well composed, however, and can be improved upon significantly. taking the reviews into account and including a better treatment of related work in writing and empirically will make this a much stronger paper. pros  strong empirical performance loglikelihood on test data  a neat idea  deep generative models are of great interest to the community cons  incremental in relation to goyal et al., 2017  needs better treatment of related work  the writing is confusing and the technical exposition is not clear enough", "accepted": 0}
{"paper_id": "iclr_2018_SknC0bW0-", "review_text": "this paper combines multiple existing ideas in bayesian optimization continuousfidelity, use of gradient information and knowledge gradient to develop their proposed cfkg method. while the methodology seems neat and effective, the reviewers and ac found that the presented approach was not quite novel enough in light of existing work to justify acceptance to iclr. continuous fidelity bayesian optimization is well studied and knowledge gradient  derivative information was presented at nips. the combination of these things seems quite sensible but not sufficiently novel unless the empirical results were really compelling. pros  the paper is clear and writing is of high quality  bayesian optimization is interesting to the community and compelling methods are potentially practically impactful  outperforms existing methods on the chosen benchmarks cons  is an incremental combination of existing methods  the paper claims too much", "accepted": 0}
{"paper_id": "iclr_2018_rJoXrxZAZ", "review_text": "tl;dr of paper for sequential prediction, in order to scale up the model size without increasing inference time, use a model that predicts multiple timesteps at once. in this case, use an lstm on top of a wavenet for audio synthesis, where the lstm predicts n steps for every wavenet forward pass. the main result is being able to train bigger models, by increasing wavenet depth, without increasing inference time. the idea is simple and intuitive. im interested in seeing how well this approach can generalize to other sequential prediction domains. i suspect that its easier in the waveform case because neighboring samples are highly correlated. i am surprised by how much an improvement however, there are a number of important design decisions that are glossed over in the paper. here are a few that i am wondering about  how well do other multistep decoders do? for example, another natural choice is using transposed convolutions to upsample multiple timesteps. fully connected layers? how does changing the number of lstm layers affect performance?  why does the wavenet output a single timestep? why not just have the multistep decoder output all the timesteps?  how much of a boost does the separate training give over joint training? if you used the idea suggested in the previous point, you wouldnt need this separate training scheme.  how does performance vary over changing the number of steps the multistep decoder outputs? the paper also reads like it was hastily written, so please go back and fix the rough edges. right now, the paper feels too coupled to the existing deep voice 2 system. as a research paper, it is lacking important ablations. ill be happy to increase my score if more experiments and results are provided. this paper presents hybridnet, a neural speech and other audio synthesis system vocoder that combines the popular and effective wavenet model with an lstm with the goal of offering a model with faster inferencetime audio generation. summary the proposed model, hybridnet is a fairly straightforward variation of wavenet and thus the paper offers a relatively low novelty. there is also a lack of detail regarding the human judgement experiments that make the significance of the results difficult to interpret. low novelty of approach  impact assessment the proposed model is based closely on wavenet, an existing stateoftheart vocoder model. the proposal here is to extend wavenet to include an lstm that will generate samples between wavenet samples  thus allowing wavenet to sample at a lower sample frequency. wavenet is known for being relatively slow at testtime generation time, thus allowing it to run at a lower sample frequency should decrease generation time. the introduction of a local lstm is perhaps not a sufficiently significant innovation. another issue that lowers the assessment of the likely impact of this paper is that there are already a number of alternative mechanism to deal with the sampling speed of wavenet. in particular, the cited method of ramachandran et al 2017 uses caching and other tricks to achieve a speed up of 21 times over wavenet compared to the 24 times speed up of the proposed method. the authors suggest that these are orthogonal strategies that can be combined, but the combination is not attempted in this paper. there are also other methods such as samplernn mehri et al. 2017 that are faster than wavenet at inference time. the authors do not compare to this model. inappropriate evaluation while the model is motivated by the need to reduce the generation of wavenet sampling, the evaluation is largely based on the quality of the sampling rather than the speed of sampling. the results are roughly calibrated to demonstrate that hybridnet produces higher quality samples when roughly adjusted for sampling time. the more appropriate basis of comparison is to compare sample time as a function of sample quality. experiments few details are provided regarding the human judgment experiments with mechanical turkers. as a result it is difficulty to assess the appropriateness of the evaluation and therefore the significance of the findings. i would also be much more comfortable with this quality assessment if i was able to hear the samples for myself and compare the quality of the wavenet samples with hybridnet samples. i will also like to compare the wavenet samples generated by the authors implementation with the wavenet samples posted by van den oord et al 2017. minor comments  questions how, specifically, is validation error defined in the experiments? there are a few language glitches distributed throughout the paper. by generating multiple samples at once with the lstm, the model is introducing some independence assumptions between samples that are from neighbouring windows and are not conditionally independent given the context produced by wavenet. this reduces significantly the generality of the proposed technique. pros  attempting to solve the important problem of speeding up autoregressive generation.  clarity of the writeup is ok, although it could use some polishing in some parts.  the work is in the right direction, but the paucity of results and lack of thoroughness reduces somewhat the works overall significance. cons  the proposed technique is not particularly novel and it is not clear whether the technique can be used to get speedups beyond 2x  something that is important for realworld deployment of wavenet.  the amount of innovation is on the low side, as it involves mostly just fairly minor architectural changes.  the absolute results are not that great mos 3.8 is not close to the sota of 4.4  4.5 the paper presents a hybrid architecture which combines wavenet and lstm for speedingup raw audio generation. the novelty of the method is limited, as its a simple combination of existing techniques. the practical impact of the approach is rather questionable since the generated audio has significantly lower mos scores than the stateoftheart wavenet model.", "accepted": 0}
{"paper_id": "iclr_2018_S1NHaMW0b", "review_text": "the paper proposes a regularisation technique based on shakeshake which leads to the state of the art performance on the cifar10 and cifar100 dataset. despite good results on cifar, the novelty of the method is low, justification for the method is not provided, and the impact of the method on tasks beyond cifar classification is unclear.", "accepted": 0}
{"paper_id": "iclr_2018_B16yEqkCZ", "review_text": "this paper presents an interesting idea that is related to imitation learning, safe exploration, and intrinsic motivation. however, in its current state the paper needs improvement in clarity. there are also some concerns about the number of hyperparameters involved. finally, the experimental results are not completely convincing and should reflect existing baselines in one of the areas described above.", "accepted": null}
{"paper_id": "iclr_2018_rkc_hGb0Z", "review_text": "the reviewers are unanimous that the paper is not sufficiently clear and could be improved with better empirical results.", "accepted": 0}
{"paper_id": "iclr_2018_By5ugjyCb", "review_text": "all of the reviewers agree that the experimental results are promising and the proposed activation function enables a decent degree of quantization. however, the main concern with the approach is its limited novelty compared to previous work on clipped activation functions. minor comments  even though pact is very similar to relu, the names are very different.  please include a plot showing the proposed activation function as well.", "accepted": null}
{"paper_id": "iclr_2018_SJD8YjCpW", "review_text": "an empirical study of weight sharing for neural networks is interesting, but all of the reviewers found the experiments insufficient without enough baseline comparisons.", "accepted": 0}
{"paper_id": "iclr_2019_B1gabhRcYX", "review_text": "the first reviewer summarizes the contribution well this paper combines a cnn that computes both a multiscale feature pyramid and a depth prediction, which is expressed as a linear combination of depth bases. this is used to define a dense reprojection error over the images, akin to that of dense or semidense methods. then, this error is optimized with respect to the camera parameters and depth linear combination coefficients using levenbergmarquardt lm. by unrolling 5 iterations of lm and expressing the dampening parameter lambda as the output of a mlp, the optimization process is made differentiable, allowing backpropagation and thus learning of the networks parameters. strengths while combining deep learning methods with bundle adjustment is not new, reviewers generally agree that the particular way in which that is achieved in this paper is novel and interesting. the authors accounted for reviewer feedback during the review cycle and improved the manuscript leading to an increased rating. weaknesses weaknesses were addressed during the rebuttal including better evaluation of their predicted lambda and comparison with codeslam. contention this paper was not particularly contentious, there was a score upgrade due to the efforts of the authors during the rebuttal period. consensus this paper addresses an interesting area of research at the intersection of geometric computer vision and deep learning and should be of considerable interest to many within the iclr community. the discussion of the paper highlighted some important nuances of terminology regarding the characterization of different methods. this paper was also rated the highest in my batch. as such, i recommend this paper for an oral presentation.", "accepted": 1}
{"paper_id": "iclr_2019_r1lYRjC9F7", "review_text": "all reviewers agree that the presented audio data augmentation is very interesting, well presented, and clearly advancing the state of the art in the field. the authors rebuttal clarified the remaining questions by the reviewers. all reviewers recommend strong acceptance oral presentation at iclr. i would like to recommend this paper for oral presentation due to a number of reasons including the importance of the problem addressed data augmentation is the only way forward in cases where we do not have enough of training data, the novelty and innovativeness of the model, and the clarity of the paper. the work will be of interest to the widest audience beyond iclr.", "accepted": 1}
{"paper_id": "iclr_2019_B1G5ViAqFm", "review_text": "1. describe the strengths of the paper. as pointed out by the reviewers and based on your expert opinion.  the paper tackles an interesting and challenging problem with a novel approach.  the method gives improves improved performance for the surface reconstruction task. 2. describe the weaknesses of the paper. as pointed out by the reviewers and based on your expert opinion. be sure to indicate which weaknesses are seen as salient for the decision i.e., potential critical flaws, as opposed to weaknesses that the authors can likely fix in a revision. the paper  lacks clarity in some areas  doesnt sufficiently explain the tradeoffs between performing all computations in the spectral domain vs the spatial domain. 3. discuss any major points of contention. as raised by the authors or reviewers in the discussion, and how these might have influenced the decision. if the authors provide a rebuttal to a potential reviewer concern, its a good idea to acknowledge this and note whether it influenced the final decision or not. this makes sure that author responses are addressed adequately. reviewers had a divergent set of concerns. after the rebuttal, the remaining concerns were  the significance of the performance improvements. the ac believes that the quantitative and qualitative results in table 3 and figures 5 and 6 show significant improvements with respect to two recent methods.  a feeling that the proposed method could have been more efficient if more computations were done in the spectral domain. this is a fair point but should be considered as suggestions for improvement and future work rather than grounds for rejection in the acs view. 4. if consensus was reached, say so. otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another. the reviewers did not reach a consensus. the final decision is aligned with the more positive reviewer, ar1, because ar1 was more confident in hisher review and because of the additional reasons stated in the previous section.", "accepted": 0}
{"paper_id": "iclr_2019_B1xJAsA5F7", "review_text": "the revisions made by the authors convinced the reviewers to all recommend accepting this paper. therefore, i am recommending acceptance as well. i believe the revisions were important to make since i concur with several points in the initial reviews about additional baselines. it is all too easy to add confusion to the literature by not including enough experiments.", "accepted": null}
{"paper_id": "iclr_2019_B1xVTjCqKQ", "review_text": "this paper studies deep convolutional architectures to perform compressive sensing of natural images, demonstrating improved empirical performance with an efficient pipeline. reviewers reached a consensus that this is an interesting contribution that advances datadriven methods for compressed sensing, despite some doubts about the experimental setup and the scope of the theoretical insights. we thus recommend acceptance as poster.", "accepted": 1}
{"paper_id": "iclr_2019_B1xf9jAqFQ", "review_text": "the authors obtain nice speed improvements by learning to skip and jump over input words when processing text with an lstm. at some points the reviewers considered the work incremental since similar ideas have already been explored, but at the end two of the reviewers ended up endorsing the paper with strong support.", "accepted": null}
{"paper_id": "iclr_2019_BJfYvo09Y7", "review_text": "a hierarchical method is presented for developing humanoid motion control, using lowlevel control fragments, egocentric visual input, recurrent highlevel control. it is likely the first demonstration of 3d humanoids learning to do memoryenabled tasks using only proprioceptive and headbased egocentric vision. the use of control fragments as opposed to mocapclipbased skills allows for finergrained repurposing of pieces of motion, while still allowing for mocapbased learning weaknesses it is largely a mashup up of previously known results r2. caveat this can be said for all research at some sufficient level of abstraction. the motions are jerky when transitions happen between control fragments r2,r3. there are some concerns as to whether the method compares against other methods; the authors note that they are either not directly comparable, i.e., solving a different problem, or are implicitly contained in some of the comparisons that are performed in the paper. overall, the reviewers and ac are in broad agreement regarding the strengths and weaknesses of the paper. the ac believes that the work will be of broad interest. demonstrating memoryenabled, visiondriven, mocapimitating skills is a broad step forward. the paper also provides a further datapoint as to which combinations of method work well, and some of the specific features required to make them work. the paper could acknowledge motion quality artifacts, as noted by the reviewers and in the online discussion. suggest to include peng et al 2017 as some of the most relevant related hrl humanoid control work, as per the reviews  discussion.", "accepted": 1}
{"paper_id": "iclr_2019_BJg4Z3RqF7", "review_text": "this paper proposes a ganbased method to recover images from a noisy version of it. the paper builds upon existing works on ambientgan and csgan. by combining the two approaches, the work finds a new method that performs better than existing approaches. the paper clearly has new interesting ideas which have been executed well. two of the reviewers have voted in favour of acceptance, with one of the reviewer providing an extensive and detailed review. the third reviewer however has some doubts which were not resolved completely after the rebuttal. upon reading the work myself, i am convinced that this will be interesting to the community. however, i will recommend the authors to take the comments of reviewer 2 into account and do whatever it takes to resolve issues pointed by the reviewer. during the review process, another related work was found to be very similar to the approach discussed in this work. this work should be cited in the paper, as a prior work that the authors were unaware of. httpsarxiv.orgabs1812.04744 please also discuss any new insights this work offers on top of this existing work. given that the above suggestions are taken into account, i recommend to accept this paper.", "accepted": 1}
{"paper_id": "iclr_2019_BJgRDjR9tQ", "review_text": "strengths this paper presents a very interesting connection between gans and robust estimation in the presence of corrupted training data. the conceptual ideas are novel and can likely be extended in many further directions. i would not be surprised if this opens up a new line of research.  weaknesses the paper is poorly written. due to disagreement among the authors and my interest in the topic, i read the paper in detail myself. i think it would be difficult for a nonexpert to understand the key ideas and i strongly encourage the authors to carefully revise the paper to reach a broader audience and highlight the key insights. additionally, the experiments are only on toy data.  discussion one of the reviewers was concerned about the lack of efficiency guarantees for the proposed algorithm indeed, the algorithm requires training gans which are currently beyond the reach of theory and finicky in practice. that reviewer points to the fact that most papers in the robustness literature are concerned with computational efficiency and is concerned that ignoring this sidesteps one of the key challenges. the reviewer is also concerned about the restriction to parametric or nearlyparametric families e.g. gaussians and elliptical distributions. other reviewers were more positive and did not see these as major issues.  decision in my opinion, the lack of efficiency guarantees is not a huge issue, as the primary contribution of the paper is pointing out a nonobvious conceptual connection between two literatures. the restriction to parametric families is more concerning, but it seems possible this could be removed with further developments. the main reason for accepting the paper despite concerns about the writing is the importance of the conceptual connection. i think this connection is likely to lead to a new line of research and would like to get it out there as soon as possible.  comments despite the accept decision, i again urge the authors to improve the quality of exposition to ensure that a large audience can appreciate the ideas.", "accepted": null}
{"paper_id": "iclr_2019_BJlxm30cKm", "review_text": "this paper is an analysis of the phenomenon of example forgetting in deep neural net training. the empirical study is the first of its kind and features convincing experiments with architectures that achieve near stateoftheart results. it shows that a portion of the training set can be seen as support examples. the reviewers noted weaknesses such as in the measurement of the forgetting itself and the training regiment. however, they agreed that their concerns we addressed by the rebuttal. they also noted that the paper is not forthcoming with insights, but found enough value in the systematic empirical study it provides.", "accepted": 1}
{"paper_id": "iclr_2019_BJxhijAcY7", "review_text": "the reviewers noticed that the paper undergone many editions and raise concern about the content. they encourage improving experimental section further and strengthening the message of the paper.", "accepted": null}
{"paper_id": "iclr_2019_BkeU5j0ctQ", "review_text": "this paper combines two different types of existing optimization methods, cemcmaes and ddpgtd3, for policy optimization. the approach resembles erl but demonstrates good better performance on a variety of continuous control benchmarks. although i feel the novelty of the paper is limited, the provided promising results may justify the acceptance of the paper.", "accepted": null}
{"paper_id": "iclr_2019_Bkg2viA5FQ", "review_text": "the paper generalizes the concept of hindsight, i.e. the recycling of data from trajectories in a goalbased system based on the goal state actually achieved, to policy gradient methods. this was an interesting paper in that it scored quite highly despite all three reviewers mentioning incrementality or a relative lack of novelty. although the authors naturally took some exception to this, ac personally believes that properly executed, contributions that seem quite straightforward in hindsight pun partly intended can be valuable in moving the field forward a clean and didactic presentation of theory backed by welldesigned and extensive empirical investigation both of which are adjectives used by reviewers to describe the empirical work in this paper can be as valuable, or moreso, than a poorly executed but highernovelty works. to quote anonreviewer3, hpg is almost certainly going to end up being a widely used addition to the rl toolbox. feedback from reviewers prompted extensive discussion and a direct comparison with hindsight experience replay which reviewers agreed added significant value to the manuscript, earning it a postrebuttal unanimous rating of 7. it is therefore my pleasure to recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2019_Bkg8jjC9KQ", "review_text": "this paper investigates the usage of the extragradient step for solving saddlepoint problems with nonmonotone stochastic variational inequalities, motivated by gans. the authors propose an assumption weakerdiffrerent than the pseudomonotonicity of the variational inequality for their convergence analysis that they call coherence. interestingly, they are able to show the asympotic last iterate convergence for the extragradient algorithm in this case in contrast to standard results which normally requires averaging of the iterates for the stochastic and mototone variational inequality such as the cited work by gidel et al.. the authors also describe an interesting difference between the gradient method without the extragradient step mirror descent vs. with that they called optimistic mirror descent. r2 thought the coherence condition was too related to the notion of pseudomonoticity for which one could easily extend previous known convergence results for stochastic variational inequality. the ac thinks that this point was well answered by the authors rebuttal and in their revision the conditions are sufficiently different, and while there is still much to do to analyze non variational inequalities or having realistic assumptions, this paper makes some nontrivial and interesting steps in this direction. the ac thus sides with expert reviewer r1 and recommends acceptance.", "accepted": null}
{"paper_id": "iclr_2019_Bkl-43C9FQ", "review_text": "the paper presents a simple and effective convolution kernel for cnns on spherical data convolution by a linear combination of differential operators. the proposed method is efficient in the number of parameters and achieves strong classification and segmentation performance in several benchmarks. the paper is generally well written but the authors should clarify the details and address reviewer comments for example, claritynotations of equations in the revision.", "accepted": null}
{"paper_id": "iclr_2019_BkzeUiRcY7", "review_text": "the paper addresses a variant of multiagent reinforcement learning that aligns well with realworld applications  it considers the case where agents may have individual, diverging preferences. the proposed approach trains a manager agent which coordinates the selfinterested worker agents by assigning them appropriate tasks and rewarding successful task completion through contract generation. the approach is empirically validated on two gridworld domains resource collection and crafting. the reviewers point out that this formulation is closely related to the principleagent problem known in the economics literature, and see a key contribution of the paper in bringing this type of problem into the deep rl space. the reviewers noted several potential weaknesses they asked to clarify the relation to prior work, especially on the principleagents work done in other areas, as well as connections to real world applications. in this context, they also noted that the significance of the contribution was unclear. several modeling choices were questioned, including the choice of using rulebased agents for the empirical results presented in the main paper, and the need for using deep learning for contract generation. they asked the authors to provide additional details regarding scalability and sample complexity of the approach. the authors carefully addressed the reviewer concerns, and the reviewers have indicated that they are satisfied with the response and updates to the paper. the consensus is to accept the paper. the ac is particularly pleased to see that the authors plan to open source their code so that experiments can be replicated, and encourages them to do so in a timely manner. the ac also notes that the figures in the paper are very small, and often not readable in print  please increase figure and font sizes in the camera ready version to ensure the paper is legible when printed.", "accepted": null}
{"paper_id": "iclr_2019_ByeMB3Act7", "review_text": "this paper introduces an approach for improving the scalability of neural network models with large output spaces, where naive softmax inference scales linearly with the vocabulary size. the proposed approach is based on a clustering step combined with percluster, smaller softmaxes. it retains differentiability with the gumbel softmax trick. the experimental results are impressive. there are some minor flaws, however theres consensus among the reviewers the paper should be published.", "accepted": 1}
{"paper_id": "iclr_2019_ByeSdsC9Km", "review_text": "all reviewers recommend acceptance. the problem is an interesting one. the method is interesting. authors were responsive in the reviewing process. good work. i recommend acceptance", "accepted": null}
{"paper_id": "iclr_2019_ByetGn0cYX", "review_text": "this paper presents a new approach for posing control as inference that leverages sequential monte carlo and bayesian smoothing. there is significant interest from the reviewers into this method, and also an active discussion about this paper, particularly with respect to the optimism bias issue. the paper is borderline and the authors are encouraged to address the desired clarifications and changes from the reviewers.", "accepted": 1}
{"paper_id": "iclr_2019_BygqBiRcFQ", "review_text": "the paper gives an extension of scattering transform to noneuclidean domains by introducing scattering transforms on graphs using diffusion wavelet representations, and presents a stability analysis of such a representation under deformation of the underlying graph metric defined in terms of graph diffusion. concerns of the reviewers are primarily with what type of graphs is the primary consideration small world social networks or point cloud submanifold samples and experimental studies. technical development like deformation in the proposed graph metric is motivated by submanifold scenarios in computer vision, and whether the development is well suitable to social networks in experiments still needs further investigations. the authors make satisfied answers to the reviewers questions. the reviewers unanimously accept the paper for iclr publication.", "accepted": 1}
{"paper_id": "iclr_2019_BylE1205Fm", "review_text": "1. describe the strengths of the paper. as pointed out by the reviewers and based on your expert opinion. the proposed method performed well on 3 visual content transfer problems. 2. describe the weaknesses of the paper. as pointed out by the reviewers and based on your expert opinion. be sure to indicate which weaknesses are seen as salient for the decision i.e., potential critical flaws, as opposed to weaknesses that the authors can likely fix in a revision.  the paper is hard to follow at times  the problem being addressed is technically interesting but not wellmotivated. that is, the question why is this of interest to the iclr community was not wellanswered. 3. discuss any major points of contention. as raised by the authors or reviewers in the discussion, and how these might have influenced the decision. if the authors provide a rebuttal to a potential reviewer concern, its a good idea to acknowledge this and note whether it influenced the final decision or not. this makes sure that author responses are addressed adequately. there were no major points of contention. 4. if consensus was reached, say so. otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another. the reviewers reached a consensus that the paper should be accepted.", "accepted": null}
{"paper_id": "iclr_2019_BylQV305YQ", "review_text": "the reviewers that provided extensive and technically welljustified reviews agreed that the paper is of high quality. the authors are encouraged to make sure all concerns of these reviewers are properly addressed in the paper.", "accepted": 1}
{"paper_id": "iclr_2019_ByleB2CcKm", "review_text": "while the reviews of this paper were somewhat mixed 7,6,4, i ended up favoring acceptance because of the thorough author responses, and the novelty of what is being examined. the reviewer with a score of 4, argues that this work is not a good fit for iclr, but, although tailoring new metrics may not be a common area that is explored, i dont believe that its outside the range of iclrs interest, and therefore also more unique.", "accepted": null}
{"paper_id": "iclr_2019_Bylmkh05KX", "review_text": "this paper is about unsupervised learning for asr, by matching the acoustic distribution, learned unsupervisedly, with a prior phonelm distribution. overall, the results look good on timit. reviewers agree that this is a well written paper and that it has interesting results. strengths  novel formulation for unsupervised asr, and a nontrivial extension to previously proposed unsupervised classification to segmental level.  well written, with strong results. improved results and analysis based on review feedback. weaknesses  results are on timit  a small phone recognition task.  unclear how it extends to large vocabulary asr tasks, and tasks that have large scale training data, and rnns that may learn implicit lms. the authors propose to deal with this in future work. overall, the reviewers agree that this is an excellent contribution with strong results. therefore, it is recommended that the paper be accepted.", "accepted": null}
{"paper_id": "iclr_2019_Byxpfh0cFm", "review_text": "the paper proposes several subsampling policies to achieve a clear reduction in the size of augmented data while maintaining the accuracy of using a standard data augmentation method. the paper in general is clearly written and easy to follow, and provides sufficiently convincing experimental results to support the claim. after reading the authors response and revision, the reviewers have reached a general consensus that the paper is above the acceptance bar.", "accepted": null}
{"paper_id": "iclr_2019_ByzcS3AcYX", "review_text": "the paper proposes using gans for disentangling style information from speech content, and thereby improve style transfer in tts. the review and responses for this paper have been especially thorough! the authors significantly improved the paper during the review process, as pointed out by the reviewers. inclusion of additional baselines, evaluations and ablation analysis helped improve the overall quality of the paper and helped alleviate concerns raised by the reviewers. therefore, it is recommended that the paper be accepted for publication.", "accepted": null}
{"paper_id": "iclr_2019_H1fU8iAqKX", "review_text": "the overall consensus after an extended discussion of the paper is that this work should be accepted to iclr. the backandforth between reviewers and authors was very productive, and resulted in substantial clarification of the work, and modification trending positive of the reviewer scores.", "accepted": 1}
{"paper_id": "iclr_2019_H1gKYo09tX", "review_text": "overall this paper presents a few improvements over the code2vec model of alon et al., applying it to seq2seq tasks. the empirical results are very good, and there is fairly extensive experimentation. this is a relatively crowded space, so there are a few natural baselines that were not compared to, but i dont think that comparison to every single baseline is warranted or necessary, and the authors have done an admirable job. one thing that still is quite puzzling is the strength of the ast nodes only baseline, which the authors have given a few explanations for using nodes helps focus on variables, and also there is an effect of combining together things that are close together in the ast tree. still, this result doesnt seem to mesh with the overall story of the paper all that well, and again opens up some obvious questions such as whether a transformer model trained on only ast nodes would have done similarly, and if not why not. this paper is very much on the borderline, so if there is space in the conference i think it would be a reasonable addition, but there could also be an argument made that the paper would be stronger in a resubmission where the above questions are answered.", "accepted": null}
{"paper_id": "iclr_2019_H1lJJnR5Ym", "review_text": "pros  novel, general idea for hard exploration domains  multiple additional tricks  ablations, control experiments  wellwritten paper  excellent results on montezuma cons  low sample efficiency 2b frames  unresolved questions nonepisodic intrinsic rewards  could have done better applestoapples comparisons to baselines the reviewers did not reach consensus on whether to accept or reject the paper. in particular, after multiple rounds of discussion, reviewer 1 remains adamant that the downsides of the paper outweigh its good points. however, given that the other three reviewers argue strongly and credibly for acceptance, i think the paper should be accepted.", "accepted": 1}
{"paper_id": "iclr_2019_HJGkisCcKm", "review_text": "the paper describes a method which, given a music waveform, generates another recording of the same music which should sound as if it was performed by different instruments. the model is an autoencoder with a wavenetlike domainspecific decoder and a shared encoder, trained with an adversarial domain confusion loss. even though the method is constructed mostly from existing components, the reviewers found the results interesting and convincing, and recommended the paper for acceptance.", "accepted": 1}
{"paper_id": "iclr_2019_HJgd1nAqFX", "review_text": "this paper considers the task of web navigation, i.e. given a goal expressed in natural language, the task is to navigate webs by filling up fields and clicking links. the proposed model uses reinforcement learning, introducing a novel extension where the graph embedding of the pages is incorporated into the qfunction. the results are sound, and the paper is overall wellwritten. the reviewers and ac note the following potential weaknesses. the primary concern that was raised was the novelty. since the task could potentially be framed as semantic parsing, reviewer 4 mentioned there may be readily available approaches for baselines that the authors did not consider. the comparison to semantic parsing required a more detailed discussion, pointing not only the differences but also the similarities, that would encourage the two communities to explore novel approaches to their tasks. further, reviewer 2 was concerned about the limited novelty, given the extensive work that combines gnn and rl, such as nervenet. the authors provided comments and a revision to address these issues. they described why it is not trivial to formulate their setup as a semantic parsing problem, partly due to the fact that the environment is partially observable. similarly, the authors described the differences between the proposed approach and methods like nervenet, such as the use of a dynamic graph and offpolicy rl, making the latter not a viable baseline for the task. these changes addressed most of the concerns raised by the reviewers. the reviewers agreed that this paper should be accepted.", "accepted": null}
{"paper_id": "iclr_2019_HkgEQnRqYQ", "review_text": "this paper proposes a knowledge graph completion approach that represents relations as rotations in a complex space; an idea that the reviewers found quite interesting and novel. the authors provide analysis to show how this model can capture symmetryassymmetry, inversions, and composition. the authors also introduce a separate contribution of selfadversarial negative sampling, which, combined with complex rotational embeddings, obtains state of the art results on the benchmarks for this task. the reviewers and the ac identified a number of potential weaknesses in the initial paper 1 the evaluation only showed the final performance of the approach, and thus it was not clear how much benefit was obtained from adversarial sampling vs the scoring model, or further, how good the results would be for the baselines if the same sampling was used, 2 citation and comparison to a closely related approach toruse, and 3 a number of presentation issues early on in the paper. the reviewers appreciated the authors comments and the revision, which addressed all of the concerns by including 1 additional experiments to performance with and without selfadversarial sampling, and comparisons to toruse, 2 improved presentation. with the revision, the reviewers agreed that this is a worthy paper to include in the conference.", "accepted": null}
{"paper_id": "iclr_2019_HkxaFoC9KQ", "review_text": "the paper presents a family of models for relational reasoning over structured representations. the experiments show good results in learning efficiency and generalization, in boxworld grid world and starcraft 2 minigames, trained through reinforcement impalaoffpolicy a2c. the final version would benefit from more qualitative andor quantitative details in the experimental section, as noted by all reviewers. the reviewers all agreed that this is worthy of publication at iclr 2019. e.g. the paper clearly demonstrates the utility of relational inductive biases in reinforcement learning. r3", "accepted": null}
{"paper_id": "iclr_2019_HkzSQhCcK7", "review_text": "the paper presents a generative model of sequences based on the vae framework, where the generative model is given by cnn with causal and dilated connections. novelty of the method is limited; it mainly consists of bringing together the idea of causal and dilated convolutions and the vae framework. however, knowing how well this performs is valuable the community. the proposed method appears to have significant benefits, as shown in experiments. the result on mnist is, however, so strong that it seems incorrect; more digging into this result, or sourcecode, would have been better.", "accepted": null}
{"paper_id": "iclr_2019_HyM7AiA5YX", "review_text": "this paper proposes adding a second objective to the training of neural network classifiers that aims to make the distribution over incorrect labels as flat as possible for each training sample. the authors describe this as maximizing the complement entropy. rather than adding the crossentropy objective and the negative complement entropy term since the complement entropy should be maximized while the crossentropy is minimized, this paper proposes an alternating optimization framework in which first a step is taken to reduce the crossentropy, then a step is taken to maximize the complement entropy. extensive experiments on image classification cifar10, cifar100, svhn, tiny imagenet, and imagenet, neural machine translation iwslt 2015 englishvietnamese task, and smallvocabulary isolatedword recognition google commands, show that the proposed twoobjective approach outperforms training only to minimize crossentropy. experiments on cifar10 also show that models trained in this framework have somewhat better resistance to singlestep adversarial attacks. concerns about the presentation of the adversarial attack experiments were raised by anonymous commenters and one of the reviewers, but these concerns were addressed in the revision and discussion. the primary remaining concern is a lack of any theoretical guarantees that the alternating optimization converges, but the strong empirical results compensate for this problem.", "accepted": 1}
{"paper_id": "iclr_2019_Hygxb2CqKm", "review_text": "the paper presents both theoretical analysis based upon lambdastability and experimental evidence on stability of recurrent neural networks. the results are convincing but is concerns with a restricted definition of stability. even with this restriction acceptance is recommended.", "accepted": null}
{"paper_id": "iclr_2019_HyxnZh0ct7", "review_text": "the reviewers disagree strongly on this paper. reviewer 2 was the most positive, believing it to be an interesting contribution with strong results. reviewer 3 however, was underwhelmed by the results. reviewer 1 does not believe that the contribution is sufficiently novel, seeing it as too close to existing multitask learning approaches. after considering all of the discussion so far, i have to agree with reviewer 2 on their assessment. much of the meta learning literature involves changing the base learner for a fixed architecture and seeing how it affects performance. there is a temptation to chase performance by changing the architecture, adding new regularizers, etc., and while this is important for practical reasons, it does not help to shed light on the underlying fundamentals. this is best done by considering carefully controlled and well understood experimental settings. even still, the performance is quite good relative to popular base learners. regarding novelty, i agree it is a simple change to the base learner, using a technique that has been tried before in other settings linear regression as opposed to classification, however its use in a meta learning setup is novel in my opinion, and the new experimental comparison regression on top of pretrained cnn features helps to demonstrate the utility of its use in the metalearning settings. while the novelty can certainly be debated, i want to highlight two reasons why i am opting to accept this paper 1 simple and effective ideas are often some of the most impactful. 2 sometimes taking ideas from one area e.g., multitask learning and demonstrating that they can be effective in other settings e.g., metalearning can itself be a valuable contribution. i believe that the metalearning community would benefit from reading this paper.", "accepted": null}
{"paper_id": "iclr_2019_HyzMyhCcK7", "review_text": "a novel approach for quantized deep neural nets is proposed, which is more principled than commonly used straightthrough gradient method. a theoretical analysis of the algorithms converegence is presented, and empirical results show advantages of the proposed approach.", "accepted": 1}
{"paper_id": "iclr_2019_HyztsoC5Y7", "review_text": "the authors consider the use of maml with model based rl and applied this to robotics tasks with very encouraging results. there was definite interest in the paper, but also some concerns over how the results were situated, particularly with respect to the related research in the robotics community. the authors are strongly encouraged to carefully consider this feedback, as they have been doing in their responses, and address this as well as possible in the final version.", "accepted": null}
{"paper_id": "iclr_2019_S1ecm2C9K7", "review_text": "the authors identify a source of bias that occurs when a model overestimates the importance of weak features in the regime where sufficient training data is not available. the bias is characterized theoretically, and demonstrated on synthetic and real datasets. the authors then present two algorithms to mitigate this bias, and demonstrate that they are effective in experimental evaluations. as noted by the reviewers, the work is wellmotivated and clearly presented. given the generally positive reviews, the ac recommends that the work be accepted. the authors should consider adding additional text describing the details concerning figure 3 in the appendix.", "accepted": null}
{"paper_id": "iclr_2019_S1lDV3RcKm", "review_text": "the paper proposes an adversarial framework that learns a generative model along with a mask generator to model missing data and by this enables a gan to learn from incomplete data. the method builds on ambientgan but it is a novel and clever adjustment to the specific problem setting of learning from incomplete data, that is of high practical interest.", "accepted": null}
{"paper_id": "iclr_2019_S1xtAjR5tX", "review_text": "the paper proposes the idea of using optimal transport to evaluate the semantic correspondence between two sets of words predicted by the model and ground truth sequences. strong empirical results are presented which support the use of optimal transport in conjunction with loglikelihood for training sequence models. i appreciate the improvements to the manuscript during the review process, and i encourage the authors to address the rest of the comments in the final version.", "accepted": null}
{"paper_id": "iclr_2019_S1zz2i0cY7", "review_text": "this paper addresses the issue of numerical roundingoff errors that can arise when using latent variable models for data compression, e.g., because of differences in floating point arithmetic across different platforms sender and receiver. the authors propose using neural networks that perform integer arithmetic integer networks to mitigate this issue. the problem statement is well described, and the presentation is generally ok, although it could be improved in certain aspects as pointed out by the reviewers. the experiments are properly carried out, and the experimental results are good. thank you for addressing the questions raised by the reviewers. after taking into account the authors responds, there is consensus that the paper is worthy of publication. i therefore recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2019_SJeXSo09FQ", "review_text": "all reviewers gave an accept rating 9, 7 6. a clear accept  just not strong enough reviewer support for an oral.", "accepted": 1}
{"paper_id": "iclr_2019_SJxsV2R5FQ", "review_text": "pros  the paper is wellwritten and precise  the proposed method is novel  valuable for realworld problems cons  reviewer 2 expresses some concern about the organization of the paper and overgenerality in the exposition  there could be more discussion of scalability", "accepted": 1}
{"paper_id": "iclr_2019_SJzR2iRcK7", "review_text": "this paper provides a technique to learn multiclass classifiers without multiclass labels, by modeling the multiclass labels as hidden variables and optimizing the likelihood of the input variables and the binary similarity labels. the majority of reviewers voted to accept.", "accepted": null}
{"paper_id": "iclr_2019_SJzSgnRcKX", "review_text": "pros  thorough analysis on a large number of diverse tasks  extending the probing technique typically applied to individual encoder states to testing for presence of certain linguistic information based on pairs of encoders states corresponding to pairs of words  the comparison can be useful when deciding which representations to use for a given task cons  nothing serious, it is solid and important empirical study the reviewers are in consensus.", "accepted": null}
{"paper_id": "iclr_2019_SkgQBn0cF7", "review_text": "this paper explores the use of multistep latent variable models of the dynamics in imitation learning, planning, and finding subgoals. the reviewers found the approach to be interesting. the initial experiments were a main weakpoint in the initial submission. however, the authors updated the experimental results to address these concerns to a significant degree. the reviewers all agree that the paper is above the bar for acceptance. i recommend accept.", "accepted": null}
{"paper_id": "iclr_2019_Sklsm20ctX", "review_text": "the paper proposes a new method to improve exploration in sparse reward problems, by having two agents competing with each other to generate shaping reward that relies on how novel a newly visited state is. the idea is nice and simple, and the results are promising. the authors implemented more baselines suggested in initial reviews, which was also helpful. on the other hand, the approach appears somewhat ad hoc. it is not always clear why and when the method works, although some intuitions are given. one reviewer gave a nice suggestion of obtaining further insights by running experiments in less complex environments. overall, this work is an interesting contribution.", "accepted": null}
{"paper_id": "iclr_2019_SyMDXnCcF7", "review_text": "this paper provides a meanfieldtheory analysis of batch normalization. first there is a negative result as to the necessity of gradient explosion when using batch normalization in a fully connected network. they then provide further insights as to what can be done about this, along with experiments to confirm their theoretical predictions. the reviewers and random commenters found this paper very interesting. the reviewers were unanimous in their vote to accept.", "accepted": null}
{"paper_id": "iclr_2019_Syl7OsRqY7", "review_text": "the paper presents a method for coarse and fine inference for question answering. it originally measured performance only on wikihop and then later added experiments on triviaqa. the results are good. one of the concerns regarding the paper was the novelty of the work, and lack of enough experiments. however, the addition of triviaqa results allays some of that concern. id suggest citing the paper by swayamdipta et al from last year that attempted coarse to fine inference for triviaqa multimention learning for reading comprehension with neural cascades. swabha swayamdipta, ankur p. parikh and tom kwiatkowski. proceedings of iclr 2018. overall, there is relative consensus that the paper is good with a new method and some strong results.", "accepted": 0}
{"paper_id": "iclr_2019_r1e13s05YX", "review_text": "the paper focuses on hybrid pipelines that contain blackboxes and neural networks, making it difficult to train the neural components due to nondifferentiability. as a solution, this paper proposes to replace blackbox functions with neural modules that approximate them during training, so that endtoend training can be used, but at test time use the original black box modules. the authors propose a number of variations offline, online, and hybrid of the two, to train the intermediate auxiliary networks. the proposed model is shown to be effective on a number of synthetic datasets. the reviewers and ac note the following potential weaknesses 1 the reviewers found some of the experiment details to be scattered, 2 it was unclear what happens if there is a mismatch between the auxiliary network and the black box function it is approximating, especially if the function is one, like sorting, that is difficult for neural models to approximate, and 3 the text lacked description of realworld tasks for which such a hybrid pipeline would be useful. the authors provide comments and a revision to address these concerns. they added a section that described the experiment setup to aid reproducibility, and incorporated more details in the results and related work, as suggested by the reviewers. although these changes go a long way, some of the concerns, especially regarding the mismatch between neural and black box function, still remain. overall, the reviewers agreed that the issues had been addressed to a sufficient degree, and the paper should be accepted.", "accepted": null}
{"paper_id": "iclr_2019_rJg4J3CqFm", "review_text": "an interesting and original idea of embedding words into the very low dimensional wasserstein space, i.e. clouds of points in a lowdimensional space  as the space is lowdimensional 2d, it can be directly visualized.  i could imagine the technique to be useful in social  human science for data visualization, the visualization is more faithful to what the model is doing than tsne plots of highdimensional embeddings  though not the first method to embed words as densities but seemingly the first one which shows that multimodality  multiple senses are captured except for models which capture discrete senses  the paper is very well written  the results are not very convincing but show that embeddings do capture word similarity even when training the model on a small dataset  the approach is not very scalable hence evaluation on 17m corpus  the method cannot be used to deal with data sparsity, though very interesting for visualization  this is mostly an empirical paper i.e. an interesting application of an existing method the reviewers are split. one reviewer is negative as they are unclear what the technical contribution is but seems a bit biased against empirical papers. another two find the paper very interesting.", "accepted": 0}
{"paper_id": "iclr_2019_rJlDnoA5Y7", "review_text": "this is a metareview with the recommendation, but i will ultimately leave the final call to the programme chairs, as this submission has a number of valid concerns. the proposed approach is one of the early, principled one to using fixed dense vectors for computing the predictive probability without resorting to softmax, that scales better than and work almost as well as softmax in neural sequence modelling. the reviewers as well as public commentators have noticed some potentially significant short comings, such as instability of learning due to numerical precision and the inability of using beam search perhaps due to the suboptimal calibration of probabilities under vmf. however, i believe these two issues should be addressed as separate followup work not necessarily by the authors themselves but by a broader community who would find this approach appealing for their own work, which would only be possible if the authors presented this work and had a chance to discuss it with the community at the conference. therefore, i recommend it be accepted.", "accepted": null}
{"paper_id": "iclr_2019_rJzLciCqKm", "review_text": "this manuscript proposes a new algorithm for learning from positive and unlabeled data. the motivation for this work includes cases of selection bias, where the positive label is correlated with observation. the resulting procedure is shown to learn a scoring function that preserves the classposterior ordering, and can thus be thresholded to obtain a classifier. the problem addressed is interesting, and the approach sounds reasonable. the writing seems to be well done, particularly after the rebuttal when the work was better placed in context. the reviewers and ac note issues with the evaluation of the proposed method. in particular, the authors do not provide a sufficiently convincing empirical evaluation on real data.", "accepted": null}
{"paper_id": "iclr_2019_rklz9iAcKQ", "review_text": "because of strong support from two of the reviewers i am recommending accepting this paper. however, i believe reviewer 1s concerns should be taken seriously. although i disagree with the reviewer that a general framework method is a bad thing, i agree with them that additional experiments would be valuable.", "accepted": 1}
{"paper_id": "iclr_2019_ryGfnoC5KQ", "review_text": "this submission follows on a line of work on online learning of a recurrent net, which is an important problem both in theory and in practice. it would have been better to see even more realistic experiments, but already with the set of experiments the authors have conducted the merit of the proposed approach shines.", "accepted": null}
{"paper_id": "iclr_2019_ryGgSsAcFQ", "review_text": "the paper shows limitations on the types of functions that can be represented by deep skinny networks for certain classes of activation functions, independently of the number of layers. with many other works discussing capabilities but not limitations, the paper contributes to a relatively underexplored topic. the settings capture a large family of activation functions, but exclude others, such as polynomial activations, for which the considered type of obstructions would not apply. also a concern is raised about it not being clear how this theoretical result can shed insight on the empirical study of neural networks. the authors have responded to some of the comments of the reviewers, but not to all comments, in particular comments of reviewer 1, whos positive review is conditional on the authors addressing some points. the reviewers are all confident and are moderately positive, positive, or very positive about this paper.", "accepted": 1}
{"paper_id": "iclr_2019_ryggIs0cYQ", "review_text": "this paper proposes switchable normalization sn that leans how to combine three existing normalization techniques for improved performance. there is a general consensus that that the paper has good quality and clarity, is well motivated, is sufficiently novel, makes clear contributions for training deep neural networks, and provides convincing experimental results to show the advantages of the proposed sn.", "accepted": null}
{"paper_id": "iclr_2019_rylIAsCqYm", "review_text": "the reviewers all agreed that this paper makes a strong contribution to iclr by providing the first asynchronous analysis of a nesterovaccelerated coordinate descent method.", "accepted": 1}
{"paper_id": "iclr_2019_B1GIB3A9YX", "review_text": "the paper presents an explicit memory that directly contributes to more efficient exploration. it stores trajectories to novel states, that serve as training data to learn to reach those states again through iterative subgoals. the description of the method is quite clear, the method is not completely novel but has some merit. most weaknesses of the paper come from the experimental section too specific environmentssolutions, lack of points of comparisons, lacking some details. we strongly encourage the authors to add additional experimental evidence, and details. in its current form, the paper is not sufficient for publication at iclr 2019. reviewers wanted to note that the blog post from uber goexplore did _not_ affect their evaluation of this paper.", "accepted": 0}
{"paper_id": "iclr_2019_B1GIQhCcYm", "review_text": "the paper formulates the problem of unsupervised onetomany image translation and addresses the problem by minimizing the mutual information. the reviewers and ac note the critical limitation of novelty and comparison of this paper to meet the high standard of iclr. ac decided that the authors need more works to publish.", "accepted": 0}
{"paper_id": "iclr_2019_B1MAJhR5YX", "review_text": "the paper seeks to obtain faster means to count or approximately count of the number of linear regions of a neural network. the paper improves bounds and makes an interesting contribution to a long line of work. a consistent concern of the reviewers is the limited applicability of the method. the empirical evaluation can serve to better assess the accuracy of theoretical bounds that have been obtained in previous works, but the practical utility is not as clear yet. this is a borderline case. the reviewers lean towards a positive rating of the paper, but are not particularly enthusiastic about the paper. the paper makes good contributions, but is just not convincing enough. i think that the work program that the authors suggest in their responses could lead to a stronger paper in the future. in particular, the exploration of necessary and sufficient conditions for different neural networks to be equivalent and the use of number of linear regions when analyzing neural networks, seem to be very promising directions.", "accepted": null}
{"paper_id": "iclr_2019_B1MbDj0ctQ", "review_text": "the overall view of the reviewers is that the paper is not quite good enough as it stands. the reviewers also appreciates the contributions so taking the comments into account and resubmit elsewhere is encouraged.", "accepted": 0}
{"paper_id": "iclr_2019_B1MhpiRqFm", "review_text": "pros  a method that obtains convergence results using a using timedependent not fixed or statedependent softmax temperature. cons  theoretical contribution is not very novel  some theoretical results are dubious  mismatch of boltzmann updates and epsilongreedy exploration  the authors seem to have intended to upload a revised version of the paper, but unfortunately, they changed only title and abstract, not the pdf  and consequently the reviewers did not change their scores. the reviewers agree that the paper should be rejected in the submitted form.", "accepted": 0}
{"paper_id": "iclr_2019_B1ethsR9Ym", "review_text": "1. describe the strengths of the paper. as pointed out by the reviewers and based on your expert opinion.  the paper tackles an interesting and relevant problem for iclr guided image modification of images in this case of facial attributes.  the proposed method is in general wellexplained although some details are lacking 2. describe the weaknesses of the paper. as pointed out by the reviewers and based on your expert opinion. be sure to indicate which weaknesses are seen as salient for the decision i.e., potential critical flaws, as opposed to weaknesses that the authors can likely fix in a revision.  the training set of faces and associated attributes were annotated using a pretrained model which introduced a bias into the annotations used for training the method.  the experimental results werent convincing. the qualitative results showed no clear advantage of the proposed method and the quantitative comparison to stargan only considered two attribute manipulations and only found a statistically significant different in performance for one of those. the second weakness was the key determining factor in the acs final recommendation. 3. discuss any major points of contention. as raised by the authors or reviewers in the discussion, and how these might have influenced the decision. if the authors provide a rebuttal to a potential reviewer concern, its a good idea to acknowledge this and note whether it influenced the final decision or not. this makes sure that author responses are addressed adequately. there were no major points of contention and no author feedback. 4. if consensus was reached, say so. otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another. the reviewers reached a consensus that the paper should be rejected.", "accepted": 0}
{"paper_id": "iclr_2019_B1fPYj0qt7", "review_text": "this paper proposes using a tensor train low rank decomposition for compressing neural network parameters. however the paper falls short on multiple fronts 1lack of comparison with existing methods 2 no baseline experiments. further there are concerns about correctness of the math in deriving the algorithms, convergence and computational complexity of the proposed method. i strongly suggest taking the reviews into account before submitting the paper it again.", "accepted": 0}
{"paper_id": "iclr_2019_B1lXGnRctX", "review_text": "the paper describes the use of tactile sensors for exploration. an important topic which has been addressed in various previous publications, but is unsolved to date. the research and the paper are unfortunately in a raw state. rejected unanimously by the reviewers, without rebuttal chances used by the authors.", "accepted": 0}
{"paper_id": "iclr_2019_BJGVX3CqYm", "review_text": "the paper proposes a quantization framework that learns a different bit width per layer. it is based on a differentiable objective where the gumbel softmax approach is used with an annealing procedure. the objective trades off accuracy and model size. the reviewers generally thought the idea has merit. quoting from discussion comments r4 the paper cited by anonreviewer 3 is indeed close to the current submission, but in my opinion the strongest contribution of this paper is the formulation from architecture search perspective. the approach is general, and seems to be reasonably efficient resnet 18 took less than 5 hours the main negatives are the comparison to other methods. in the rebuttal, the authors suggested in multiple places that they would update the submission with additional experiments in response to reviewer comments. as of the decision deadline, these experiments do not appear to have been added to the document. in the discussion r4 this paper seems novel enough to me, but i agree that the prior work should at least be cited and compared to. this is a general weakness in the paper, the comparison to relevant prior works is not sufficient. r3 not only novel, but more general han the prior work mentioned, but the discussion  experiments do not seem to capture this. with a range of scores around the borderline threshold for acceptance at iclr, this is a difficult case. on the balance, it appears that shortcomings in the experimental results are not resolved in time for iclr 2019. the missing results include ablation studies promised to r4 and a comparison to darts promised to r3 we plan to perform the suggested experiments of comparing with exhaustive search and darts. the results will be hopefully updated before the revision deadline and the cameraready if the paper is accepted. these results are not present and could not be evaluated during the reviewdiscussion phase.", "accepted": null}
{"paper_id": "iclr_2019_BJeRg205Fm", "review_text": "this paper proposes to quantify the uncertainty of neural network models with beta, dirichlet and dirichletmultinomial likelihood. this paper is clearly written with a sound main idea. however, it is a common practice to model different types of data with different likelihood, although the proposed distributions are not usually used for network output. all the reviewers therefore considered this paper to be of limited novelty. reviewer 2 also had a concern about the mixed experimental results of the proposed method. reviewer 3 raised the concern that this paper did not model the uncertainty of prediction from the uncertainty of the model parameters. it is a common consideration in a bayesian approach and i encourage the authors to discussed different sources of uncertainty in future revisions.", "accepted": 0}
{"paper_id": "iclr_2019_BJgQB20qFQ", "review_text": "this paper provides a new approach for progressive planning on discrete state and action spaces. the authors use lstm architectures to iteratively select and improve local segments of an existing plan. they formulate the rewriting task as a reinforcement learning problem where the action space is the application of a set of possible rewriting rules. these models are then evaluated on a simulated job scheduling dataset and halide expression simplification. this is an interesting paper dealing with an important problem. the proposed solution based on combining several existing pieces is novel. on the negative side, the reviewers thought the writing could be improved, and the main ideas are not explained clearly. furthermore, the experimental evaluation is weak.", "accepted": null}
{"paper_id": "iclr_2019_BJgbzhC5Ym", "review_text": "this paper proposes a principled solution to the problem of joint sourcechannel coding. the reviewers find the perspectives put forward in the paper refreshing and that the paper is well written. the background and motivation is explained really well. however, reviewers found the paper limited in terms of modeling choices and evaluation methodology. one major flaw is that the experiments are limited to unrealistic datasets, and does not evaluate the method on a realistic benchmarks. it is also questioned whether the errorcorrecting aspect is practically relevant.", "accepted": 0}
{"paper_id": "iclr_2019_BJgsN3R9Km", "review_text": "the submission proposes a method that combines sparsification and low rank projections to compress a neural network. this is in line with nearly all stateoftheart methods. the specific combination proposed in this instance are svd for lowrank and localized group projections lgp for sparsity. the main concern about the paper is the lack of stronger comparison to sota compression techniques. the authors justify their choice in the rebuttal, but ultimately only compare to relatively straightforward baselines. the additional comparison with e.g. table 6 of the appendix does not give sufficient information to replicate or to know how the reduction in parameters was achieved. the scores for this paper were borderline, and the reviewers were largely in consensus with their scores and the points raised in the reviews. given the highly selective nature of iclr, the overall evaluations and remaining questions about the paper and comparison to baselines indicates that it does not pass the threshold for acceptance.", "accepted": null}
{"paper_id": "iclr_2019_BJll6o09tm", "review_text": "this paper proposes a simple modification of the adam optimizer, introducing a hyperparameter p with value in the range 0,12 parameterizing the parameter update theta_new  theta_old  mvp where p12 falls back to the standard adamamsgrad optimizer, and p0 falls back to a variant of sgd with momentum. the authors motivate the method by pointing out that  through the value of p, one can interpolate between sgd with momentum and adamamsgrad. by choosing a value of p smaller than 0.5, one can therefore use perform optimization that is partially adaptive.  the method shows good empirical performance. the paper contains an inaccuracy, which we hope will be solved before the final version. the authors argue that the 1sqrtv term in adam results in a lower learning rate, and the authors argue that the effective learning rate easily explodes section 3 because of this term, and that a more aggressive learning rate is more appropriate. this last point is false; the value of 1sqrtv can be smaller or larger than 1 depending on the value of v, and that a decrease in value of p can result in either an increase or decrease in effective learning rate, depending on the value of v. the value of v is a function of the scale of loss function, which can really be arbitrary. in case of very highdimensional predictions, for example, the scale of the loss function is often proportional with the dimensionality of variable to be modeled, which can be arbitrarily large, e.g. in image or video modeling the loss function tends to be of a much larger scale than with classification. the authors promise to include a comparison to adamw loshchilov, 2017 that includes tuning of the weight decay parameter. the lack of this experiments makes it more difficult to make a conclusion regarding the performance relative to adamw. however, the methods offer potentially orthogonal and combinable advantages. loshchilov, 2017 httpsarxiv.orgpdf1711.05101.pdf", "accepted": 1}
{"paper_id": "iclr_2019_BJxYEsAqY7", "review_text": "the paper describes knowledge distillation methods. as noted by all reviewers, the methods are very similar to the prior art, so there is not enough novelty for the paper to be accepted. the reviewers opinion didnt change after the rebuttal.", "accepted": 0}
{"paper_id": "iclr_2019_BkfPnoActQ", "review_text": "this paper proposes a combination of three techniques to improve the learning performance of atari games. good performance was shown in the paper with all three techniques together applied to dqn. however, it is hard to justify the integration of these techniques. it is also not clear why the specific decisions were made when combining them. more comprehensive experiments, such as a more systematic ablation study, are required to convince the benefits of individual components. furthermore, it seems very hard to tell whether the improvement of existing approaches, such as apex dqn, was from using the proposed techniques or a deeper architecture tables 1245. overall, this paper is not ready for publication.", "accepted": null}
{"paper_id": "iclr_2019_Bklzkh0qFm", "review_text": "the authors propose an architecture for learning and predicting graphs with relations between nodes. the approach is a combination of recent research efforts into graph attention networks and relational graph convolutional networks. the authors are commended for their clear and direct writing and presentation and their honest claims and their empirical setup. however, the paper simply doesnt have much to offer to the community, since the algorithmic contributions are marginal and the results unimpressive. while the authors justify the submission in terms of the difficult implementation and the extensive experiments, this is not enough to support its publication at a top conference. rather, this could be a technical report.", "accepted": 0}
{"paper_id": "iclr_2019_Bkx8OiRcYX", "review_text": "all reviewers agree to reject. while there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.", "accepted": 0}
{"paper_id": "iclr_2019_BkxgbhCqtQ", "review_text": "the reviewers agree this paper is not good enough for iclr.", "accepted": 0}
{"paper_id": "iclr_2019_ByecAoAqK7", "review_text": "this paper is essentially an application of dual learning to multilingual nmt. the results are reasonable. however, reviewers noted that the methodological novelty is minimal, and there are not a large number of new insights to be gained from the main experiments. thus, i am not recommending the paper for acceptance at this time.", "accepted": 0}
{"paper_id": "iclr_2019_ByfbnsA9Km", "review_text": "the paper challenges claims about crossentropy loss attaining max margin when applied to linear classifier and linearly separable data. this is important in moving forward with the development of better loss functions. the main criticism of the paper is that the results are incremental and can be easily obtained from previous work. the authors expressed certain concerns about the reviewing process. in the interest of dissipating any doubts, we collected two additional referee reports. although one referee is positive about the paper, four other referees agree that the paper is not strong enough.", "accepted": 1}
{"paper_id": "iclr_2019_BygIV2CcKm", "review_text": "this paper proposes and endtoend trainable architecture for data augmentation, by defining a parametric model for data augmentation using spatial transformers and gans and optimizing validation classification error through the notion of influence functions. experiments are reported on mnist and cifar10. this is a borderline submission. reviewers found the theoretical framework and problem setup to be solid and promising, but were also concerned about the experimental setup and the lack of clarity in the manuscript. in particular, one would like to evaluate this model against similar baselines e.g. ratner et al on a largescale classification problem. the ac, after taking these comments into account and making hisher own assessment, recommends rejection at this time, encouraging the authors to address the above comments and resubmit this promising work in the next conference cycle.", "accepted": null}
{"paper_id": "iclr_2019_ByghKiC5YX", "review_text": "i appreciate the willingness of the authors to engage in vigorous discussion about their paper. although several reviewers support accepting this submission, i do not find their arguments for acceptance convincing. the paper considers automated methods for finding errors in text classification models. i believe it is valuable to study the errors our models make in order to understand when they work well and how to improve them. crucially, in the later case, we should demonstrate how to use the errors we find to close the loop and create better models. a paper about techniques to find errors for text models should make a sufficiently large contribution to be accepted. i view the following hypothetical contributions as the most salient in this specific case thus my decision reduces to determining if any of these conditions have been met. a paper need not achieve all of these things, any one of them would suffice 1. show that the errors found can be used to meaningfully improve the models. this requires building a better model than the one probed by the method and convincingly demonstrating that it is superior in an important way that is relevant to the original goals of the application. ideally it would also consider alternative, simpler ways to improve the models e.g. making them larger. 2. show that errors are difficult to find, but that the proposed method is nonetheless capable of finding errors and that the method is nonobvious to a researcher in the field. this is not applicable here because errors are extremely easy to find on the test set and from labeling more data. if we demand an automated method, then the greedy algorithm does not qualify as sufficiently nonobvious and it seems to work fine, making the gumbel method unnecessary. 3. show that the particular specific errors found are qualitatively different from other errors in their implications and that they provide a unique and important insight. i do not believe this submission attempts to show this type of contribution. one example of this type of paper would be a paper that does a comparative study of the errors that different models make and finds something interesting potentially yielding a path to improved models. 4. generate a new, more difficultinteresting, dataset by finding errors of one or more trained models given that the authors use human labelers to validate examples this is potentially another path. here is an example of a paper using adversarial techniques in this way httpsarxiv.orgabs1808.05326 however, i believe the paper would need to be rethought and rewritten to make this sort of contribution. ultimately, the authors and reviews supporting acceptance must explain the contribution succinctly and convincingly. the reviewers most strongly advocating for accepting this submission seem to be saying that there is a valuable new method and probabilistic framework proposed here for finding model errors. i believe researchers in the field could have easily come up with the greedy algorithm a standard approach to discrete optimization problems proposed here without needing to read the paper. furthermore, i believe the other more complicated gumbel algorithm proposed is not necessary given the similarly effective and simpler greedy algorithm. if the authors believe that the gumbel algorithm provides applicationrelevant advantages over the greedy algorithm, then they should specify how these errors will be used and rewrite the paper to make the greedy algorithm a baseline. however, i do not believe the experimental results support this idea.", "accepted": 1}
{"paper_id": "iclr_2019_Byxz4n09tQ", "review_text": "the authors propose a scheme to compress models using studentteacher distillation, where training data are augmented using examples generated from a conditional gan. the reviewers were generally in agreement that 1 that the experimental results generally support the claims made by the authors, and 2 that the paper is clearly written and easy to follow. however, the reviewers also raised a number of concerns 1 that the experiments were conducted on smallscale tasks, 2 the use of the compression score might be impractical since it would require retraining a compressed model, and is affected by the effectiveness of the compression algorithm which is an additional confounding factor. the authors in their rebuttal address 2 by noting that the student training was not too expensive, but i believe that this cost is task specific. overall, i think 1 is a significant concern, and the ac agrees with the reviewers that an evaluation of the techniques on largescale datasets would strengthen the paper.", "accepted": null}
{"paper_id": "iclr_2019_H1ERcs09KQ", "review_text": "while this was a borderline paper, concerns about the novelty and significance of the presented work exist on the part of all reviewers, and no reviewer was willing to argue for acceptance. many good points to the work exist, and a stronger case on these issues would greatly strengthen the paper overall. i look forward to a future submission.", "accepted": null}
{"paper_id": "iclr_2019_H1MBuiAqtX", "review_text": "the authors present an interesting approach but there were multiple significant concerns with the clarity of the presentation, and some concern with the significance of the experimental results.", "accepted": 0}
{"paper_id": "iclr_2019_H1e6ij0cKQ", "review_text": "this is an interesting approach to use reinforcement learning to replace crf for sequence tagging, which would potentially be beneficial when the tag set is gigantic. unfortunately the conducted experiments do not really show this, which makes it difficult to see whether the proposed approach is indeed a viable alternative to crf for sequence tagging with a large tag set. this sentiment was shared by all the reviewers, and r1 especially pointed out major and minor issues with the submission and was not convinced by the authors response.", "accepted": 0}
{"paper_id": "iclr_2019_H1fsUiRcKQ", "review_text": "the paper combines the ideas of vat and bad gan, replacing the fake samples in bad gan objective with vat generated samples. the motivation behind using the k1 ssl framework with vat examples remains unclear, particularly in the light of prop. 2 which shows smoothness of classifier around the unlabeled examples is enough which vat already encourages. r2 and r3 have raised the point of limited insight and lack of motivation behind combining vat and bad gan objectives in this way. r2 and r3 are also concerned about the empirical results which show only marginal improvements over vatbadgan in most settings. ac feels that the idea of the paper is interesting but agrees with r2r3 that the proposed objective is not motivated well enough what is the precise advantage of using k1 ssl formulation with vat examples?. the paper really falls on the borderline and could be improved if this point is addressed convincingly.", "accepted": null}
{"paper_id": "iclr_2019_H1gDgn0qY7", "review_text": "the paper presents a novel view on adversarial examples, where models using relu are inherently sensitive to adversarial examples because relu activations yield a polytope of examples with exactly the same activation. reviewers found the finding interesting and novel but argue it is limited in impact. i also found the idea interesting but the paper could probably be improved as all reviewers have remarked. overall, i found it borderline but probably not enough for acceptance.", "accepted": 0}
{"paper_id": "iclr_2019_H1ldNoC9tX", "review_text": "the paper proposes an algorithm for semisupervised learning, which incorporate biased negative data into the existing pu learning framework. the reviewers and ac commonly note the critical limitation of practical value of the paper and results are rather straightforward. ac decided the paper might not be ready to publish as other contributions are not enough to compensate the issue.", "accepted": null}
{"paper_id": "iclr_2019_H1xAH2RqK7", "review_text": "while there was some support for the ideas presented, the majority of the reviewers did not think the submission was ready for presentation at iclr. concerns raised included that the experiments needed more work, and the paper needs to do a better job of distinguishing the contributions beyond those of past work.", "accepted": 0}
{"paper_id": "iclr_2019_HJMRvsAcK7", "review_text": "this is an interesting topic but the reviewers had substantial concerns on the clarity and significance of the contribution.", "accepted": 0}
{"paper_id": "iclr_2019_HJeABnCqKQ", "review_text": "the paper proposes an extension to reinforcement learning with selfimitation siloh et al. 2018. it is based on the idea of leveraging previously encountered highreward trajectories for reward shaping. this shaping is learned automatically using an adversarial setup, similar to gail ho  ermon, 2016. the paper clearly presents the proposed approach and relation to previous work. empirical evaluation shows strong performance on a 2d point mass problem designed to examine the algorithms behavior. of particular note are the insightful visualizations in figure 2 and 3 which shed light on the algorithms learning behavior. empirical results on the mujoco domain show that the proposed approach is particularly strong under delayedreward 20 steps and noisyobservation settings. the reviewers and ac note the following potential weaknesses the paper presents an empirical validation showing improvements over ppo, in particular in mujoco tasks with delayed rewards and with noisy observations. however, given the close relation to sil, a direct comparison with the latter algorithm seems more appropriate. reviewers 2 and 3 pointed out that the empirical validation of sil was more extensive, including results on a wide range of atari games. the authors provided results on several hardexploration atari games in the rebuttal period, but the results of the comparison to sil were inconclusive. given that the main contribution of the paper is empirical, the reviewers and the ac consider the contribution incremental. the reviewers noted that the proposed method was presented with little theoretical justification, which limits the contribution of the paper. during the rebuttal phase, the authors sketched a theoretical argument in their rebuttal, but noted that they are not able to provide a guarantee that trajectories in the replay buffer constitute an unbiased sample from the optimal policy, and that policy gradient methods in general are not guaranteed to converge to a globally optimal policy. the ac notes that conceptual insights can also be provided by motivating algorithmic or modeling choices, or through detailed analysis of the obtained results with the goal to further understanding of the observed behavior. any such form of developing further insights would strengthen the contribution of the submission.", "accepted": null}
{"paper_id": "iclr_2019_HJeQToAqKQ", "review_text": "connecting different fields and bringing new insights to machine learning are always appreciated. but since it is challenging to do it needs to be done well. this paper falls short here.", "accepted": 0}
{"paper_id": "iclr_2019_HJei-2RcK7", "review_text": "the reviewers all agree that the work is interesting, but none have stood out and championed the paper as exceptional. the reviewers note that the paper is wellwritten, contributes a methodological innovation, and provides compelling experiments. however, given the reviewers positive but unenthusiastic scores, and after discussion with pcs, this paper does not meet the bar for acceptance into iclr.", "accepted": null}
{"paper_id": "iclr_2019_HJlY0jA5F7", "review_text": "the paper proposes a novel sample based evaluation metric which extends the idea of fid by replacing the latent features of the inception network by those of a dataset specific vae and the fid by the mean fid of the classconditional distributions. furthermore, the paper presents interesting examples for which fid fails to match the human judgment while the new metric does not. all reviewers agree, that while these ideas are interesting, they are not convinced about the originality and significance of the contribution and believe that the work could be improved by a deeper analysis and experimental investigation.", "accepted": 0}
{"paper_id": "iclr_2019_HJxYwiC5tm", "review_text": "this paper attempts to answer its suggestive title by arguing that this generic lack of invariance in large cnn architectures is due to aliasing introduced during the downsampling stages. this paper received mixed reviews. positive aspects include the clarity and exhaustive empirical setups, whereas negative aspects focused on the lack of substance behind some of the claims. ultimately, the ac took these considerations into account and made hisher own assessment, summarized here. the main claim of this paper implies the following modern cnns are unable to build invariance to small shifts, but somehow are able to learn far more complex invariances involving lighting, pose, texture, etc. this must be empirically verified beyond reasonable doubt, and the ac thinks that the current experimental setup does not achieve this threshold. as mentioned by reviewers and by public comments, the preprocessing pipeline is a key factor that may be confounding the analysis, and this should be better analysed. for example, as mentioned in the reviews below, the shift in the image can be either done by inpainting, cropping, or using a fixed background. the authors claim that there are no qualitative differences between those preprocessing choices, but by inspecting figures 2b and 8c, the ac notices a severe change in jaggedness; in other words, the choice of preprocessing does affect the quantitative measures of unstability, even though the qualitative assessment unstable in all setups is the same. in particular, using noncentered crops should be the default setup, since it requires no preprocessing. it is confusing that it appears in the appendix instead of the inpainting version of figure 2b. this is important, since it implies that the analysis is mixing two perturbations the actual action of the translation group and the choice of preprocessing, and that the latter is by no means negligible. i would suggest the authors to perform the following experiment to disentangle the effect of translation by the effect of preprocessing. since the translation forms a group, for any shift applied to the image, one can undo it by applying the inverse shift. say one applies a shift to image x of d pixels and obtains xtx,d as a result by using whatever border handling procedure. if border effects were negligible, then xtx,d should give us back x, so a good measure of how unstable the network is is to measure the difference in prediction between x,x and x. if predicting x is as unstable as predicting x, it follows that the network is actually unstable to the border effect introduced by t. given this, the ac recommends rejection at this time, and encourages the authors to resubmit their work by addressing the above point.", "accepted": null}
{"paper_id": "iclr_2019_HkMwHsCctm", "review_text": "the strength of the paper is that it designs an lpbased algorithm for training neural networks with runtime exponential in the number of parameters and linear in the size of the datasets and the algorithm works for worstcase datasets. as reviewer 2 and reviewer 3 pointed out, the cons include a its not clear why the algorithm provides any theoretical insights on how to design in the future polynomial time algorithm  it seems that the algorithms are inherently exponential time and b its not clear whether the algorithm is practically at all relevant. the ac also noted that bruteforce search algorithm is also exponential in  parameters and linear in size of datasets, and the authors agreed with it. this leaves the main contribution of the paper be that it works for the worst datasets. however, theoretically speaking, its not clear this should be counted as a feature for algorithm design because we cannot go beyond the intractability without making assumptions on the data and in the acs opinion, the big open question is how to make additional assumptions on the data instead of removing them. in summary, the drawback b makes this a purely theoretical paper and the theoretical significance of the paper is unclear due to a. therefore based on a, the ac decided to recommend reject, although the ac suggested the authors to resubmit to other top theory or ml theory conferences which may better evaluate the theoretical significance of the paper.", "accepted": 1}
{"paper_id": "iclr_2019_Hke8Do0cF7", "review_text": "the reviewers agree this paper is not good enough for iclr.", "accepted": 0}
{"paper_id": "iclr_2019_HyMRUiC9YX", "review_text": "while the paper contains significant information, most insights have already been revealed in previous work as noted by r1. the empirical novelty is therefore limited and the authors do not provide theoretical analysis to complement this.", "accepted": 0}
{"paper_id": "iclr_2019_HyVbhi0cYX", "review_text": "this paper claims results showing relu networks or a particular architecture for that are nphard to learn. the authors claim that results that essentially show this such as those by livni et al. are unsatisfactory as they only show this for relu networks that are fully connected. however, the authors fail to criticize their own paper for only showing this result for a network with 3 gates. for the same reason that the livni et al. results dont imply anything for fully connected networks, these results dont imply anything for larger networks. conceivably certain gadgets could be created to ensure that the larger networks are essentially forced to ignore the rest of the gates. this line of research isnt terribly interesting and furthermore the paper is not particularly well written. for learning relus, it is already known assuming conjectures based on hardness of improper pac learning that functions that can be represented as a single hidden layer relu network cannot be learned even using a much larger network in polynomial time see for instance the livni et al. paper, etc.. proving nphardness results for proper isnt as useful as they usually are very restricted in terms of architectures the learning algorithm is allowed to use. however, if they do want to show such results, i think the nphardness of learning 2term dnf formulas will be a much easier starting point. also, i think there is a flaw in the proof of lemma 4.1. the function f cannot be represented by the networks the authors claim to use. in particular the 1eta outside the max0, x term is not acceptable. the main result of this work is to prove that a twolayer neural network with 2 hidden neurons in the first layer and 1 hidden neuron in the second layer is nphard to train when all activation functions are relu. similar results with the hardthresholding activation function were proved by blum  rivest in 1988. the main proof consists of a reduction from the 2hyperplane separability problem which was known to be equivalent to nns with hardthresholding activation functions. quality moderate. good solid formal results but nothing really surprising. clarity the manuscript is mostly wellwritten, and the authors gave proper credit to prior works. the only minor issue is perhaps the abuse of the variable w in introduction and the same variable w with an entirely different meaning in the rest of the paper. it would make more sense to change the ws in introduction to ds. originality this work is largely inspired by blum  rivests work, and builds heavily on some previous work including megiddo and edelsbrunner et al. while there is certainly some novelty in extending prior work to the relu activation function, it is perhaps fair to say the originality is moderate. significance while the technical construction seems plausible and correct, the real impact of the obtained results is perhaps rather limited. this is one of those papers that it is certainly nice to have all details worked out but none of the obtained results is really surprising or unexpected. while i do agree there is value in formally documenting the authors results, this conference is perhaps not the right venue. other comments the discussion on page 2 related literature seems odd. wouldnt the results of livni et al and dasgupta et al already imply the nphardness of fully connected relu networks, in a way similar to how one obtains corollary 3.2? if this is correct, then the contribution of this work is basically a refined complexity analysis where the relu network is shrunken to 2 layers with 3 nuerons? i really wish the authors had tried to make their result more general, which in my opinion would make this paper more interesting and novel can you extend the proof to a family of activation functions? it is certainly daunting to write separate papers to prove such a complexity result for every activation function... the authors also conveniently made the realizability assumption. what about the more interesting nonrealizable case? the construction to prove theorem 3.4 bears some similarity to a related result in zhang et al. the hard sorting part appears to be different. this paper shows that training of a 3 layer neural network with 2 hidden nodes in the first layer and one output node is npcomplete. this is an extension of the result of blum and rivest88. the original theorem was proved for threshold activation units and the current paper proves the same result for relu activations. the authors do this by reducing the 2affine separability problem to that of fitting a neural network to data. the reduction is well written and is clever. this is a reasonable contribution although it does not add significantly to the current state of the art. dear authors, all reviewers agreed that, while the problem considered was of interest, the theoretical result presented in this work was of too limited scope to be of interest for the iclr audience. based on their comments, you might want to consider a more theoreticallyoriented venue for such a submission.", "accepted": 0}
{"paper_id": "iclr_2019_Hye-LiR5Y7", "review_text": "the paper proposes an approach for transfer learning by assigning weights to source samples and learning these jointly with the network parameters. reviewers had a few concerns about experiments, some of which have been addressed by the authors. the proposed approach is simple which is a positive but it is not evaluated on any of the regular transfer learning benchmarks eg, the ones used in kornblith et al., 2018 do better imagenet models transfer better?. the tasks used in the paper, such as cifar noisy  cifar and svhn04  mnist59, are artificially constructed, and the paper falls short of demonstrating the effectiveness of the approach on real settings. the paper is on the borderline with current scores and the lack of regular transfer learning benchmarks in the evaluations makes me lean towards not recommending acceptance.", "accepted": null}
{"paper_id": "iclr_2019_HyefgnCqFm", "review_text": "this paper introduces a few training methods to fit the dynamics of a pde based on observations. quality not great. the authors seem unaware of much related work both in the numerics and deep learning communities. the experiments arent very illuminating, and the connections between the different methods are never clearly and explicitly laid out in one place. clarity poor. the intro is long and rambly, and the main contributions arent clearly motivated. a lot of time is spent mentioning things that could be done, without saying when this would be important or useful to do. an algorithm box or two would be a big improvement over the many long english explanations of the methods, and the diagrams with cycles in them. originality not great. there has been a lot of work on fitting dynamics models using nns, and also attempting to optimize pde solvers, which is hardly engaged with. significance this work fails to make its own significance clear, by not exploring or explaining the scope and limitations of their proposed approach, or comparing against more baselines from the large set of related literature.", "accepted": null}
{"paper_id": "iclr_2019_HyesW2C9YQ", "review_text": "the reviewers raised a number of concerns including the usefulness of the presented dataset given that the collected data is acted rather than naturalistic and the large body of research in affective computing explains that models trained on acted data cannot generalise to naturalistic data, no methodological novelty in the presented work, and relatively uninteresting application with very limited realworld application it remains unclear whether having better empathetic dialogues would be truly crucial for any reallife application and, in addition, all work is based on acted rather than realworld data. the authors rebuttal addressed some of the reviewers concerns but not fully especially when it comes to usefulness of the data. overall, i believe that the effort to collect the presented database is noble and may be useful to the community to a small extent. however, given the unrealism of the data and, in turn, very limited if any generalisability of the presented to realworld scenarios, and lack of methodological contribution, i cannot recommend this paper for presentation at iclr.", "accepted": 0}
{"paper_id": "iclr_2019_HygUOoC5KX", "review_text": "adversarial defense is a tricky subject, and the authors are to be commended for their novel approach to this problem. the reviewers all agree that there is promise in this approach. however, after reviewing the discussion, they have all come to the conclusion that the robustness of your generative model needs to be more thoroughly explored. regarding gradient masking, there are other attacks like a manifold attack that use gradients that can be explored as well. regarding spsa, it would be helpful perhaps to also include other numerical gradient attacks to ensure that spsa is stronger and working as intended. essentially, the reviewers would all like to see a more streamlined version of this paper that removes any doubt about the efficacy of the generative approach. once that is established, additional properties and features can be explored. also note that for the purposes of these reviews and discussion, schott et al. was considered as concurrent work and not prior work.", "accepted": 1}
{"paper_id": "iclr_2019_Hylnis0qKX", "review_text": "this work presents a reconstruction gan with an additional classification task in the objective loss function. evaluations are carried out on medical and nonmedical datasets. reviewers raise multiple concerns around the following  novelty all reviewers  inadequate comparison baselines all reviewers  inadequate citations. r2  r3 authors have not offered a rebuttal. recommendation is reject. work may be more suitable as an application paper for a medical conference or journal.", "accepted": 0}
{"paper_id": "iclr_2019_HyxOIoRqFQ", "review_text": "the paper presents an original approach to replace inefficient discrete autoregressive posterior sampling by a parallel sampling procedure based on fixedpoint iterations reminiscent of normalizing flow, but for discrete variables. all reviewers liked the idea, and found that it was an original and promising approach. but all agreed the paper was poorly written and very unclear. all also found the experimental section lacking, in clarity and scope. authors did not provide a rebuttal. overall a potentially really promising idea, but the paper is not yet ripe.", "accepted": 0}
{"paper_id": "iclr_2019_HyxhusA9Fm", "review_text": "this paper introduces a newly collected dataset of natural language interactions between a tourist and a guide for localization and navigation. the paper also includes baseline experiments with a reasonably novel approach. the task is well motivated although an open question remains due to gps, comment by reviewer 1, but the description of the dataset and collection, approach and experiments were not ideal in the first version of the paper. much of the information was pushed to the appendix and it was hard to follow the paper without going back and forth, and even then some points were missing. authors rewrote parts of the paper to address these concerns, but there are still some open questions. for example, is it possible to have subtasks, given the task is complex and may not be easy to accomplish as a whole? or could simple lstm be another baseline the final review of the third reviewer?", "accepted": 0}
{"paper_id": "iclr_2019_S1en0sRqKm", "review_text": "the paper presents an interesting empirical analysis showing that increasing the batch size beyond a certain point yields no decrease in time to convergence. this is an interesting finding, since it indicates that parallelisation approaches might have their limits. on the other hand, the study does not allow the practitioners to tune their hyperparamters since the optimal batch size is dependent on the model architecture and the dataset. furthermore, as also pointed out in an anonymous comment, the batch size is very large compared to the size of the benchmark sets. therefore, it would be nice to see if the observation carries over to largescale data sets, where the number of samples in the minibatch is still small compared to the total number of samples.", "accepted": 1}
{"paper_id": "iclr_2019_S1gBz2C9tX", "review_text": "the paper proposes to use importance resampling ir as an alternative to the more popular importance sampling is approach to offpolicy rl. the hope is to reduce variance, as shown in experiments. however, there is no analysis whywhen ir will be better than is for variance reduction, and a few baselines were suggested by reviewers. while the authors rebuttal was helpful in clarifying several issues, the overall contribution does not seem strong enough for iclr, on both theoretical and empirical sides. the high variance of is is known, and the following work may be referenced for better 1st order updates when is weights are used karampatziakis  langford uai11. in section 3, the paper says that most offpolicy work uses d_mu, instead of d_pi, to weigh states. this is true, but in the current context infinitehorizon rl, there are more recent works that should probably be referenced httpproceedings.mlr.pressv70hallak17a.html httpspapers.nips.ccpaper7781breakingthecurseofhorizoninfinitehorizonoffpolicyestimation", "accepted": null}
{"paper_id": "iclr_2019_S1lwRjR9YX", "review_text": "the paper according to reviewers needs more work for publication and significantly more clarifications. the reviewers are not convinced on publishing even after intensive discussion that the ac read in full. the ac recommends further improvements on the paper to address better reviewers concerns.", "accepted": 0}
{"paper_id": "iclr_2019_S1xLZ2R5KQ", "review_text": "this paper proposes a framework of image restoration by searching for a map in a trained gan subject to a degradation constraint. experiments on mnist show good performance in restoring the images under different types of degradation. the main problem as pointed out by r1 and r3 is that there has been rich literature of image restoration methods and also several recent works that also utilized gan, but the authors failed to make comparison any of those baselines in the experiments. additional experiments on natural images would provide more convincing evidence for the proposed algorithm. the authors argue that the restoration tasks in the experiments are too difficult for tv to work. it would be great to provide actual experiments to verify the claim.", "accepted": 0}
{"paper_id": "iclr_2019_SJg7IsC5KQ", "review_text": "the reviewers agree that providing more insights on why batch normalization work is an important topic of investigation, but they all raised several problems with the current submission which need to be addressed before publication. the ac thus proposes revise and sesubmit.", "accepted": 0}
{"paper_id": "iclr_2019_SJlpM3RqKQ", "review_text": "this paper focuses on communication efficient federated learning fl and proposes an approach for training large models on heterogeneous edge devices. the paper is wellwritten and the approach is promising, but all reviewers pointed out that both novelty of the approach and empirical evaluation, including comparison with stateofart, are somewhat limited. we hope that suggestions provided by the reviewers will be helpful for extending and improving this work.", "accepted": 0}
{"paper_id": "iclr_2019_SkGNrnC9FQ", "review_text": "the diffusion maps framework is used to embed a given collection of datasets into diffusion coordinates that capture intrinsic geometry. then a correspondence map is constructed between datasets by finding rotations that align these coordinates. the approach is interesting. the reviewers, however, found the empirical analysis somewhat simplistic with inadequate comparisons to other correspondence construction methods in the literature.", "accepted": 0}
{"paper_id": "iclr_2019_SkGQujR5FX", "review_text": "the paper needs more revisions and better presentation of empirical study.", "accepted": null}
{"paper_id": "iclr_2019_SkVRTj0cYQ", "review_text": "following the unanimous vote of the reviewers, this paper is not ready for publication at iclr. the greatest concern was that the novelty beyond past work has not been sufficiently demonstrated.", "accepted": 0}
{"paper_id": "iclr_2019_Skf-oo0qt7", "review_text": "some expert reviewers have raised novelty issues, that the authors have addressed in detail. still, these expert reviewers are not entirely convinced. if this were a journal, i would recommend a major revision or rejectandresubmit in order to allow the authors to anticipate the reviewers concerns in the body of the paper and get some fresh reviews. i compliment the authors on the diligence they have put into the rebuttal stage, and look forward to reading the next version of the work. i will note that the bounds by bartlett, foster, and telgarsky and then the pacbayes versions by neyshabur et al. are numerically vacuous empirically, and so whether those bounds or these bounds for rnns explain generalization is up for debate.", "accepted": 0}
{"paper_id": "iclr_2019_SkghN205KQ", "review_text": "this paper proposes searchguided training for structured prediction energy networks spens. the reviewers found some interest in this approach, though were somewhat underwhelmed by the experimental comparison and the details provided about the method. r1 was positive and recommends acceptance; r2 and r3 thought the paper was on the incremental side and recommend rejection. given the space restriction to this years conference, we have to reject some borderline papers. the ac thus recommends the authors to take the reviewers comments in consideration for a revise and resubmit.", "accepted": 0}
{"paper_id": "iclr_2019_Skl6k209Ym", "review_text": "the reviewers are polarized on this paper and the overall feeling is that it is not quite ready for publication. there is also an interesting interpretability aspect that, while given as a motivation for the approach, is never really explored beyond showing some figures of alignments. one of the main concerns of the methods effectiveness in practice is the computational cost. there is also concern from one of the reviewers that the formulation could result in creating sparse matching maps where only a few pixels get matched. the authors provide some justification for why this wouldnt happen, and this should be put in a future draft. even better would be to show statistics to demonstrate empirically that this doesnt happen. there were a number of clarifications that were brought up during the discussion, and the authors should go over this carefully and update the draft to resolve these issues. there is also a typo in the title that should be fixed.", "accepted": 0}
{"paper_id": "iclr_2019_Sklqvo0qt7", "review_text": "i enjoyed reading the paper myself and agree with some of the criticisms raised by the reviewers, but not all of them. in particular, i dont think its a major issues that this work studies an explicit regularization scheme because the state of our understanding of generalization in deep learning is so embarrassingly poor!! unlike a lot of work, this work is engaging with the approximation error and developing risk bounds called generalization error here ... not my favorite term for the risk! rather than just controlling the generalization gap. the simple proof in the bounded noiseless case was nice to see. on the other hand, not being familiar with the work of klusowski and barron 2016, im not willing to overrule the reviewers on judgments that this work is not novel enough. i would suggest the authors take control of this and paint a more detailed picture of how these two bodies of work relate, including how the proof techniques and arguments overlap. some other comments 1. your theorem requires lambda  4, but then youre using lambda  0.1. this seems problematic to me. 2. your nonvacuous upper bound is pathnormsqrtn ... but do the numbers in the table include the constants? looking at the constants that are likely to show up, 4bn sqrt2 log 2d, they are easily contributing a factor greater than 10 which would make these bounds vacuous as well. you need to explain how you are calculating these numbers more carefully. 3. several times arora et al and neyshabur et al are cited when reference is being made to numerical experiments to show that existing bounds are vacuously large. but dziugaite and roy, who you cite for the term nonvacuous, made an earlier analysis of pathnorm bounds in their appendix and point out that they are vacuous. 4. the paper does not really engage with the fact that you are unlikely to be exactly minimizing the functional j. any hope of bridging this gap? 5. the experiments in general are a bit too vaguely described. also, you control squared error but then only report classification error. would be interested to see both.", "accepted": 0}
{"paper_id": "iclr_2019_SkxxIs0qY7", "review_text": "the paper proposes an original and interesting alternative to gans for optimizing a proxy to jensenshannon divergence for discrete sequence data. experimental results seem promising. official reviewers were largely positive based on originality and results. however, as it currently stands, the paper still makes false claims that are not well explained or supported, in particular its repeated central claim to provide a lowvariance, biasfree algorithm to optimize js. given that these central issues were clearly pointed out in a review from a prior submission of this work to another venue review reposted on the current openreview thread on nov. 6, the ac feels that the authors had had plenty of time to look into them and address them in the paper, as well as occasions to reference and discuss relevant related work pointed in that review. the current version of the paper does neither. the algorithm is not unbiased for at least two reasons pointed out in discussions a in practice a parameterized mediator will be unable to match the true pg, at best yielding a useful biased estimate not unlike how gans parameterized discriminator induces bias. b one would need to use reinforce or similar to get an unbiased estimate of the gradient in eq. 13, a key detail omitted from the paper. from the discussion thread it is possible that authors were initially confused about the fact that this fundamental issue did not disappear with eq. 13 they commented most important idea we want to present in this paper is how to avoid incorporating reinforce. please refer to eq.13, which is the key to the success of this.. but rather, as guessed by a commentator, that a heuristic implementation, not explained in the paper, dropped the reinforce term thus effectively trading variance for bias. on december 4th authors posted a justification confirming heuristically dropping the reinforce terms when taking the gradient of eq. 13, and said they could attach detailed analysis and experiment results in the cameraready version. however if one of the most important idea of the paper is how to avoid reinforce as still implied and highlighted in the abstract, the ac finds it worrisome that the paper had no explanation of when and how this was done, and no analysis of the bias induced by unreportedly dropping the term. the approach remains original, interesting, and potentially promising, but as it currently stands, ac and sac agreed that inexact theoretical overclaiming and insufficient justification and indepth analysis of key heuristic shortcutstradeoffs however useful are too important for their fixing to be entrusted to a final cameraready revision step. a major revision that clearly adresses these issues in depth both in how the approach is presented and in supporting experiments will constitute a much more convincing, sound, and impactful research contribution.", "accepted": null}
{"paper_id": "iclr_2019_Skz-3j05tm", "review_text": "this paper describes a graph convolutional network gcn approach to capture relational information in natural language as well as knowledge sources for goaloriented dialogue systems. relational information is captured by dependency parses, and when there is code switching in the input language, word cooccurrence information is used instead. experiments on the modified dstc2 dataset show significant improvements over baselines. the original version of the paper lacked comparison to some sota baselines as also raised by the reviewers, these are included in the revised version. although the results show improvements over other approaches, it is arguable bleu and rouge scores are not good enough for this task. inclusion of human evaluation in the results would be very useful.", "accepted": null}
{"paper_id": "iclr_2019_SyVhg20cK7", "review_text": "this work introduces a rewardshaping scheme for multiagent settings based on the tderror of other agents. overall, reviewers were positive about the direction and the presentation but had a variety of concerns and questions and felt more experiments were necessary to validate the claims of flexibility and scalability, with results more comparable to the scale of the contemporary multiagent literature. one note in particular a feedforward q network is used in a partially observable environment, which the authors seemed to dismiss in their rebuttal. i agree with the reviewer that this is an important consideration when comparing to baselines which were developed with recurrent networks in mind. a revised manuscript addressed concerns with the presentation but did not introduce new results or plots, and reviewers were not convinced to alter their evaluation. there is agreement that this is an interesting paper, so i recommend that the authors conduct a more thorough empirical evaluation and submit to another venue.", "accepted": null}
{"paper_id": "iclr_2019_SyfXKoRqFQ", "review_text": "this paper introduced an adaptive importance sampling strategy to select minibatches to speed up the convergence of network training. the method is well motivated and easy to follow. the main concerns raised by the reviewers are limited novelty of the proposed simple idea compared to related recent work, and moderate empirical performance. the authors argue that the particular choice of the adaptive sampling method comes after trying various methods. i believe providing more detailed discussion and comparison with different methods together with the active bias paper would help the readers appreciate the insights conveyed in this paper. the authors provide some additional experiments in the revision. it would make the whole experiment section a lot stronger and convincing if the authors could run more thorough experiments on extra challenging datasets and include all the results int the main text. additional experiment to clarify the merit of the proposed method on either faster convergence or lower asymptotic error would also improve the contribution of this paper.", "accepted": null}
{"paper_id": "iclr_2019_r1esnoAqt7", "review_text": "this paper presents a dataset for measuring disentanglement in learned representations. it consists of mnist digits, sometimes transformed in various ways, and labeled with a variety of attributes. this dataset is used to measure statistics of various learned models. measuring disentanglement is certainly an important problem in our field. this dataset seems to be well designed, and i would recommend its use for papers studying disentanglement. the experiments are welldesigned. while the reviewers seem bothered by the fact that its limited to mnist, this doesnt strike me as a problem. we continue to learn a lot from mnist, even today. but producing a useful dataset isnt by itself a significant enough research contribution for an iclr paper. id recommend publication if a it were very different from currently existing datasets, b constructing it required overcoming significant technical obstacles, or c the dataset led to particularly interesting findings. regarding a, there are already datasets of similar complexity which have groundtruth attributes useful for measuring disentanglement, such as dsprites and 3d faces. regarding b, the construction seems technically straightforward. regarding c, the experimental findings are plausible and consistent with past findings which is a good validation of the dataset but not obviously interesting in their own right. so overall, this seems like a useful dataset, but i cannot recommend publication at iclr.", "accepted": 0}
{"paper_id": "iclr_2019_r1fE3sAcYQ", "review_text": "pros  nicely written paper  clear and precise with a derivation of the loss function cons noveltyimpact i think all the reviewers acknowledge that you are doing something different in the neural brainwashing nb problem than is done in the typical catastropic forgetting cf setting. you have one dataset and a set of models with shared weights; the cf setting has one model and trains on different datasetstasks. but whereas solving the cf problem would solve a major problem of continual machine learning, the value of solving the nb problem is harder to assess from this paper... the main application seems to be improving neural architecture search. at the metalevel, the techniques used to derive the main loss are already well known and the result similar to ewc, so they dont add a lot from the analysis perspective. i think it would be very helpful to revise the paper to show a range of applications that could benefit from solving the nb problem and that the technique you propose applies more broadly.", "accepted": null}
{"paper_id": "iclr_2019_r1fiFs09YX", "review_text": "the paper extends maml so that a learned behavior can be quickly sampleefficiently adapted to a new agend allied or opponent. the approach is tested on two simple tasks in 2d gridworld environments chasing and path blocking. the experiments are very limited, they do not suffice to support the claims about the method. the authors did not enter a rebuttal and all the reviewers agree that the paper is not good enough for iclr.", "accepted": 0}
{"paper_id": "iclr_2019_r1gOe209t7", "review_text": "all reviewers recommend reject and there is no rebuttal. there is no basis on which to accept the paper.", "accepted": 0}
{"paper_id": "iclr_2019_r1xwS3RqKQ", "review_text": "the reviewers unanimously agreed the paper did not meet the bar of acceptance for iclr. they raised questions around the technical correctness of the paper, as well as the experimental setup. the authors did not address any reviewer concerns, or provide any response. therefore, i recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2019_rJgSV3AqKQ", "review_text": "the paper is a premature submission that needs significant improvement in terms of conceptual, theoretical, and empirical aspects.", "accepted": 0}
{"paper_id": "iclr_2019_rJxXDsCqYX", "review_text": "this paper presents two extensions of relation networks rns to represent a sentence as a set of relations between words 1 dependencybased constraints to control the influence of different relations within a sentence and 2 recurrent extension of rns to propagate information through the tree structure of relations. pros the notion of relation networks for sentence representation is potentially interesting. cons the significance of the proposed methods compared to existing variants of treernns is not clear r1. r1 requested empirical comparisons against treernns since the proposed methods are also of tree shape, but the authors argued back that such experiments are necessary beyond bilstm baselines. verdict reject. the proposed methods build on relatively incremental ideas and the empirical results are rather inconclusive.", "accepted": 0}
{"paper_id": "iclr_2019_rkglvsC9Ym", "review_text": "the reviewers agree that the paper is wellwritten, and they all seem to like the general idea. one of the earlier criticisms was that you did not compare against other robust loss functions, but you have partially rectified that by comparing to l1 in the appendix. as per the request of reviewer 2 i would also compare to the huber loss. one remaining concern is the lack of theoretical justification, which could help address the comment of reviewer 3 regarding blurry images from location uncertainty. the other concern is that you should compare your method using fid scores from a standard implementation so that your numbers are comparable to other papers. some of the reviewers were impressed, but confused by your relatively low scores.", "accepted": 0}
{"paper_id": "iclr_2019_rkl4M3R5K7", "review_text": "four reviewers have evaluated this paper. the reviewers have raised concerns about the specific formulation used for adversarial example generation which requires further clarity in motivation and interpretation. the reviewers have also made the point that the experimental evaluation is against previous work which tried to solve a different problem black box based attack and hence the conclusions are unconvincing.", "accepted": 0}
{"paper_id": "iclr_2019_rkx0g3R5tX", "review_text": "all reviewers agree that the paper is not quite ready for publication.", "accepted": 0}
{"paper_id": "iclr_2019_rkxhX209FX", "review_text": "the paper addresses sampleefficient robust policy search borrowing ideas from active learning. the reviews raised important concerns regarding 1 the complexity of the proposed technique, which combines many separate pieces and 2 the significance of experimental results. the empirical setup adopted is not standard in rl, and a clear comparison against epopt is lacking. i appreciate the changes made to address the comment, and i encourage the authors to continue improving the paper by simplifying the model and including a few baseline comparisons in the experiments.", "accepted": null}
{"paper_id": "iclr_2019_ryGiYoAqt7", "review_text": "the authors take two algorithmic components that were proposed in the context of discreteaction rl  priority replay and parameter noise  and evaluate them with ddpg on continuous control tasks. the different approaches are nicely summarized by the authors, however the contribution of the paper is extremely limited. there is no novelty in the proposed approaches, the empirical evaluation is inconclusive and limited, and there is no analysis or additional insights or results. the ac and the reviewers agree that this paper is not strong enough for iclr.", "accepted": 0}
{"paper_id": "iclr_2019_ryfz73C9KQ", "review_text": "this paper proposed an unsupervised learning algorithm for predictive modeling. the key idea of using ncecpc for predictive modeling is interesting. however, major concerns were raised by reviewers on the experimental designempirical comparisons and paper writing. overall, this paper cannot be published in its current form, but i think it may be dramatically improved for a future publication.", "accepted": 0}
{"paper_id": "iclr_2019_rygunsAqYQ", "review_text": "the manuscript proposes a novel estimation technique for generative models based on fast nearest neighbors and inspired by maximum likelihood estimation. overall, reviewers and ac agree that the general problem statement is timely and interesting, and the subject is of interest to the iclr community the reviewers and acs note weakness in the evaluation of the proposed method. in particular, reviewers note that the parzenbased loglikelihood estimate is known to be unreliable in highdimensions. this makes a quantitative evaluation of the results challenging, thus other metrics should be evaluated. reviewers also expressed concerns about the strengths of the baselines compared. additional concerns are raised with regards to scalability which the authors address in the rebuttal.", "accepted": 0}
{"paper_id": "iclr_2019_ryxDUs05KQ", "review_text": "the paper presents a gan for learning a target distribution that is defined as the difference between two other distributions. the reviewers and ac note the critical limitation of novelty and appealing results of this paper to meet the high standard of iclr. ac thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.", "accepted": 0}
{"paper_id": "iclr_2019_ryxOIsA5FQ", "review_text": "this work proposes a method for both instance and feature based transfer learning. the reviewers agree that the approach in current form lacks sufficient technical novelty for publication. the paper would benefit from experiments on larger datasets and with more analysis into the different aspects of the proposed model.", "accepted": 0}
{"paper_id": "iclr_2019_ryxhynC9KX", "review_text": "the authors provide a convolutional neural network for predicting the satisfiability of sat instances. the idea is interesting, and the main novelty in the paper is the use of convolutions in the architecture and a procedure to predict a witness when the formula is satisfiable. however, there are concerns about the suitability of convolutions for this problem because of the permutation invariance of sat. empirically, the resulting models are accurate correctly predicting satunsat 9099 of the time while taking less time than some existing solvers. however, as pointed out by the reviewers, the empirical results are not sufficient to demonstrate the effectiveness of the approach. i want to thank the authors for the great work they did to address the concerns of the reviewers. the paper significantly improved over the reviewing period, and while it is not yet ready for publication, i want to encourage the authors to keep pushing the idea to further and improve the experimental results.", "accepted": null}
{"paper_id": "iclr_2019_ryxjH3R5KQ", "review_text": "this paper proposes direct sparse optimization dsonas to obtain neural architectures on specific problems at a reasonable computational cost. regularization by sparsity is a neat idea, but similar idea has been discussed by many pruning papers. model pruning formulation for neural architecture search based on sparse optimization is claimed to be the main contribution, but its debatable if such contribution is strong worse accuracy, more computation, more parameters than mnas less search time, but also worse search quality. the effect of each proposed technique is appropriately evaluated. however, the reviewers are concerned that the proposed method does not outperform the existing stateoftheart methods in terms of classification accuracy. theres also some concerns about the search space of the proposed method. it is debatable about claim that the first nas algorithm to perform direct search on imagenet and the first method to perform direct search without block structure sharing. given the acceptance rate of iclr should be 30, i would say this paper is good but not outstanding.", "accepted": null}
{"paper_id": "iclr_2020_H1lBj2VFPS", "review_text": "this paper considers the question of how to quantize deep neural networks, for processors operating on lowprecision integers. the authors propose a methodology and have evaluated it thoroughly. the reviewers all agree that this question is important in practice, though there was disagreement about how novel a contribution this paper is specifically, and on its clarity. the clarity questions were resolved on rebuttal, so i lean to accepting the paper.", "accepted": 0}
{"paper_id": "iclr_2020_BJeS62EtwH", "review_text": "this paper presents a method for extracting knowledge consistency between neural networks and understanding their representations. reviewers and ac are positive on the paper, in terms of insightful findings and practical usages, and also gave constructive suggestions to improve the paper. in particular, i think the paper can gain much attention for iclr audience. hence, i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_H1gzR2VKDH", "review_text": "this paper proposes a method that uses subgoals for planning when using video prediction. the reviewers thought that the paper was clearly written and interesting. the reviewer questions and concerns were mostly addressed during the discussion phase, and the reviewers are in agreement that the paper should be accepted.", "accepted": null}
{"paper_id": "iclr_2020_HJgK0h4Ywr", "review_text": "this manuscript proposes and evaluates new metrics for measuring the quality of disentangled representations for both supervised and unsupervised settings. the contributions include conceptual definitions and empirical evaluation. in reviews and discussion, the reviewers and ac note missing or inadequate empirical evaluation with many available methods for learning disentangled representations. on the writing, reviewers mentioned that the conciseness of the manuscript could be improved. the reviewers also mentioned incomplete references and discussion of prior work, which should be improved.", "accepted": null}
{"paper_id": "iclr_2020_ryxgJTEYDr", "review_text": "in contrast to many current hierarchical reinforcement learning approaches, the authors present a decentralized method that learns low level policies that decide for themselves whether to act in the current state, rather than having a centralized higher level meta policy that chooses between low level policies. the reviewers primarily had minor concerns about clarity, reward scaling, and several other issues that were clarified by the authors. the only outstanding concern is that of whether transferpretraining is required for the experiments to work or not. while this is an interesting question that i would encourage authors to address as much as possible, it does not seem like a dealbreaker in light of the reviewers agreement on the core contribution. thus, i recommend this paper for acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_r1lL4a4tDB", "review_text": "the authors propose to decompose control in a pomdp into learning a model of the environment via a vrnn and learning a feedforward policy that has access to both the environment and environment model. they argue that learning the recurrent environment model is easier than learning a recurrent policy. they demonstrate improved performance over existing stateoftheart approaches on several po tasks. reviewers found the motivation for the proposed approach convincing and the experimental results proved the effectiveness of the method. the authors response resolved reviewers concerns, so as a result, i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_rJehNT4YPr", "review_text": "this paper proposes a new way of comparing classifiers, which does not use fixed test set and adaptively sample it from an arbitrarily large corpus of unlabeled images, i.e. replacing the conventional testsetbased evaluation methods with a more flexible mechanism. the main proposal is to build a test set adaptively in a manner that captures how classifiers disagree, as measured by the wordnet tree. as noted by r2, this work has the potential to be of interest to a broad audience and can motivate many subsequent works. while the reviewers acknowledged the importance of this work, they raised several concerns 1 the proposed approach is immature to be considered for benchmarking yet r1,r4, 2 selecting k and studying its influence on the performance  r1, r3, r4, 3 the proposed approach requires data annotation which might not be straightforward  r3, r4. the authors provided a detailed rebuttal addressing the reviewer concerns. there is reviewer disagreement on this paper. the comments from r3 were valuable for the discussion, but at the same time too brief to be adequately addressed by the authors. the comments from emergency reviewer were helpful in making the decision. ac decided to recommend acceptance of the paper seeing its valuable contributions towards rethinking the evaluation of current sota models.", "accepted": 1}
{"paper_id": "iclr_2020_H1gmHaEKwB", "review_text": "the rebuttal period influenced r1 to raise their rating of the paper. the most negative reviewer did not respond to the author response. this work proposes an interesting approach that will be of interest to the community. the ac recommends acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_ryxOUTVYDH", "review_text": "this paper proposes an ensemble method to identify noisy labels in the training data of supervised learning. the underlying hypothesis is that examples with label noise require memorization. the paper proposes methods to identify and remove bad training examples by retaining only the training data that maintains low losses after perturbations to the model parameters. this idea is developed in several candidate ensemble algorithms. one of the proposed ensemble methods exceeds the performance of stateoftheart methods on mnist, cifar10 and cifar100. the reviewers found several strengths and a few weaknesses in the paper. the paper was well motivated and clear. the proposed solution was novel and plausible. the experiments were comprehensive. the reviewers identified several parts of the paper that could be more clear or where more detail could be provided, including a complexity analysis and extended experiments. the author response addressed the reviewer questions directly and also in a revised document. in the discussion phase, the reviewers were largely satisfied that their concerns were addressed. this paper should be accepted for publication as the paper presents a clear problem and solution method along with convincing evidence of methods merits.", "accepted": 1}
{"paper_id": "iclr_2020_SJxDDpEKvH", "review_text": "this paper provides a fresh application of tools from causality theory to investigate modularity and disentanglement in learned deep generative models. it also goes one step further towards making these models more transparent by studying their internal components. while there is still margin for improving the experiments, i believe this paper is a timely contribution to the iclrml community. this paper has highvariance in the reviewer scores. but i believe the authors did a good job with the revision and rebuttal. i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_SJgMK64Ywr", "review_text": "the submission applies architecture search to find effective architectures for video classification. the work is not terribly innovative, but the results are good. all reviewers recommend accepting the paper.", "accepted": 1}
{"paper_id": "iclr_2020_Bkeeca4Kvr", "review_text": "the authors propose a method for fewshot learning for graph classification. the majority of reviewers agree on the novelty of the proposed method and that the problem is interesting. the authors have addressed all major concerns.", "accepted": 0}
{"paper_id": "iclr_2020_BJgza6VtPB", "review_text": "main content blind review 1 summarizes it well recently many language gan papers have been published to overcome the so called exposure bias, and demonstrated improvements in natural language generation in terms of sample quality, some works propose to assess the generation in terms of diversity, however, quality and diversity are two conflicting measures that are hard to meet. this paper is a groundbreaking work that proposes receiver operating curve or pareto optimality for quality and diversity measures, and shows that simple temperature sweeping in mle generates the best qualitydiversity curves than all language gan models through comprehensive experiments. it points out a good target that language gans should aims at.  discussion the main reservation was the originality of the idea of using temperature sweep in the softmax. however, it turns out this idea came from the authors in the first place, which they have not been able to state directly due to the anonymity requirement. per the program chairs instruction to direct this to the area chair, i think this has been handled correctly.  recommendation and justification this paper should be accepted. it provides readers with insight in that it illuminates a misconception of how important exposure bias has been assumed to be, and provides a less expensive mle based way to train than gan counterparts.", "accepted": 1}
{"paper_id": "iclr_2020_HylsTT4FvB", "review_text": "all three reviewers agree that the paper provide an interesting study on the ability of generative adversarial networks to model geometric transformations and a simple practical approach to how such ability can be improved. acceptance as a poster is recommended.", "accepted": 1}
{"paper_id": "iclr_2020_HJe_yR4Fwr", "review_text": "this works presents a new and interesting notion of margin for deep neural networks that incorporates representation at all layers. it then develops generalization bounds based on the introduced margin. the reviewers pointed some concerns, including some notation issues, complexity in case of residual networks, removal of exponential dependence on depth, and dependence on a hard to compute quantity  kappadv. some of these concerns were addressed by the authors. at the end, most of the reviewers find the notion of alllayer margin introduced in this paper a very novel and promising idea for characterizing generalization in deep networks. agreeing with reviewers, i recommend accept. however, i request the authors to accommodate remaining comments concerns raised by r1 in the final version of your paper. in particular, in your response to r1 you mentioned for one case you saw improvement even with dropout, but that is not mentioned in the revision; please include related details in the draft.", "accepted": 1}
{"paper_id": "iclr_2020_SkxybANtDB", "review_text": "the paper proposes a bayesian approach for timeseries regression when the explanatory timeseries influences the response timeseries with a time lag. the time lag is unknown and allowed to be nonstationary process. reviewers have appreciated the significance of the problem and novelty of the proposed method, and also highlighted the importance of the application domain considered by the paper.", "accepted": 1}
{"paper_id": "iclr_2020_rkgAGAVKPr", "review_text": "while the reviewers have some outstanding issues regarding the organization and clarity of the paper, the overall consensus is that the proposed evaluation methods is a useful improvement over current standards for metalearning.", "accepted": 1}
{"paper_id": "iclr_2020_HJeO7RNKPr", "review_text": "this work proposes a cnn architecture for joint depth and camera motion estimation from videos. the paper presents a differentiable formulation of the problem to allow its endtoend learning, and the reviewers unanimously find the proposed approach reasonable and agree that this is a solid paper. some of the reviewers find the method itself to be too mechanical, but they all agree that this is a wellengineered solution.", "accepted": 1}
{"paper_id": "iclr_2020_rkenmREFDr", "review_text": "this paper proposes a new framework for improved nearest neighbor search by learning a space partition of the data, allowing for better scalability in distributed settings and overall better performance over existing benchmarks. the two reviewers who were most confident were both positive about the contributions and the revisions. the one reviewer who recommended reject was concerned about the metric used and whether comparison with baselines was fair. in my opinion, the authors seem to have been very receptive to reviewer comments and answered these issues to my satisfaction. after author and reviewer engagement, both r1 and myself are satisfied with the addition of the new baselines and think the authors have sufficiently addressed the major concerns. for the final version of the paper, id urge the authors to take seriously r4s comment regarding clarity and add algorithmic details as per their suggestion.", "accepted": 0}
{"paper_id": "iclr_2020_HJx8HANFDH", "review_text": "this paper proposes techniques to improve training with batch normalization. the paper establishes the benefits of these techniques experimentally using ablation studies. the reviewers found the results to be promising and of interest to the community. however, this paper is borderline due in part due to the writing notation issues and because it does not discuss related work enough. we encourage the authors to properly address these issues before the camera ready.", "accepted": 0}
{"paper_id": "iclr_2020_S1xtORNFwH", "review_text": "the paper proposes to compress convolutional neural networks via weight sharing across filters of each convolution layer. a fast convolution algorithm is also designed for the convolution layer with this approach. experimental results show i effectiveness in cnn compression, ii acceleration on the tasks of image classification, object detection and neural architecture search. while the authors addressed most of reviewers concerns, the weakness of the paper which remains is that no wallclock runtime numbers only flops are reported  so efficiency of the approach in practice in uncertain.", "accepted": 1}
{"paper_id": "iclr_2020_rkgHY0NYwr", "review_text": "the work presents a novel and effective solution to learning reusable motor skills. the urgency of this problem and the considerable rebuttal of the authors merits publication of this paper, which is not perfect but needs community attention.", "accepted": 1}
{"paper_id": "iclr_2020_HklUCCVKDB", "review_text": "while prior work has shown the potential of using uncertainty to tackle catastrophic forgetting e.g. by appropriate updates to the posterior, this paper goes further and proposes a strategy to adapt the learning rate based on the uncertainty. this is a very reasonable idea since, in practice, learning rate control is one of the simplest and most understood techniques to fight catastrophic forgetting. the overall approach ends up being a wellmotivated strategy for controlling the learning rate of the parameters according to a notion of their importance. of course now the question is if this work uses a good proxy for importance so further ablation studies would help, but the current results already show a clear benefit.", "accepted": 1}
{"paper_id": "iclr_2020_SkgsACVKPH", "review_text": "this paper proposes a method to improve the training of sparse network by ensuring the gradient is preserved at initialization. the reviewers found that the approach was well motivated and well explained. the experimental evaluation considers challenging benchmarks such as imagenet and includes strong baselines.", "accepted": null}
{"paper_id": "iclr_2020_Skxuk1rFwB", "review_text": "this paper presents a method that hybridizes the strategies of linear programming and interval bound propagation to improve adversarial robustness. while some reviewers have concerns about the novelty of the underlying ideas presented, the method is an improvement to the sota in certifiable robustness, and has become a benchmark method within this class of defenses.", "accepted": 1}
{"paper_id": "iclr_2020_HygegyrYwH", "review_text": "this paper studies how much overparameterization is required to achieve zero training error via gradient descent in one hidden layer neural nets. in particular the paper studies the effect of margin in data on the required amount of overparameterization. while the paper does not improve in the worse case in the presence of margin the paper shows that sometimes even logarithmic width is sufficient. the reviewers all seem to agree that this is a nice paper but had a few mostly technical concerns. these concerns were sufficiently addressed in the response. based on my own reading i also find the paper to be interesting, well written with clever proofs. so i recommend acceptance. i would like to make a suggestion that the authors do clarify in the abstract intro that this improvement can not be achieved in the worst case as a shallow reading of the manuscript may cause some confusion that logarithmic width suffices in general.", "accepted": 1}
{"paper_id": "iclr_2020_S1ltg1rFDS", "review_text": "this paper addresses an important and relevant problem in reinforcement learning learning from offpolicy data, taking into account the offsets in the visitation distribution of states. this has the promise of lowering variance even with long horizon rollouts. existing methods have required access to the behavior policy or have required data from the stationary distribution. the novel proposed approach instead uses an alternative method, based on the fixed point of the backward flow operator, to calculate the importance ratios required for policy evaluation in discrete and continuous environments. in the initial version of the submission, several concerns were expressed regarding both the quality of the paper and clarity. the authors have updated the paper to address these concerns to the satisfaction of the reviewers, who are now unanimously in favor of acceptance.", "accepted": null}
{"paper_id": "iclr_2020_Hygab1rKDS", "review_text": "four reviewers have assessed this paper and they have scored it as 6666 after rebuttal. nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the cameraready submission. especially, the authors should take care to make this paper accessible understandable to the ml community as iclr is a ml venue rather than quantum physics one. failure to do so will likely discourage the generosity of reviewers toward this type of submissions in the future.", "accepted": 1}
{"paper_id": "iclr_2020_rJgzzJHtDB", "review_text": "the authors develop a novel technique to train networks to be robust and accurate while still being efficient to train and evaluate. the authors propose robust dynamic inference networks that allows inputs to be adaptively routed to one of several output channels and thereby adjust the inference time used for any given input. they show the line of investigation initiated by authors is very interesting and should open up a new set of research questions in the adversarial training literature. the reviewers were in consensus on the quality of the paper and voted in favor of acceptance. one of the reviewers had concerns about the evaluation in the paper, in particular about whether carefully crafted attacks could break the networks studied by the authors. however, the authors performed additional experiments and revised the paper to address this concern to the satisfaction of the reviewer. overall, the paper contains interesting contributions and should be accepted.", "accepted": 1}
{"paper_id": "iclr_2020_HyxnMyBKwB", "review_text": "this paper studies the optimal value function for the gamblers problem, and presents some interesting characterizations thereof. the paper is well written and should be accepted.", "accepted": null}
{"paper_id": "iclr_2020_r1xCMyBtPS", "review_text": "this paper proposes a method to improve alignments of a multilingual contextual embedding model e.g., multilingual bert using parallel corpora as an anchor. the authors show the benefit of their approach in a zeroshot xnli experiment and present a word retrieval analysis to better understand multilingual bert. all reviewers agree that this is an interesting paper with valuable contributions. the authors and reviewers have been engaged in a thorough discussion during the rebuttal period and the revised paper has addressed most of the reviewers concerns. i think this paper would be a good addition to iclr so i recommend accepting this paper.", "accepted": null}
{"paper_id": "iclr_2020_B1gZV1HYvS", "review_text": "the paper proposes an extension to the popular generative adversarial imitation learning framework that considers multiagent settings with correlated policies, i.e., where agents actions influence each other. the proposed approach learns opponent models to consider possible opponent actions during learning. several questions were raised during the review phase, including clarifying questions about key components of the proposed approach and theoretical contributions, as well as concerns about related work. these were addressed by the authors and the reviewers are satisfied that the resulting paper provides a valuable contribution. i encourage the authors to continue to use the reviewers feedback to improve the clarity of their manuscript in time for the camera ready submission.", "accepted": 1}
{"paper_id": "iclr_2020_BJgWE1SFwS", "review_text": "this submission proposes to use neural networks in combination with pairwise choice markov chain models for choice modelling. the deep network is used to parametrize the pcmc and in so doing improve generalization and inference. strengths the formulation and theoretical justifications are convincing. the improvements are nontrivial and the approach is novel. weaknesses the text was not always easy to follow. the experimental validation is too limited initially. this was addressed during the discussion by adding an additional experiment. all reviewers recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_SJexHkSFPS", "review_text": "this paper studies the setting in reinforcement learning where the next action must be sampled while the current action is still executing. this refers to continuous time problems that are discretised to make them delayaware in terms of the time taken for action execution. the paper presents adaptions of the bellman operator and qlearning to deal with this scenario. this is a problem that is of theoretical interest and also has practical value in many realworld problems. the reviewers found both the problem setting and the proposed solution to be valuable, particularly after the greatly improved technical clarity in the rebuttals. as a result, this paper should be accepted.", "accepted": null}
{"paper_id": "iclr_2020_r1xMH1BtvB", "review_text": "this paper investigates the tasks used to pretrain language models. the paper proposes not using a generative tasks filling in masked tokens, but instead a discriminative tasked recognising corrupted tokens. the authors empirically show that the proposed method leads to improved performance, especially in the limited compute regime. initially, the reviewers had quite split opinions on the paper, but after the rebuttal and discussion phases all reviewers agreed on an accept recommendation. i am happy to agree with this recommendation based on the following observations  the authors provide strong empirical results including relevant ablations. reviews initially suggested a limitation to classification tasks and a lack of empirical analysis, but those issues have been addressed in the updated version.  the problem of pretraining language model is relevant for the ml and nlp communities, and it should be especially relevant for iclr. the resulting method significantly outperforms existing methods, especially in the low compute regime.  the idea is quite simple, but at the same time it seems to be a quite novel idea.", "accepted": 1}
{"paper_id": "iclr_2020_SklkDkSFPB", "review_text": "two reviewers recommend acceptance. one reviewer is negative, however, does not provide reasons for rejection. the ac read the paper and agrees with the positive reviewers. in that the paper provides value for the community on an important topic of network compression.", "accepted": 0}
{"paper_id": "iclr_2020_SJgIPJBFvH", "review_text": "this paper provides a valuable survey, summary, and empirical comparison of many generalization quantities from throughout the literature. it is comprehensive, thorough, and will be useful to a variety of researchers both theoretical and applied.", "accepted": 1}
{"paper_id": "iclr_2020_S1g8K1BFwS", "review_text": "the paper proposes a novel method to calibrate a knowledge graph embedding method when ground truth negatives are not available. essentially, the method relies on generating corrupted triples as negative examples to be used by known approaches platt scaling and isotonic regression. this is claimed as the first approach of probability calibration for knowledge graph embedding models, which is considered to be very relevant for practitioners working on knowledge graph embedding although this is a narrow audience. the paper does not propose a wholly novel method for probability calibration. instead, the value in experimental insights provided. some reviewers would have liked to see a more indepth analysis, but reviewers appreciated the thoroughness of the results in the clear articulation of the findings and the fact that multiple datasets and models are studied. there was an animated discussion about this paper, but the paper seems a useful contribution to the iclr community and i would like to recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_rkxNh1Stvr", "review_text": "this paper presents a method to model uncertainty in deep learning regressors by applying a posthoc procedure. specifically, the authors model the residuals of neural networks using gaussian processes, which provide a principled bayesian estimate of uncertainty. the reviewers were initially mixed and a fourth reviewer was brought in for an additional perspective. the reviewers found that the paper was well written, well motivated and found the methodology sensible and experiments compelling. anonreviewer4 raised issues with the theoretical exposition of the paper going so far as to suggest that moving the theory into the supplementary and using the reclaimed space for additional clarifications would make the paper stronger. the reviewers found the author response compelling and as a result the reviewers have come to a consensus to accept. thus the recommendation is to accept the paper. please do take the reviewer feedback into account in preparing the camera ready version. in particular, please do address the remaining concerns from anonreviewer4 regarding the theoretical portion of the paper. it seems that the methodological and empirical portions of the paper are strong enough to stand on their own and therefore the recommendation for an accept. adding theory just for the sake of having theory seems to detract from the message particularly if it is irrelevant or incorrect as initially pointed out by the reviewer.", "accepted": 1}
{"paper_id": "iclr_2020_Skx82ySYPH", "review_text": "this paper proposes a solid if somewhat incremental improvement on an interesting and wellstudied problem. i suggest accepting it.", "accepted": 1}
{"paper_id": "iclr_2020_SylO2yStDr", "review_text": "this paper presents layerdrop, which is a method for structured dropout which allows you to train one model, and then prune to a desired depth at test time. this is a simple method which is exciting because you can get a smaller, more efficient model at test time for free, as it does not need fine tuning. they show strong results on machine translation, language modelling and a couple of other nlp benchmarks. the reviews are consistently positive, with significant author and reviewer discussion. this is clearly an approach which merits attention, and should be included in iclr.", "accepted": 1}
{"paper_id": "iclr_2020_rkl03ySYDH", "review_text": "the paper makes a reasonable contribution to generative modeling for unsupervised scene decomposition. the revision and rebuttal addressed the primary criticisms concerning the qualitative comparison and clarity, which caused some of the reviewers to increase their rating. i think the authors have adequately addressed the reviewer concerns. the final version of the paper should still strive to improve clarity, and strengthen the evaluation and ablation studies.", "accepted": 0}
{"paper_id": "iclr_2020_Hyl7ygStwB", "review_text": "the authors propose a novel way of incorporating a large pretrained language model bert into neural machine translation using an extra attention model for both the nmt encoder and decoder. the paper presents thorough experimental design, with strong baselines and consistent positive results for supervised, semisupervised and unsupervised experiments. the reviewers all mentioned lack of clarity in the writing and there was significant discussion with the authors. after improvements and clarifications, all reviewers agree that this paper would make a good contribution to iclr and be of general use to the field.", "accepted": null}
{"paper_id": "iclr_2020_SygKyeHKDH", "review_text": "this paper tackles hardexploration rl problems using learning from demonstrations. the idea is to combine the existing r2d2 algorithms with imitation learning from human demonstrations. experiments are conducted on a new set of challenging tasks, highlighting limitations of strong current baseline while highlighting the strength of the proposed approach. the contribution is twofolds the proposed algorithm which clear outperforms previous sota agents and the set of benchmarks. all reviewers being positive about this paper, i therefore recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_ByglLlHFDS", "review_text": "the paper proposes a new algorithm called expected information maximization eim for learning latent variable models while computing the iprojection solely based on samples. the reviewers had several questions, which the authors sufficiently answered. the reviewers agree that the paper should be accepted. the authors should carefully read the reviewer questions and comments and use them to improve their final manuscript.", "accepted": null}
{"paper_id": "iclr_2020_Sye_OgHFwH", "review_text": "in this paper, the authors present adversarial attacks by semantic manipulations, i.e., manipulating specific detectors that result in imperceptible changes in the picture, such as changing texture and color, but without affecting their naturalness. moreover, these tasks are done on two large scale datasets imagenet and mscoco and two visual tasks classification and captioning. finally, they also test their adversarial examples against a couple of defense mechanisms and how their transferability. overall, all reviewers agreed this is an interesting work and well executed, complete with experiments and analyses. i agree with the reviewers in the assessment. i think this is an interesting study that moves us beyond restricted pixel perturbations and overall would be interesting to see what other detectors could be used to generate these type of semantic manipulations. i recommend acceptance of this paper.", "accepted": null}
{"paper_id": "iclr_2020_SyevYxHtDB", "review_text": "the paper proposed an optimizationbased defense against model stealing attacks. a criticism of the paper is that the method is computationally expensive, and was not demonstrated on more complex problems like imagenet. while this criticism is valid, other reviewers seem less concerned by this because the sota in this area is currently focused on smaller problems. after considering the rebuttal, there is enough reviewer support for this paper to be accepted.", "accepted": 1}
{"paper_id": "iclr_2020_rylmoxrFDH", "review_text": "the authors study neural networks with binary weights or activations, and the socalled differentiable surrogates used to train them. they present an analysis that unifies previously proposed surrogates and they study critical initialization of weights to facilitate trainability. the reviewers agree that the main topic of the paper is important in particular initialization heuristics of neural networks, however they found the presentation of the content lacking in clarity as well as in clearly emphasizing the main contributions. the authors imporved the readability of the manuscript in the rebuttal. this paper seems to be at acceptance threshold and 2 of 3 reviewers indicated low confidence. not being familiar with this line of work, i recommend acceptance following the average review score.", "accepted": 0}
{"paper_id": "iclr_2020_HyebplHYwB", "review_text": "this paper introduces a way to measure dataset similarities. reviewers all agree that this method is novel and interesting. a few questions initially raised by reviewers regarding models with and without likelihood, geometric exposition, and guarantees around gw, are promptly answered by authors, which raised the score to all weak accept.", "accepted": null}
{"paper_id": "iclr_2020_S1erpeBFPB", "review_text": "this paper proposes using flushreload to infer the deep network architecture of another program, when the two programs are running on the same machine as in cloud computing or similar. there is some disagreement about this paper; the approach is thoughtful and well executed, but one reviewer had concerns about its applicability and realism. upon reading the authors rebuttal i believe these to be largely addressed, or at least as realistically as one can in a single paper. therefore i recommend acceptance.", "accepted": 0}
{"paper_id": "iclr_2020_H1xscnEKDr", "review_text": "this paper studies the problem of defending deep neural network approaches for image classification from physically realizable attacks. it first demonstrates that adversarial training with pgd attacks and randomized smoothing exhibit limited effectiveness against three of the highest profile physical attacks. then, it proposes a new abstract adversarial model, where an adversary places a small adversarially crafted rectangle in an image, and develops two approaches for efficiently computing the resulting adversarial examples. empirical results show the effectiveness. overall, a good paper. the rebuttal is convincing.", "accepted": 1}
{"paper_id": "iclr_2020_BJlS634tPr", "review_text": "this paper proposes an improvement to the popular darts approach, speeding it up by performing the search in a subset of channels. the improvements are robust, and code is available for reproducibility. the rebuttal cleared up initial concerns, and after the private discussion among reviewers now all reviewers give accepting scores. because the improvements seem somewhat incremental and only applied to darts, r3 argued against an oral, and even the most positive reviewer agreed that a poster format would be best for presentation. i therefore strongly recommend recommendation, as a poster.", "accepted": 1}
{"paper_id": "iclr_2020_Skgvy64tvr", "review_text": "this paper presents new nonlinearity function which specially affects regions of the model which are densely valued. the nonlinearity is simple it retains only topk highest units from the input, while truncating the rest to zero. this also makes the models more robust to adversarial defense which depend on the gradients. the nonlinearity function is shown to have better adversarial robustness on cifar10 and svhn datasets. the paper also presents theoretical analysis for why the nonlinearity is a good function. the authors have already incorporated major suggestions by the reviewers and the paper can make significant impact on the community. thus, i recommend its acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_H1ldzA4tPr", "review_text": "this paper proposes using objectcentered graph neural network embeddings of a dynamical system as approximate koopman embeddings, and then learning the linear transition matrix to model the dynamics of the system according to the koopman operator theory. the authors propose adding an inductive bias a block diagonal structure of the transition matrix with shared components to limit the number of parameters necessary to learn, which improves the computational efficiency and generalisation of the proposed approach. the authors also propose adding an additional input component that allows for external control of the dynamics of the system. the reviewers initially had concerns about the experimental section, since the approach was only tested on toy domains. the reviewers also asked for more baselines. the authors were able to answer some of the questions raised during the discussion period, and by the end of it all reviewers agreed that this is a solid and novel piece of work that deserves to be accepted. for this reason i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_rJxbJeHFPS", "review_text": "this paper proposes a framework which qualifies how well given neural architectures can perform on reasoning tasks. from this, they show a number of interesting empirical results, including the ability of graph neural network architectures for learn dynamic programming. this substantial theoretical and empirical study impressed the reviewers, who strongly lean towards acceptance. my view is that this is exactly the sort of work we should be showcasing at the conference, both in terms of focus, and of quality. i am happy to recommend this for acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_Bkeb7lHtvH", "review_text": "the paper considers the problem of training neural networks asynchronously, and the gap in generalization due to different local minima being accessible with different delays. the authors derive a theoretical model for the delayed gradients, which provide prescriptions for setting the learning rate and momentum. all reviewers agreed that this a nice paper with valuable theoretical and empirical contributions.", "accepted": 1}
{"paper_id": "iclr_2020_SJxUjlBtwB", "review_text": "the paper introduces a generative approach to reconstruct 3d images for cryoelectron microscopy cryoem. all reviewers really liked the paper, appreciate the challenging problem tackled and the proposed solution. acceptance is therefore recommended.", "accepted": 1}
{"paper_id": "iclr_2020_SJe5P6EYvS", "review_text": "this paper presents a new twist on the typical lstm that applies several rounds of gating on the history and input, with the end result that the lstms transition function is effectively contextdependent. the performance of the model is illustrated on several datasets. in general, the reviews were positive, with one score being upgraded during the rebuttal period. one of the reviewers complained that the baselines were not adequate, but in the end conceded that the results were still worthy of publication. one reviewer argued very hard for the acceptance of this paper papers that are as clear and informative as this one are few and far between. ... as such, i vehemently argue in favor of this paper being accepted to iclr.", "accepted": 1}
{"paper_id": "iclr_2020_ryxdEkHtPS", "review_text": "the paper empirically studies the behaviour of deep policy gradient algorithms, and reveals several unexpected observations that are not explained by the current theory. all three reviewers are excited about this work and recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_rkeZIJBYvr", "review_text": "the reviewers generally agreed that the paper presents a compelling method that addresses an important problem. this paper should clearly be accepted, and i would suggest for it to be considered for an oral presentation. i would encourage the authors to take into account the reviewers suggestions many of which were already addressed in the rebuttal period and my own suggestion. the main suggestion i would have in regard to improving the paper is to position it a bit more carefully in regard to prior work on bayesian metalearning. this is an active research field, with quite a number of papers. there are two that are especially close to the vi method that the authors are proposing gordon et al. and finn et al. 2018. for example, the graphical model in figure 2 looks nearly identical to the ones presented in these two prior papers, as does the variational inference procedure. there is nothing wrong with that, but it would be appropriate for the authors to discuss this prior work a bit more diligently  currently the relationship to these prior works is not at all apparent from their discussion in the related work section. a more appropriate way to present this would be to begin section 3.2 by stating that this framework follows prior work  there is nothing wrong with building on prior work, and the significant and important contribution of this paper is no way diminished by being upfront about which parts are inspired by previous papers.", "accepted": 1}
{"paper_id": "iclr_2020_BkluqlSFDS", "review_text": "the authors presented a federate learning algorithm which constructs the global model layerwise by matching and averaging hidden representations. they empirically demonstrate their method outperforms existing federated learning algorithms this paper has received largely positive reviews. unfortunately one reviewer wrote a very short review but was generally appreciative of the work. fortunately, r1 wrote a detailed review with very specific questions and suggestions. the authors have addresses most of the concerns of the reviewers and i have no hesitation in recommending that this paper should be accepted. i request the authors to incorporate all suggestions made by the reviewers.", "accepted": 1}
{"paper_id": "iclr_2020_BJlLvnEtDB", "review_text": "this paper aims to analyze cnn representations in terms of how well they measure the perceptual severity of image distortions. in particularly, a sensitivity to changes in visual frequency and b orientation selectivity was used. although the reviewers agree that this paper presents some interesting initial findings with a promising direction, the majority of the reviewers three out of four find that the paper is incomplete, raising concerns in terms of experimental settings and results. multiple reviewers explicitly asked for additional experiments to confirm whether the presented empirical results can be used to improve results of an image generation. responding to the reviews, the authors added a superresolution experiment in the appendix, which the reviewers believe is the right direction but is still preliminary. overall, we believe the paper reports interesting findings but it will require a series of additional work to make it ready for the publication.", "accepted": 0}
{"paper_id": "iclr_2020_HyeuP2EtDB", "review_text": "the paper proposes an algorithm for zeroshot generalization in rl via learning a scoring a function from. the reviewers had mixed feelings, and many were not from the area. a shared theme was doubts about the significance of the experimental setting, and also the generality of the approach. as this is my field, i read the paper, and recommend rejection at this time. the proposed method is quite laborious and requires quite a bit of assumptions on the environments to work, as well as fine tuning parameters for each considered task number of regions, etc. i also agree that the evaluation is not convincing  stronger baselines need to be considered and the experiments to better address the zeroshot transfer aspect that the paper is motivated by. i encourage the authors to take the review feedback into account and submit a future version to another venue.", "accepted": 0}
{"paper_id": "iclr_2020_rylUOn4Yvr", "review_text": "the paper proposes a gradient rescaling method to make deep neural network training more robust to label noise. the intuition of focusing more on easier examples is not particularly new, but empirical results are promising. on the weak side, no theoretical justification is provided, and the method introduces extra hyperparameters that need to be tuned. finally, more discussions on recent sota methods e.g., lee et al. 2019 as well as further comprehensive evaluations on various cases, such as asymmetric label noise, semantic label noise, and openset label noise, would be needed to justify and demonstrate the effectiveness of the proposed method.", "accepted": 0}
{"paper_id": "iclr_2020_H1ekF2EYDH", "review_text": "this paper presents a largescale automatically extracted knowledge base in chinese which contains information about entities and their relations present in academic papers. the authors have collected several papers that come from around 38 different domains. as such this is a dataset creation paper where the authors have used existing methodologies to perform relation extraction in chinese. after having read the reviews and followup replies by authors, the main criticisms of the paper still hold. in addition to the lack of technical contribution, i feel that the writing of the paper can be improved a lot, for example, i would like to see a table with some example entities and relations extracted. that said, with further improvements this paper could potentially be a good contribution to lrec which is focused on dataset creation. in its current form, i recommend the paper to be rejected.", "accepted": 0}
{"paper_id": "iclr_2020_BJlgt2EYwr", "review_text": "this paper studies differentiable neural architecture search, focusing on a problem identified with the approximated gradient with respect to architectural parameters, and proposing an improved gradient estimation procedure. the authors claim that this alleviates the tendency of darts to collapse on degenerate architectures consisting of e.g. all skip connections, presently dealt with via early stopping. reviewers generally liked the theoretical contribution, but found the evidence insufficient to support the claims. requests for experiments by r1 with matched hyperparameters were granted and several reviewers felt this strengthened the submission, though relegated to an appendix, but after a lengthy discussion reviewers still felt the evidence was insufficient. r1 also contended that the authors were overly dogmatic regarding automl  that the early stopping heuristic was undesirable because of the additional human knowledge involved. i appreciate the sentiment but find this argument unconvincing  while it is true that a great deal of human knowledge is still necessary to make architecture search work, the aim is certainly to develop foolproof automatic methods. as reviewers were still unsatisfied with the empirical investigation after revisions and found that the weight of the contribution was insufficient for a 10 page paper, i recommend rejection at this time, while encouraging the authors to take seriously the reviewers requests for a systematic study of the source of the empirical gains in order to strengthen their paper for future submission.", "accepted": 0}
{"paper_id": "iclr_2020_BJepq2VtDB", "review_text": "the paper presented an adaptive stochastic gradient descent method with layerwise normalization and decoupled weight decay and justified it on a variety of tasks. the main concern for this paper is the novelty is not sufficient. the method is a combination of lars and adamw with slight modifications. although the paper has good empirically evaluations, theoretical convergence proof would make the paper more convincing.", "accepted": 0}
{"paper_id": "iclr_2020_SkxW23NtPH", "review_text": "this paper presents a new reinforcement learning based approach to device placement for operations in computational graphs and demonstrates improvements for large scale standard models. the paper is borderline with all reviewers appreciating the paper even the reviewer with the lowest score. the reviewer with the lowest score is basing the score on minor reservation regarding lack of detail in explaining the experiments. based upon the average score rejection is recommended. the reviewers comments can help improve the paper and it is definitely recommended to submit it to the next conference.", "accepted": 0}
{"paper_id": "iclr_2020_rJlUhhVYvS", "review_text": "thanks to reviewers and authors for an interesting discussion. it seems the central question is whether learning to identify correct bijections should be part of graph classification problems, or whether this leads to bias and overfitting. reviews are generally negative, putting this in the lower third of the submissions. the paper, however, inspired an interesting discussion, and i would encourage the authors to continue this line of work, addressing the question of bias and overfitting more directly, possibly going beyond dataset evaluation and, for example, thinking about how to evaluate whether training on nonisomorphic graphs leads to better offtraining set generalization.", "accepted": 0}
{"paper_id": "iclr_2020_rklFh34Kwr", "review_text": "this paper proposes a variant of hamiltonian monte carlo for bayesian inference in deep learning. although the reviewers acknowledge the ambition, scope and novelty of the paper they still have a number of reservations regarding experimental results and claims regarding need for hyperparameter tuning. the overall score consequently falls below acceptance. rejection is recommended. these reservations made by the referees should definitely be addressable before next conference deadline so looking forward to see the paper published asap.", "accepted": 0}
{"paper_id": "iclr_2020_H1gza2NtwH", "review_text": "the reviewers all appreciated the importance of the topic understanding the local geometry of loss surfaces of large models is viewed as critical to understand generalization and design better optimization methods. however, reviewers also pointed out the strength of the assumptions and the limitations of the empirical study. despite the claim that these assumptions are weaker than those made in prior work, this did not convince the reviewers that the conclusion could be applied to common loss landscapes. i encourage the authors to address the points made by the reviewers and submit an updated version to a later conference.", "accepted": 0}
{"paper_id": "iclr_2020_B1x8anVFPr", "review_text": "this paper investigates layer normalization and learning rate warmup in transformers, demonstrating that placing layer norm inside the residual connection preln leads to better behaved gradients than postln placement. doing so allows the learning rate warmup stage to be removed, leading to faster training. reviewers were mildly positive about the submission, commenting on the interesting insight provided about transformers, as well as the clear, focused motivation and contribution. however they also stated that it seem rather incremental of a contribution, as preln placement has been introduced before, and found it confusingly written at times. r2 clearly read it very closely, and had many detailed comments and discussions with authors and other reviewers. they had concerns about the relationship of this work with gradient clipping. the authors deserve credit for quickly investigating this in further experiments. interestingly, the found that even with gradient clipping, postln models still needed the learning rate warmup stage, although this issue went away with smaller clipped values or much lower learning rates. overall, r2 appears to find the papers motivation very compelling, but the insights incomplete and not fully satisfactory, while all reviewers find the novelty rather limited. i think a future submission that forges closer connections between the empirical findings and the theoretical interpretations would be of a great interest to the community, but in its current form is probably unsuitable for publication at iclr 2020.", "accepted": null}
{"paper_id": "iclr_2020_Bylp62EKDH", "review_text": "the authors propose a novel distance metric learning approach. reviews were mixed, and while the discussion was interesting to follow, some issues, including novelty, comparison with existing approaches, and impact, remain unresolved, and overall, the paper does not seem quite ready for publication.", "accepted": 1}
{"paper_id": "iclr_2020_rkl_Ch4YwS", "review_text": "one reviewer is positive, while the others recommend rejection. the authors did not submit a rebuttal, thus the reviewers kept their original assessment.", "accepted": 0}
{"paper_id": "iclr_2020_rklz16Vtvr", "review_text": "this paper proposes a method for finding neural architecture which, through the use of selective branching, can avoid processing portions of the network on a perdatapoint basis. while the reviewers felt that the idea proposed was technically interesting and wellpresented, they had substantial concerns about the evaluation that persisted postrebuttal, and lead to a consensus rejection recommendation.", "accepted": 0}
{"paper_id": "iclr_2020_BJlkgaNKvr", "review_text": "the paper investigates why adversarial training can sometimes degrade model performance on clean input examples. the reviewers agreed that the paper provides valuable insights into how adversarial training affects the distribution of activations. on the other hand, the reviewers raised concerns about the experimental setup as well as the clarity of the writing and felt that the presentation could be improved. overall, i think this paper explores a very interesting direction and such papers are valuable to the community. its a borderline paper currently but i think it could turn into a great paper with another round of revision. i encourage the authors to revise the draft and resubmit to a different venue.", "accepted": 0}
{"paper_id": "iclr_2020_rygHe64FDS", "review_text": "main content blind review 2 summarizes it well this paper investigates the security of distributed asynchronous sgd. authors propose zeno, workerserver asynchronous implementation of sgd which is robust to byzantine failures. to ensure that the gradients sent by the workers are correct, zeno server scores each worker gradients using a reference gradient computed on a secret validation set. if the score is under a given threshold, then the worker gradient is discarded. authors provide convergence guarantee for the zeno optimizer for nonconvex function. in addition, they provide an empirical evaluation of zeno on the cifar10 datasets and compare with various baselines.  discussion reviews are generally weak on the limited novelty of the approach compared with zeno, but the rebuttal of the authors on nov 15 is fair too long to summarize here.  recommendation and justification i do not feel strongly enough to override the weak reviews but if there is room in the program i would support a weak accept.", "accepted": 0}
{"paper_id": "iclr_2020_ryxMW6EtPB", "review_text": "this paper proposes looking at the duality gap to measure performance. however, the metric is just an upperbound on the true metric of interest, and therefore its value can be ambiguous. the reviewers found the paper to be in an unacceptable form and was clearly hastily prepared. they were also skeptical about the novelty of the result as well as the comprehensiveness of the experiments. this paper would require extensive revisions before any potential acceptance. reject", "accepted": null}
{"paper_id": "iclr_2020_B1ldb6NKDr", "review_text": "this paper presents an approach combining multiagent with hierarchical rl in a custommade simulated humanoid robotics setting. although it is an interesting premise and has a compelling motivation multiagent, realworld interaction, humanoid robotics, the reviewers had some trouble pinpointing what the significant contributions are. partly this is due to lack of clarity in the presentation, such as with overlong sections eg 5.2, unclear descriptions, mistakes in the text, etc. reviewers also remarked that this paper might be trying to do too much, without performing the necessary experimentscomparisons and analyses needed to interpret the contributions of each component. this work is definitely promising and has the potential to make a nice contribution, given some additional care experiments, analyses and rewritingpolishing. as it is, its probably a bit premature for publication at iclr.", "accepted": 0}
{"paper_id": "iclr_2020_Skerzp4KPS", "review_text": "this paper proposes a data augmentation method based on generative adversarial networks by training several gans on subsets of the data which are then used to synthesise new training examples in proportion to their estimated quality as measured by the inception score. the reviewers have raised several critical issues with the work, including motivation it can be harder to train a generative model than a discriminative one, novelty, complexity of the proposed method, and lack of comparison to existing methods. perhaps the most important one is the inadequate empirical evaluation. the authors didnt address any of the raised concerns in the rebuttal. i will hence recommend the rejection of this paper.", "accepted": null}
{"paper_id": "iclr_2020_SylUzpNFDS", "review_text": "main content blind review 3 summarizes it well this paper proposes a new loss for training models that predict where events occur in a sequence when the training sequence has noisy labels. the central idea is to smooth the label sequence and prediction sequence and compare these rather than to force the model to treat all errors as equally serious. the proposed problem seems sensible, and the method is a reasonable approach. the evaluations are carried out on a variety of different tasks piano onset detection, drum detection, smoking detection, video action segmentation.  discussion the reviewers were concerned about the relatively low level of novelty, simplicity of the proposed approach which the authors argue could be seen as a feature rather than a flaw, given its good performance, and inadequate motivation.  recommendation and justification after the authors revision in response to the reviews, this paper could be a weak accept if not for the large number of stronger submissions.", "accepted": 0}
{"paper_id": "iclr_2020_SkexNpNFwS", "review_text": "this paper proposes applying potential flow generators in conjunction with l2 optimal transport regularity to favor solutions that move input points as little as possible to output points drawn from the target distribution. the resulting pipeline can be effective in dealing with, among other things, imagetoimage translation tasks with unpaired data. overall, one of the appeals of this methodology is that it can be integrated within a number of existing generative modeling paradigms e.g., gans, etc.. after the rebuttal and discussion period, two reviewers maintained weak reject scores while one favored strong acceptance. with these borderlinemixed scores, this paper was discussed at the metareview level and the final decision was to side with the majority, noting that a revision which fully addresses reviewer comments could likely be successful at a future venue. as one important lingering issue, r1 pointed out that the optimality conditions of the proposed approach are only enforced on sampled trajectories, not actually on the entire space. the rebuttal concedes this point, but suggests that the method still seems to work. but as an improvement, the suggestion is made that randomly perturbed trajectories could help to mitigate this issue. however, no experiments were conducted using this modification, which could be helpful in building confidence in the reliability of the overall methodology. additionally, from my perspective the empirical validation could also be improved to help solidify the contribution in a revision. for example, the imagetoimage translation experiments with celeba were based on a linear pca embedding and feedforward networks. it would have been nice to have seen a more sophisticated setup for this purpose as discussed in section 5, especially for a nontheoretical paper with an ostensibly practicallyrelevant algorithmic proposal. and consistent with reviewer comments, the paper definitely needs another pass to clean up a number of small grammatical mistakes.", "accepted": 1}
{"paper_id": "iclr_2020_Syg7VaNYPB", "review_text": "the authors propose generative latent flow which uses autoencoder to learn latent representations and normalizing flows to map that distribution. the reviewers feel that there is limited novelty since it is a straightforward combination of existing ideas.", "accepted": 0}
{"paper_id": "iclr_2020_BkxtNaEYDr", "review_text": "the paper puts forward a theoretical investigation of the learnability of treestructured boolean circuits with neural networks. the authors identify local correlations, ie correlation of each internal target circuit gate with the target output, as critical property for characterizing learnability by layerwise training. the reviewers agree that the paper is well written and content to be correct to the best of their knowledge. however, they have reservations about the strength of the assumptions about the target functions as well as the layerwise training procedure. i think this paper is slightly below acceptance threshold for iclr, which is a quite applied conference. the assumptions are quite strong, ie local correlations and the topology of the circuit to be known as well as layerwise training, and possibly too far removed from current deep learning practice.", "accepted": 0}
{"paper_id": "iclr_2020_H1gpET4YDB", "review_text": "this paper proposes blockwise masked attention mechanisms to sparsify transformer architectures, the main motivation being reducing the memory usage with long sequence inputs. the resulting model is called blockbert. the paper falls in a trend of recent papers compressingsparsifyingdistilling transformer architectures, a very relevant area of research given the daunting resources needed to train these models. while the proposed contribution is very simple and interesting, it also looks a rather small increment over prior work, namely sparse transformer and adaptive span transformer, among others. experiments are rather limited and the memorytime reduction is not overwhelming 18.736.1 less memory, 12.025.1 less time, while final accuracy is sometimes sacrificed by a few points. no comparison to other adaptively sparse attention transformer architectures correia et al. emnlp 19 or sukhbaatar et al. acl 19 which should as well provide memory reductions due to the sparsity of the gradients, which require less activations to be cached. i suggest addressing this concerns in an eventual resubmission of the paper.", "accepted": 0}
{"paper_id": "iclr_2020_HyezBa4tPB", "review_text": "this paper proposes adding a dirichlet distribution as a wrapper on top of a black box classifier in order to better capture uncertainty in the predictions. this paper received four reviews in total with scores 1,1,1,6. the reviewer who gave the weak accept found the paper well written, easy to follow and intuitive. the other reviewers, however, were primarily concerned about the empirical evaluation of the method. they found the baselines too weak and werent convinced that the method would work well in practice. the reviewers also cited a lack of comparison to existing literature for their scores. one reviewer noted that while the method addresses aleatoric uncertainty, it doesnt provide any mechanism for epistemic uncertainty, which would be necessary for the applications motivating the work. the authors did not provide a response and thus there was no discussion.", "accepted": null}
{"paper_id": "iclr_2020_HkeeITEYDr", "review_text": "this paper studies the robust reinforcement learning problem in which the constraint on model uncertainty is captured by the wasserstein distance. the reviewers expressed concerns regarding novelty with respect to prior work, the presentation or the results, and unconvincing experiments. in its current form the paper is not ready for acceptance to iclr2020.", "accepted": 0}
{"paper_id": "iclr_2020_B1x9ITVYDr", "review_text": "after reading the authors response, all the reviwers still think that this paper is a simple extension of gradient masking, and can not provide the robustness in neural networks.", "accepted": 0}
{"paper_id": "iclr_2020_rJe1DTNYPH", "review_text": "all reviewers suggest rejection. beyond that, the more knowledgable two have consistent questions about the motivation for using the cckl objective. as such, the exposition of this paper, and justification of the work could use improvement, so that experienced reviewers understand the contributions of the paper.", "accepted": 0}
{"paper_id": "iclr_2020_SJlbvp4YvS", "review_text": "the authors propose to extend modelbasedmodelfree hybrid methods e.g., mve, steve to stochastic environments. they use an ensemble of probabilistic models to model the environment and use a lower confidence bound of the estimate to avoid risk. they found that their proposed method yields stateoftheart performance over previous methods. the valid concerns by reviewers 1  4 were not addressed by the authors and although the authors responded to reviewer 3, they did not revise the paper to address their concerns. the ideas and results in this paper are interesting, but without addressing the valid concerns raised by reviewers, i cannot recommend acceptance.", "accepted": 0}
{"paper_id": "iclr_2020_H1xSOTVtvH", "review_text": "the paper presents a technique for learning rl agents to generalize well to unseen environments. all reviewers and ac think that the paper has some potential but is a bit below the bar to be accepted due to the following facts a limited experiments, i.e., consider more appealing baselinesscenarios and provide more experimental details. b the proposed methodidea is simplereasonable, but not super novel, i.e., not enough considering the iclr high standard potentially enough for a workshop paper. hence, i think this is a borderline paper toward rejection.", "accepted": 0}
{"paper_id": "iclr_2020_H1lKd6NYPS", "review_text": "there was extension discussion of the paper between the reviewers. its clear that the reviewers appreciated the main idea in the paper, and the notion of an online metacritic that accelerates the rl process is definitely very appealing. however, there were unanswered questions about what the method is actually doing that make me reticent to recommend acceptance at this point. i would refer the authors to r3 and r1 for an indepth discussion of the issues, but the short summary is that its not clear whether, if, and how the metaloss in this case actually converges, and what the metacritic is actually doing. in the absence of a theoretical understanding for what the modification does to accelerate rl, we are left with the empirical experiments, and there it is necessary to consider alternative hypotheses and perform detailed ablation analyses to understand that the method really works for the reasons stated by the authors and not some of the alternative explanations, see e.g. r3. while there is nothing wrong with a result that is primarily empirical, it is important to analyze that the empirical gains really are happening for the reasons claimed, and to carefully study convergence and asymptotic properties of the algorithm. the comparatively diminished gains with the stronger algorithms td3 and especially sac make me more skeptical. therefore, i would recommend that the paper not be accepted at this time, though i encourage the authors to resubmit with a more indepth experimental evaluation.", "accepted": 0}
{"paper_id": "iclr_2020_HylhuTEtwr", "review_text": "the consensus among all reviewers was to reject this paper, and the authors did not provide a rebuttal.", "accepted": null}
{"paper_id": "iclr_2020_ryx6daEtwr", "review_text": "this paper aims to estimate the 3d location and orientation of vehicle from a 2d image. instead of using a cnnbased 3d detection pipeline, the authors propose to detect the vehicles wheel grounding points and then using the ground plane constraint for the estimation. all three reviewers provided unanimous rating of rejection. many concerns are raised by the reviewers, including poor generalization to new situations, small improvement over prior work, low presentation quality, the lack of detailed description of the experiments, etc. the authors did not respond to the reviewers comments. the ac agrees with the reviewers comments, and recommend rejection.", "accepted": null}
{"paper_id": "iclr_2020_SJxRKT4Fwr", "review_text": "the paper proposes a solution based on selfattention rnn to addressing the missing value in spatiotemporal data. i myself read through the paper, followed by a discussion with the reviewers. we agree that the model is reasonable, and the results are promising. however, there is still some room for improvement 1. the selfattention mechanism is not new. the specific way proposed in the paper is an interesting tweak of existing models, but not brand new per se. most importantly, it is unclear if the proposed way is the optimal one and where the performance improvement comes from. as the reviewer suggested, more thorough empirical analysis should be performed for deeper insights of the model. 2. the datasets were adopted from existing work, but most of them do not have such complex models as the one proposed in the paper. therefore, the suggestion for bigger datasets is valid. given the considerations above, we agree that while the paper has a lot of good materials, the current version is not ready yet. addressing the issues above could lead to a good publication in the future.", "accepted": 0}
{"paper_id": "iclr_2020_HklRKpEKDr", "review_text": "this work extends previous work castellini et al with parameter sharing and lowrank approximations, for pairwise communication between agents. however the work as presented here is still considered too incremental, in particular when compared to castellini et al. the advances such as parameter sharing and lowrank approximation are good but not enough of a contribution. authors efforts to address this concern did not change reviewers judgment. therefore, we recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_Bkx29TVFPr", "review_text": "the paper proposes an implicit function approach to learning the modes of multimodal regression. the basic idea is interesting, and is clearly related to density estimation, which the paper does not discuss. based on the reviews and the fact that the authors did not submit a helpful rebuttal, i recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_BJlyi64FvB", "review_text": "this paper investigated the effect of network width on learned features using activation atlases. from the current view of deep learning, the novelty of the paper is limited. as all reviews rejected the paper and the authors gave up rebuttal, i choose to reject the paper.", "accepted": 0}
{"paper_id": "iclr_2020_rJlqoTEtDB", "review_text": "after reading the authors rebuttal, the reviewers still think that this is an incremental work, and the theory and experiments .are inconsistent. the authors are encouraged to consider the the reivewers comments to improve the paper.", "accepted": 1}
{"paper_id": "iclr_2020_HJghoa4YDB", "review_text": "this paper provides convergence results for nonlinear td under lazy training. this paper tackles the important and challenging task of improving our theoretical understanding of deep rl. we have lots of empirical evidence qlearning and td can work with nns, and even empirical work that attempts to characterize when we should expect it to fail. such empirical work is always limited and we need theory to supplement our empirical knowledge. this paper attempts to extend recent theoretical work on the convergence of supervised training of nn to the policy evaluation setting with td. the main issue revolves around the presentation of the work. the reviewers found the paper difficult to read ok for theory work. but, the paper did not clearly discuss and characterize the significance of the work how limited is the lazy training regime, when would it be useful? now that we have this result, do we have any more insights for algorithm design improving nonlinear td, or comments about when we expect nn policy evaluation to work? this all reads like the paper needs a better intro and discussion of the implications and limitations of the results, and indeed this is what the reviewers were looking for. unfortunately the author response and paper submitted were lacking in this respect. even the strongest advocates of the work found it severely lacking explanation and discussion. they felt that the paper could be accepted, but only after extensive revision. the direction of the work is important. the work is novel, and not a small undertaking. however, to be published the authors should spend more time explaining the framework, the results, and the limitations to the reader.", "accepted": 0}
{"paper_id": "iclr_2020_rJxG3pVKPB", "review_text": "the paper considers the task of sequence to sequence modelling with multivariate, realvalued time series. the authors propose an encoderdecoder based architecture that operates on fixed windows of the original signals. the reviewers unanimously criticise the lack of novelty in this paper and the lack of comparison to existing baselines. while rev 1 positively highlights human evaluation contained in the experiments, they nevertheless do not think this paper is good enough for publication as is. the authors did not submit a rebuttal. i therefore recommend to reject the paper.", "accepted": 0}
{"paper_id": "iclr_2020_H1gS364FwS", "review_text": "this paper performs event extraction from amharic texts. to this end, authors prepared a novel amharic corpus and used a hybrid system of rulebased and learningbased systems. overall, while all reviewers admit the importance of addressing lowresource language and the value of the novel amharic corpus, they are not satisfied with the quality of the current paper as a scientific work. most importantly, although the attempt of even extraction might be new on amharic, there have been many works on other languages. it should be clearly presented what are the nontrivial languagespecific challenges on amharic and how they are solved, otherwise it seems just an engineering of existing techniques on a new dataset. also, all reviewers are fairly concerned about the presentation and clarity of the paper. unfortunately, no revised paper is uploaded and we cannot confirm how authors response is reflected. for those reasons, i would like to recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_BJg9hTNKPH", "review_text": "this paper is an empirical studies of methods to stabilize offline ie, batch rl methods where the dataset is available up front and not collected during learning. this can be an important setting in e.g. safety critical or production systems, where learned policies should not be applied on the real system until their performance and safety is verified. since policies leave the area where training data is present, in such settings poor performance or divergence might result, unless divergence from the reference policy is regularized. this paper studies various methods to perform such regularization. the reviewers are all very happy about the thoroughness of the empirical work. the work only studies existing methods and combination thereof, so the novelty is limited by design. the paper was also considered well written and easy to follow. the results were very similar between the considered regularizers, which somehow limits the usefulness of the paper as practical guideline although at least now we know that perhaps we do not need to spend a lot of time choosing the best between these. bigger differences were observed between value penalties versus policy regularization. this seems to correspond to theoretical observations by neu et al httpsarxiv.orgabs1705.07798, 2017, which is not cited in the manuscript. although unpublished, i think that work is highly relevant for the current manuscript, and id strongly recommend the authors to consider its content. some minor comments about the paper are given below. on the balance, the strong point of the paper is the empirical thoroughness and clarity, whereas novelty, significance, and theoretical analysis are weaker points. due to the high selectivity of iclr, i unfortunately have to recommend rejection for this manuscript. i have some minor comments about the contents of the paper  the manuscript contains the line under this definition, such a behavior policy \u03c0b is always welldefined even if the dataset was collected by multiple, distinct behavior policies. wouldnt simply defining the behavior as a mixture of the underlying behavior policies when known work equally well?  the paper mentions several earlier works that regularize policies update using the kl from a reference policy or to a reference policy. the paper of peters is cited in this context, although there the constraint is actually on the kl divergence between stateaction distributions, resulting in a different type of regularization.", "accepted": null}
{"paper_id": "iclr_2020_B1lCn64tvS", "review_text": "sat is npcomplete karp, 1972 due its intractable exhaustive search. as such, heuristics are commonly used to reduce the search space. while usually these heuristics rely on some indomain expert knowledge, the authors propose a generic method that uses rl to learn a branching heuristic. the policy is parametrized by a gnn, and at each step selects a variable to expand and the process repeats until either a satisfying assignment has been found or the problem has been proved unsatisfiable. the main result of this is that the proposed heuristic results in fewer steps than vsids, a commonly used heuristic. all reviewers agreed that this is an interesting and wellpresented submission. however, both r1 and r2 rightly according to my judgment point that at the moment the paper seems to be conducting an evaluation that is not entirely fair. specifically, vsids has been implemented within a framework optimized for running time rather than number of iterations, whereas the proposed heuristic is doing the opposite. moreover, the proposed heuristic is not stressedtest against larger datasets. so, the authors take a heuristicframework that has been optimized to operate specifically well on large datasets where running time is what ultimately makes the difference scale it down to a smaller dataset and evaluate it on a metric that the proposed algorithm is optimized for. at the same time, they do not consider evaluation in larger datasets and defer all concerns about scalability to the one of industrial use vs answering ml questions related to whether or not it is possible to stretch existing rl techniques to learn a branching heuristic. this is a valid point and not all techniques need to be super scalable from iteration day 0, but this being ml, we need to make sure that our evaluation criteria are fair and that we are comparing apples to apples in testing hypotheses. as such, i do not feel comfortable suggesting acceptance of this submission, but i do sincerely hope the authors will take the reviewers feedback and improve the evaluation protocols of their manuscript, resulting in a stronger future submission.", "accepted": 1}
{"paper_id": "iclr_2020_rJxFpp4Fvr", "review_text": "the authors propose a notion of feature robustness, provide a straightforward decomposition of risk in terms of this robustness measure, and then provide some empirical evidence for their perspective. across the board, the reviewers raised issues with missing related work, which the authors then addressed. i will point out that some things the authors say about pacbayes are false. e.g., in the rebuttal the authors say that pacbayes is limited to 01 error. it is generally trivial to obtain bounds for bounded loss. for unbounded loss functions, there are bounds based on, e.g., sub gaussian assumptions. despite improvements in connections with related work, reviewers continued to find the theoretical contributions to be marginal. even the empirical contributions were found to be marginal.", "accepted": 0}
{"paper_id": "iclr_2020_rJlwAa4YwS", "review_text": "this paper presents a new view of latent variable learning as learning lattice representations. overall, the reviewers thought the underlying ideas were interesting, but both the description and the experimentation in the paper were not quite sufficient at this time. id encourage the authors to continue on this path and take into account the extensive review feedback in improving the paper!", "accepted": 0}
{"paper_id": "iclr_2020_S1xqRTNtDr", "review_text": "this paper proposes a way to lean contextdependent policies from demonstrations, where the context represents behavior labels obtained by annotating demonstrations with differences in behavior across dimensions and the reduced in 2 dimensions. results are conducted in the domain of starcraft. the main concerns from the reviewers related to the papers novelty as pointed by r2 and experiments particularly the lack of comparison with other methods and the evaluation of only 4 out of the 62 behaviour clusters, as pointed by r3. as such, i cannot recommend acceptance, as current results do not provide strong empirical evidence about the superiority of the method against other alternatives.", "accepted": 0}
{"paper_id": "iclr_2020_B1gi0TEFDB", "review_text": "this paper investigates gradient sparsification using topk for distributed training. starting with empirical studies, the authors propose a distribution for the gradient values, which is used to derive bounds on the topk sparsification. the topk approach is further improved using a procedure that is easier to parallelize. the reviewers and ac agree that the problem studied is timely and interesting. however, this manuscript also received quite divergent reviews, resulting from differences in opinion about the rigor and novelty of the results, and perhaps issues with unstated assumptions. in reviews and discussion, the reviewers also noted issues with clarity of the presentation, some of which were corrected after rebuttal. in the opinion of the ac, the manuscript is not appropriate for publication in its current state.", "accepted": 0}
{"paper_id": "iclr_2020_SyglyANFDr", "review_text": "this paper proposes a modification of sgd to do distributionallyrobust optimization of deep networks. the main idea is sensible enough, however, the inadequate handling of baselines and relatively toy nature of the experiments means that this paper needs more work to be accepted.", "accepted": 0}
{"paper_id": "iclr_2020_Byx91R4twB", "review_text": "this paper addresses the tasks of video generation and prediction and shows impressive results on the datasets such as kinetics600. there is a reviewer disagreement on this paper. ac can confirm that all three reviewers have read the rebuttal and have contributed to a long discussion. the reviewers have raised the following concerns that were viewed as critical issues when making the final decision r1 and r3 expressed the concerns regarding limited technical novelty of the proposed approach in light of the prior works, e.g. mocogan and tganv2. r3 suggests, that the proposed method shows advantage that might be due to the large computational resources available to train the model. providing a comparison of the proposed model and the relevant baselines on the kinetics dataset is desirable to access the benefits of the proposed approach r1. ac also agrees with the r2 about the potential impact this work could have in the community. however, given that the reviewers have raised important concerns and have given suggestions, the paper needs too many revisions for acceptance at this time. we hope the reviews are useful for improving and revising the paper.", "accepted": 0}
{"paper_id": "iclr_2020_B1x2eCNFvH", "review_text": "the paper introduces an approach for semisupervised learning based on local label propagation. while reviewers appreciate learning a consistent embedding space for prediction and label propagation, a few pointed out that this paper does not make it clear how different it is from preview work wu et al, iscen et al., zhuang et al., in addition to complexity calculation, or pseudolabel accuracy. these are important points that werent included to the degree that reviewersreaders can understand, and reviewers seem to not change their minds after authors wrote back. this suggests the paper can use additional cycles of polishingediting to make these points clear. we highly recommend authors to carefully reflect on reviewers both pros and cons of the paper to improve the paper for your future submission.", "accepted": 0}
{"paper_id": "iclr_2020_BylalAEtvB", "review_text": "while there was some support for this paper, there was not enough support to accept it for publication at iclr. the following concern is characteristic of the concerns raised by the reviewers the main contribution of this paper is hard to discern, but the ideas presented are interesting. other reviewers said it was hard to read and not ready for publication.", "accepted": 0}
{"paper_id": "iclr_2020_ryxIZR4tvS", "review_text": "the paper proposes two methods for link prediction in knowledge hypergraphs. the first method concatenates the embedding of all entities and relations in a hyperedge. the second method combines an entity embedding, a relation embedding, and a weighted convolution of positions. the authors demonstrate on two datasets derived by the authors from freebase, that the proposed methods work well compared to baselines. the paper proposes direct generalizations of knowledge graph approaches, and unfortunately does not yet provide a comprehensive coverage of the possible design space of the two proposed extensions. the authors should be commended for providing the source code for reproducibility. one of the reviewers who was unfortunately also the most negative, was time pressed. unfortunately, the discussion period was not used by the reviewers to respond to the authors rebuttal of their concerns. even discounting the most negative review, this paper is on the borderline, and given the large number of submissions to iclr, it unfortunately falls below the acceptance threshold in its current form.", "accepted": 0}
{"paper_id": "iclr_2020_HyxwZRNtDr", "review_text": "while the reviewers agreed that the problem of learning robust policies is an important one, there were a number of major concerns raised about the paper, and as a result i would recommend that the paper not be accepted at this time. the important points are 1 limited novelty in light of prior work in this area see r2 and r3; 2 a number of missing comparisons see r2. there is also a bit of confusion in the reviews, which i think stems from a somewhat unclear statement in the paper of the problem formulation. while there is nothing wrong with assuming access to a parameterized simulator and studying robustness under parametric variation, this is of course a much stronger assumption than some prior work on robust reinforcement learning. clarity on this point is crucial, and there are a large number of prior methods that can likely do well in this setting e.g., based on system id, etc..", "accepted": 0}
{"paper_id": "iclr_2020_SkxcZCNKDS", "review_text": "this paper studies maximum entropy reinforcement learning in more detail. maximum entropy is a popular strategy in modern rl methods and also seems used in human and animal decision making. however, it does not lead to optimize expected utility. the authors propose a setting in which maximum entropy rl is an optimal solution. the authors were quite split on the paper, and there has been an animated discussion between the reviewers among each other and with the authors. the technical quality is good, although one reviewer commented on the restricted setting of the experiments bandit problems. the authors have addressed this by adding an additional experiment. futhermore, two reviewers commented that the clarity of the paper could be improved. a larger part of the discussion also the private discussion revolved around relevance and significance, especially of the metapomdp setting that takes up a large part of the manuscript.  a reviewer mentioned that after reading the paper, it does not become more clear why maximum entropy rl works well in practice. the discussion even turned to why maxentropyrl might be unreasonable from the point of view of needing a metapomdp with markov assumptions, which doesnt help shed light on its empirical success. the metapomdp setting does not seem to reflect the use cases where maximum entropy rl has done well in emperical studies.  another reviewer mentioned that earlier papers have investigated maximum entropy rl, and that the paper tries to offer a new perspective with the metapomdp setting. the discussion of this discussion was not deemed complete in current state and needs more attention splitting the paper into two along these lines is a possibility mooted by two of the reviewers. a particular example was the doctorpatient example, where in the metapomdp setting the doctor would repeatedly attempt to cure a fixed sampled illness, rather than e.g. solving for a new illness each time. based on the discussion, i would conclude that the topic broached by the paper is very relevant and timely, however, that the paper would benefit from a round of major revision and resubmission rather than being accepted to iclr in current form.", "accepted": 1}
{"paper_id": "iclr_2020_S1xsG0VYvB", "review_text": "this paper explores the role of excitatory and inhibitory neurons, and how their properties might differ based on simulations. a few issues were raised during the review period, and i commend the authors for stepping up to address these comments and run additional experiments. it seems, though, that the reviewers worries were born out in the results of the additional experiments 1. the object classification task is not really relevant to elicit the observed behavior and 2. inhibitory neurons are not essential at least when training with batch norm. i hope the authors can make improvements in light of these observations, and discuss their implications in a future version of this paper.", "accepted": 1}
{"paper_id": "iclr_2020_ryeQmCVYPS", "review_text": "the reviewers wondered about the practical application of this method, given that the performance was lower. the reviewers were also surprised by some of your claims and wanted you to explore them more deeply. on the positive side, the reviewers found your experiments to be very thorough. you also performed additional experiments during the rebuttal period. we hope that those experiments will help you to build a better paper as you work towards publishing this work.", "accepted": 0}
{"paper_id": "iclr_2020_BJeFQ0NtPS", "review_text": "the paper proposed a nonautoregressive attention based encoderdecoder model for texttosepectrogram using attention distillation. it is shown to bring good speedup to conventional autoregressive ones. the paper further adopted vae for the vocoder training which trains from scratch although performs worse than existing method e.g. clarinet. the main concerns for this paper come from the unclear presentation  as the reviewer pointed out, therere some misleading claims that the speedup gains was obtained without the consideration of the full context i.e. not including the whole inference time.  the paper failed to clear present the architectures developedused in the paper and the differences from those used in the literature. the reviewers suggested the use of diagram to aid the presentation.  the two contributions are unbalanced presented. due to the complexities involved, its better to explain things in more details. the authors acknowledged the reviewers comments during rebuttal, but did not make any changes to the paper.", "accepted": 0}
{"paper_id": "iclr_2020_r1xfECEKvr", "review_text": "thank you for an interesting read. this paper is an experimental paper which argues the importance of epistemicmodel uncertainty in applications for electronic health records ehr. the main arguments are the following 1. current metrics on dataset level cannot reveal uncertainty in prediction on personal level; 2. when evaluated on personal level, deterministic nns with different random initialisations can produce very different predictions thus require consideration of model uncertainty i am not exactly sure if iclr is the best venue for this submission, as there is quite little innovation in modelling methodology, and the empirical analysis is domain specific. i feel this paper is more suitable to e.g. mlhc or miccai which focus on data analysismachine learning methods applied to healthcare science. still i think the set of experiments in the paper is overall supportive to the main argument that the authors is trying to make. possible improvements 1. the histograms in figure 3  4 clearly show that, deterministic nns trained with different initialisations produces diverse predictions on individual patients. i commend the authors for presenting these visualisations, and i think it would be more useful to quantify this phenomenon on dataset level, e.g. compute the mean and variance of this variation of individual predictions. 2. i would expect to see an improvement of ece for the bayesiandeep ensemble models. the table a.3 so marginal improvements, and i wonder how would this result support the authors claim? also how do the eceace metrics look like when computed on subgroups? apart from section 3.3, in general i think the paper writing is clear to me. the loss sensitive optimal decision method is interesting, but a lot of details are missing 1. the presentation in section 3.3 is unclear, e.g. minimising eq. 4 w.r.t. what? whats the definition of decision region? also what exactly is the mathematical form of the associated cost used in the experiments? 2. if i understand it correctly, in experiments the optimal thresholding method has only been applied to individual networks in the ensemble. if so what is the intention of discussing eq. 5 in the first place? also it is unclear to me how this method performs in the deep ensemblebayesian rnn case. see e.g. httpsarxiv.orgpdf1805.03901.pdf for a relevant approach. i am not an expert in the field of model uncertainty summary  contributions this paper discusses the important problem of model uncertainty in the output of ml models developed for clinical applications. the authors illustrate the underlying concepts using rnns which are popular in the medical ml literature by applying these to two datasets. they argue that bayesian rnns with bayesian embeddings should be the models of choice in such settings as they explicitly allow the expression of uncertainty whereby obtaining confidence intervals etc is easy. the other advantage is the fewer number of parameters that need to be stored to get such statistics. novelty  some of the ideas presented are standard or wellknown properties to most ml practitioners. for instance, the relationship between mean and variance in fig 2 or the uncertainty in predictions  optimal decisions in fig 3. is the point of the paper to make it more obvious?  it is certainly the case that medicine practioners are not as aware of these issues, but to reach that audience this paper would do better in a venue that caters to that community. however, the paper needs to address the concerns first so as to not confuse that community  the bayesian rnn models being discussed are not novel either and their properties have been discussed in the corresponding papers probably not in such detail and with examples.  what is the value of the bernoulli distribution? isnt the single output from a well calibrated model is enough to give the same information. writing the paper is very well written and has good figures and examples to explain the ideas. the one area that can be improved is the contributions section. results  the authors do not discuss the related issue of model calibration in much detail. it is unclear what additional information we are gaining from the authors perspective of model uncertainty. a well calibrated model as well as other ways of obtaining confidence intervals via hypothesis tests would serve just as well.  are the conclusions derived on the specific datasets general?  the results showing grouplevel biases are not very helpful and come across as anecdotal. these can be derived from most other models too. one major issue with this paper is that the proposed metric, model uncertainty, is identical to data uncertainty in terms of measuring the uncertainty for an individual patient. the fundamental problem is that a mixture of bernoulli distributions, which the authors define as model uncertainty, contains equivalent information to a single bernoulli distribution, which the authors define as data uncertainty. the authors dont seem to acknowledge that when a neural network outputs a single bernoulli parameter, that parameter is already able to capture all necessary uncertainty information. now, whether those output bernoulli parameters are correctly calibrated and methods for improving calibration is a completely different topic not explored in this paper. this is a fundamental problem for the paper because the paper is structured with the objective of creating good estimates of those mixtures. due to the equivalency mentioned above, knowing good estimates of these mixtures is not useful for understanding the uncertainty for an individual patient because the information contained in that mixture about a particular patient is identical to the information contained in a single bernoulli centered at the mean. do note that knowing the mixture does provide some information about the model class, but thats a completely separate discussion and irrelevant when we are primarily concerned with knowing the uncertainty for a single prediction. the authors need to better justify why we should care about model uncertainty when data uncertainty is already able to capture all of the uncertainty in the problem. one experiment i think would be interesting would be a thorough comparison in auroc and calibration between the mean bernoulli from the rnn ensembles or bayesian rnn compared to the single bernoulli from a single rnn. its well known that ensembling is often helpful for improving calibration and accuracy, but it is often underexplored with neural networks due to limited compute resources. it would be interesting to see both the improvement on the single model due to ensembling and whether the bayesian rnn enables you to achieve similar improvements with less compute. note that the distribution of bernoullis is still irrelevant for this experiment, as we can simplify that distribution into a single mean bernoulli parameter that captures the same information and do our analysis on that parameter. the paper considers an important problem in medical applications of deep learning, such as variabilitystability of models predictions in face of various perturbations in the model e.g., random seed, and evaluates different approaches to capturing model uncertainty. however, it appears to be little innovation in terms of machinelearning methodology, so iclr might not be the best venue for this work, while perhaps other venues focused more on medical applications might be a better fit.", "accepted": 0}
{"paper_id": "iclr_2020_SJeoE0VKDS", "review_text": "the two most experienced reviewers recommended the paper be rejected. the submission lacks technical depth, which calls the significance of the contribution into question. this work would be greatly strengthened by a theoretical justification of the proposed approach. the reviewers also criticized the quality of the exposition, noting that key parts of the presentation was unclear. the experimental evaluation was not considered to be sufficiently convincing. the review comments should be able to help the authors strengthen this work.", "accepted": 0}
{"paper_id": "iclr_2020_HJg6VREFDH", "review_text": "this paper proposes a new way to stabilise gan training. the reviews were very mixed but taken together below acceptance threshold. rejection is recommended with strong motivation to work on the paper for next conference. this is potentially an important contribution.", "accepted": 1}
{"paper_id": "iclr_2020_BygfrANKvB", "review_text": "the authors present a new approach to improve performance for retrosynthesis using a seq2seq model, achieving significant improvement over the baseline. there are a number of lingering questions regarding the significance and impact of this work. hence, my recommendation is to reject.", "accepted": null}
{"paper_id": "iclr_2020_ryex8CEKPr", "review_text": "this manuscript proposes feature selection inspired by knockoffs, where the generative models are implemented using modern deep generative techniques. the resulting procedure is evaluated in a variety of empirical settings and shown to improve performance. the reviewers and ac agree that the problem studied is timely and interesting, as knockoffs combined with generative models have recently shown promise for inferential problems. however, the reviewers were unconvinced about the motivation of the work, and the strength of the empirical evaluation results. in the option of the ac, this work might be improved by focusing both conceptually and empirically on applications where inferential variable selection is most relevant e.g. causal settings, healthcare applications, and so on.", "accepted": 0}
{"paper_id": "iclr_2020_SJeeL04KvH", "review_text": "this manuscript proposes strategies to improve both the robustness and accuracy of federated learning. two proposals are online reinforcement learning for adaptive hyperparameter search, and local distribution matching to synchronize the learning trajectories of different local models. the reviewers and ac agree that the problem studied is timely and interesting, as it addresses known issues with federated learning. however, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and clarity of the conceptual and empirical results. taken together, the acs opinion is that the paper may not be ready for publication.", "accepted": 0}
{"paper_id": "iclr_2020_r1gNLAEFPS", "review_text": "this paper addresses the classic medial image segmentation by combining neural ordinary differential equations nodes and the level set method. the proposed method is evaluated on kidney segmentation and salient object detection problems. reviewer 1 provided a brief review concerning iclr is not the appropriate venue for this work. reviewer 2 praises the underlying concept being interesting, while pointing out that the presentation and experiments of this work is not ready for publication yet. reviewer 3 raises concerns on whether the methods are presented properly. the authors did not provide responses to any concerns. given these concerns and overall negative rating two weak reject and one reject, the ac recommends reject.", "accepted": 0}
{"paper_id": "iclr_2020_Syeu8CNYvS", "review_text": "this work applies deep kernel learning to the problem of few shot regression for modeling biological assays. to deal with sparse data on new tasks, the authors propose to adapt the learned kernel to each task. reviews were mixed about the method and experiments, some reviewers were satisfied with the author rebuttal while others did not support acceptance during the discussion period. some reviewers ultimately felt that the experimental results were too weak to warrant publication. on the binding task the method is comparable with simpler baselines, and some felt that the gains on antibacterial were unconvincing. other reviewers felt that there remained simpler baselines to compare with, for example ablating the affects of learning the kernel with simple hand picking one. while authors commented they tried this, there were no details given on the results or what exactly they tried. based on the reviewer discussion, the work feels too preliminary in its current form to warrant publication in iclr. however, given that there are clearly some interesting ideas proposed in this work, i recommend resubmitting with stronger experimental evidence that the method helps over baselines.", "accepted": 1}
{"paper_id": "iclr_2020_B1xewR4KvH", "review_text": "this work explores how to leverage structure of this input in decision trees, the way this is done for example in convolutional networks. all reviewers agree that the experimental validation of the method as presented is extremely weak. authors have not provided a response to answer the many concerns raised by reviewers. therefore, we recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_HJx4PAEYDH", "review_text": "the submission proposes a variant of a transformer architecture that does not use positional embeddings to model local structural patterns but instead adds a recurrent layer before each attention layer to maintain local context. the approach is empirically verified on a number of domains. the reviewers had concerns with the paper, most notably that the architectural modification is not sufficiently novel or significant to warrant publication, that appropriate ablations and baselines were not done to convincingly show the benefit of the approach, that the speed tradeoff was not adequately discussed, and that the results were not compared to actual sota results. for these reasons, the recommendation is to reject the paper.", "accepted": 0}
{"paper_id": "iclr_2020_SkgvvCVtDS", "review_text": "this paper present a learning method for speeding up of lp, and apply it to the tsp problem. reviewers and ac agree that the idea is quite interesting and promising. however, i think the paper is far from being ready to publish in various aspects a much more editorial efforts are necessary b the tps application of small scale is not super appealing hence, i recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_BJlxdCVKDB", "review_text": "this paper aims at making a deep rl policy interpretable and verifiable by distilling the policy represented by a deep neural network into an ensemble of decision trees. this should be done without hurting the performance of the policy. the authors achieve this by extending the existing viper algorithm. the resulting approach can imitate the deep policy better compared with viper while preserving verifiability. experiments show that the proposed method improves in terms of cumulative reward and error rate over viper in four benchmark tasks. the amount of improvement over the original viper is not convincing given the presented results. moreover, reviewers uniformly agree that the contribution of this work is incremental. i therefore recommend to reject this paper.", "accepted": 0}
{"paper_id": "iclr_2020_SygLu0VtPH", "review_text": "this paper is a very borderline case. mixed reviews. r2 score originally 4, moved to 5 rounded up to wa 6, but still borderline. r1 was 6 wa and r3 was 3 wr. r2 expert on this topic, r1 and r3 less so. ac has carefully read the reviewsrebuttalcomments and looked closely at the paper. ac feels that r2s review is spot on and that the contribution does not quite reach iclr acceptance level, despite it being interesting work. so the ac feels the paper cannot be accepted at this time. but the work is definitely interesting  the authors should improve their paper using r2s comments and resubmit.", "accepted": 0}
{"paper_id": "iclr_2020_S1e__ANKvB", "review_text": "several approaches can be used to feed structured data to a neural network, such as convolutions or recurrent network. this paper proposes to combine both roads, by presenting molecular structures to the network using both their graph structured and a serialized representation smiles, that are processed by a framework combining the strenth of graph neural network and the sequential transformer architecture. the technical quality of the paper seems good, with r1 commenting on the performance relative to sota seq2seq based methods and r3 commenting on the benefits of using more plausible constraints. the problem of using data with complex structure is highly relevant for iclr. however, the novelty was deemed on the low side. as a very competitive conference, this is one of the key aspects necessary for successful iclr papers. all reviewers agree that the novelty is too low for the current high bar of iclr.", "accepted": 0}
{"paper_id": "iclr_2020_rJg9OANFwS", "review_text": "the paper proposes two approaches to topic modeling supervised by survival analysis. the reviewers find some problems in novelty, algorithm and experiments, which is not ready for publish.", "accepted": 0}
{"paper_id": "iclr_2020_S1e5YC4KPS", "review_text": "this paper provides an approach to improve the differentially private sgd method by leveraging a differentially private version of the lottery mechanism, which reduces the number of parameters in the gradient update and the dimension of the noise vectors. while this combination appears to be interesting, there is a nontrivial technical issue raised by reviewer 3 on the sensitivity analysis in the paper. r3 brought up this issue even after the rebuttal. this issue needs to be resolved or clarified for the paper to be published.", "accepted": 0}
{"paper_id": "iclr_2020_r1xpF0VYDS", "review_text": "there was some support for the ideas presented, but this paper was on the borderline, and ultimately not able to be accepted for publication at iclr. concerns raised included level of novelty, and clarity of the exposition to an ml audience.", "accepted": 0}
{"paper_id": "iclr_2020_ryl0cAVtPH", "review_text": "the paper addresses the question of why warm starting could result in worse generalization ability than training from scratch. the reviewers agree that increasing the circumstances in which warm starting could be applied is of interest, in particular to reduce training time and computational resources. however, the reviewers were unanimous in their opinion that the paper is not suitable for publication at iclr in its current form. concerns included that the analysis was not sufficiently focused and the experiments too small scale. as the analysis component of the paper was considered to be limited, the experimental results were insufficient on the balance to push the paper to an acceptable state.", "accepted": 0}
{"paper_id": "iclr_2020_HylloR4YDr", "review_text": "solid, but not novel enough to merit publication. the reviewers agree on rejection, and despite authors adaptation, the paper requires more work and broader experimentation for publication.", "accepted": 0}
{"paper_id": "iclr_2020_S1xXiREKDB", "review_text": "this paper proposes to use the gan i.e., minimax framework for adversarial training, where another neural network was introduced to generate the most effective adversarial perturbation by finding the weakness of the classifier. the rebuttal was not fully convincing on why the proposed method should be superior to existing attacks.", "accepted": 0}
{"paper_id": "iclr_2020_Hkl_sAVtwr", "review_text": "this paper proposes a compressed sensing cs method which employs deep image prior dip algorithm to recovering signals for images from noisy measurements using untrained deep generative models. a novel learned regularization technique is also introduced. experimental results show that the proposed methods outperformed the existing work. the theoretical analysis of early stopping is also given. all reviewers agree that it is novel to combine the deep learning method with compressed sensing. the paper is well written and overall good. however the reviewers also proposed many concerns about method and the experiments, but the authors gave no rebuttal almost no revisions were made on the paper. i would suggest the author to consider the reviewers concern seriously and resubmit the paper to another conference or journal.", "accepted": 0}
{"paper_id": "iclr_2020_ryga2CNKDH", "review_text": "the paper proposed a method to evaluate latent variable based generative models by estimating the compression in the latents rate and the distortion in the resulting reconstructions. while reviewers have clearly appreciated the theoretical novelty in using ais to get an upper bound on the rate, there are concerns on missing empirical comparison with other related metrics precisionrecall and limited practical applicability of the method due to large computational cost. authors should consider comparing with pr metric and discuss some directions that can make the method practically as relevant as other related metrics.", "accepted": 1}
{"paper_id": "iclr_2020_Hye-p0VFPB", "review_text": "this paper presents an energyefficient architecture for quantized deep neural networks based on decomposable multiplication using macs. although the proposed approach is shown to be somehow effective, two reviewers pointed out that the very similar idea was already proposed in the previous work, bitblade 1. as the authors did not submit a rebuttal to defend this critical point, id like to recommend rejection. i recommend authors to discuss and clarify the difference from 1 in the future version of the paper. 1 sungju ryu, hyungjun kim, wooseok yi, jaejoon kim. bitblade area and energyefficient precisionscalable neural network accelerator with bitwise summation. dac2019", "accepted": 0}
{"paper_id": "iclr_2020_r1g1CAEKDH", "review_text": "this paper adds a new model to the literature on representation learning from correlated variables with some common and some private dimensions, and takes a variational approach based on wyners common information. the literature in this area includes models where both of the correlated variables are assumed to be available as input at all times, as well as models where only one of the two may be available; the proposed approach falls into the first category. pros the reviewers generally agree, as do i, that the motivation is very interesting and the resulting model is reasonable and produces solid results. cons the model is somewhat complex and the paper is lacking a careful ablation study on the components. in addition, the results are not a clear win for the proposed model. the authors have started to do an ablation study, and i think eventually an interesting story is likely to come out of that. but at the moment the paper feels a bit too preliminaryinconclusive for publication.", "accepted": 0}
{"paper_id": "iclr_2020_S1xGCAVKvr", "review_text": "this paper proposes an improved over andrychowicz et al metaoptimizer that tries to to learn better strategies for training deep machine learning models. the paper was reviewed by three experts, two of whom recommend weak reject and one who recommends reject. the reviewers identify a number of significant concerns, including degree of novelty and contribution, connections to previous work, completeness of experiments, and comparisons to baselines. in light of these reviews and since the authors have unfortunately not provided a response to them, we cannot recommend accepting the paper.", "accepted": 0}
{"paper_id": "iclr_2020_HJxN0CNFPB", "review_text": "this paper proposes a new type of polynomial nn called ladder polynomial nn lpnn which is easy to train with general optimization algorithms and can be combined with techniques like batch normalization and dropout. experiments show it works better than fms with simple classification and regression tasks, but no experiments are done in more complex tasks. all reviewers agree the paper addresses an interesting question and makes some progress but the contribution is limited and there are still many ways to improve.", "accepted": 0}
{"paper_id": "iclr_2020_r1lHAAVtwr", "review_text": "the paper proposes a hierarchical diversity promoting regularizer for neural networks. experiments are shown with this regularizer applied to the last fullyconnected layer of the network, in addition to l2 and energy regularizers on other layers. reviewers found the paper wellmotivated but had concerns on writingreadability of the paper and that it provides only marginal improvements over existing simple regularizers such as l2. i would encourage the authors to look for scenarios where the proposed regularizer can show clear improvements and resubmit to a future venue.", "accepted": 0}
{"paper_id": "iclr_2020_H1eY00VFDB", "review_text": "the paper proposes a new loss function which adds to the training objective another term that pulls the current parameters of a neural network further away from the parameters at a previous time step. intuitively, this aims to push the current parameters further to the local optimum. on a variety of benchmarks, optimizing the proposed loss function achieves better results than just optimizing the training loss. the paper is well written and easy to follow. however, i am not entirely convinced about the intuition of the proposed method and i think further investigation are necessary. while the method is simple and general, it also seems to be rather heuristic and requires carefully chosen hyperparameters. having said that, the empirical evidence shows that the proposed loss function consistently improves performance. the following details should be addressed further  i am a bit confused by the definition of the loss function. in equation 1 it seems that the term on the left represents the training objective. if that is correct than equation 2 second case contains the training objective twice?  f in section 3 after equation 2 is not properly defined  could it happen that the proposed loss function leads to divergence, for example if the parameter from a previous time step thetatp is close to the optimum theta_star?  what is the motivation to use the l1 norm? how does this choice affect convergence compared to lets l2 norm?  section 4.1 typo in first paragraph k instead of kappa  section 4.1 the results would be more convincing if all networks were trained multiple times with a different random initialization and table 1 would include the mean and std.  why is no warmup period used for the gan experiments?  section 4.3 why is kappa increase by 1 for the speech recognition experiments where as by 2 for all other experiments?  i suggest to increase the line width of all figures since they are somewhat hard to identify on a print version.  why is the momentum set to 0.5 for sgd in the ablation study? most frameworks use a default value of 0.9.  i would like to see the affect of the warmup period to the performance in the ablation study.  how does the choice of learning rate schedule, such as for example cosine annealing, affect the loss function? post rebuttal  i thank the authors for clarifying my questions and providing additional experiments. i think that especially the additional ablation studies and reporting the mean and std of multiple trials make the contribution of the paper more convincing. hence, i increased my score. this paper presents the retrospective loss to optimize neural network training. the idea behind the retrospective loss is to add a penalization term between the current model to the model from a few iterations before. extensive experimental results on a wide range of datasets are provided to show the effectiveness of the retrospective loss. the retrospective loss is additionally controlled by two hyperparameters, the strength parameter k and the update frequency t_p. this loss, measured in l1 norm, is added to the training objective. the geometric intuition of the added loss term is that this pushes the model away from the model at iteration t_p. the paper argues that this shrinks the parameter space of the loss function. one of the concern regards the writing of the paper.  algorithm 1 and figure 6 look very blurry, which i think are both below the publication standard.  the introduction could be written to be more helpful, such as providing more context on why the obtained experimental results are important e.g. getting stateoftheart results on the datasets studied in the experiments  the related work contrasts with previous work which is not clear because the precise contribution has not been stated at the point. more detailed questions  what are the standard deviations for the experimental results as you reported in table 4 but not in other experiments?  im curious whether the use of l1 norm is critical or not in the retrospective loss. this paper introduces a further regularizer, retrospection loss, for training neural networks, which leverages past parameter states. the authors added several ablation studies and extra experiments during the rebuttal, which are helpful to show that their method is useful. however, this is still one of those papers that essentially proposes an additional heuristic to train deep news, which is helpful but not clearly motivated from a theoretical point of view despite the intuitions. yes, it provides improvements across tasks but these are all relatively small, and the method is more involved. therefore, i am recommending rejection.", "accepted": 1}
{"paper_id": "iclr_2020_Byg9AR4YDB", "review_text": "this paper proposes a dedicated deep models for analysis of multiplexed ion beam imaging by timeofflight mibitof. the reviewers appreciated the contributions of the paper but not quite enough to make the cut. rejection is recommended.", "accepted": 0}
{"paper_id": "iclr_2020_BJeTCAEtDB", "review_text": "the paper proposed the use of a lossy transform coding approach to to reduce the memory bandwidth brought by the storage of intermediate activations. it has shown the proposed method can bring good memory usage while maintaining the the accuracy. the main concern on this paper is the limited novelty. the lossy transform coding is borrowed from other domains and only the use of it on cnn intermediate activation is new, which seems insufficient.", "accepted": 1}
{"paper_id": "iclr_2020_H1gyy1BtDS", "review_text": "the authors study generalization in distributed representation learning by describing limits in accuracy and complexity which stem from information theory. the paper has been controversial, but ultimately the reviewers who provided higher scores presented weaker and fewer arguments. by recruiting an additional reviewer it became clearer that, overall the paper needs a little more work to reach iclr standards. the main suggestions for improvements have to do with improving clarity in a way that makes the motivation convincing and the practicality more obvious. boosting the experimental results is a complemental way of increasing convincingness, as argued by reviewers.", "accepted": 1}
{"paper_id": "iclr_2020_HygXkJHtvB", "review_text": "there has been significant discussion in the literature on the effect of the properties of the curvature of minima on generalization in deep learning. this paper aims to shed some light on that discussion through the lens of theoretical analysis and the use of a bayesian jeffreys prior. it seems clear that the reviewers appreciated the work and found the analysis insightful. however, a major issue cited by the reviewers is a lack of compelling empirical evidence that the claims of the paper are true. the authors run experiments on very small networks and reviewers felt that the results of these experiments were unlikely to extrapolate to large scale modern models and problems. one reviewer was concerned about the quality of the exposition in terms of the writing and language and care in terminology. unfortunately, this paper falls below the bar for acceptance, but it seems likely that stronger empirical results and a careful treatment of the writing would make this a much stronger paper for future submission.", "accepted": null}
{"paper_id": "iclr_2020_HklCk1BtwS", "review_text": "the paper studies word embeddings using the matrix factorization framework introduced by levy et al 2015. the authors provide a theoretical explanation for how the hyperparameter alpha controls the distance between words in the embedding and a method to estimate the optimal alpha. the authors also provide experiments showing the alpha found using their method is close to the alpha that gives the highest performance on the wordsimilarity task on several datasets. the paper received 2 weak rejects and 1 weak accept. the reviews were unchanged after the rebuttal, with even the review for weak accept r2 indicating that they felt the submission to be of low quality. initially, reviewers commented that while the work seemed solid and provided insights into the problem of learning word embeddings, the paper needed to improve their positioning with respect to prior work on word embeddings and add missing citations. in the revision, the authors improved the related work, but removed the conclusion. the current version of the paper is still low quality and has the following issues 1. the paper exposition still needs improvement and it would benefit from another review pass following r3s suggestions, the authors have made various improvements to the paper, including modifying the terminology and contextualizing the work. however, as r3 suggests, the paper still needs more rewriting to clearly articulate the contribution and how it relates to prior work throughout the paper. in addition, the conclusion was removed and the paper still needs an editing pass as there are still many languagegrammar issues. page 5 inherites  inherits page 5 top knn  top k 2. more experimental evaluation is needed. for instance, r1 suggested that the authors perform additional experiments on other tasks e.g. ner, pos tagging. the authors indicated that this was not a focus of their work as other works have already looked at the impact of alpha on other task. while prior works has looked at the correlation of alpha vs performance on the task, they have not looked at whether alpha estimated the method proposed by the author will give good performance on these tasks as well. including such analysis will make this a stronger paper. overall, there are some promising elements in the paper but the quality of the paper needs to be improved. the authors are encouraged to improve the paper by adding more experimental evaluation on other tasks, improving the writing, as well as incorporating other reviewer comments and resubmit to an appropriate venue.", "accepted": 0}
{"paper_id": "iclr_2020_BJg7x1HFvB", "review_text": "though the reviewers thought the ideas in this paper were interesting, they questioned the importance and magnitude of the contribution. though it is important to share empirical results, the reviewers were not sure that there was enough for this paper to be accepted.", "accepted": 0}
{"paper_id": "iclr_2020_BJgkbyHKDS", "review_text": "this paper studies the empirical performance of invertible generative models for compressive sensing, denoising and in painting. one issue in using generative models in this area has been that they hit an error floor in reconstruction due to model collapse etc i.e. one can not achieve zero error in reconstruction. the reviewers raised some concerns about novelty of the approach and thoroughness of the empirical studies. the authors response suggests that they are not claiming novelty w.r.t. to the approach but rather their use in compressive techniques. my own understanding is that this error floor is a major problem and removing its effect is a good contribution even without any novelty in the techniques. however, i do agree that a more thorough empirical study would be more convincing. while i can not recommend acceptance given the scores i do think this paper has potential and recommend the authors to resubmit to a future venue after a through revision.", "accepted": 0}
{"paper_id": "iclr_2020_HylNWkHtvB", "review_text": "this paper proposes an adaptive gradient method for optimization in deep learning called avagrad. the authors argue that avagrad greatly simplifies hyperparameter search over e.g. adam and demonstrate competitive performance on benchmark image and text problems. in thorough reviews, thorough author response and discussion by the reviewers which are are all appreciated a few concerns about the work came to light and were debated. one reviewer was compelled by the author response to raise their recommendation to weak accept. however, none of the reviewers felt strongly enough to champion the paper for acceptance and even the reviewer assigning the highest score had reservations. a major issue of debate was the treatment of hyperparameters, i.e. that the authors tuned hyperparameters on a smaller problem and then assumed these would extrapolate to larger problems. in a largely empirical paper this does seem to be a significant concern. the space of adaptive optimizers for deep learning is a crowded one and thus the empirical or theoretical burden of proof of superiority is high. the authors state regarding a concurrent submission when hyperparameters are properly tuned, echoing our results on this matter, however, it seems that the reviewers disagree that the hyperparameters are indeed properly tuned in this paper. its due to these remaining reservations that the recommendation is to reject.", "accepted": 0}
{"paper_id": "iclr_2020_rkgAb1Btvr", "review_text": "this paper presents a new method for detecting outofdistribution ood samples. a reviewer pointed out that the paper discovers an interesting finding and the addressed problem is important. on the other hand, other reviewers pointed out theoreticalempirical justifications are limited. in particular, i think that experimental supports why the proposed method is superior beyond the existing ones are limited. i encourages the authors to consider more scenarios of ood detection e.g., datasets and architectures and more baselines as the problem of measuring the confidence of neural networks or detecting outliers have rich literature. this would guide more comprehensive understandings on the proposed method. hence, i recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_rJgffkSFPS", "review_text": "this paper proposes to use graph convolutional networks gcns in bayesian optimization for neural architecture search. while the paper title includes multiobjective, this component appears to only be a posthoc evaluation of the pareto front of networks evaluated using a singleobjective search  this could be performed for any method that evaluates more than one network. performance on nasbench101 appears to be very good. in the private discussion of reviewers and ac, several issues were raised, including whether the approach is compared fairly to lanas and whether the gcn will predict well for large search spaces. also, unfortunately, no code is provided, making it unclear whether the work is reproducible. the reviewers unanimously agreed on a weak rejection score. i concur with this assessment and therefore recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_r1e4MkSFDr", "review_text": "this paper presents a continuous cnn model that can handle nonuniform time series data. it learns the interpolation kernel and convolutional architectures in an endtoend manner, which is shown to achieve higher performance compared to na\u00efve baselines. all reviewers scored weak reject and there was no strong opinion to support the paper during discussion. although i felt some of the reviewers comments are missing the points, i generally agree that the novelty of the method is rather straightforward and incremental, and that the experimental evaluation is not convincing enough. particularly, comparison with more recent stateoftheart point process methods should be included. for example, 13 claim better performance than rmtpp. considering that the contribution of the paper is more on empirical side and ccnn is not only the solution for handing nonuniform time series data, i think this point should be properly addressed and discussed. based on these reasons, id like to recommend rejection. 1 xiao et al., modeling the intensity function of point process via recurrent neural networkss, aaai 2017. 2 li et al., learning temporal point processes via reinforcement learning, nips 2018. 3 turkmen et al, fastpoint scalable deep point processes, ecmlpkdd 2019.", "accepted": 0}
{"paper_id": "iclr_2020_rkxVz1HKwB", "review_text": "this paper discusses new methods to perform adversarial attacks on salience maps. in its current form, this paper in its current form has unfortunately has not convinced several of the reviewerscommenters of the motivation behind proposing such a method. i tend to share the same opinion. i would encourage the authors to rethink the motivation of the work, and if there are indeed solid use cases to express them explicitly in the next version of the paper.", "accepted": 0}
{"paper_id": "iclr_2020_SkxV7kHKvr", "review_text": "all three reviewers are consistently negative on this paper. thus a reject is recommended.", "accepted": 0}
{"paper_id": "iclr_2020_SJeUm1HtDH", "review_text": "this paper investigates using sound to improve classification, motion prediction, and representation learning all from data generated by a real robot. all the reviewers were intrigued by the work. the paper provides experiments on real robots never a small task, and a dataset for the community, and a sequence of illustrative experiments. because the paper combines existing techniques, its main contribution is the empirical demonstrations of the utility of using sound. overall, it was not quite enough for the reviewers. the main issues were 1 motion prediction is perhaps expected given the physical setup, 2 lack of comparison with other approaches, 3 lack of diversity in the demonstrations 10 objects, one domain. the authors added two new experiments with a different setup, further demonstrating their claims. in addition the authors highlighted that the novelty of this task means there are no clear baselines to which r3 agreed. the new experiments are briefly described in the response and visuals on a website, but the authors did not update the paper. the new experiments could potentially significantly strength the paper. however, the terse description in the response and the supplied visuals made it difficult for the reviewers to judge their contribution. overall, this is certainly a very interesting direction. the results on real world data demonstrate promise, even if they are not the benchmarking style the community is used too.", "accepted": 0}
{"paper_id": "iclr_2020_r1la7krKPS", "review_text": "the authors propose two measures of calibration that dont simply rely on the top prediction. the reviewers gave a lot of useful feedback. unfortunately, the authors didnt respond.", "accepted": 0}
{"paper_id": "iclr_2020_S1xJ4JHFvS", "review_text": "the paper addresses an important problem of finding a good tradeoff between generalization and convergence speed of stochastic gradient methods for training deep nets. however, there is a consensus among the reviewers, even after rebuttals provided by the authors, that the contribution is somewhat limited and the paper may require additional work before it is ready to be published.", "accepted": 0}
{"paper_id": "iclr_2020_BJe4V1HFPr", "review_text": "this paper proposes a twostage adversarial training approach for learning a disentangled representation of style and content of anime images. unlike the previous style transfer work, here style is defined as the identity of a particular anime artist, rather than a set of uninterpretable style features. this allows the trained network to generate new anime images which have a particular content and are drawn in the style of a particular artist. while the approach works well, the reviewers voiced concerns about the method overly complicated and somewhat incremental and the quality of the experimental section lack of good baselines and quantitative comparisons at least in terms of the disentanglement quality. it was also mentioned that releasing the code and the dataset would strengthen the appeal of the paper. while the authors have addressed some of the reviewers concerns, unfortunately it was not enough to persuade the reviewers to change their marks. hence, i have to recommend a rejection.", "accepted": 0}
{"paper_id": "iclr_2020_BylB4kBtwB", "review_text": "the paper discusses audio source separation with complex nns. the approach is good and may increase an area of research. but the experimental section is very weak and needs to be improved to merit publication.", "accepted": 0}
{"paper_id": "iclr_2020_HJl8SkBYPr", "review_text": "the authors leverage advances in semisupervised learning and data augmentation to propose a method for active learning. the al method is based on the principle that a model should consistently label across perturbationaugmentations of examples, and thus propose to choose samples for active learning based on how much the estimated label distribution changes based on different perturbations of a given example. the method is intuitive and the experiments provide some evidence of efficacy. however, during discussion there was a lingering question of novelty that eventually swayed the group to reject this paper.", "accepted": null}
{"paper_id": "iclr_2020_BylPSkHKvB", "review_text": "the paper proposed a new seq2seq method to implement natural language to formal language translation. fixed length tensor product representations are used as the intermediate representation between encoder and decoder. experiments are conducted on mathqa and algolist datasets and show the effectiveness of the methods. intensive discussions happened between the authors and reviewers. despite of the various concerns raised by the reviewers, a main problem pointed by both reviewer3 and reviewer4 is that there is a gap between the theory and the implementation in this paper. the other reviewer 2 likes the paper but is less confident and tend to agree with the other two reviewers.", "accepted": 1}
{"paper_id": "iclr_2020_Skg5r1BFvB", "review_text": "this work considers the popular lqr objective but with a,b unknown and dynamically changing. at each time a context c,d is observed and it is assumed there exist a linear map theta from c,d to a,b. the particular problem statement is novel, but is heavily influenced by other mdp settings and the also follows very closely to previous works. the algorithm seems computationally intractable a problem shared by previous work this work builds on and so in experiments a gross approximation is used. reviewers found the work very stylized and did not adequately review related work. for example, little attention is paid to switching linear systems and the recent lqr advances are relegated to a list of references with no discussion. the reviewers also questioned how the theory relates to the traditional setting of lqr regret, say, if c,d were identity at all times so that theta  a,b. this paper received 3 reviews a third was added late to the process and my own opinion influenced the decision. while the problem statement is interesting, the work fails to put the paper in context with the existing work, and there are some questions of algorithm methods.", "accepted": 0}
{"paper_id": "iclr_2020_rJecSyHtDS", "review_text": "the paper proposes a new problem setting of predicate zeroshot learning for visual relation recognition for the setting when some of the predicates are missing, and a model that is able to address it. all reviewers agreed that the problem setting is interesting and important, but had reservations about the proposed model. in particular, the reviewers were concerned that it is too simple of a step from existing methods. one reviewer also pointed towards potential comparisons with other zeroshot methods. following that discussion, i recommend rejection at this time but highly encourage the authors to take the feedback into account and resubmit to another venue.", "accepted": 0}
{"paper_id": "iclr_2020_HyxPIyrFvH", "review_text": "the authors show that models trained to satisfy adversarial robustness properties do not possess robustness to naturally occuring distribution shifts. the majority of the reviewers agree that this is not a surprising result especially for the choice of natural distribution shifts chosen by the authors for instance it would be better if the authors compare to natural distribution shifts that look similar to the adversarial corruptions. moreover, this is a survey study and no novel algorithms are presented, so the paper cannot be accepted on that merit either.", "accepted": 0}
{"paper_id": "iclr_2020_SJefPkSFPr", "review_text": "the authors take inspiration from regulatory fit theory and propose a new parameter for policy gradient algorithms in rl that can manage the regulatory focus of an agent. they hypothesize that this can affect performance in a problemspecific way, especially when trading off between broad exploration and risk. the reviewers expressed concerns about the usefulness of the proposed algorithm in practice and a lack of thorough empirical comparisons or theoretical results. unfortunately, the authors did not provide a rebuttal, so no further discussion of these issues was possible; thus, i recommend to reject.", "accepted": 0}
{"paper_id": "iclr_2020_B1eXvyHKwS", "review_text": "this paper studies adversarial training in the linear classification setting, and shows a rate of convergence for adversarial training of o1log t to the hard margin svm solution under a set of assumptions. while 2 reviewers agree that the problem and the central result is somewhat interesting though r3 is uncertain of the applicability to deep learning, i agree that useful insights can often be gleaned from studying the linear case, reviewers were critical of the degree of clarity and rigour in the writing, including notation, symbol reuse, repetitionsredundancies, and clarity surrounding the assumptions made. no updates to the paper were made and reviewers did not feel their concerns were addressed by the rebuttals. i therefore recommend rejection, but would encourage the authors to continue refining their paper in order to showcase their results more clearly and didactically.", "accepted": null}
{"paper_id": "iclr_2020_SyecdJSKvr", "review_text": "after reading the authors rebuttal, the reviewer still hold that the main contribution is just the simple combination of already known losses. and the paper need to pay more attention on the clarity of the paper.", "accepted": 0}
{"paper_id": "iclr_2020_r1xxKJBKvr", "review_text": "the paper proposes passnet, which is an architecture that produces a 2d map of probability of successful completion of a soccer pass. the architecture has some similarities with unet and has downsampling and upsampling modules with a set of skipconnections between them. the reviewers raised several issues  novelty compared to unet  lack of ablation studies  uncertainty about what probabilities mean and issues regarding output interpretation. the authors have tried to address these concerns in their rebuttal and provided additional experiments. they also argue that the application area sport analytics of the paper is novel. even though the application area is interesting and might lead to new problems, this paper did not get enough support from reviewers to justify its acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_S1xKYJSYwS", "review_text": "this paper proposes to represent the distribution w.r.t. which neural architecture search nas samples architectures through a variational autoencoder, rather than through a fully factorized distribution as previous work did. in the discussion, a few things improved causing one reviewer to increase hisher score from 1 to 3, but it became clear that the empirical evaluation has issues, with a different search space being used for the method than for the baselines. there was unanimous agreement for rejection. i agree with this judgement and thus recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_rkgl51rKDB", "review_text": "this paper combines pearl with hac to create a hierarchical metarl algorithm that operates on goals at the high level and learns lowlevel policies to reach those goals. reviewers remarked that its wellpresented and wellorganized, with enough details to be mostly reproducible. in the experiments conducted, it appears to show strong results. however there was strong consensus on two major weaknesses that render this paper unpublishable in its current form 1 the continuous control tasks used dont seem to require hierarchy, and 2 the baselines dont appear to be appropriate. reviewers remarked that a vital missing baseline is her, and that its unfair to compare to pearl, which is a more general metarl algorithm. the authors dont appear to have made revisions in response to these concerns. all reviewers made useful and constructive comments, and i urge the authors to take them into consideration when revising for a future submission.", "accepted": 0}
{"paper_id": "iclr_2020_rkgb9kSKwS", "review_text": "this paper proposes a new formulation of the nonlocal block and interpret it from the graph view. the idea is interesting and the experimental results seems to be promising. reviewer has two major concerns. the first is the presentation, which is not clear enough. the second is the experimental design and analysis. the authors add more video dataset in the revision, but still lack comprehensive experimental analysis for videobased applications. overall, the idea of nonlocal block from graph view is interesting. however, the presentation of the paper needs further polish and thus does not meet the standard of iclr", "accepted": 0}
{"paper_id": "iclr_2020_HygS91rYvH", "review_text": "the paper proposes to get universal adversarial examples using few test samples. the approach is very close to the khrulkov  oseledets, and the abstract for some reason claims that it was proposed independently, which looks like a very strange claim. overall, all reviewers recommend rejection, and i agree with them.", "accepted": 0}
{"paper_id": "iclr_2020_rJe_cyrKPB", "review_text": "the authors use a tucker decomposition to represent the weights of a network, for efficient computation. the idea is natural, and preliminary results promising. the main concern was lack of empirical validation and comparisons. while the authors have provided partial additional results in the rebuttal, which is appreciated, a thorough set of experiments and comparisons would ideally be included in a new version of the paper, and then considered again in review.", "accepted": 0}
{"paper_id": "iclr_2020_BkxackSKvH", "review_text": "this paper proposes a method for learning sentence embeddings such that entailment and contradiction relationships between sentence pairs can be inferred by a simple parameterfree operation on the vectors for the two sentences. reviewers found the method and the results interesting, but in private discussion, couldnt reach a consensus on what if any substantial valuable contributions the paper had proven. the performance of the method isnt compellingly strong in absolute or relative terms, yielding doubts about the value of the method for entailment applications, and the reviewers didnt see a strong enough motivation for the line of work to justify publishing it as a tentative or exploratory effort at iclr.", "accepted": null}
{"paper_id": "iclr_2020_HklliySFDS", "review_text": "this manuscript describes a continual learning approach where individual instances consist of sequences, such as language modeling. the paper consists of a definition of a problem setting, tasks in that problem setting, baselines not based on existing continual learning approaches, which the authors argue is to highlight the need for such techniques, but with which the reviewers took issue, and a novel architecture. reviews focused on the gravity of the contribution. r1 and r2, in particular, argued that the paper is written as though the problembenchmark definition is the main contribution. r2 mentions that in spite of this, the methods section jumps directly into the candidate architecture. as mentioned above, several reviewers also took issue with the fact that existing cl techniques are not employed as baselines. the authors engaged with reviewers and promised updates, but did not take the opportunity to update their paper. as many of the reviewers comments remain unaddressed and the authors updates did not materialize, i recommend rejection, and encourage the authors to incorporate the feedback they have received in a future submission.", "accepted": 0}
{"paper_id": "iclr_2020_SyxGoJrtPr", "review_text": "this paper proposes a new training technique to produce a learned model robust against adversarial attacks  without explicitly training on example attacked images. the core idea being that such a training scheme has the potential to reduce the cost in terms of training time for obtaining robustness, while also potentially increasing the clean performance. the method does so by proposing a version of label smoothing and doing two forms of data augmentations gaussian noise and mixup. the reviewers were mixed on this work. two recommended weak reject while one recommended weak accept. all agreed that this work addressed an important problem and that the proposed solution was interesting. the authors and reviewers actively engaged in a discussion, in some cases with multiple back and forths. the main concern of the reviewers is the inconclusive experimental evidence. though the authors did demonstrate strong performance on pgd attacks, the reviewers had concerns about some attack settings like epsilon and how that may unfairly disadvantage the baselines. in addition, the results on cw presented a different story than the results with pgd. therefore, we do not recommend this work for acceptance in its current form. the work offers strong preliminary evidence of a potential solution to provide robustness without direct adversarial training, but more analysis and explanation of when each component of their proposed solution should increase robustness is needed.", "accepted": 0}
{"paper_id": "iclr_2020_H1gx3kSKPS", "review_text": "the paper proposes a generative model that jointly trains an implicit generative model and an explicit energy based model using steins method. there are concerns about technical correctness of the proofs and the authors are advised to look carefully into the points raised by the reviewers.", "accepted": 0}
{"paper_id": "iclr_2020_BJg_2JHKvH", "review_text": "this paper offers a novel method for semisupervised learning using gmms. unfortunately the novelty of the contribution is unclear, and the majority of the reviewers find the paper is not acceptable in present form. the ac concurs.", "accepted": null}
{"paper_id": "iclr_2020_SklgTkBKDr", "review_text": "this paper presents two new architectures that model latent intermediate utilities and use nonadditive utility aggregation to estimate the set utility based on the computed latent utilities. these two extensions are easy to understand and seem like a simple extension to the existing rnn model architectures, so that they can be implemented easily. however, the connection to choquet integral is not clear and no theory has been provided to make that connection. hence, it is hard for the reader to understand why the integral is useful here. the reviewers have also raised objection about the evaluation which does not seem to be fair to existing methods. these comments can be incorporated to make the paper more accessible and the results more appreciable.", "accepted": 0}
{"paper_id": "iclr_2020_SJlgTJHKwB", "review_text": "this paper claims to present a modelagnostic continual learning framework which uses a queue to work with delayed feedback. all reviewers agree that the paper is difficult to follow. i also have a difficult time reading the paper. in addition, all reviewers mentioned there is no baseline in the experiments, which makes it difficult to empirically analyze the strengths and weaknesses of the proposed model. r2 and r3 also have some concerns regarding the motivation and claim made in the paper, especially in relation to previous work in this area. the authors did not respond to any of the concerns raised by the reviewers. it is very clear that the paper is not ready for publication at a venue such as iclr at the current state, so i recommend rejecting the paper.", "accepted": null}
{"paper_id": "iclr_2020_rJxBa1HFvS", "review_text": "this paper studies the problem of estimating the value function in an rl setting by learning a representation of the value function. while this topic is one of general interest to the iclr community, the paper would benefit from a more careful revision and reorganization following the suggestions of the reviewers.", "accepted": null}
{"paper_id": "iclr_2020_B1xu6yStPH", "review_text": "this paper proposes exaid, a method to detect adversarial attacks by building on the advances in explainability particularly shap, where activitymaplike explanations are used to justify and validate decisions. though it may have some valuable ideas, the execution is not satisfying, with various issues raised in comments. no rebuttal was provided.", "accepted": 0}
{"paper_id": "iclr_2020_rJe7CkrFvS", "review_text": "the paper is about exploration in deep reinforcement learning. the reviewers agree that this is an interesting and important topic, but the authors provide only a slim analysis and theoretical support for the proposed methods. furthermore, the authors are encouraged to evaluate the proposed method on more than a single benchmark problem.", "accepted": 0}
{"paper_id": "iclr_2020_SJeOAJStwB", "review_text": "this paper studies the problem of federated learning for noni.i.d. data, and looks at the hyperparameter optimization in this setting. as the reviewers have noted, this is a purely empirical paper. there are certain aspects of the experiments that need further discussion, especially the learning rate selection for different architectures. that said, the submission may not be ready for publication at its current stage.", "accepted": 0}
{"paper_id": "iclr_2020_r1glygHtDB", "review_text": "the paper proposes an architecture for semantic instance segmentation learnable from coarse annotations and evaluates it on two microscopy image datasets, demonstrating its advantage over baseline. while the reviewers appreciate the details of the architecture, they note the lack of evaluation on any of popular datasets and the lack of comparisons with baselines that would be more close to stateoftheart. the authors do not address this criticism convincingly. it is not clear, why e.g. the cityscapes or voc pascal datasets, which both have reasonably accurate annotations, cannot be used for the validation of the idea. if the focus is on the precision of the result near the boundaries, then one can always report the error near boundaries this is a standard thing to do. note that the performance of the baseline models is far from saturated near boundaries i.e. the errors are larger than mistakes of annotation. at this stage, the paper lacks convincing evaluation and comparison with prior art. given that this is first and foremost application paper, lacking some very novel ideas as pointed out by e.g. rev1, better evaluation is needed for acceptance.", "accepted": null}
{"paper_id": "iclr_2020_BJeVklHtPr", "review_text": "the paper is rejected based on unanimous reviews.", "accepted": 0}
{"paper_id": "iclr_2020_ByxHJeBYDB", "review_text": "this paper trains a transformer to extrapolate learning curves, and uses this in a modelbased rl framework to automatically tune hyperparameters. this might be a good approach, but its hard to know because the experiments dont include direct comparisons against existing hyperparameter optimizationadaptation techniques either the ones based on extrapolating training curves, or standard ones like bayesopt or pbt. the presentation is also fairly informal, and its not clear if a reader would be able to reproduce the results. overall, i think theres significant cleanup and additional experiments needed before publication in iclr.", "accepted": 0}
{"paper_id": "iclr_2020_B1xGxgSYvH", "review_text": "this paper provides a new theoretical framework for domain adaptation by exploring the compression and adaptability. reviewers and ac generally agree that this paper discusses about an important problem and provides new insight, but it is not a thorough theoretical work. the reviewers identified several key limitations of the theory such as unrealistic condition and approximation. some important points still require more work to make the framework practical for algorithm design and computation. the presentation could also be improved. hence i recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_HygFxxrFvB", "review_text": "this provides a new method, called dpautogan, for the problem of differentially private synthetic generation. the method uses private autoencoder to reduce the dimension of the data, and apply private gan on the latent space. the reviewers think that there is not sufficient justification for why this is a good approach for synthetic generation. they also think that the presentation is not ready for publication.", "accepted": 0}
{"paper_id": "iclr_2020_r1ghgxHtPH", "review_text": "the paper proposes an interesting idea of inserting gaussian convolutions into convnet in order to increase and to adapt effective receptive fields of network units. the reviewers generally agree that the idea is interesting and that the results on cityscapes are promising. however, it is hard not to agree with reviewer 3, that validation on a single dataset for a single task is not sufficient. this criticism is unaddressed.", "accepted": 0}
{"paper_id": "iclr_2020_HJxTgeBtDr", "review_text": "the paper diligently setup and conducted multiple experiments to validate their approach  bucketizating attributions of data and analyze them accordingly to discover deeper insights eg biases. however, reviewers pointed out that such bucketing is tailored to tasks where attributions are easily observed, such as the one of the focus in this paper ner. while manuscript proposes this approach as general, reviewers failed to seem this point. another reviewer recommended this manuscript to become a journal item rather than conference, due to the length of the page in appendix 17. there were some confusions around writings as well, pointed out by some reviewers. we highly recommend authors to carefully reflect on reviewers both pros and cons of the paper to improve the paper for your future submission.", "accepted": 1}
{"paper_id": "iclr_2020_SJgSflHKDr", "review_text": "the authors discuss how to predict generalization gaps. reviews are mixed, putting the submission in the lower half of this years submissions. i also would have liked to see a comparison with other divergence metrics, for example, l1, mmd, hdistance, discrepancy distance, and learned representations e.g., bert, laser, etc., for language. without this, the empirical evaluation of fd is a bit weak. also, the obvious next step would be trying to minimize fd in the context of domain adaptation, and the question is if this shouldnt already be part of your paper? suggestions the amazon reviews are timestamped, enabling you to run experiments with drift over time. see 0 for an example. 0 httpswww.aclweb.organthologyw186210", "accepted": 0}
{"paper_id": "iclr_2020_S1ghzlHFPS", "review_text": "while reviewers find this paper interesting, they raised number of concerns including the novelty, writing, experiments, references and clear mention of the benefit. unfortunately, excellent questions and insightful comments left by reviewers are gone without authors answers.", "accepted": 0}
{"paper_id": "iclr_2020_rkedXgrKDH", "review_text": "this article studies the length of onedimensional trajectories as they are mapped through the layers of a relu network, simplifying proof methods and generalising previous results on networks with random weights to cover different classes of weight distributions including sparse ones. it is observed that the behaviour is similar for different distributions, suggesting a type of universality. the reviewers found that the paper is well written and appreciated the clear description of the places where the proofs deviate from previous works. however, they found that the results, although adding interesting observations in the sparse setting, are qualitatively very close to previous works and possibly not substantial enough for publication in iclr. the revision includes some experiments with trained networks and updates the title to better reflect the contribution. however, the reviewers did not find this convincing enough. the article would benefit from a deeper theory clarifying the observations that have been made so far, and more extensive experiments connecting to practice.", "accepted": 0}
{"paper_id": "iclr_2020_H1l2mxHKvr", "review_text": "this paper tackles the interesting problem of metalearning in problem spaces where training tasks are scarce. two criticisms that seems to shared across reviewers are that i it is debatable how novel the space of meta learning with few tasks is, especially since there arent established standard for how many training tasks should be available, and ii the paper could use more comparisons with baseline methods and ablations to understand the contributions. as an ac, i downweight criticism i because i dont feel the paper has to be creating a new problem definition; its acceptable to make advances within an existing space. however, criticism ii seems to remain. after conferring with reviewers it seems that the rebuttal was not strong enough to significantly alter the reviewers opinions on this issue, and so the paper does not have enough support to justify acceptance. the paper certainly addresses interesting issues, and i look forward to seeing a revisedimproved version at another venue.", "accepted": 0}
{"paper_id": "iclr_2020_BygY4grYDr", "review_text": "as the reviewers point out, the core contribution might be potentially important but the current execution of the paper makes it difficult to gauge this importance. in the light of this, this paper does not seem ready for appearance in a conference like iclr.", "accepted": 0}
{"paper_id": "iclr_2020_B1x3EgHtwB", "review_text": "the paper develops linear overparameterization methods to improve training of small neural network models. this is compared to training from scratch and other knowledge distillation methods. reviewer 1 found the paper to be clear with good analysis, and raised concerns on generality and extensiveness of experimental work. reviewer 2 raised concerns about the correctness of the approach and laid out several other possibilities. the authors conducted several other experiments and responded to all the feedback from the reviewers, although there was no final consensus on the scores. the review process has made this a better paper and it is of interest to the community. the paper demonstrates all the features of a good paper, but due to a large number of strong papers, was not accepted at this time.", "accepted": 0}
{"paper_id": "iclr_2020_BkeYSlrYwH", "review_text": "the paper introduces an ensemble of rl agents that share knowledge amongst themselves. because there are no theoretical results, the experiments have to carry the paper. the reviewers had rather different views on the significance of these experiments and whether they are sufficient to convincingly validate the learning framework introduced. overall, because of the high bar for iclr acceptance, this paper falls just below the threshold.", "accepted": 1}
{"paper_id": "iclr_2020_H1e3HlSFDr", "review_text": "this paper proposes to add constraints to the rl problem within a variational method. the hope is to specify a safe vs nonsafe states. the reviewers were not convinced that this paper makes the cut for iclr. moreover, there was no rebuttal from the authors, so it didnt give the reviewer a chance to reconsider their opinion. based on the current ratings, i recommend to reject this paper.", "accepted": null}
{"paper_id": "iclr_2020_BkljIlHtvS", "review_text": "this paper presents a number of experiments involving the modelagnostic metalearning maml framework, both for the purpose of understanding its behavior and motivating specific enhancements. with respect to the former, the paper argues that deeper networks allow earlier layers to learn generic modeling features that can be adapted via later layers in a taskspecific way. the paper then suggests that this implicit decomposition can be explicitly formulated via the use of metaoptimizers for handling adaptations, allowing for simpler networks that may not require generic modelingspecific layers. at the end of the rebuttal and discussion phases, two reviewers chose rejection while one preferred acceptance. in this regard, as ac i did not find clear evidence that warranted overriding the reviewer majority, and consistent with some of the evaluations, i believe that there are several points whereby this paper could be improved. more specifically, my feeling is that some of the conclusions of this paper would either already be expected by members of the community, or else would require further empirical support to draw more firm conclusions. for example, the fact that earlier layers encode more generic features that are not adapted for each task is not at all surprising such lowlevel features are natural to be shared. moreover, when the linear model from section 3.2 is replaced by a deep linear network, clearly the model capacity is not changed, but the effective number of parameters which determine the gradient update will be significantly expanded in a seemingly nontrivial way. this is then likely to be of some benefit. consequently, one could naturally view the extra parameters as forming an implicit metaoptimizer, and it is not so remarkable that other trainable metaoptimizers might work well. indeed cited references such as park  oliva, 2019 have already applied explicit metaoptimizers to maml and fewshot learning tasks. and based on table 2, the proposed factorized metaoptimizer does not appear to show any clear advantage over the metacurvature method from park  oliva, 2019. overall, either by using deeper networks or an explicit trainable metaoptimizer, there are going to be more adaptable parameters to exploit and so the expectation is that there will be room for improvement. even so, i am not against the message of this paper. rather it is just that for an empiricallybased submission with close ties to existing work, the bar is generally a bit higher in terms of the quality and scope of the experiments. as a final lesser point, the paper argues that metaoptimizers allow for the decomposition of modeling and adaptation as mentioned above; however, i did not see exactly where this claim was precisely corroborated empirically. for example, one useful test could be to recreate figure 2 but with the metaoptimizer in place and a shallower network architecture. the expectation then might be that general features are no longer necessary.", "accepted": 0}
{"paper_id": "iclr_2020_Bkl2UlrFwr", "review_text": "the submission proposes a method for learning a graph structure and node embeddings through an iterative process. smoothness and sparsity are both optimized in this approach. the iterative method has a stopping mechanism based on distance from a ground truth. the concerns of the reviewers were about scalability and novelty. since other methods have used the same costs for optimization, as well as other aspects of this approach, there is little contribution other than the iterative process. the improvement over lds, the most similar approach, is relatively minor. although the paper is promising, more work is required to establish the contributions of the method. recommendation is for rejection.", "accepted": 0}
{"paper_id": "iclr_2020_B1eZweHFwr", "review_text": "this paper proposes a smoothingbased certification against various forms of transformations, such as rotations, translations. the reviewers have concerns on the novelty of the work and several technical issues. the authors have made efforts to address some of issues, but the work may still significantly benefit from a throughout improvement in both presentation and technical contribution.", "accepted": 0}
{"paper_id": "iclr_2020_rJx7wlSYvB", "review_text": "the main contribution is a bayesian neural net algorithm which saves computation at test time using a vector quantization approximation. the reviewers are on the fence about the paper. i find the exposition somewhat hard to follow. in terms of evaluation, they demonstrate similar performance to various bnn architectures which require monte carlo sampling. but there have been lots of bnn algorithms that dont require sampling e.g. pbp, bayesian dark knowledge, mackays delta approximation, so it seems important to compare to these. i think there may be promising ideas here, but the paper needs a bit more work before it is to be published at a venue such as iclr.", "accepted": 1}
{"paper_id": "iclr_2020_r1gIwgSYwr", "review_text": "this paper proposes pacbayes bounds for metalearning. the reviewers who are most knowledgeable about the subject and who read the paper most closely brought up several concerns regarding novelty especially a description of how the proposed bounds relate to those in prior works pentina el al. 2014, galanti et al. 2016 and amit and meir 2018 and regarding clarity. the reviewers found theoretical analysis and proofs hard to follow. for these reasons, the paper isnt ready for publication at this time. see the reviewers comments for details.", "accepted": 1}
{"paper_id": "iclr_2020_BylldxBYwH", "review_text": "the authors present a physicsaware models for inpainting fluid data. in particular, the authors extend the vanilla unet architecture and add losses that explicitly bias the network towards physically meaningful solutions. while the reviewers found the work to be interesting, they raised a few questionsobjections which are summarised below 1 novelty the reviewers largely found the idea to be novel. i agree that this is indeed novel and a step in the right direction. 2 experiments the main objection was to the experimental methodology. in particular, since most of the experiments were on simulated data the reviewers expected simulations where the test conditions were a bit more different than the training conditions. it is not very clear whether the training and test conditions were different and it would have been useful if the authors had clarified this in the rebuttal. the reviewers have also suggested a more thorough ablation study. 3 organisation the authors could have used the space more effectively by providing additional details and ablation studies. unfortunately, the authors did not engage with the reviewers and respond to their queries. i understand that this could have been because of the poor ratings which would have made the authors believe that a discussion wouldnt help. the reviewers have asked very relevant qs and made some interesting suggestions about the experimental setup. i strongly recommend the authors to consider these during subsequent submissions. based on the reviewer comments and lack of response from the authors, i recommend that the paper cannot be accepted.", "accepted": 0}
{"paper_id": "iclr_2020_S1eZOeBKDS", "review_text": "the paper presents a model for learning spiking representations. the basic model is a a deep autoencoder trained endtoend with a biophysical generative model and results are presented on emg and semg data, with the aim to motivate further research in selfsupervised learning. the reviewers raised several points about the paper. reviewer 1 raised concerns about lack of context on surrounding work, clarity of the model itself and motivating the loss. reviewer 2 pointed out strengths of the paper in its simplicity and the importance of this problem, but also raised concerns about the papers clarity, again motivations on the loss function and sensibility of design choices. the authors responded to the feedback from reviewer 1, but overall the reviewer did not think their scores should be changed. the paper in its current form is not yet ready for acceptance, and we hope there has been useful feedback from the reviewing process for their future research.", "accepted": null}
{"paper_id": "iclr_2020_HJxDugSFDB", "review_text": "an actorcritic method is introduced that explicitly aims to learn a good representation using a stochastic latent variable model. there is disagreement among the reviewers regarding the significance of this paper. two of the three reviewers argue that several strong claims made in the paper that are not properly backed up by evidence. in particular, it is not sufficiently clear to what degree the shown performance improvement is due to the stochastic nature of the model used, one of the key points of the paper. i recommend that the authors provide more empirical evidence to back up their claims and then resubmit.", "accepted": 1}
{"paper_id": "iclr_2020_Byeq_xHtwS", "review_text": "the paper has several clarity and novelty issues.", "accepted": 0}
{"paper_id": "iclr_2020_rke3OxSKwr", "review_text": "the paper proposes a method of training latencylimited waitk decoders for online machine translation. the authors investigate the impact of the value of k, and of recalculating the transformers decoder hidden states when a new source token arrives. they significantly improve over stateoftheart results for germanenglish translation on the wmt15 dataset, however there is limited novelty wrt previous approaches. the authors responded in depth to reviews and updated the paper with improvements, for which there was no reviewer response. the paper presents interesting results but imo the approach is not novel enough to justify acceptance at iclr.", "accepted": 0}
{"paper_id": "iclr_2020_rkxMKerYwr", "review_text": "this paper studies the transfer of representations learned by deep neural networks across various datasets and tasks when the network is pretrained on some dataset and subsequently finetuned on the target dataset. on the theoretical side the authors analyse twolayer fully connected networks. in an extensive empirical evaluation the authors argue that an appropriately pretrained networks enable better loss landscapes improved lipschitzness. understanding the transferability of representations is an important problem and the reviewers appreciated some aspects of the extensive empirical evaluation and the initial theoretical investigation. however, we feel that the manuscript needs a major revision and that there is not enough empirical evidence to support the stated conclusions. as a result, i will recommend rejecting this paper in the current form. nevertheless, as the problem is extremely important i encourage the authors to improve the clarity and provide more convincing arguments towards the stated conclusions by addressing the issues raised during the discussion phase.", "accepted": 0}
{"paper_id": "iclr_2020_B1gNKxrYPB", "review_text": "the paper studies the problem of graph learning with attributes, and propose a 2d graph convolution that models the node relation graph and the attribute graph jointly. the paper proposes and efficient algorithm and models intraclass variation. empirical performance on 20ng, lcora, and wiki show the promise of the approach. the authors responded to the reviews by updating the paper, but the reviewers unfortunately did not further engage during the discussion period. therefore it is unclear whether their concerns have been adequately addressed. overall, there have been many strong submissions on graph neural networks at iclr this year, and this submission as is currently stands does not quite make the threshold of acceptance.", "accepted": 0}
{"paper_id": "iclr_2020_ByxloeHFPS", "review_text": "this paper pursues an ambitious goal to provide a theoretical analysis hrl in terms of regret bounds. however, the exposition of the ideas has severe clarity issues and the assumptions about hmdps used are overly simplistic to have an impact in rl research. finally, there is agreement between the reviewers and ac that the novelty of the proposed ideas is a weak factor and that the paper needs substantial revision.", "accepted": 0}
{"paper_id": "iclr_2020_Bye-sxHFwB", "review_text": "this paper proposes a neural network architecture that represents each neuron with input and output embeddings. experiments on cifar show that the proposed method outperforms baseline models with a fully connected layer. i like the main idea of the paper. however, i agree with r1 and r2 that experiments presented in the paper are not enough to convince readers of the benefit of the proposed method. in particular, i would like to see a more comprehensive set of results across a suite of datasets. it would be even better, although not necessary, if the authors apply this method on top of different base architectures in multiple domains. at the very least, the authors should run an experiment to compare the proposed approach with a feed forward network on a simpletoy classification dataset. i understand that these experiments require a lot of computational resources. the authors do not need to reach sota, but they do need to provide more empirical evidence that the method is useful in practice. i also would like to see more discussions with regards to the computational cost of the proposed method. how much slowerfaster is traininginference compared to a fully connected network? the writing of the paper can also be improved. there are many a few typos throughout the paper, even in the abstract. i recommend rejecting this paper for iclr, but would encourage the authors to polish it and run a few more suggested experiments to strengthen the paper.", "accepted": 0}
{"paper_id": "iclr_2020_H1lMogrKDH", "review_text": "the paper studies nonspiking hudgkinhuxley models and shows that under few simplifying assumptions the model can be trained using conventional backpropagation to yield accuracies almost comparable to stateoftheart neural networks. overall, the reviewers found the paper wellwritten, and the idea somewhat interesting, but criticized the experimental evaluation and potential low impact and interest to the community. while the method itself is sound, the overall assessment of the paper is somewhat below whats expected from papers accepted to iclr, and im thus recommending rejection.", "accepted": 0}
{"paper_id": "iclr_2020_B1xoserKPH", "review_text": "this paper report empirical implications of privacy leaks in language models. reviewers generally agree that the results look promising and interesting, but the paper isnt fully developed yet. a few pointed out that framing the paper better to better indicate broader implications of the observed symptoms would greatly improve the paper. another pointed out better placing this work in the context of other related work. overall, this paper could use another cycle of polishingenhancing the results.", "accepted": 0}
{"paper_id": "iclr_2020_H1x-3xSKDr", "review_text": "this article studies the effects of bn on robustness. the article presents a series of experiments on various datasets with noise, pgd adversarial attacks, and various corruption benchmarks, that show a drop in robustness when using bn. it is suggested that a main cause of vulnerability is the tiling angle of the decision boundary, which is illustrated in a toy example. the reviewers found the contribution interesting and that the effect will impact many dnns. however, they the did not find the arguments for the tiling explanation convincing enough, and suggested more theory and experimental illustration of this explanation would be important. in the rebuttal the authors maintain that the main contribution is to link bn and adversarial vulnerability and consider their explanation reasonable. in the initial discussion the reviewers also mentioned that the experiments were not convincing enough and that the phenomenon could be an effect of gradient masking, and that more experiments with other attack strategies would be important to clarify this. in response, the revision included various experiments, including some with various initial learning schedules. the revision clarified some of these issues. however, the reviewers still found that the reason behind the effect requires more explanations. in summary, this article makes an important observation that is already generating a vivid discussion and will likely have an impact, but the reviewers were not convinced by the explanations provided for these observations.", "accepted": 0}
{"paper_id": "iclr_2020_Hkxi2gHYvH", "review_text": "the paper proposes to use the representation learned via cpc to do reward shaping via clustering the embedding and providing a reward based on the distance from the goal. the reviewers point out some conceptual issues with the paper, the key one being that the method is contingent on a random policy being able to reach the goal, which is not true for difficult environments that the paper claims to be motivated by. one reviewer noted limited experiment runs and lack of comparisons with other reward shaping methods. i recommend rejection, but hope the authors find the feedback helpful and submit a future version elsewhere.", "accepted": 0}
{"paper_id": "iclr_2021_nIAxjsniDzg", "review_text": "there is a clear consensus over all reviewers that this is a very strong empirical analysis, with actionable insights that should prove quite useful both to researchers and practitioners. i have no doubt that many will use it as a reference when implementing and using rl algorithms especially since the authors said they would release their code. this is thus a clear accept, that in my opinion would deserve an oral presentation, so as to better disseminate its key findings.", "accepted": 1}
{"paper_id": "iclr_2021_rC8sJ4i6kaH", "review_text": "the paper looks into theoretical analysis of selftraining beyond the existing linear case and considers deep networks under additional assumption on data. namely expansion and minimal overlap in the neighborhood of examples in different classes. the results shed some light on selftraining algorithms that use input consistency regularizers. although the assumptions are very hard to check for all input distributions, the authors make an attempt by considering output of biggan generator. in summary, the paper is a great first step in understanding selftraining for deep networks. the paper is overall clearly written. please add the explanation of assumption 4.1 as requested by reviewer 4. pros  given the extensive use of selftraining the paper is of great importance to the community extending the analysis of selftraining to deep networks the paper is clearly written and easy to follow cons the assumptions are very hard to validate on all datasets", "accepted": 1}
{"paper_id": "iclr_2021_Mk6PZtgAgfq", "review_text": "the paper presents a variance reduction technique to the straightthrough version of the gumbelsoftmax estimator. the technique is relying on the truncated gumbel of maddison et al. i share the excitement of the reviewers about this work and i expect this technique to further influence the field.", "accepted": 1}
{"paper_id": "iclr_2021_dYeAHXnpWJ4", "review_text": "this paper studies why input gradients can give meaningful feature attributions even though they can be changed arbitrarily without affecting the prediction. the claim in this paper is that the learned logits in fact represent class conditional probabilities and hence input gradients given meaningful feature attributions. the main concern is that this claim is verified very indirectly, by adding a regularization term that promotes logits learning class conditional probabilities and observing that input gradient quality also improves. nevertheless, there are interesting insights in the paper and the questions it asks are very timely and important, and overall, it could have a significant impact on further research in this area.", "accepted": 1}
{"paper_id": "iclr_2021_cPZOyoDloxl", "review_text": "the paper is studying a new intrinsic motivation rl setup in a dynamic environment, where the authors minimize the state entropy instead of the common approach of maximizing it. the resulting idea is simple but also surprising that it works so well. all reviewers appreciated the new problem formulation of using dynamic environments and found the idea very promsing. in addition, they identified the following strengths of the paper  the experiments are exhaustive, identifying many domains where the approach can be applied  the presented results are compelling  the paper is well written  the paper introduces a new problem setup that has not been studied before i agree with the reviewers that this paper contains many interesting contributions and therefore recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2021_g-wu9TMPODo", "review_text": "the paper seeks to understand how training overparametrized models e.g., those based on neural networks to zero training accuracy even when the test error is small i.e., benign overfitting can introduce vulnerabilities in the form of adversarial examples and how to remedy the situation. the paper implicates label noise as one of the causes of adversarial robustness, and suboptimal representations learned as part of the training as another. the claims are supported both theoretically and empirically. a good paper overall, accept!", "accepted": 1}
{"paper_id": "iclr_2021_WiGQBFuVRv", "review_text": "this paper proposes an approach to probabilistic time series forecasting based on combining autoregressive deep learning models with normalizing flows. in terms of strengths, time series forecasting is a fundamental problem. the proposed approach is a reasonable combination of existing model components that provides a flexible, endtoend trainable framework for multivariate probabilistic forecasting. the experiments are wellconducted and the results outperform recently published methods. while the reviewers raised a number of questions, all of the reviewers agree that their questions have be answered satisfactorily by the authors during the discussion and the paper should be accepted. the authors should be sure to incorporate the reviewer suggestions and author responses into the final paper.", "accepted": 1}
{"paper_id": "iclr_2021_rumv7QmLUue", "review_text": "this paper proposes a broad framework for unifying various pruning approaches and performs detailed analyses to make recommendations about the settings in which various approaches may be most useful. reviewers were generally excited by the framework and analyses, but had some concerns regarding scale and the papers focus on structured pruning. the authors included new experiments however, which mostly addressed reviewer concerns. overall, i think is a strong paper which will likely be provide needed grounding for pruning frameworks and recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2021_O7ms4LFdsX", "review_text": "this paper presents an approach for learning disentangled static and dynamic latent variables for sequence data. in terms of learning objective, the paper extends wasserstein autoencoder to sequential data, and this approach is novel and wellmotivated; the aggregated posterior for static variables comes out naturally and plays an important role for regularization this appears to be new for sequence data. the authors also studies how to model additional categorical variables for weakly supervised learning in real scenarios. the main steps generation and inference were illustrated by graphical models with clarity, and rigorous statements are provided to back them up. experimental results demonstrate the advantages of proposed method, in terms of disentanglement performance and generation quality. the reviewers think this paper makes nice contributions to the sequential generative model community.", "accepted": null}
{"paper_id": "iclr_2021_m1CD7tPubNy", "review_text": "this submission explores how certain common padding choices can induce spatial biases in convolutional networks. it looks into alternative padding schemes which mitigate these issues and demonstrates significant performance improvements in widely used convnets. reviewers generally agreed that this is an important point that should be more widely understood in the community, and that the proposed changes are relatively simple to adopt, so this work is likely to be impactful. most reviewers thought the paper was wellwritten, describing the problem well, and the analysis wellexecuted. most reviewers acknowledged that most of the weaknesses described in their initial reviews were welladdressed by the authors responses and manuscript updates. given the strength of the analysis and the impact for many practitioners, i recommend the submission be accepted with a spotlight presentation.", "accepted": 1}
{"paper_id": "iclr_2021_wS0UFjsNYjn", "review_text": "this paper addresses a method for unsupervised metalearning where a vae with gaussian mixture prior is used and setlevel inference, taking episodespecific dataset as input, is performed to calculate its posterior. in the metatesting phase, semisupervised learning with the learned vae is used to fast adapt to fewshow learning. reviewers are satisfied with the author responses, agreeing that the method is a principled way to tackle unsupervised metalearning.", "accepted": 1}
{"paper_id": "iclr_2021_o966_Is_nPA", "review_text": "this paper introduces a novel pruning algorithm for neural networks, gently regularizing the weights away through weight decay and using hessian information instead of simple magnitude. all in all an idea that is simple and effective, and could be of interest to a large audience. ac", "accepted": 1}
{"paper_id": "iclr_2021_01olnfLIbD", "review_text": "the paper presents a scalable data poisoning algorithm for targeted attacks, using the idea of designing poisoning patterns which align the gradients of the real objective and the adversarial objective. this intuition is supported by theoretical results, and the paper presents convincing experimental results about the effectiveness of the model. the reviewers overall liked the paper. however, they requested a number of clarifications and some additional work, which should be incorporated in the final version however, the authors are not required to use the wording as poison integrity poison availability. in particular, it would be great to see the experiment the authors suggested in their response to reviewer 2 about the effectiveness of their method for multiple targets this is important to better understand the limitations of the proposed approach.", "accepted": null}
{"paper_id": "iclr_2021_Qm7R_SdqTpT", "review_text": "all three reviewers agree on accepting the paper and think that the proposed approach will be of interest for those working in vdieo prediction. the authors are asked to include the extra discussion with r3 as part of the paper and include the proposed changes by r2 to provide more thorough experimentation. the paper is recommended as a poster presentation.", "accepted": null}
{"paper_id": "iclr_2021_XjYgR6gbCEc", "review_text": "this paper proposes a unified way of data augmentation using a latent embedding space  it learns a continuous latent space for transformation, and finds effective directions to traverse in this space for data augmentation. the proposed approach combines existing approaches for data augmentation, e.g., adversarial training, triplet loss, and joint training. the paper also identifies input examples where the model had low performance and creates harder examples that help the model improve its performance. it is evaluated on multiple corresponding to text, table, timeseries and image modalities and outperforms sota except on image data. the paper has responded to the reviewers feedback to provide more detailed experiments with stronger baselines and also ablation studies to show the effectiveness of different components of the approach. the results can be further improved by thorough empirical comparison to other sota methods, and by using other loss functions e.g.,center loss, large margin loss and other contrastive losses as alternatives to the triplet loss proposed in the paper. some reviewers have pointed out that the paper is somewhat limited in its novelty, since it combines existing offtheshelf moduleslosses and similar methods have been tried in the past  the novel contributions of the paper should be clearly highlighted in the revised submission.", "accepted": null}
{"paper_id": "iclr_2021_ixpSxO9flk3", "review_text": "the authors proposed to train an energy based model with a hierachical variational approximations. the entropy can be tricky in hierarchical variational approximations. the authors suggest using the auxillary samples to guide an importance samples to compute the gradient of the entropy. they evaluate their approach on a slew of models. the idea is straightfoward and could potentially be applied to other hierarchical variational models out side of the energybased model setting. the authors were responsive and clarified many agressive questions. id ask the authors to clean up two things  equation 8 would be easier to follow if it kept the expectation from equation 6 thereby making z_0 feel like it materialize out of thin air  a more detailed discusion of when the proposal is good and what could be missed out when relying on the generating z to center the proposal", "accepted": 1}
{"paper_id": "iclr_2021_DEa4JdMWRHp", "review_text": "the proposed approach is interesting and is differentiated enough from the recent body of work on neural network granger causal modeling as it offers a mechanism for detecting signs of causality. the authors have satisfactorily addressed the points raised in the reviews. in particular relationship with prior work and novelty of the contributions are now clearly articulated. the added discussion on the superiority of tcdf on simulated fmri experiments is insightful. though prediction error is only a proxy for the task at hand, the readers will appreciate the added evaluation. the proposed approach to stability evaluation leveraging the timereversal trick is novel and particularly pertinent, and could motivate some interesting followup work on this topic. it is also important that the authors have characterized the computational advantage of the approach.", "accepted": 1}
{"paper_id": "iclr_2021_IMPA6MndSXU", "review_text": "this paper studies the problem of unsupervised domain translation. here translation does not refer to language translation. instead, it refers to the idea of transferring highlevel semantic features. specifically, the authors look at digit style transfer between mnistpostal address numbers and svhnstreet view house numbers and sketches to reals. the visuals look very convincing and the empirical results are strong, too. there is one weaker review but the authors address the concerns in their response and the reviewer did unfortunately not respond despite promting.", "accepted": 0}
{"paper_id": "iclr_2021_aYuZO9DIdnn", "review_text": "this paper studies the patchbased convolutional kernels for image classification, and finds that making the kernel dependent on data is necessary for designing competitive kernels for image classification. the proposed simple method shows comparable results to those endtoend deeper architectures on cifar10 and imagenet datasets. all reviewers feel that the paper is interesting, important, and the performance is impressive. during the rebuttal, the authors have addressed most of the questions and concerns raised by the reviewers. in particular, authors have clarified the motivation, discussed the model size of the proposed method requested by r1, added precise details about the spectrum definition and intrinsic dimension requested by r4, and taken the suggestions from all reviewers to improve their paper. after rebuttal, all reviewers agree on accepting the paper. after checking the discussions between the authors and reviewers, i am convinced that the original concerns of the reviewers are addressed. hence, i recommend that this paper be accepted.", "accepted": null}
{"paper_id": "iclr_2021_pAbm1qfheGk", "review_text": "the paper combines flowbased and energybased models to generate molecular conformations given a molecular graph. for this, a continuous flow model is used to map the graphbased molecular representation into a distribution over conformations. an energybased model ebm is used to further help the model capture longrange atomic interactions. the proposed method is compared with strong baselines cvgae, graphdg, and rdkit. the authors addressed most of the reviewers concerns in the rebuttal. all the reviewers agree on acceptance.", "accepted": null}
{"paper_id": "iclr_2021_uxpzitPEooJ", "review_text": "this paper presents a way to use gnns to learn edge weights of a coarsened graph given the node mapping from the original graph to the coarsened graph. the paper is wellwritten and the approach is wellmotivated as learning makes it easy to adapt the edge weights to different tasks and objectives, as illustrated in the graph laplacian and rayleigh quotient examples. all the reviewers gage positive reviews for this paper, hence i recommend accepting this paper. the reason for not promoting this paper further to spotlight or oral is that the paper addressed a relatively small problem, learning the edge weights given the node mapping, and the proposed method is quite simple. therefore this papers impact could be limited. one suggestion to the authors is to present more results on downstream tasks, i.e. how does the proposed coarsening algorithm improve downstream task performance, instead of just losses defined without a downstream task in mind. example things to consider does this approach improve graph classification accuracy? does this improve downstream gnn models efficiency without sacrificing accuracy?", "accepted": null}
{"paper_id": "iclr_2021_45uOPa46Kh", "review_text": "the authors propose to take a tokenlevel generative approach to the task of visionlanguage navigation r2rr4r. the reviewers raise a number of concerns which should be noted in the final version of this work. the primary concern revolves around generality. how will this approach generalize to more sophisticated generative and discriminative models? to what extent is the model relying on the short instructionaction sequences to succeed and would not perform well on longer instructions, longer trajectories, or more abstract language. finally, the discussion of the uninformed prior is interesting because while clean, reviewers note there is no realistic grounded language scenario in which an uninformative prior makes sense.", "accepted": 1}
{"paper_id": "iclr_2021_bEoxzW_EXsa", "review_text": "the reviewers have different views on the papers but agreed that the paper can be accepted. however, they suggested some points of improvements including the writing clarity and style and experiments showing strong improvements compared to wgan.", "accepted": 1}
{"paper_id": "iclr_2021_FOyuZ26emy", "review_text": "the authors carefully study a class of unsupervised learning models called selfexpressive deep subspace clustering sedsc models, which involve clustering data arising from mixtures of complex nonlinear manifolds. the main contribution is to show that the sedsc formulation itself suffers from fundamental degeneracies, and that the experimental gains reported in the literature may be due to adhoc preprocessing. the contributions are compelling, and all reviewers appreciated the paper. despite the paper being of somewhat narrow focus, my belief is that negative results of this nature are useful and timely. i recommend an accept.", "accepted": null}
{"paper_id": "iclr_2021_1OCTOShAmqB", "review_text": "this paper investigates the training dynamics of simple neural attention mechanisms, in a controlled setting with clear but rather strict assumptions. some reviewers expressed caution about the applicability of the assumptions in practice, but nevertheless there is agreement that the results deepen our understanding and enrich our toolkit for reasoning about attention. in support of this, in the discussion period, it was emphasized that the work uses different techniques than most current work in this direction. i am therefore confident that the paper will be useful, and recommend acceptance. i strongly encourage the authors to improve the clarity of the work and thorough citation, as suggested by the reviewers.", "accepted": 1}
{"paper_id": "iclr_2021_-mWcQVLPSPy", "review_text": "three experts in the field recommend accepting the paper ratings 7,7,6 after the author response, appreciating the improvements the authors made. note the ac is mainly disregarding r3s rating, as r3 did not respond to the early request of the ac to clarify their review, did not respond to the authors request for clarification, and did not participate in any discussion past their initial short review. the solid experimental evaluation and an original methodology for zeroshot learning speak for accepting the paper. the area chair is certain about accepting the paper, but not fully confident if it should be poster or spotlight.", "accepted": 0}
{"paper_id": "iclr_2021_q8qLAbQBupm", "review_text": "the paper offers a more systematic treatment of various symmetryrelated results in the current literature. concretely, the invariance properties exhibited by loss functions associated with neural networks give rise to various dynamical invariants of gradient flows. the authors address these dynamical invariants in a unified manner and study them wrt different variants of gradient flows aimed at reflecting different algorithmic aspects of real training processes. the simplicity and the generality of dynamical invariants are both the strength and the weakness of the approach. on one hand, they provide a simple way of obtaining nontrivial generalities for the dynamics of learning processes. on the other hand, they abstracts away the very structure of neural networks from which they derive, and hence only allow relatively generic statements. perhaps the approach should be positioned more as a conceptual method for studying invariant loss functions. overall, although the technical contributions in the paper are rather incremental, the conceptual contribution of using dynamical invariants to unify and somewhat simplify existing analyses in a clear and clean symmetrybased approach is appreciated by the reviews and warrant a recommendation for borderline acceptance.", "accepted": 1}
{"paper_id": "iclr_2021_2VXyy9mIyU3", "review_text": "dear authors, thank you very much for your very detailed feedback to the reviewers. they have highly contributed to clarifying some of the concerns raised by the reviewers and improved their understanding of this paper. overall, all the reviewers acknowledge the merit of this paper and thus i suggest acceptance of this paper. however, as reviewer 4 pointed out, there are conceptual and theoretical issues that need to be more carefully addressed. please clarify these issues in the final version of the paper.", "accepted": 1}
{"paper_id": "iclr_2021_9l0K4OM-oXE", "review_text": "this paper introduces neural attentiondistillation; a new scheme for erasing backdoors in a poisoned neural network. the paper performs an empirical evaluation of their proposed method against 6 stateoftheart backdoor attacks. the authors show that attentiondistillation succeeds by using only a small fraction of clean training data without any performance degradation. in addition, the authors have provided ablation studies to clarify the contribution of each component in their proposed approach. reviewers find the simplicity and effectiveness of the approach an important attribute that may lead this work to have a high impact in the field. the paper is wellwritten, and all reviewers rate it on the accept side. i concur with their opinions and comments and i recommend accept.", "accepted": null}
{"paper_id": "iclr_2021_dV19Yyi1fS3", "review_text": "quantization is an important practical problem to address. the proposed method which quantizes a different random subset of weights during each forward is simple and interesting. the empirical results on roberta and efficientnetb3 are good, in particular, for int4 quantization. during the rebuttal, the authors further included quantization results on resnet which were suggested by the reviewers. this additional experiment is important for comparing this proposed approach with the existing methods which do not have quantization results on the models in this paper.", "accepted": 0}
{"paper_id": "iclr_2021_R0a0kFI3dJx", "review_text": "the paper introduces a new step size rule for the extragradientmirrorprox algorithm, building upon and improving the results of bach  levy for the deterministic convexconcave setups. the proposed adaptation of egmirrorprox  dubbed adaprox in the submitted paper  has the rate interpolation property, which means that it provides orderoptimal rates for both smooth and nonsmooth problems, without any knowledge of the problem class or the problem parameters for the input instance. the paper also demonstrates that the same algorithm can handle certain barrierbased problems, using regularizers based on the finsler metric. the consensus of the reviews was that the theory presented in the paper is solid and interesting. the main concerns shared by a subset of the reviews were regarding the practical usefulness of the proposed method. in particular, the method exhibits large constants in the convergence bounds and cannot handle stochastic setups. further, the empirical evidence provided in the paper was deemed insufficient to demonstrate the algorithms competitiveness on learning problems. if possible, the authors are advised to provide more convincing empirical results in a revised version, or, alternatively, to tone down the claims regarding the practical performance of the method.", "accepted": null}
{"paper_id": "iclr_2021_NTEz-6wysdb", "review_text": "the paper attempts to improve retrieval in open domain question answering systems, which is a very important problem. in this regards, the authors propose to utilize crossattention scores from a seq2seq reader models as signal for training retrieval systems. this approach overcomes typical low amount of labelled data available for retriever model. the reviewers reached a consensus that the proposed approach are interesting and novel. the proposed approach establish new stateoftheart performance on three qa datasets, although the improvements over previous methods are marginal. overall, reviewers agree that the paper will be beneficial to the community and thus i recommend an acceptance to iclr.", "accepted": null}
{"paper_id": "iclr_2021_tu29GQT0JFy", "review_text": "all the reviewers highlight that the paper addresses the important issue of extending deep latent variable models to handle missing non at random data, which are known to be very difficult. the authors suggest modeling the mechanism of missing values and perform inference using amortized importance weighted variational inference and demonstrate the capacities of their approach on many experiments. the paper highlight the tradeoff between the complexity of the data model and that of the missing data mechanism. the authors appropriately answer reviewers comments, add new experiments varying the percentage of missing values, and give more details on the methodological part. i also think that this is a valuable contribution to the community, that the literature is well covered the historical statistical litterature and the ml one, and that it provides new insights and methods to tackle this difficult problem.", "accepted": 0}
{"paper_id": "iclr_2021_NQbnPjPYaG6", "review_text": "this paper presents a series of negative results regarding the convergence of deterministic, reasonable algorithms in minmax games. the defining characteristic of such algorithms is that a the algorithms fixed points are critical points of the game; and b they avoid strict maxima from almost any initialization. the authors then construct a range of simple 2dimensional market games in which every reasonable algorithm fails to converge, from almost any initialization. the paper received three positive recommendations and one negative, with all reviewers indicating high confidence. after my own reading of the paper, i concur with the majority view that the papers message is an interesting one for the community and will likely attract interest in iclr. in more detail, i view the authors result as a cautionary tale, not unlike the neurips 2019 spotlight paper of vlatakisgkaragkounis et al, and a concurrent arxiv preprint by hsieh et al. 2020. in contrast to the type of cyclingrecurrence phenomena that are welldocumented in bilinear games and which can be resolved through the use of extragradient methods, the nonconvergence phenomena described by the authors of this paper appear to be considerably more resilient, as they apply to all reasonable algorithms. determining whether gans or other practical applications of minmax optimization can exhibit such phenomena is an important open question, and one which needs to be informed by a deeper understanding of the theory. i find this paper successful in this regard and i am happy to recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2021_ZK6vTvb84s", "review_text": "all reviewers agreed that the paper proposes some interesting and novel ideas on the use of ot for pooling. it also provides some nice insights and strong experimental results. as suggested by one of the reviewer, a discussion about the impact of the number of references may be of interest though.", "accepted": null}
{"paper_id": "iclr_2021_V8jrrnwGbuc", "review_text": "the paper offers novel insights about memorization, the process by which deep neural networks are able to learn examples with incorrect labels. the core insight is that late layers are responsible for memorization. the paper presents a thorough examination of this claim from different angles. the experiments involving rewinding late layers are especially innovative. the reviewers found the insights valuable and voted unanimously for accepting the paper. the sentiment is well summarized by r2 the findings of the paper are interesting. it shows the heterogeneity in layers and training stage of the neural net. i would like to bring to your attention the coherent gradients paper see also r1 comment. this and other related papers already discusses the effect of label permutation on the gradient norm. please make sure you discuss this related work. as a minor comment, please improve the resolution of all figures in the paper. in summary, it is my pleasure to recommend the acceptance of the paper. thank you for submitting your work to iclr, and please make sure you address all remarks of the reviewers in the cameraready version.", "accepted": null}
{"paper_id": "iclr_2021_Iz3zU3M316D", "review_text": "clarity the paper is wellwritten with illustrative figures. originality the originality of the paper is relatively restricted, mainly due to the resemblance with the work 1. however, there are important differences, that the authors nicely pointed out, and we encourage them to include these in the final version of the paper. significance the paper points out a relevant issue in using normalization techniques such as batch normalization together with momentumbased optimization algorithms in training deep neural networks. while the paper could be considered another algorithms for training nns, the papers illustrates nicely the main arguments, and is backed up with more than sufficient experimental results. main pros  in the main pros, ac and reviewers admit the phenomenal job in responding to reviewers questions and requests  the paper provides experimental results on various tasks and datasets to demonstrate the advantage of the proposed method.  after the reviews, the authors also reinforced their empirical investigation by reporting standard deviation of the results, which allows to better appreciate the performances of sgdp and adamp. finally, they also added the experiments with higher weight decay, showing that indeed 1e4 was the best value. main cons  one reviewer requires more explanation why the proposed update in equation 12 yields smaller norms w_t1 than the momentumbased update in equation 8.", "accepted": null}
{"paper_id": "iclr_2021_bjkX6Kzb5H", "review_text": "this article proposes a weakly supervised fewshot learning method for medical imaging segmentation. while initially, the article presented several problems indicated by the reviewers, e.g., the explanation of the novelty and contributions, the explanation of the method, and the experimental evaluation, the authors made a great effort addressing most of the reviewers comments and uploaded an updated version of the article. however, still, the evaluation part of the article is a bit weak. but the article contains interesting contributions. accordingly, i recommend accepting the paper at iclr2021.", "accepted": 0}
{"paper_id": "iclr_2021_0OlrLvrsHwQ", "review_text": "summary the authors observe that a range of laplaciantype operators used in graph neural networks can be embedded in a parametric family, so that the precise form of the laplacian used can be determined by the learning process. empirical evaluation and some limited theoretical analysis are provided. discussion the authors have provided detailed replies and also additional experiments. that has addressed major concerns, and most reviewers now agree the paper is good. one reviewer is more skeptical, mostly regarding presentation. i agree with some of the points raised in this regard, but see them as less of an issue  i would consider the presentation improvable, but acceptable. one weakness i should mention is that the two theorems provided are frankly trivial. i appreciate this is only a conference submission, but i would nonetheless call the fact that symmetric matrices have real eigenvalues theorem 1 an observation, not a result. that similarly holds for any direct consequence of gershgorins theorem theorem 2. the entire page used to state this could perhaps be put to better use for additional empirical results. recommendation the program committee the ac and program chairs were hesitating about this paper but decided to recommend acceptance. the idea is neat and simple, presentation and empirical evaluation are fine, if improvable we strongly recommend the authors to invest time. what is phrased as theory is trivial, but also admittedly not the main focus of the paper.", "accepted": null}
{"paper_id": "iclr_2021_blfSjHeFM_e", "review_text": "this paper introduced a new ode integration scheme that allows constantmemory gradient computation. i was concerned that the low order of convergence of this method would make it impractical, but the authors performed extensive experiments and got impressive results. overall the paper addresses one of the main practical difficulties with large neural ode models. the authors satisfactorily addressed the reviewers concerns in the discussion.", "accepted": null}
{"paper_id": "iclr_2021_A2gNouoXE7", "review_text": "this paper proposes a method for bilingual lexicon induction. the proposed method is efficient, it optimizes a reconstruction and transfer loss. extensive experiments are reported, and the methods provides improvements over prior work. overall, the paper brings together prior ideas in a useful way.", "accepted": 1}
{"paper_id": "iclr_2021_vcopnwZ7bC", "review_text": "this paper presents the ordermemory policy network ompn, an architecture for modelling a hierarchy of subtasks and discovering task decompositions from demonstration data. results are presented on a compositional gridworld task craft and on a simulated robotics task dial. the reviewers agree that the proposed method is novel and interesting, that the paper addresses an important problem, and that it is wellwritten. one main criticism by the reviewers, the lack of experimental evaluation of different hyperparameter choices, such as the depth of the memory stack and the expected number of subtasks, has to a large part already been addressed in the revision by the authors. the total number of hyperparameters that need to be tuned, however, is quite large and the authors are encouraged to revise their claim our central message is that ompn is a general offtheshelf model for task decompositions in this light. the paper is borderline, and could clearly benefit from a revised, stronger presentation and more extensive experimental evaluation, but i am confident that the authors can use the time until the cameraready version is due to address some of the remaining feedback by the reviewers, and hence i think that this paper can be accepted. the authors are further encouraged to take the following additional reviewer feedback into account, which was brought up during the internal discussion period 1 the complexity of the proposed method could be better justified by more thoroughly investigating the effectiveness of using a multilevel hierarchy e.g., by running experiments on more complicated and hierarchical tasks with multiple branches. 2 further strengthening downstream performance evaluation, such as in imitation learning in addition to the already presented behavioral cloning results andor reinforcement learning, would further strengthen the paper and demonstrate that the discovered decomposition is indeed useful.", "accepted": 0}
{"paper_id": "iclr_2021_BM---bH_RSh", "review_text": "existing works mostly focus on model compression for the classification task. this paper aims for an efficient recommendation system that can well balance the model compression and model accuracy, which therefore brings in new challenges and opportunities. the authors propose to unify the model compression and feature embedding compression and develop an effective and reasonable solution. the concerns raised by the reviewers have been well fixed and all reviewers agree on the papers contribution. the paper is therefore recommended for acceptance.", "accepted": null}
{"paper_id": "iclr_2021_uQfOy7LrlTR", "review_text": "dear authors, as you have noticed this paper was not easy to review. i have hence invited 2 additional reviewers which i strongly respect and are very knowledgeable. after carefully reading the paper myself, i have to agree with one of the reviewers who said ... it your paper makes a good contribution to the literature ..... to be honest, we were working in my group on a very similar approach but did not manage to finish it and i know how hard it is. to conclude, when preparing to the final version, please try to go over the reviews, i am sure they can make your paper even stronger", "accepted": 1}
{"paper_id": "iclr_2021_U_mat0b9iv", "review_text": "the authors present a new theoretical framework that establishes that any network can be approximated by pruning a polynomially larger random binary networks, and also an algorithm for pruning binary nets. the results are important in the general context of the strong lottery ticket hypothesis, and are of both theoretical and practical interest. although some of the ideas and technical contributions can be seen as a combination of prior tools and algorithms, the experimental findings are very novel. some further concerns of clarity and novelty were addressed by the authors.", "accepted": 0}
{"paper_id": "iclr_2021_rsogjAnYs4z", "review_text": "on the positive side, this is a quite nice empirical exploration of the interaction between data parallelism and sparsity for training neural networks. the experiments are broad and detailed. on the negative side, the empirical results recapitulate what would be expected and what has already been seen in the literature, as the authors themselves point out we note that our observation is consistent with the results of regular network training presented in shallue et al., 2019; zhang et al., 2019.. and the theory presented, while it does explain the results nicely, is a trivial reformulation of the standard convergence result given in equation 2. so while this is an interesting paper and the reviewers rated it positively on average, the sparsity exploration is _much_ more novel than the dataparallelism exploration, and there are significant novelty weaknesses that need to be taken into consideration.", "accepted": null}
{"paper_id": "iclr_2021_Iw4ZGwenbXf", "review_text": "the authors propose an intriguing alternative to ift or unrolled gd as a method for optimizing through arg min layers in a neural net, by using a differentiable samplingbased optimization approach. i found the general idea in the paper to be intriguing and thoughtprovoking. the reviewers generally seem to have also appreciated the method, and many of the reviewers concerns were addressed by the authors during the rebuttal. although the paper does have a number of flaws  in particular, the evaluation is a bit hard to appreciate, since improvement over prior work is either unclear, or no meaningful comparison is offered,  i think in this case the benefits outweigh the downsides. the work is far from perfect, but the ideas that are presented are interested and valuable to the community, and i think that iclr attendees will appreciate learning about this work. i would encourage the authors however to improve the paper, and especially the empirical evaluation, as much as possible for the cameraready, and to take reviewer comments into account insofar as feasible. im also not sure how much i buy the overfitting to hyperparameters argument for unrolled gd, and a less charitable interpretation is that the authors present this issue largely to make up for the comparative lack of other benefits. thats not necessarily a bad thing, but i think making such a big deal of it is a bit strange. its probably fair to say at this stage that the actual benefits of this approach are a bit modest though improvements in runtime are a good thing..., but the idea is interesting, and may spur future research.", "accepted": 0}
{"paper_id": "iclr_2021_3hGNqpI4WS", "review_text": "quality the algorithm is thoroughly evaluated and several interesting experiments are included in the appendix.  clarity the paper is generally well written.  originality the proposed approach is a small but novel improvement over existing algorithms to the best of the reviewers and my knowledge. the concept of deploymentefficiency is, in my opinion not novel, since it seems mostly a rebranding of what the mbrl community traditionally refers to as dataefficiency  although i agree that deploymentefficiency is indeed a more accurate term.  significance of this work the paper deal with a relevant and timely topic. however, the paper does not compare to the larger mbrl literature. hence, it is difficult to gauge the significance of this work.  overall this manuscript offers a good contribution to the topic of modelbased reinforcement learning algorithms.  minor comments  i suggest removing the word impressive from the abstract. this is a subjective term, which should be avoided.  in my personal opinion, it would be nice to include experiments with more stateoftheart baselines such as pets and poplin, for which code is available online. it is unclear to me how much the improvement in performance depends on the algorithm itself compared to just having larger batch sizes. from this perspective, figure 5 in appendix b is probably the most interesting insight of the manuscript, to me.", "accepted": 1}
{"paper_id": "iclr_2021_Vd7lCMvtLqg", "review_text": "this paper proposes a method to cope with large vocabulary sizes. the idea is to find a small number of anchor words and to express every other word as a sparse nonnegative linear combination of them. they give an endtoend method for training, and give a statistical interpretation of their algorithm as a bayesian nonparametric prior in particular an indian restaurant process. they give extensions that allow them to deduce the optimal number of anchors which allows them to avoid needing to tune this hyperparameter. finally they give a variety of experiments, particularly in language and recommendation tasks. the results on language are particularly impressive, and in the author response period, at the behest of a reviewer, they were able to extend the experiments to the amazon review dataset which contains 233m reviews on 43.5 m items by 15.2 m users. this paper is a nice combination of a simple but powerful idea, and a range of experiments demonstrating its utility. other papers have proposed related ideas, but here the main novelty is in 1 using a small number of anchors that can incorporate domain knowledge and 2 using a sparse linear transformation to express other words in this basis. one reviewer did not find the bayesian nonparametric interpretation to be fruitful, since it does not lead to techniques for handling growing datasets e.g. if the ideal number of anchors changes over time.", "accepted": null}
{"paper_id": "iclr_2021_aCgLmfhIy_f", "review_text": "this paper proposes a method for regularizing the pretraining of an embedding function for relation extraction from text that encourages wellformed clusters among the relation types. experiments on fewrel, semeval 2010 task 8, and a proposed fuzzyred dataset show that the proposed prototype method generally outperforms prior stateoftheart, including mtb soares et al., 2019, which was the strongest. the key, novel idea is to model prototype representations for target relations as part of the learning process. a contribution of the work is to show that learning prototype representations are useful in supervised deep learning architectures even beyond fewshot learning. this additional learning objective is useful as an inductive bias, and is perhaps of interest even beyond relation extraction research. reviewers generally found the proposed method sound and intuitive, and the original set of experiments promising. some of the reviewers raised concerns about the setup of the experiments, including the relationship between the pretraining and target tasks, and the need for several additional baselines. the authors were able to address these concerns, and the reviewers did not raise any followup concerns.", "accepted": 0}
{"paper_id": "iclr_2021_wQRlSUZ5V7B", "review_text": "this paper proposes a novel technique to learn a disentangled latent space using vaes and semisupervision. the technique is based on a careful specification of the joint distribution where the labels inform a factorisation of the distribution over continuous latent factors. the technique allows for inference, generation, and intervention in a tractable way. the paper is wellwritten, the formulation is original, and the experiments convincing. there were some confusions that were mostly resolved during the discussion. in addition to the expert reviews attached, i would like to remark that i too find the formulation interesting and elegant. and if i may add to the discussion, oivae outputinterpretable vaes by ainsworth et al presented at icml18 is a related piece of work that did not occur to me earlier, but which the authors could still relate to id certainly enjoy reading about the authors views on that line of work.", "accepted": null}
{"paper_id": "iclr_2021_gJYlaqL8i8", "review_text": "all reviewers agree that this paper is worth publishing. it investigates a novel idea on how to adaptively prioritise experiences from replay based on relative withinbatch importance. the empirical investigation is thorough, and while the performance improvements are not stunning, the benefit is surprisingly consistent across many environments.", "accepted": null}
{"paper_id": "iclr_2021_rkQuFUmUOg3", "review_text": "the authors proposed a meta learning framework for nas, namely metad2a meta datasettoarchitecture, that can stochastically generate graphs architectures from a given set dataset via a datasetarchitecture latent space learned with amortized metalearning. each dataset is encoded via a set encoder and the architecutres are obtained via a graph decoder. metad2a is trained once on a database consisting of datasets and pretrained networks and can rapidly search a neural architecture for a novel dataset. while the set encoder and graph decoder for nas have been introduced by existing work, the main contribution of the paper is to show that the metalearning of a datasetconditioned architecture generation framework can enable fast generation of a good architecture without training on the target dataset. the proposed method is interesting and effective, however it requires an existing pool of good architectures for a given task, which may limit its applicability to a diverse set of realworld problems. i strongly encourage the authors to include experiments on a larger pool of architectures than the nasbench201 search space to show the strength of their proposed method in generating good architectures. while training metad2a with pairs of metaimagenet and randomly sampled graph shows that the proposed framework can generate graphs with different types of edges, it doesnt show that it can successfully metalearn to produce better architectures for a new task from an existing pool of good architectures. we believe that many of the reviewers comments were addressed in the rebuttal, so while the scores are low, they do not reflect neither the contribution nor the reviewers opinion well e.g., r3, in his last post, seems to suggest that his review should be updated but it has not happened.", "accepted": 0}
{"paper_id": "iclr_2021_4qR3coiNaIv", "review_text": "the reviewers agree that the submitted paper is of high quality and provides a promising approachframework for bayesian irl. certain concerns regarding details of the implementation and evaluation have already been addressed by the authors during the rebuttal phase, and also the title of the paper was adjusted in line with discussions with the reviewers. for the final paper, the authors should make sure to clearly highlight the advances of inferring a distribution over rewards this is already partly done by the added grid world experiments and discuss relations to vaes as the initially had in mind and even in the paper title. beyond that, the should of course also address other reviewers comments.", "accepted": null}
{"paper_id": "iclr_2021_QO9-y8also-", "review_text": "this paper asks a simple question do extremeactivating synthetic images for a cnn unit help a human observer to predict that units response to natural images, compared with maximallyminimally activating natural images. they authors conducted welldesigned human studies and found that the synthetic images provide useful information for prediction, but that the benefit is smaller than that provided by simply presenting people with other natural images that maximally or minimally activate a unit. the paper provides one reasonable metric for evaluating feature visualizations. feature visualizations are widely used, but there are very few objective metrics for evaluating them. this methodological contribution is the main contribution of this work and could be impactful. although the conclusion is not very surprising, the paper makes a potentially good contribution to the literature.", "accepted": 1}
{"paper_id": "iclr_2021_kE3vd639uRW", "review_text": "the paper presents a bidirectional pooling layer inspired by the classical lifting scheme from signal processing. liftdownpool is able to preserve structure and details in different subbands, whereas liftuppool is able to generate a refined up sampled feature map using the detail subbands. this is very useful for image to image translation tasks and all tasks that involve up scaling. this is a solid contribution with extensive and thorough experiments and direct practical usage, clear accept.", "accepted": 1}
{"paper_id": "iclr_2021_Qun8fv4qSby", "review_text": "there is a substantial contribution in identifying novel questionsissues, as this paper certainly does. neither i nor the reviewers have seen this issue of transient nonstationary before, and the authors make a compelling case for it, especially in the supervised setting with the cifar experiments. it is less compelling through the rl experiments. as such, this paper is likely to inspire new work within the field. to me, figure 1 is the most interesting aspect of the whole paper. the initial approach by the authors is questionable in its effectiveness, and is likely to be improved by others in the future. some of the results in figure 3 are questionable, especially when you look at the individual curves in figure 8. so overall, this means that the authors have identified a truly novel issue, and proposed an initial method that is just okay. theyve done a nice job investigating this in a supervised setting, and need to push further in the rl setting. the question is whether the novel contribution of the problem outweighs that the algorithm and its evaluation could use improvement. the reviewers debated this in the discussion, with points on both sides, but the novelty of the questionissue even if the investigation could use work is likely to inspire further work in this direction. other notes the authors could have evaluated the impractical version of their algorithm proposed in the first paragraph of section 4.2. this would inform 1 whether their parallel training approximation is close to the optimal algorithm, and 2 whether the optimal impractical algorithm is capable of improving generalization significantly. if the latter is true, it would leave open a huge avenue of investigation to find better approximate solutions.", "accepted": 1}
{"paper_id": "iclr_2021_W3Wf_wKmqm9", "review_text": "this paper introduces clearning, an approach to integrate temporal abstractions to valuebased methods. specifically, it uses accessibility functions that estimate horizonaware value functions for goalreaching rl problems. such an approach allows tradingoff reliability and speed. after careful consideration im recommending the acceptance of this paper. the main weaknesses raised by the reviewers were addressed during the rebuttal, including the improvement of presentation and the introduction of new experiments and baselines. there were not many actionable criticisms left after the discussion and the reviewers acknowledged that the paper has improved since its first version. for the final version of the manuscript, i recommend the authors to further take r2s commentssuggestions into consideration. further incorporating the discussion about tdms in the main text will improve clarity, better position the paper, and increase its likelihood of having impact.", "accepted": null}
{"paper_id": "iclr_2021_qbH974jKUVy", "review_text": "the paper seeks to empirically study and highlight how disentanglement of latent representations relates to combinatorial generalization. in particular, the main argument is to show that models fail to perform combinatorial generalization or extrapolation while succeeding in other ways. this is a borderline paper. for empirical studies it is also less agreed upon in general where one should draw the line about sufficient coverage of experiments, i.e., the burden of proof for primarily empirically derived insights. the initial submission clearly did not meet the necessary standard as the analysis was based on a single dataset and studied only two methods vae and betavae. the revised version of the manuscript now includes additional experiments an additional dataset and two new methods, still offering largely consistent pattern of observations, raising the paper to its current borderline status. some questions remain about the new results esp the decoder.", "accepted": 1}
{"paper_id": "iclr_2021_AAes_3W-2z", "review_text": "this paper proposes a novel and interesting embedding of graphs emulating the wasserstein distance. the experiments are good and the authors did a detailed answer taking into account the comments of the reviewer. the responses were appreciated and the ac recommends the paper to be accepted.", "accepted": 1}
{"paper_id": "iclr_2021_w2mYg3d0eot", "review_text": "the paper proves new rates of convergence for stochastic subgradient under an interpolation condition. the analysis is rather simple but it produces better rates than previously known, which all reviewers agree is interesting. as pointed out by the reviewers, this work has the potential to help the community better understand optimization with overparametrized neural networks where convexity or other related assumptions play a role. to the authors, please add a citation to pegasos as requested by the reviewers.", "accepted": 1}
{"paper_id": "iclr_2021_J8_GttYLFgr", "review_text": "the paper proposes and studies a new so2equivariant convolution layer for vehicle and pedestrian trajectory prediction. the experiments are detailed and demonstrate the effectiveness of the approach in relation to nonequivariant models.", "accepted": null}
{"paper_id": "iclr_2021_chPj_I5KMHG", "review_text": "this paper presents a new approach to grounding languagebased rl tasks via an intermediate semantic representation, in an architecture called languagegoalbehavior lgb. the architecture permits learning a mapping from internal goals to behavior gb separately from learning a mapping from language to internal goals lg, and prior to flexibly combining all three lgb. the architecture is studied in a specific implementation called decstr. the architecture has multiple desired attributes including support for intrinsic motivation, decoupling skill acquisition from language grounding, and strategy switching. the experiments demonstrate the utility of different components in the architecture with a variety of ablation results. the reviews initially found the paper to be poorly organized with required content described only in the appendix r1, r2, r4, with unclear main contributions r1, r2, r4, and with results restricted to demonstrations r3. despite these reservations, the reviewers found the content to be potentially relevant though narrow in scope. the authors substantially revised the paper. they improved its organization, clarified contributions, separated the architecture from the specific examples, and improved the experimental baselines. after reading the revised paper, the reviewers agreed that the papers organization and insights were improved, making the new papers contribution and insight clear. the experimental baselines were also improved, providing more support for the potential utility of the proposed method. three reviewers indicate to accept this paper for its contribution of a novel approach to grounding language and behavior with an intermediate semantic representation. no substantial concerns were raised on the content of the revised paper. the paper is therefore accepted.", "accepted": 0}
{"paper_id": "iclr_2021_TVjLza1t4hI", "review_text": "the approach is novel and according to the reviewers comments addresses a relevant and important problem on eeg data analysis. differences to related work are discussed. methods and experimental results are sound. the authors have provided a comprehensive response to the reviews.", "accepted": null}
{"paper_id": "iclr_2021_v9hAX77--cZ", "review_text": "this paper proposes a model for predicting edits to trees given an edit specification that comes either from the ground truth beforeafter state gold setting, like reconstruction error of autoencoder or from the beforeafter state of an analogous edit. the problem setting follows mostly from yin et al 2019. there are several shortcomings of this paper 1. the technical novelty of the model is somewhat limited, as its an assembly of components that have been used in related work. authors insist in the discussion on the novelty of the tree edit encoder sec 3.2, but i think this is overstated. the related treeedit models e.g., tarlow et al 2019 perform a very similar encoding in the decoder when training with teacherforcing. while its true that decoders are typically thought of as monolithic entities that generate a sequence of edits from a state, inside the teacherforced training, the models are computing a representation of a prefix of ground truth edits, which are then repeatedly used to predict next edits. afaiu, the proposal is basically to use this hidden representation as the edit encoder. 2. the claim that the approach is more language agnostic than dinella et al 2020 also seems shaky, as the authors admit in their response that languagespecific grammars need to be handled specially. e.g., i expect that the authors of dinella et al would find it easier to extend their existing code to use a new language than to adapt this approach. 3. the submission relies too heavily on the gold setting where the target output is fed as an input, and im skeptical of their characterization of yin et als intentions when the authors say in comments, because of this, models that are able to reproduce the desired output effectively have a demonstrably better inductive bias that allows them to do so efficiently. this was the original motivation expressed by yin et al. 2019. i dont see this stated in the yin et al paper. i see yin et al. characterizing this setting as an upper bound and saying better performance with the goldstandard edit does not necessarily imply better more generalizable edit representation. yin et al., 2019. its worrying that the proposed model only seems to do better in this setting, which would be very easy to game if one were aiming to directly optimize for it. having said this, 1 is not a standard way to think about encoding edits, 2 is debatable, and we can hope that future work does not treat improvements in the gold setting as a valid research goal. further, there is another contribution around imitation learning that the reviewers appreciate. in total, reviewers did an excellent job and generally believe the paper should be accepted. i wont go against that recommendation.", "accepted": 1}
{"paper_id": "iclr_2021_lf7st0bJIA5", "review_text": "the paper proposes to do unsupervised discovery of 3d physical objects. the core idea is to decompose the scene into primitives that contain a a segment; b 3d position and dynamics; and c appearance. these are combined with a physics model and renderer to discover objectsprimitives by watching videos; the core supervisory signal used is that one should be able to reconstruct future scenes and that objectsprimitives ought to be physically consistent. the system is tested on synthetic data as well as real videos of blocks. the reviewers were positive about many aspects but, at the time of submission had a number of concerns. these were, in view of of many of the four reviewers, largely addressed. these are as follows  one overarching concern r3, r4 was the experiments that the papers title and motivation focused heavily on 3d but the experiments lacked a 3d experiment of any variety. the authors addressed this by adding 3d iou and recall. while numbers are low for iou, this is a challenging area and the ac appreciates this as did r3 and r4.  another concern is the data itself r4,r1. r4 in particular cites the synthetic nature of it as a stumbling block; r1 is similarly concerned about the difficulty of the backgrounds and the rigidity of the objects. the ac thinks that the data is sufficient for this paper given the overall paper focus, methodological contributions, and particular set of claims. however, the ac is highly sympathetic to r4s arguments and thinks more realistic real data beyond the additional data of towers of blocks in front of a white sheet would substantially improve the impact of the paper and the direction of research.  the last contentfocused concern was disagreement that the system is unsupervised r2,r4. the authors have addressed this with experiments using a hardcoded system that uses a heuristic based on the bottom coordinate, which obtains good results as well. all reviewers with this concern seem satisfied although the ac would note this assumes a single ground plane, which ties into concerns about the data although this is a small nitpick.  r2 had substantial concerns about the legibility and reproducibility of the paper. these have been largely addressed in the revision, as far as the ac can tell. the paper is an good contribution on a challenging and important problem. while the ac shares some of r4s concerns about the data and indeed how data difficulty and method interact, the ac finds the revised paper compelling and recommends acceptance.", "accepted": null}
{"paper_id": "iclr_2021_Qk-Wq5AIjpq", "review_text": "the paper provides a method for constructing pac confidence scores for pretrained deep learning classifiers. the reviewers were all positive about the paper. pros  has provable guarantees on the reliability of the prediction. such guarantees are quite desirable in practice.  the problem of neural network uncertainty is important and timely problem, especially in safetycritical applications.  the method is simple and wellmotivated.  strong empirical performance.  interesting applications to fast dnn inference and safe planning. cons  lack of generalization guarantees the guarantees in the paper only hold on the training set; but in practice, performance in test is whats important.  only a handful of baselines tested against, most of which if not all were naive.", "accepted": null}
{"paper_id": "iclr_2021_cu7IUiOhujH", "review_text": "this paper introduces supervised contrastive learning loss on top of typical crossentropy loss for finetuning language model for downstream tasks. while the idea is simple and has been used in vision literature as pointed out by r1  r4, its application lm is first introduced in this paper. the experimental gain is small in the regular setting but clearer gains in a fewshot learning setting and noisy training dataset through back translation setting. overall the paper is clearly written and experiments are carefully studied. during the discussion phase, the authors provided results on the full glue dataset as well as other ablation studies e.g., cece recommended by r2, improving the paper.", "accepted": null}
{"paper_id": "iclr_2021_Cz3dbFm5u-", "review_text": "the authors did a nice job of responding to the concerns of reviewers during the discussion phase which increased reviewer scores. because of this i will vote to accept. the authors should carefully edit the paper for typos, grammatical errors, and style errors. some examples  abstract make this one paragraph without a line break  end of 1st paragraph in intro so there is an urge  so there is an urgent  start of 3rd paragraph in intro stateoftheart cryptographic  the stateoftheart cryptographic  last paragraph of 2.1 to solve above  to solve the above  end of 2.3 compared to the lightweight instahide and texthide, mpc and he are of advantages in the security guarantees so far.  compared to the lightweight methods instahide and texthide, mpc and he provide much stronger security guarantees. i also urge the authors to please double check the reviewer comments when preparing a newer version to ensure all concerns are taken into account.", "accepted": null}
{"paper_id": "iclr_2021_c9-WeM-ceB", "review_text": "the reviewers all agreed on accepting this paper, stating that it makes a compelling point about the usefulness of saliency methods to diagnose generalization. the reviewers found that the experiments were a strong point and applauded the thorough hyperparameter tuning and reruns for statistical significance. one reviewer commented that the paper was too dense with information, so much so as to make it difficult to digest. however, overall this seems like an interesting paper that is relevant to the community and will hopefully foster some good discussion about the shortcomings and future directions of saliency methods.", "accepted": null}
{"paper_id": "iclr_2021_fw-BHZ1KjxJ", "review_text": "the paper proposes an approach to learn sparse embeddings for documentslabels which can be trained by using multiple gpus in parallel, and are more amenable to nearest neighbor search. the paper certainly seemed to have botched comparison to snrm and requires to fix the claims in section 5.1. but, the impressive performance on extreme classification tasks is quite convincing. also, reviewers in general are quite enthusiastic about the paper. so we would recommend the paper for acceptance, but authors certainly need to take comments of reviewers into account especially around baselines and comparison to snrm.", "accepted": 0}
{"paper_id": "iclr_2021_vYeQQ29Tbvx", "review_text": "the paper provides an astonishingly simple experiment the parameters in the network are fixed, but only the parameters in the batchnorm taking less than 1 of the total number of parameters are trained and also the last linear layer is trained. the resulting networks provide better accuracies than training a random subset of the network. another part of this work is the study of the effect of beta and gamma when doing full training. pros  all the reviewers agree this is an interesting and important observation.  contribution is clear and paper is wellwritten  in future, better understanding of different parameters may cons a concern has been raised by one of the reviewers that it is more like a technical report some previous work which studies the effect of gamma was not mentioned. i think, the most interesting part is training only beta and gamma. it will provide a ground for theoretical investigations of the properties of deep neural network models, and maybe lead to more efficient training algorithms.", "accepted": 1}
{"paper_id": "iclr_2021_VcB4QkSfyO", "review_text": "the paper is interested in the lipschitz constant estimation of deep equilibrium models. the estimation of this constant provides us the ability to certify classification decisions and understand robustness as well as has important bearings on the generalization ability of a neural network. overall a solid theoretical contribution with rigorous theory in a wellwritten paper.", "accepted": null}
{"paper_id": "iclr_2021_ehJqJQk9cw", "review_text": "the paper proposes a personalized federated learning method, which personalizes by computing a weighted combination of neighboring compatible models. reviewers uniformly liked the quality of writing and level of novelty, and agree on the relevance of the problem and solution. the solution was deemed creative and particularly impactful in the important case of heterogeneous data on each node, and experiments showed convincing improvements. the discussion between reviewers and authors was constructive and has lead to further improvements of the paper. slight concerns remained on privacy with all models stored on the server, and breath of personalized fl benchmarks used, but reviewers agreed the contributions overall are still significant enough. future work remains on the theory of the proposed model.", "accepted": null}
{"paper_id": "iclr_2021_vujTf_I8Kmc", "review_text": "this paper proposes a metalearning method that learns structured features based on constellation modules. exploiting object parts and their relationships is a promising direction for fewshot learning as anonreviewer3 described. the effectiveness of the proposed method is demonstrated with experiments using standard benchmark, and ablation study.", "accepted": null}
{"paper_id": "iclr_2021_-_Zp7r2-cGK", "review_text": "this paper proposes replacing the softmax of deep nns with a kernelbased gaussian mixture model, to allow for perclass multimodality. results show that the method is competitive with other output modifications such as the largemargin softmax. the two primary concerns of the reviewers were the lack of largescale image classification results and theoretical guarantees. the authors have added cifar100 results. moreover, the authors agree that theoretical results would be nice to have, but such results are nontrivial and likely require a pacbayes treatment. i find the method to be wellmotivated and that the paper demonstrates sufficient experimental rigor. given the popularity of the softmax throughout deep learning, this paper will likely be of interestor at least, be of potential useto a large part of the iclr community. i encourage the authors to add the imagenet results to the final version.", "accepted": 1}
{"paper_id": "iclr_2021_AJY3fGPF1DC", "review_text": "this paper considers the problem of identification of causal effects under the unsupervised domain adaptation setting. the authors assume the invariance of the causal structure and use it to regularize the predictor of causal effects. the method is interesting and looks effective, although this assumption may not hold always true e.g., in some domains, some causal influences may disappear, leading to extra conditional independence relations. hope the authors will update the paper to address the concerns raised by the reviewers, especially to conduct a sensitivity analysis of the framework to misspecification of the causal structure and make the motivation for the used evaluation metrics clear, and also provide a more thorough review of related work.", "accepted": 1}
{"paper_id": "iclr_2021_b6BdrqTnFs7", "review_text": "this paper proposes an approach to training language instruction following agents that aims to improve their compositional generalization., by means of an entropy regularization method to reduce redundant dependency on input. all four expert reviewers agreed that the paper is not ready for publication in its current form. of biggest concern is the fact that the reviewers could not interpret the exposition of the method, so were unable to be sure exactly how the method worked. this can be addressed in a future submission by clearer presentation. another concern was that the authors only consider a single benchmark, and fail to situate the work relative to other grounded language learning tasks and datasets. thus, reviewers were concerned about the generality of the method, and suspected it may be too specific to the gscan setup. that said, the reviewers were all impressed by the strong results on the gscan benchmark. it strikes me that there is some interesting insight here that can be derived from this impressive performance, that may also be applicable to other grounded language learning settings. however, to make the paper acceptable for publication the authors must do a much better job of communicating how their method works, what that specific insight is and how it is relevant beyond the gscan dataset ideally via direct experimentation in other settings.", "accepted": 0}
{"paper_id": "iclr_2021_a7gkBG1m6e", "review_text": "thank you for your submission to iclr. overall the reviewers and i think that this paper presents some nice contributions to the adversarial attacks literature, demonstrating a lowsamplecomplexity, physicallyrealizable attack in a domain of clear importance and interest in machine learning. the move to considering more in the loop adversarial examples is particularly compelling, and the threat model and improvement over bo methods are both compelling here. the main downside of this paper, of course, is the fact that the physical adversarial examples are of course nothing of the sort they are simulated. rather, they are just simulated in a manner that may plausibly be slightly more amenable to realworld deployment. the authors claim that they dont carry out an evaluation on a real system because it is dangerous is a bit overly dramatic the tests could easily be carried out in a controlled environment, and demonstration on an actual physical system even, e.g., and rc car would vastly improve the impact of this work. as it is, the paper is borderline, but ultimately slightly below the high bar set by iclr publications. i would strongly encourage the authors to reconsider the inclusion of the word physical in the title, as it honestly sets expectations high for a promise that the paper cannot deliver on, or even better to run real experiments on even a small physical system, demonstrating the transferability there. the paper ultimately has the potential for a high impact in this field, if these issues are addressed.", "accepted": null}
{"paper_id": "iclr_2021_ToWi1RjuEr8", "review_text": "this paper aims to develop a simple yet efficient deep rl algorithm for offpolicy rl. the proposed method uses advantages to as weight in regression, which is an extension of the known method of rewardweighted regression. the paper is in general nicely written, and it comes with a set of theoretical analyses and experiments. while all reviewers admit that the approach is interesting and the work makes an attempt to solve an important yet open problem, there are several aspects of the paper that make it not ready for publication in its current form  novelty as pointed out by reviewers, the proposed method appears to be a minor modification of existing offpolicy solvers. although the use of advantages as weights makes intuitive sense, it is unclear why and how the new method significantly differs from and outperforms existing methods. going forward, it would be helpful if the authors could present more convincing argumentsexperiments to demonstrate the power of arw, relative to similar existing methods.  experiments provide some insights into the difference between several algorithms, but the results are not strong enough to support the claim of the paper. please see reviewers comments for more details. we strongly recommend the authors to take these comments into consideration and develop more rigorous experiments to demonstrate advantages of awr.  theoretical analysis is limited. as r2, r3 mentioned, the theory analysis in the paper seems to not match the algorithm, and there remain bugs, this it doesnt add to the paper. although theory might not be the focus of the paper, if the authors decide into include theoretical analysis, the analysis would hopefully provide insights into why and by how much the approach is better.", "accepted": 0}
{"paper_id": "iclr_2021_YtgKRmhAojv", "review_text": "this paper proposes to use a single parametric householder reflection to represent orthogonal weight matrices. it demonstrates that this is sufficient provided that we make the reflection direction a function of the input vector. it is also demonstrated under which conditions this modified transformation is invertible. the derivations are sound. this insight allows for cheaper forwarding of the model but it also comes with extra costs it has an increased computational cost for inversion e.g. requires optimisation and, importantly, it does not allow to cache the od matrix so it is not clear there is an advantage of the method over exp maps when we have parameter sharing e.g. as in rnns, since the action of the matrix has to be recomputed everytime. the presented experiments are ok, but comparisons to other potentially more efficient methods are lacking as pointed out by the reviewers. as it stands it is not clear that this is an idea of broad interest, perhaps more suited to a specialised venue such as a workshop.", "accepted": 0}
{"paper_id": "iclr_2021_GNv-TyWu3PY", "review_text": "the paper proposes an algorithm with sublinear regret for the problem of routing users through a network with unknown congestion functions over an infinite time horizon. the reviewers generally appreciated the main contribution of this work. one of the reviewers also felt that, although it may be possible to obtain the main result using more standard techniques, it is not clear whether doing so is an easy extension of the prior work. following the discussion, all of the reviewers agreed that the paper missed important related work and it needs a major revision that incorporates the extensive feedback of reviewer 2. for these reasons, i recommend reject.", "accepted": 1}
{"paper_id": "iclr_2021_tY38nwwdCDa", "review_text": "the paper introduces an augmentation technique that, given an image with a detected object, keeps the object and removes the background. the reviewers expressed numerous valid concerns about the papers novelty, the setting assumption that theres a single object, the scalability of the approach and the experimental setup, including the baselines used. the authors have not addressed these concerns.", "accepted": 0}
{"paper_id": "iclr_2021_io-EI8C0q6A", "review_text": "this work mainly applies wav2vec 2.0 to multilingual speech recognition and lacks of novelty. the various pretraining and finetuning mixmatch are specific to the speech recognition task. as suggested by reviewers, it is recommended to resubmit to a speech conference. also the paper lacks comparisons to sota on one of the well studied task i.e. babel in the speech field. the main factor for the decision is lack of novelty.", "accepted": 0}
{"paper_id": "iclr_2021_-yo2vfTt_Cg", "review_text": "the paper considers adaptive stochastic optimization methods and shows that they can be reinterpreted as first order trust region methods with an ellipsoidal trust region, they consider a related second order method, and they show convergence properties and empirical results. the results are of interest, but the significance of some of the results is not clear. part of this has to do with substance, and part of this has to do with presentation that can be improved. empirical results are weak, including appropriate baselines and details of the empirical results.", "accepted": 0}
{"paper_id": "iclr_2021_IrofNLZuWF", "review_text": "the paper studies the problem of stochastic optimization where the gradient noise process is nonstationary. while this is an important problem in the community, the reviewers find that the assumptions are poorly justified. while the authors provided extensive feedback, the reviewers did not change their initial assessment. this paper can therefore not be accepted in its current form. i think the reviewers provided some very critical and useful feedback and i therefore strongly encourage the authors to take advantage of this feedback to resubmit their paper to another venue.", "accepted": 0}
{"paper_id": "iclr_2021_vNw0Gzw8oki", "review_text": "the paper presents a framework for incorporating physics knowledge through, potentially incomplete, differential equations into the deep kernel learning approach of wilson et al. the reviewers found the paper addresses an important problem and presents good results. however, one of the main issues raised by r1 is that, although the proposed method can be applied to broader settings such as that of incomplete differential equations, there are still regimes where the comparison is not only possible but perhaps insightful. an example baseline is the work of lorenzi and filippone, constraining the dynamics of deep probabilistic models icml, 2018. another critical issue, raised by r4, is the insufficient clarity in the presentation. many of the concerns raised by this reviewer were clarified in the discussion and i thank the authors for their engagement. however, the ac believes some of the points raised by r4 in this regard were left unaddressed in the paper and the manuscript does indeed require at least one more iteration. the format violation concerns raised during the reviewing process did not affect the decision on this paper, as the pcs confirmed that they did not meet the bar for desk rejection and recommended to assess the paper on its technical merits.", "accepted": 1}
{"paper_id": "iclr_2021_iMKvxHlrZb3", "review_text": "this paper proposed an extension of the sign model as an efficient and scalable solution to handle prediction problems on heterogeneous graphs with multiple edge types. the approach is quite simple 1 sample subsets of edge types, then construct graphs with these subsets of edge types and 2 compute node features on each such graph as if they have only a single edge type, 3 then aggregate the representations from multiple graphs into one using an attention mechanism, and 4 train mlps on node representations as in sign. results show that such a simple method can produce quite good results, and is very efficient and scalable. the reviewers of this paper put it on the borderline, with 3 out of 4 leaning toward rejection. the most common criticism is the lack of novelty. indeed this paper is an extension of prior work sign, and the proposed approach is simple. however, i personally think the simplicity and the great empirical results is rather the strength of this paper. the authors also did a good job addressing reviewers comments and concerns in the discussions, but a few reviewers unfortunately didnt actively engage in the process. id really encourage the authors to improve and highlight the strength of this paper more and submit to the next venue.", "accepted": 0}
{"paper_id": "iclr_2021_ES9cpVTyLL", "review_text": "the reviews are concerned about the noveltyincremental nature of the paper and partially also about the conclusions drawn from the experiments. the authors did not take the chance to write a response.", "accepted": 0}
{"paper_id": "iclr_2021_2nm0fGwWBMr", "review_text": "although the paper is clearly written overall and well motivated, reviewers raised several crucial concerns and, unfortunately, the authors did not respond to reviews. during the discussion, reviewers agree with that this submission is not ready for publication. in particular, empirical evaluation is not thorough as important baselines are not included and discussion is not convincing. i will therefore reject the paper. for future submission, i strongly recommend the authors to do author response. there are many cases where the reviewers change their scores based on the interaction between the authors and the reviewers, which is healthy for the review process.", "accepted": 0}
{"paper_id": "iclr_2021_sgNhTKrZjaT", "review_text": "due to uniformly unfavourable reviews and lack of author engagement in the discussion period, this paper is rejected.", "accepted": 0}
{"paper_id": "iclr_2021_Kao09W-oe8", "review_text": "the paper begins with an observation in standard trained cnns that the correlations in the output channels are high. building upon this the paper proposes a new optimizer which modifies the gradients to encourage corelations among output channels. they provide a theoretical foundation for the method, by deriving the gradient through placing a riemannian metric on the manifold of parameter tensors which encourages smoothness along the output channel dimension. two variants one based on a sobolev metric are proposed and are experiments are provided. the underlying idea and the derivation of the gradients were generally appreciated by the reviewers. however some reviewers maintained their concern regarding the effectiveness of the performed experimentation. the gains demonstrated are relatively small over the baselines and more importantly the baselines are quite far off the state of the art baselines for the particular problems. this is the primary reason for my recommendation as experiments are the only source of understanding whether the method is effective there is little theory  mostly at an intuitive level to justify the form of the optimizer. overall, i strongly encourage the authors to explore the idea further and strengthen the paper with stronger baselines perhaps on larger datasets and resubmit.", "accepted": 0}
{"paper_id": "iclr_2021_6X_32jLUaDg", "review_text": "this paper proposed an unsupervised domain adaptation method for 3d lidarbased object detection. four reviewers provided detailed reviews 3 rated marginally above acceptance threshold, and 1 rated ok but not good enough  rejection. the reviewers appreciated simple yet effective idea, the well motivated method, the comprehensiveness of the experiments, and well written paper. however, major concerns are also raised regarding the core technical contributions on the proposed approach. the acs look at the paper, the review, the rebuttal, and the discussion. given the concerns on the core technical contributions, the high competitiveness of the iclr field, and the lack of enthusiastic endorsements from reviewers, the acs believe this work is not ready to be accepted to iclr yet and hence a rejection decision is recommended.", "accepted": 0}
{"paper_id": "iclr_2021_8EGmvcCVrmZ", "review_text": "the paper proposes to introduce ideas from singular theory to deep learning. all reviewers agree that the work is not yet ready for publication. the key issue seems to boil down to the fact that the paper does not propose nor verify any clearly motivated scientific hypothesis. relatedly, the work includes many too broad or unscientific claims such as to understand why classical measures of capacity fail to say anything meaningful about dnns. such statements should be given more precisely and with a proper citation. based on this i have to recommend rejecting the paper. at the same time, i would like to thank the authors for submitting the work for consideration to iclr. i hope the feedback will be useful for improving the work.", "accepted": 0}
{"paper_id": "iclr_2021_M_eaMB2DOxw", "review_text": "the paper received reviews from experts in representation of invariant functions. they all have expressed concerns regarding the novelty of the technical contributions, and the lack of appropriate comparisons to existing results. this applies in particular to representation of symmetric functions using neural networks which was largely covered by previous works, as acknowledged by the authors. the authors are encouraged to consider the valuable inputs by the reviewers and revise accordingly.", "accepted": 0}
{"paper_id": "iclr_2021_oXQxan1BWgU", "review_text": "the paper proposes a variant of maml for metalearning on tasks with a hierarchical tree structure. the proposed algorithm is evaluated on synthetic datasets, and it compares favorably to maml. the reviewers identified several significant weaknesses, including 1 the experimental evaluation is limited, and it only includes small synthetic datasets; 2 the proposed algorithm is incremental over maml. the reviewers agreed that the paper cannot be accepted in its current form. i recommend reject.", "accepted": 0}
{"paper_id": "iclr_2021_3FkrodAXdk", "review_text": "this work studies statistics of ensemble models that capture the prediction diversity between ensemble members. the goal of the work is to identify or construct a metric which is predictive of the holdout accuracy achieved by the ensemble prediction. pros  studies empirically how measures of ensemble diversity relate to ensemble prediction accuracy.  proposes improvements to diversity metrics that correlate better with accuracy. cons  unclearconfusing presentation.  limited empirical validation that relies mostly on cifar10 results to justify claims  some claims made trend between ensemble diversity and accuracy, q diversity capturing not capturing negative correlations are not substantiated. all reviewers recommend this paper to be rejected and the authors did not reply to any reviews.", "accepted": 0}
{"paper_id": "iclr_2021_99M-4QlinPr", "review_text": "this paper investigate an interesting problem of multiagent rl with selfplay. we agree with the reviewers that the paper requires more work before it can be presented at a top conference. we would encourage the authors to use the reviewers feedback to improve the paper and resubmit to one of the upcoming conferences.", "accepted": 0}
{"paper_id": "iclr_2021_6IVdytR2W90", "review_text": "this submission proposes an approach for fusing representations at multiple scales to improve object detection systems. reviewers thought the paper was wellwritten and showed positive results on coco, a common object detection benchmark. however, reviewers agreed that there was not sufficient methodological novelty or empirical improvement over existing approaches to warrant acceptance at iclr several prior works have addressed multiscale fusion and reviewers did not find the evaluationablations sufficient to demonstrate the approach yielded substantial improvements over these existing approaches. i hope the authors will consider resubmitting the paper after refining it based on the reviewers feedback.", "accepted": 0}
{"paper_id": "iclr_2021_vY0bnzBBvtr", "review_text": "this paper explores the performance of qlearning in the presence of either onesided feedback or full feedback. such feedbacks play an important role in improving the resulting regret bounds, which are almost not affected by the dimension of the state and action space. the motivation of such feedback settings stems from problems like inventory control. however, the assumptions underlying the theory herein are often quite strong, which might limit the applicability of the theory. the dependency on the length per episode h can also be improved.", "accepted": 0}
{"paper_id": "iclr_2021_yfKOB5CO5dY", "review_text": "the paper presents a pacbayesian approach for metalearning that utilizes information of the task distribution in the prior. the presented localized approach allows the authors to derive an algorithm directly from the bound  this is a worthwhile contribution. nevertheless there are several concerns that were raised by the reviewers and in its current form the work is not ready to appear in iclr.", "accepted": null}
{"paper_id": "iclr_2021_6FsCHsZ66Fp", "review_text": "in this paper, the authors propose a theoretically principled neural network that inherently resists \u2113 perturbations without the help of adversarial training. although the authors insist to focus on the novel design with comprehensive theoretical supports, the reviewers still concern the insufficient empirical evaluations despite the novel idea and theoretical analysis.", "accepted": 0}
{"paper_id": "iclr_2021__adSMszz_g9", "review_text": "this paper introduces a new model, called memformer, that combines the strength of transformer networks and recurrent neural networks. while the reviewers found the idea interesting, they also raised issues regarding the experimental section. in particular, they found the results unconvincing, because of weak baselines, non standard experimental settings eg. using reporting perplexity results on bpe tokens, or evaluating on only one dataset. these concerns were not well addressed by the rebuttal. for these reasons, i recommend to reject the paper.", "accepted": 0}
{"paper_id": "iclr_2021_B9t708KMr9d", "review_text": "this paper proposes a semisupervised graph classification technique that unifies feature and label propagation techniques. the resulting algorithm is a simple extension that attains strong performance. reviewers were divided on this submission. some reviewers felt the proposed algorithm did not constitute a sufficient technical contribution given that it was a simple combination of existing techniques. i tend to agree with other reviewers that the simplicity is a benefit. however, despite the methods simplicity there was significant confusion about the details of the method and multiple reviewers flagged that the paper was difficult to read and understand. it further could benefit from additional discussion and some clarificationcleanup of the experimental results. finally, multiple reviewers asked for better situating of the proposed method with respect to prior work. given these concerns, i do not think the paper is ready for publication. i would recommend the reviewers do a thorough rewrite of the paper to address these concerns and consider resubmitting.", "accepted": 0}
{"paper_id": "iclr_2021_O1pkU_4yWEt", "review_text": "this paper tackles an important problem and includes experiments on a new domain russian documents vs english documents. unfortunately, all reviewers agree that this paper lacks novelty for publication in its current state. additional details and clarifications to the proposed approach, notably through a more thorough performance analysis, would improve the significance of the paper.", "accepted": 0}
{"paper_id": "iclr_2021_jN8TTVCgOqf", "review_text": "the paper considers using local spectral graph clustering methods such at the pprnibble method for graph neural networks. these local spectral methods are widely used in social networks, and understanding neural networks from them is interesting. in many ways, the results are interesting and novel, and they deserve to be more widely known, but there are several directions to make the work more useful to the community. these are outlined in the reviewer comments, which the authors answered partially but not completely satisfactorily. much of this has to do with explaining howwhere these these very fundamental and ubiquitous methods are useful in a particular application gnns here, and node embeddings below. an example of a paper that successfully did this is lasagne locality and structure aware graph node embedding, e. faerman, et al. proc. 2018 conference on web intelligence. that is mentioned not since it is directly relevant to this paper, but since it provides an example of how to present the use of a method such as pprnibble for the community.", "accepted": 0}
{"paper_id": "iclr_2021_nuwy7R_kemM", "review_text": "this paper  adheres to the bayesian interpretation of mc dropout and applies it to transformerbased nmt, thus approximately sampling from the nmt models posterior predictive distribution y_x_, mathcal d  as the nmt predictive distribution is over a discrete sample space, the authors compute variance of pairwise comparisons between the translation and other candidate outputs in a beam of likely translations the authors call this bleuvar. whereas the work is potentially interesting it does not seem ripe for publication. here are some of the issues id like to highlight 1. ood detection. detection in input space seems like a natural baseline. the authors argue that ood detection in output space takes the downstream task into consideration, but going through the conditional also makes the task considerably more difficult and computationally challenging. though we appreciate the authors point, we dont see it as a good enough reason to discard ood detection in input space as a serious alternative. 2. why bdl? the motivation for bayesian methods is clear, but bdl can at best approximate bayesian reasoning, thus the question does deserve an answer. the reviewers asked for experiments that demonstrate empirically the relevance of the bayesian formulation, for example, one reviewer suggested to compute bleuvar in the frequentist case, and that makes perfect sense. consider this qtheta likely underestimates posterior uncertainty, so lets say that operatornamevarthetamathcal d is rather small, then bleuvar as presented is in fact not capturing posterior predictive uncertainty due to entropy of y_x_, mathcal d, but rather sampling uncertainty due to entropy of y_x_, theta. 3. unrealistic experiments we all agreed that the experiments are weak. for example, we do not share the authors excitement for the results around a foreign language as an example of ood data point, we see it as an artificially simple case. we also expected more interesting cases of mixed domain data sets for ideas, check tasks within wmt and iwslt, as well as resources such as opus and lowresource language pairs as those in flores and more generally different levels of noise e.g., synthetic data produced by other translation engines, roundtripbacktranslations are very typical in lowresource settings. additional remarkssuggestions  in my personal view, bleuvar should not be based on biased statistics beam search introduces all sorts of unknown biases; the pairwise comparison mechanism behind bleuvar is similar to what mt researchers call minimum bayes risk decoding a frequentist criterion for making decisions under uncertainty.  we do believe the setting explored in this paper is related to confidence estimation, and even though i agree with the authors that a direct comparison is not per se needed, ce datasets could still prove useful for evaluation; though the paper has been appreciated for it dispenses with quality annotation, for it attempts to quantify estimation uncertainty or epistemic, if the authors prefer rather than sampling uncertainty or aleatoric, and for other technical contributions such as bleuvar, we think this paper needs more than subtlecareful positioning, it really needs to acknowledge the relevance of certain alternatives and evaluate against them bdl need not win every comparison, thats not so much the issue, the issue is that the current picture is too incomplete. a final personal remark. i noticed the exchange regarding the suitability of the paper to an ml vs nlp venue. i personally do not think your submission is more or less appropriate to one or the other on the grounds of its technical content. the expert reviews attached suggest enough ideas for improvements, and i would imagine an improved version of the paper having a good chance at any major ml or nlp venue.", "accepted": 0}
{"paper_id": "iclr_2021_o7YTArVXdEW", "review_text": "this paper addresses automatically learning the neighborhood size they call adaptive neighbor support for unsupervised representation learning with a vae. the neighborhood size is determined based on zscores from by estimating a normal distribution in the latent space. the paper is poorly written. there are several grammatical errors and typos that distracts from understanding the paper. in addition, the use of terminology is not precise, which adds to the confusion, as pointed out by the reviewers. acvae is better than vaeknn in table 1 but worse in scan with knn in table 3. further analysis to understand why this is so is needed. additional measures of cluster quality is recommended. as pointed out by the reviewers, this paper is below the acceptance threshold for iclr. the reviewers provided several constructive suggestions. please refer to detailed reviewer comments to help you improve your paper.", "accepted": 0}
{"paper_id": "iclr_2021_o6ndFLB1DST", "review_text": "there is a general consensus on the fact that the paper is not yet ready for publication. i encourage the authors to carefully address the detailed concerns raised by the reviewers, which include among other i the incompleteness of the literature overview, which should include the references provided by the reviewers, ii poor or bias towards the proposed approach experimental evaluation, and iii a vague treatment of key terms in the interpretability literature like feasibility e.g., to make sure that the counterfactual lie in high data density regions.", "accepted": 0}
{"paper_id": "iclr_2021_86PW5gch8VZ", "review_text": "the reviewers have a strong consensus towards rejection here, and i agree with this consensus , although i think some of the reviewers concerns are misplaced. for example, the paper does not appear to use a magnitude upper bound that would be vacuous together with a strong convexity assumption although variance bounds  strong convexity do cover only a small fraction of strongly convex learning tasks, these assumptions arent vacuous. some feedback i have that perhaps was not covered by the reviewers pros  studying the setting where the number of bits varies dynamically is very interesting although, as reviewer 3 points out, not entirely novel. there is significant possibility for improvement from this method, and your theory seems to back this up. cons  the experimental setup is weak, and is measuring the wrong thing. when we run sgd to train a model, what we really care about is when the training finishes the total wall clock time to train on some system. for compression methods with fixed compression rates, its fine to use the number of bits transmitted as a proxy, because when the number of bits transmitted is uniform over time this will be monotonic in the wallclock time. however, when the bits transmitted per iteration can change over time, this can have a difficulttopredict effect on the wallclock time, because of the potential for overlap between communication and computation where below a certain number of bits sent, the system is not communicationbound. wallclock time experiments comparing against other more modern compression methods would significantly improve this paper.", "accepted": 0}
{"paper_id": "iclr_2021_gZ2qq0oPvJR", "review_text": "this paper describes an application of reinforcement learning to theorem proving in the connection tableau calculus. the paper does a reasonable job in the application of rl techniques and the high level issues are important. however, as the reviewers note, there is little connection to the notion of analogy outside of the very general idea that rl methods learn to generalize to novel situations. i did not find the methods very original as it seems a somewhat mechanical application of rl methods. that would be fine if the empirical results were convincing or surprising. however, i found the robinson arithmetic domains not very interesting as the problems were literally arithmetic, as in 25  7, rather than theorems such as the commutativity of addition. the empirical results were not as convincing in the tptp domains where mcts seemed to dominate. also there are related papers in the area of deep learning applied to theorem proving that i believe dominate this paper learning to reason in large theories and an inequality benchmark.", "accepted": 1}
{"paper_id": "iclr_2021_PI_CwQparl_", "review_text": "this is a clear reject. none of the reviewers supports publication of this work. the concerns of the reviewers are largely valid.", "accepted": 0}
{"paper_id": "iclr_2021_W75l6XMzLq", "review_text": "this paper extends the idea of hindsight experience replay her to learn q functions with relative goals by constructing a distribution over relative goals sampled from a replay buffer using a clustering algorithm. this approach is evaluated on three multigoal rl environments and is shown to learn faster than baselines. bf pros 1. faster convergence as compared to baselines 2. interesting use of clustering in the context of her but this choice is made without strong justifications or formal arguments bf cons 1. some of the key choices made in this paper are not justified or explained property, e.g.  the goal sampling strategy, choices made in the clustering algorithm and associated heuristics, implicit assumptions e.g. r1 raised the question of using l2 distance in measuring metrics between two states 2. there are several choices made without sufficient formal arguments, verification or guarantees. the paper studies an interesting problem but could be made stronger by incorporating feedback received during the discussion period.", "accepted": 0}
{"paper_id": "iclr_2021_8mVSD0ETOXl", "review_text": "all four referees have indicated reject. severe points of criticism have been raised, concerning the lacking novelty, the experimental setup and the significance and interpretation of results. i fully agree with the reviewers in all important points, so i recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2021_aJLjjpi0Vty", "review_text": "this paper mostly received negative scores. a few reviewers pointed out that the idea of modeling user preference in the frequency domain seems novel and interesting. however, there are a few concerns around the clarity of the paper, the motivation of the proposed approach, as well as the experimental results being unconvincing both in terms of execution as well as exploration of the results. the authors did not provide a response. therefore, i recommend reject.", "accepted": 0}
{"paper_id": "iclr_2021_ijVgDcvLmZ", "review_text": "although the paper presents some interesting ideas, in general the reviewers agree that the paper lacks clear results and is not an easy read. the paper proposes a factorisation of value functions, a topic that has received quite some attention in the literature e.g. qplex, and it seems that their is not sufficient innovation in the proposed method in the paper. there are also a number of claims in the paper e.g. partial observability etc. with which some of the reviewers disagree, and should be discussed more carefully in a revised version of the article, that all in all seems to need more work.", "accepted": 0}
{"paper_id": "iclr_2021_-aThAo4b1zn", "review_text": "this paper proposed to theoretically explain why a pretrained embedding network with selfsupervised training ssl can provide representation for downstream fewshot learning fsl tasks. the review process finds that the paper may overclaim the results and that the results seem unsatisfactory. both reviewer 4 and reviewer 5 expressed concerns regarding the writing, organizing, and grammar errors of this paper. the paper needs a substantial revision to improve clarity and accessibility. as pointed out by nikunj saunshis public comment, this paper may benefit from discussing the differences from the previous works, including 1. 1 arora et al., a theoretical analysis of contrastive unsupervised representation learning, icml 2019", "accepted": 0}
{"paper_id": "iclr_2021_oev4KdikGjy", "review_text": "the paper analyzes the space of mixed sample data augmentation approaches, and proposes a new variant, fmix, based on a new masking strategy. reviewers point to the fact that fmix is only marginally better than previous approaches, that the experimental setup is unconvincing, and that the proposed analysis might not be grounded. this is a really borderline paper but i see the issues as more important than the benefits, so i recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2021_5IqTrksw9S", "review_text": "this paper proposes a new source code modeling benchmark, with the unique twist being that we not only have code source text, but we also have build information, which allows extracting richer information to construct labels from. this enables, for example, a null pointer prediction task with labels coming from an interprocedural static analysis tool. ac and reviewers agree that this is a valuable framing for a benchmark suite. unfortunately, its not clear that the benchmark in its current form delivers on the promise of the framing. much of the interest and novelty is limited to just the one nulltoken task, and reviewers raise a number of concerns including dataset size and whether the task truly measures the interprocedural reasoning that it sets out to measure. anonreviewer2 raised some good questions here that the authors promised to address in a forthcoming comment, but that didnt come before the discussion deadline. id encourage the authors to use the reviewer suggestions to more strongly establish that these tasks measure what they set out to measure, and also to consider adding other tasks that measure whether our ml models are capable of deeper  longerrange reasoning. in total, there is a lot of potential here, but the work needs another iteration before its ready for publication.", "accepted": 0}
{"paper_id": "iclr_2021_r1j4zl5HsDj", "review_text": "the authors present an adaptive model that learns a good policy by adversarial training, focusing on the setting where the query budget is very small. some experiments are carried out to validate the proposed method. the reviewers opinions turned out to be split on this paper. on one hand, all reviewers appreciated the idea of the problem and recognized its importance. on the other hand, there are have been multiple concerns regarding readability but that has improved during the discussion and about the empirical validationevaluation. based on the above, as well as my own reading, i believe this paper contains interesting ideas but, as it currently stands, is not ready for publication.", "accepted": 0}
{"paper_id": "iclr_2021_dnKsslWzLNY", "review_text": "this paper provides approximation results for functions that can be represented by hybrid quantumclassical circuits. it is felt that venues such as qip would be a more suitable venue, and perhaps some experimentssimulations could be added.", "accepted": 0}
{"paper_id": "iclr_2021_NGBY716p1VR", "review_text": "this paper first investigates the behavior e.g., catastrophic overfitting of fast adversarial training fastadv through experiments. it finds that the key to its success is the ability to recover from overfitting to weak attacks. then, it presents a simple fix fastadv that incorporates pgd adversarial training when catastrophic overfitting is observed. the resulting method is shown to be able to train for a large number of epochs. it also presents a version fastadvw that use the improved fast adversarial training as a warmup of pdgadversarial training, similar as in previous work. overall, the analysis is useful and the ideas are valid. the empirical results also show promise. however, the main weakness of such empirical analysis is that it may be sensitive to the settings e.g.,  of epochs, splitting of datasets, . the authors rebuttal also reflected such potential concerns.", "accepted": null}
{"paper_id": "iclr_2021_lEZIPgMIB1", "review_text": "we thank the authors and reviewers for engaging in a detailed and constructive discussion, and providing a revised version of the paper after the initial round of reviews. regarding quality, the work is technically correct and the amount of experiments significant. however, as highlighted by reviewers 2 and 3, some important questions remain unanswered, in particular 1 more empirical evidence to support the claim that the umap loss is a relevant for neural networks, and 2 more comparison with existing approaches beyond tsne. regarding clarity, the paper is overall clear and pleasant to read. however, after the revision round, all details about the proposed methods have been moved to the annex. while the initial version was criticized for the opposite reason all experiments were in a annex, the balance may not be found yet; e.g., the equation for the umap loss, which is at the core of the paper, would certainly find its place in the main part of the manuscript for an iclr paper. the originality is the weakest aspect of the paper besides the lack of comparison with related work. as mentioned by several reviewers, plugging the umap loss to a differentiable model is nowadays an idea that lacks originality. what would be important to justify that such a straightforward idea makes it to iclr would be to demonstrate convincingly that it outperforms existing alternative approaches. finally, regarding the significance of the work, it is limited by the lack of thorough comparison with existing method. on the other hand, if the method is implemented in a fast and easytouse package, it may find its public as illustrated by the positive evaluation of reviewer 1 from a potential user point of view.", "accepted": 1}
{"paper_id": "iclr_2021_qk0FE399OJ", "review_text": "the paper analyzes the behavior of random searchbased nas and provided new insights e.g., a low ranking correlation among top20 candidate architectures in the search phase. an extensive set of experiments were also conducted. however, most reviewers found the incremental nature and similarity with previous works to be a concern. i would encourage the authors to better position their work and better explain the novel methodological aspects.", "accepted": 0}
{"paper_id": "iclr_2021_wOI9hqkvu_", "review_text": "this paper proposes gantraining of a nonautoregressive generator for text. to circumvent the usual problems with nondifferentiability of text gans, the authors turn to gumbelsoftmax parameterisation and straightthrough estimation. there are a number of aspects to this submission and they are not always clearly positioned. i will concentrate on the two aspects that seem most crucial 1. the authors position their generator as an implicit generator, but it really isnt. if we take the continuous interpretation of the output distributions the gumbelsoftmax transformation does correspond to a tractable density, the concrete density of maddison et al, with known parameter. if we take the discrete interpretation of the output distribution gumbelargmax is just an alternative to sampling from a categorical distribution with known parameter. in either case, the generator maps the noise source to a collection of conditionally independent distributions each of which has a known parameter and analytical densitymass function. the authors do, however, train the architecture using a gantype objective as if the generator were implicit. 2. in the discussion phase the authors added that gan training overcomes the independence assumptions made by the generator. whereas that makes intuitive sense, it suddenly changes the emphasis of the contributions, from proposing an implicit generator presumably powerful for it being implicit to proposing a way to circumvent the strong independence assumptions of the generator with a mechanism other than more traditional approximate marginalisation of vaes. in their rebuttal, the authors commented on the use of nonautoregressive vaes in neural machine translation, and though those observations have indeed been made, they might well be specific to mt. the simplest and more satisfactory response would be to ablate the use of the gan objective that is, to train a nonautoregressive vae, also note that, with the same choice of likelihood, posterior collapse is rather unlikely to happen. other problems raised by reviewers were addressed in the rebuttal, and i would like to thank the authors for that. for example, ablating the nonautoregressive generator and comparing to reinforce. i believe these improved the submission. still, i cannot recommend this version for publication. i would suggest that the authors consider careful ablations of the components they see as precisely important for the results that currently seems to be the ganlike objective despite the model not, strictly speaking, requiring it.", "accepted": null}
{"paper_id": "iclr_2021_KcImcc3j-qS", "review_text": "this paper proposes a mechanism for fast sampling from the posterior over the weights of the last layer of neural network, by approximating the logits as a gaussian through equation 8. this is based on earlier work by mackay, but has some new empirical investigations. this is a very difficult case. in its favor  despite the main ideas coming from older work by mackay, they are interesting and relevant and worth resurfacing.  the experiments demonstrate some improvements in ood detection over a diagonal laplace approximation to the last layer, and is competitive in performance with a kfac laplace last layer approximation but much faster at testtime.  the authors provided early and thoughtful responses and actively tried to have a discussion with reviewers. it is a pity that the reviewers did not participate in this discussion. concerns  while interesting, it is unclear if the proposed method actually has much practical utility in its current form. the method is presented as a fast approach for uncertainty in bayesian deep networks. but eq. 8 requires such significant computations to form that whatever is gained by the fast sampling may not makeup for the cost of forming the approximation itself. this computational burden is why the approach is only applied to a last layer. table 2 and some of the surrounding discussion helps with alleviating these concerns and is most appreciated. but many basic questions persist i do we really need many samples to achieve good performance, especially from a posterior over only a last layer? we see the kl divergence decreases, but what about performance on interesting problem as a function of sample size? ii in terms of total runtimeaccuracy would this be competitive with using bayesian methods over all the parameters, even if these methods are taking fewer samples? it would be easy to try. iii besides ood detection, how does this approach generally affect accuracy or calibration? iv how would this method compare to a basic baseline like retraining the last layer several times and ensembling? v could anything be done to significantly accelerate the computations in forming eq. 8? while not all of the answers to these questions need to be favorable to the laplace bridge for acceptance, it would certainly improve the paper to at least address most of the questions explicitly. at the end of the paper, an online setting is mentioned, which i think would be amenable to this approach  it could be good to explore this direction.  it is disappointing that the reviewers did not communicate with the authors, despite commendable efforts from the authors. however, the paper continued to lack a clear champion. given the persisting lukewarm reception of reviewers, and some of the practical concerns above, it would help to have some standout result, especially since the methodology, while interesting and relevant, is not new. that does raise some expectations for the experiments. this is not an easy case. the paper has merits. and its possible some of the concerns could be addressed by simply more clearly rationalizing design decisions why would we use this approach in its current form over full bayesian methods, which are now quite fast, with fewer posterior samples?. at the same time its clear the paper in its current form is not resonating with reviewers, and there are concerns about the practical applicability and limitations. its on the borderline. having some standout results could really help this paper realize its potential.", "accepted": 0}
{"paper_id": "iclr_2021_fm58XfadSTF", "review_text": "in this paper, the authors proposed a largemarginbased domain adaptation method for crossdomain sentiment analysis. the idea of developing a largemarginbased method for domain adaptation is not new. though the proposed method contains some new ideas, the difference between the proposed method and the existing largemargin based methods needs to be discussed and studied empirically. in addition, the experimental results are not convincing some related baselines are missing and experiments need to be conducted on more datasets. though the authors did provide long responses to each reviewer, after a lot of discussions, the reviewers still find that their concerns are not well addressed. therefore, this paper is not ready to be published in iclr based on its current shape.", "accepted": null}
{"paper_id": "iclr_2021_bG_lJcLwE3p", "review_text": "the reviews are a bit mixed. while all the reviewers feel that the paper proposed an interesting mechanism to train conditional generators from a single image and demonstrated good image editing results in the experiments, there are also common concerns about the practicality of the proposed method for interactive image editing. all the reviewers asked for the computation time, and some expressed the concerns about technical contributions. while these concerns were somewhat addressed in the rebuttal, the ac feels that its a hard sell to bet on the dramatic increase of computational capacity to make the computing time from an hour to realtime. concerns about novelty also remained. given the drawbacks, the final decision was to not accept. however, this work is promising and can be made stronger for publication in a later venue.", "accepted": null}
{"paper_id": "iclr_2021_LzhEvTWpzH", "review_text": "all reviewers agreed to reject.", "accepted": 0}
{"paper_id": "iclr_2021_4K_NaDAHc0d", "review_text": "this paper considers multitask rl from the perspective of an unsupervised clustering of different tasks with an emlike algorithm. the idea is evaluated on several simple and atari domains. we thank the reviewers for their detailed responses and revision. this work still seems a little preliminary in its current form. while the empirical results seem promising, it is generally felt that it would benefit from more extensive experiments, including further comparisons to other approaches and exploring the effects of the hyperparameters on tasks with much larger numbers of clusters. it would also be beneficial to provide some theoretical results, particularly with respect to negative transfer.", "accepted": null}
{"paper_id": "iclr_2021_uRuGNovS11", "review_text": "reviewers have commented on the lack of novelty of the paper as it reads only as applying the variational inference framework of blundell et al. 2015 to deep metric learning r2 and r4. furthermore, the paper has not properly positioned itself when compared to previous works on deep variational metric learning and deep adversarial metric learning r1 and other previous literature that have studied robustness for metric learning. the argument on robustness to noisy labels needs to be expanded and better fleshed out in a future version of the paper.", "accepted": 0}
{"paper_id": "iclr_2021_9wHe4F-lpp", "review_text": "description the paper proposes an improvement to binary neural networks with realvalued skip connections between preactivations, by introducing more flexible learnable nonlinearities on the realvalued connections. the parametric nonlinearity is actually linear at initialization, which makes the training easier at the beginning. due to learnable parameters it eventually adjusts to a more complex one, able to refine the accuracy. i think this idea is a good finding.  review process and decision the reviewers initially gave low ratings to the paper, indicating that the contribution is incremental and not fully clearly presented. there was no detailed discussion with the authors, since the authors response and the rebuttal revision came in the very end of the discussion period. in the subsequent discussion phase the reviewer board has not indicated any major changes to the initial reviewsranking. the ac checked the paper and supports rejection.  details the authors are encouraged to improve the paper carefully addressing points proposed by reviewers. i think the argumentation of the paper should be improved. some explanations are intuitive, but operating with fuzzy notions and may in fact be incorrect or irrelevant. the paper should be made more precise, based on verifiable arguments. i think the following is crucial and not made clear in the paper the nonlinearities inserted before the sign function do not affect the result of sign. they indeed affect only the residual connections. furthermore, the structure of residual connections should be fully clarified to reveal that there are complete realvalued paths all the way from the input to the network to its output, made of the residual connections with their own learnable parameters and 1x1 convolutions and learnable nonlinearities and an intake from binary convolutions on the way. the learnable nonlinearities can in principle improve performance just because the realvalued paths can learn better. i paste below feedback by reviewers to authors response i believe they would agree to share it with authors but did not find a suitable way of doing it  response by r1 i acknowledge that i read and appreciated the authors answers to my questions. i think the idea of analyzing the role of nonlinearities is nice and i tend to confirm my score. but i also agree with other reviewers that, as it is, the paper has some unclear parts and would not complain if it is rejected.  response by r3 thanks for your responses to answer my questions for the paper. i agree with the results of the proposed fbtn for improved binary neural networks bnns. however, my concern about the novelty of using group convolution modules in bnns has not been addressed. i think the paper is not sufficient enough to publish at the conference. so, i do not change my rating of the paper.  response by r4 i maintained my rating when combining other reviews and responses to them, despite of their well response. it is still questionable whether fprelu, one of the main contributions they claimed, actually improves the performance of bnn remarkably. in particular, this is supported by the fact that the performance of bnn on resnet34 which the techniques in this paper were applied does not show much difference from realtobin model.", "accepted": 0}
{"paper_id": "iclr_2021_o2N6AYOp31", "review_text": "this metareview is written after considering the reviews, the authors responses, the discussion, and the paper itself. the paper proposes a training scheme for autoencoders, involving data augmentation and interpolation, that results in autoencoders for which interpolations in the latent space lead to meaningful interpolations in the image space. the paper notices that this property carries over reasonably well to datasets different from the training one. the reviewers point out that the idea is interesting r1, r2, r4 and simple r2, but the experiments are substandard r2, r4 and presentation is at times suboptimal r1. overall consensus is towards rejection. authors addressed some of the concerns in their responses, but failed to convince the reviewers to change their evaluations. i agree with the reviewers and recommend rejection at this point. the idea is indeed interesting and could be publishable if presented and evaluated well, but in the current manuscript the presentation is at times unclear or somewhat misleading e.g. presenting the method as a general image generation method, not an interpolation method and the experiments are reasonable, but not quite convincing, mainly because the architectures and the baselines are outdated as also pointed out by r1 and r4. i encourage the authors to further improve the paper and resubmit to a different venue.", "accepted": 0}
{"paper_id": "iclr_2021_rI3RMgDkZqJ", "review_text": "all reviewers appreciated the main result in the paper, which gives global optimality guarantee for constrained policy optimization for both tabular setting and ntk setting. however, there were a number of unclear parts of the paper reported by several reviewers assumptions, hyperparameter tuning, complexity dependence on the number of neurons, experimental setups. on top of it, the ac also echoes with r1s concern about the novelty of this work as it basically stacks existing results td by dalal et al., neural td by cai et al. 2019, npg by agarwal et al, csa algorithm by lan  zhou. these concerns made me reticent to recommend acceptance at this point. i strongly encourage the authors to continue their interesting work in considering the reviewer comments and strengthen the numerical experiments.", "accepted": null}
{"paper_id": "iclr_2021_Ip195saXqIX", "review_text": "the paper received four negative reviews. the overall idea was found to be interesting, but several concerns were raised. there is a general consensus that the experimental part and the results are not convincing. several comments have also been made regarding the clarity and motivation, which needs to be strengthened. r4 also mentions references from the sparse estimation literature that would help for positioning the paper. the rebuttal did address some of these points, but it was not sufficient to change their opinion. overall, the area chair agrees with the reviewers and follows their recommendation.", "accepted": 0}
{"paper_id": "iclr_2021_a5KvtsZ14ev", "review_text": "the paper received 5 reviews, one of which had positive feedback. although there are merits associated with the paper, several concerns raised in the reviews and the discussion period that prevents the paper to be accepted. it appears that experiments on noisy graphs are not properly done and competitive baselines are not used for validations. the quality of the learned graph structure is not adequately analyzed. and the experimental setup was not clearly explained. all these indicate that there is a need for a major revision before the paper can be considered for acceptance.", "accepted": null}
{"paper_id": "iclr_2021_wMIdpzTmnct", "review_text": "the paper investigates several properties of adversarial examples obtained by hardlabel attacks. there are some interesting findings in this paper, such as the connection between query efficiency and distance to the image manifold. however, all the reviewers think the paper is below the acceptance threshold due to several weaknesses, including insufficient experiments, clarity, and whether the observations made in the paper can benefit query efficiency or quality of hardlabel attacks.", "accepted": 0}
{"paper_id": "iclr_2021_HWqv5Pm3E3", "review_text": "this submission develops a novel technique for domain adaptation for the setup where only a trained model but no data from the source task is available. the authors propose to finetune the feature encoder using batch norm statistics of the features extracted. additionally their criterion also promotes increasing the the mututal information between features and target classification. the developed method is experimentally evaluated on several benchmarks. pros  the problem considered is of practical relevance and general interest in iclr community  the proposed methodology is well motivated and shows good performance cons  there is no thorough formal analysis of when the method would work and not work; not even on an intuitive level state conditions under which the proposed method should be expected to work betterworse than other state of the art optimization criteria for the same setup  alternatively to a sound theoretical analysis, the authors should provide a more extensive set of ablation experiments this was mentioned by several reviewers in the current format, it remains unclear, how the research community would benefit from the study presented.", "accepted": 0}
{"paper_id": "iclr_2021_7ZJPhriEdRQ", "review_text": "this paper is about learning the output noise variance of a vae and its effect on the generated image quality as measured by fid. the paper argues that the output variance parameter plays an important role and proposes a simple procedure, where a maximum likelihood estimate of the noise variance is estimated. experiments on some standard datasets are provided. overall, the paper is well written and has been perceived positively by the reviewers. however, the effect of observation variance has been in detail analysed by earlier work, in particular dai and wipf 2019. the novelty of the current paper is somewhat limited in scope. the paper is somewhat borderline in these respect; a much stronger experimental section would have been helpful. one key contribution of the work is empirical comparison of alternative parametrizations of the output noise. overall, the paper would be stronger if this aspect is analysed more in detail, possibly with careful comparisons with competing methods. inclusion of controlled experiments e.g. by adding extra noise to data to show how precise the noise variance estimation and how the procedure influences the convergence of other parameters would have made the paper much more impactful.", "accepted": 0}
{"paper_id": "iclr_2021_0xdQXkz69x9", "review_text": "this paper presents a method for attacking fewshot learners with poisoning a subset of support set. i believe this might be the first work to address adversarial examples for metalearners or fewshot learners, which is a timely issue. a common concern raised by most of reviewers is in the novelty of this work, in the sense that the method builds on a basic attack strategy such as pgd in the standard adversarial example setting. authors responded to this, summarizing whats new in this paper. episodic training for fewshot learners requires consuming support set instead of single training data point. it is a nature of most metalearning methods. thus, it is easily expected that the adversarial attack for fewshot learners is naturally extended to poisoning a support set or its subset instead of a single data point. certainly such extension may entail a new strategy. however, during the discussion period with reviewers, concerns on the novelty of such extension still remains. in particular, the fewshot learning algorithms do not allow big changes in the original model. the algorithms analyzed are prototypical networks that do not utilize finetuning, and maml that finetunes for a small number of prefixed steps. so the transfer of adversarial samples may not be counted as a major contribution.", "accepted": 0}
{"paper_id": "iclr_2021_L3iGqaCTWS9", "review_text": "four knowledgeable referees reviewed this paper; one reviewer weakly supports accept and other three indicate reject. even with the rebuttal, all negative reviewers have concerns on the limited novelty and marginal performance improvement, and agree that the paper is not well qualified for the high standard of iclr.", "accepted": 0}
{"paper_id": "iclr_2021_3JI45wPuReY", "review_text": "this work proposes a framework to search for the topology of an artificial neural network jointly with the network training, via a genetic algorithm that can decide structural actions, such as addition or removal of neurons and layers. an extra heuristic based on bayesian information criterion helps the optimization process decide on its decisions about the topology. they demonstrate improvements over baseline fullyconnected networks on svhn and augmented cifar10. reviewers and myself agree that this is an interesting idea, and that the paper is easy to follow. while i may not agree that we need to achieve sota on these datasets, or see large scale imagenettype experiments for novel ideas, i agree with the reviewers, esp r1s point that the current experiments are not satisfactory to meet the bar for acceptance at iclr. cifar10 and svhn are wellestablished tasks, and showing baseline accuracy of 7548 on them respectively doesnt seem to do them justice, especially when most methods even with low compute requirements can get  95 on both, for the past few years. for this work to be of interest to the broader community, it needs to be improved to incorporate at least respectable baselines on these small datasets, and perhaps be improved to work beyond fully connected networks. at this stage, we need to see a revision of the method and see improvements before an acceptance decision can be made.", "accepted": 0}
{"paper_id": "iclr_2021_PAsd7_vP4_", "review_text": "this paper is rejected. the authors contributions are  propose pfpn as an expressive action policy for continuous action spaces.  introduce a reparameterization trick for training pfpn with offpolicy methods.  experiments claiming pfpn outperforms unimodal gaussian policies and a uniform discretization scheme and that it is more sample efficient and stable across different training trials. i and the reviewers appreciate the additions by the authors. the gmm baseline is an important addition addressing concerns from several reviewers. however, i agree with r2s comment that most interesting contribution of the paper is the resampling scheme. however, there is minimal evaluation of the benefit of this scheme ... however, the added experiments with random sampling are somewhat worryingthe performance improvement of the proposed resampling scheme is quiet minor over random resampling. in the future, the authors may want to investigate the random resampling for the systems in figure 14. without resampling, the proposed method is a locationscale stateindependent gmm policy. it is interesting that this outperforms the fully statedependent gmm, and the authors could investigate that further. to justify the additional complexity of the resampling step, the authors need to perform further investigation and move that to the main text. in addition, the evaluated environments omit standard openai gym environments which the authors do have access to as evidenced by their experiments w ddpg on them in the appendix. this makes evaluating baseline method performance challenging. furthermore, the authors cite tang  agrawal 2018 which introduces a normalizing flow policy that outperforms gmm. it would be natural to compare to that baseline. finally, figurnov et al. 2018 among others shows how reparameterization gradients can be computed through gmms. the authors should explain why this is not applicable.", "accepted": 0}
{"paper_id": "iclr_2021_M3NDrHEGyyO", "review_text": "the paper is about a reinforcement learning algorithm that operates in a constrained mdp and is provided with a baseline policy. although the reviewers acknowledge that the paper has some merits wellwritten, clearly organized, significant empirical evaluation, reproducible experimental results, some concerns have been raised about the novelty of the proposed solution and of its theoretical analysis. the reviewers feel that the authors responses have not properly addressed all their doubts. the paper is borderline and i think that it is not ready for publication in the current form. i encourage the authors to update their paper following the reviewers suggestions and try to submit it in one of the forthcoming machine learning conferences.", "accepted": null}
{"paper_id": "iclr_2021_uVnhiRaW3J", "review_text": "the reviewers appreciate the importance of enforcing safety in rl, and the technical directions considered in the paper related to incorporating cost in advantage estimation. however, they express several concerns about the formulation of the problem considered and the consistency of the approach, as well as the somewhat incremental contribution w.r.t. cpo. three reviewers recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2021_kB8DkEKSDH", "review_text": "the reviewer concerns generally centered around the novelty of replacing the distance metric for a policy constraint. while the authors clarified many of the reviewer concerns and added some additional comparisons, in the end it was not clear why the proposed approach was interesting while it is true that this particular distance metric has not been evaluated in prior work, and the result would have been interesting if it resulted in some clear benefits either empirically or theoretically, in the absence of clear and unambiguous benefit, its not clear how valuable this concept really is. after discussion, the reviewers generally found the paper to not be ready for publication in its present state.", "accepted": 0}
{"paper_id": "iclr_2021_Rhl8IoYzdSI", "review_text": "the paper proposes to model uncertainty by combining quantile regression and chebyshev polynomial approximation. the paper addresses the important problem of uncertainty quantification for black box models. however, some major concerns remain after the discussion among the reviewers. in particular, there has been some concerns around the clarity of the presentation. the proposal lacks a clear use case, e.g. where satisfying constrained blackbox uncertainty problem is a musthave.", "accepted": 0}
{"paper_id": "iclr_2021_0fqoSxXBwI6", "review_text": "the authors present a method for selfsupervised learning of representations of 2d projections of 3d objects. by performing known 3d transformations of an object of interest, a encoderdecoder network is trained to estimate the applied transformation from a series of 2d projections. the proposed method is used as a regularizer and experiments are performed on supervised 3d object classification and retrieval. after seeing each others reviews, one of the main concerns from the reviewers was the relationship between the proposed method and zhang et al., cvpr 2019 i.e. aet. the two methods are conceptually very similar, and the consensus from the reviewers is that the authors did not acknowledge the overlap sufficiently and also did not provide a convincing argument as to why they think the approaches are different. in their rebuttal the authors provided some additional results on real data which is a valuable and welcome addition. however, there were still other concerns that the reviewers had e.g. r2 wanted to know why the model could not be applied directly to 3d shapes instead of 2d projections. given the above concerns specifically the relationship to aet, there is currently not enough support for accepting the paper in its current form. the authors have received detailed feedback and are encouraged to take it onboard when revising the paper in future.", "accepted": 0}
{"paper_id": "iclr_2021_oY7La6DBTLx", "review_text": "this paper explores the robustness of oneclass classifiers to geometric transformations at test time. the authors observe that some existing methods fail to detect novel images from the same class when they have undergone specific transformations at test time i.e. inplane rotations. in contrast, it is suggested that humans have no difficulty in ignoring the impact of these types of transformations. to address this issue, the authors propose to take the maximum prediction over the set of rotated versions of a given test image. the current consensus from reviewers, and this metareviewer agrees with this view, is that the paper, while not without some merit, is too narrow in focus to be of general interest in its current form. the main contribution is limited to one family of transformations, and it is not immediately clear how to generalize this to others when the entire transformation space is not easily enumerated. there are also legitimate concerns regarding if the specific issue outlined is likely to be a problem in practice see r1s comments. the authors allude to some interesting negative results related to data augmentation in their response to r4 r2 also had questions about this. the authors should consider adding these results to a future revision of the paper as it will strengthen the central message. in conclusion given the limited support, this ac also agrees that the paper is not yet ready for publication at iclr.", "accepted": 0}
{"paper_id": "iclr_2021_qiAxL3Xqx1o", "review_text": "in this paper, the authors proposed a geometric graph generator that applies a wgan model for efficient geometric interpretation. all the reviewers agree that the idea is interesting and the method has the potentials for graph generation tasks. unfortunately, the experimental part is unsatisfying, which makes the paper on the borderline. more analytic experiments should be designed to verify the properties of the proposed gggan, especially its scalability. although in the rebuttal phase the authors add a simple example to generate large but simple graphs, we would like to see more experiments and comparisons on more realworld large graphs even if the performance may not be good, the results will be constructive for both readers and authors to understand the work.", "accepted": null}
{"paper_id": "iclr_2021_Dmpi13JiqcX", "review_text": "this paper explores a methodology for learning disentangled representations using a triplet loss to find subnetworks within a transformer. the authors compare against several other methods and find that their method performs well without needing to train from scratch. the reviewers thought this paper was well written and the authors were very responsive during the review period. however, there were some questions about the experimental setup and empirical performance of the paper, leaving the reviewers wondering if the performance was convincing. we agree that there is value in exploring disentangled representations even if they do not necessarily improve performance as the authors point out, but clearly explaining the reasoning behind all analyses e.g. specifically choosing domains to introduce a spurious correlation, and justifying differences in performance is particularly important in these cases.", "accepted": null}
{"paper_id": "iclr_2021_eBHq5irt-tk", "review_text": "the authors restate mackays definition of effective dimensionality and describe its connections to posterior contraction in bayesian neural networks, model selection, widthdepth tradeoffs, double descent, and functional diversity in loss surfaces. the authors claim the effective dimensionality leads to a richer understanding of the interplay between parameters and functions in deep neural networks models. in their experiments the authors show that effective dimensionality compares favourably to alternative norm and flatness based generalization measures. strengths 1  the authors include a description of how to compute a scalable approximation to the effective dimensionality using the lanczos algorithm and hessian vector products. 2  the authors include some novel experimental results showing the effective dimensionality with respect to changes in width and depth. these results are informative in how changes in depth and width affect this metric in a different way. the same for the experiments with the double descent curve. weaknesses 1  for some reason the authors seem to have taken the concept of effective dimensionality from david mackays approximation to the model evidence in neural networks and ignored all the extra terms in such approximation. it is currently unclear why there is a need to do this and focus only on the effective dimensionality. almost all the experiments that the authors describe could have been done using a similar approximation to mackays model evidence. it is unclear why is there a need to focus just on a part of mackays approximation. the fact that the authors state that the effective dimensionality is only meaningful for models with low train loss seems indicative that david mackays approximation to the model evidence would be a better metric. 2  with the exception of the experiments for changes in the effective dimensionality as a function of the depth and width and the double descent curve, all the other experiments and results are expected and not new to anyone familiar with david mackays work. 3  the experiments on depth and width are for only one dataset and may not be representative in general. the authors should consider other additional datasets. the authors should improve the paper, including a justification for using only the effective dimensionality and not david mackays approximation to the model evidence. they should also strengthen the experiments by comparing with david mackays approximation to the model evidence and should consider additional datasets as mentioned above.", "accepted": 0}
{"paper_id": "iclr_2021_dvSExzhjG9D", "review_text": "the paper proposed a metalearning method for tuning the learning rate. in the discussion, reviewers agreed that the key issue is that the empirical evaluation is not yet sufficient to demonstrate the efficacy of the method. in particular, this is an especially pressing issue given that there are now many metalearning methods for tuning the learning rate none popular in practice though, and the paper does not compare to any of them. relatedly, most reviewers found that the novelty of the method is not clearly established and discussed in the paper. based on the above, i have to recommend rejecting the paper. i would like to thank the authors for submitting the work for consideration to iclr. i hope the feedback will be useful for improving the work.", "accepted": 0}
{"paper_id": "iclr_2021_XkI_ggnfLZ4", "review_text": "this paper explores the role of hyperparameters in the separate phases of a classic pruning pipeline mask identification and retraining. key observations include a set of the hyperparameters to search relative to a standard regime as well as the identification that the layerwise pruning rates from mask finding are intertwined with these hyperparameters and are what chiefly affects the eventual performance of the pruned network. the pros of this paper are that it works against the contemporary wisdom that the default hyperparameters for a model are the best for finding a mask for the model. instead, there are improvements to be had by identifying a set of hyperparameters that lead to worse overall model accuracy, but better masks. second, the work shows that the layerwise pruning rates are the key elements of these hyperparameters effect. the rates can in fact be transferred to more poorly performing network configurations and improve performance. the cons of this paper, as noted by the reviewers, are the somewhat unclear implications of the technique. the added guidance on directions to improve hyperparameters is valuable but does not necessarily provide a costeffect strategy to find these. at its strongest, this guidance offers practitioners a recommendation to also consider hyperparameters for the initial model. the stronger, forwardlooking implication is, instead, the connection to layerwise pruning rates. specifically, while layerwise pruning rates have been demonstrated to be important in the literature e.g., 1, there has been a limited study into the exact nature of a good set of pruning rates versus a bad set of pruning rates. where this paper stops short of a clear result, is if were to connect excessive pruning of the earlier layers, or simply the layerwise rates themselves to another property of the network e.g., gradient flow, or capacity that indicates the improved eventual performance. my recommendation is to reject. the papers core experiments are wellexecuted. however, this final detail, closing the gap between the portability of these layerwise rates and a conceptual understanding, is a key missing component. once done, that will make for a very strong paper. 1 amc automl for model compression and acceleration on mobile devices. yihui he, ji lin, zhijian liu, hanrui wang, lijia li, song han. eecv, 2018", "accepted": 0}
{"paper_id": "iclr_2021_yT7-k6Q6gda", "review_text": "this is a tricky one, hence my low confidence rating. the reviewers seem to agree that the paper is well written, easy to follow, and that it tests a relevant hypothesis that is of interest to the community. there was some disagreement as to whether the experiments are comprehensive, complete andor conclusive enough, although on balance it seems reviewers were overall satisfied barring a few additional requests which the authors addressed in their feedback. however, no reviewers support the paper strongly borderline accepts while r5 remains unconvinced and has raised a technical point in their review about the estimator of the trace of the fisher information matrix. the question r5 has raised is central to the papers methods, arguments and conclusions. in a message to acs and pcs the authors raised concerns about r5. i personally thought that while r5 could have worded their review more carefully and respectfully as i pointed out in my respose the concerns raised were otherwise motivated, the reviewer engaged in a discussion, and the arguments were laid out clearly. i side with r5 and i think that the paper should be rewritten with more clarity on this question  the problem r5 found is likely to trip up others who read or build on the paper. the authors have raised that there are two parallel submissions closely related to this one, complicating the decision making somewhat 1 httpsopenreview.netforum?idrq_qr0c1hyo 2 httpsopenreview.netforum?id3q5iqurkcf", "accepted": null}
{"paper_id": "iclr_2021_E8fmaZwzEj", "review_text": "the paper presents a method to make cnn focus more on structure rather than texture by constraining a random set of neurons per feature map to have constant activation. the paper has limited novelty and unclear analysis of the experimental results, for instance plots of accuracy vs strength of adversarial perturbation should be produced. tables are not readable and results tend to be cluttered and confusing. some comparisons seem to be cherry picked as pointed out by some reviewers. although the approach seems to be well received by the reviewers they all shared similar concerns about having a stronger motivation and better validation of the approach that is not amount of comparisons but the right comparisons that would clear doubts and make the work directly comparable to others. i strongly encourage the authors to perform a deeper analysis and to clearly work on hypothesis and validation of their work. in my opinion, although the reviewers think different, the experiments are not sufficient to validate the strong claim of the paper.", "accepted": null}
{"paper_id": "iclr_2021_sAX7Z7uIJ_Y", "review_text": "this paper addresses stochastic semantic segmentation with a twostep approach a standard segmentation network learned with crossentropy serves as a guide to calibrate a second refinement network to generate diverse predictions while their expectation matches the calibration model. the reviewers acknowledge the paper merits, e.g. the decoupling between the segmentation and generation networks. however, they also highlight serious concerns on the the clarity of the presentation, and the need for a consolidated evaluation. the ac carefully reads the paper and the discussion among authors and reviewers. despite improvements in paper presentation, the ac still considers that the paper would benefit from clarifications, e.g. the fact that the paper does not address calibration, and that stronger baselines as those mentioned by reviewers are needed for fully validating the approach. therefore, the ac recommends rejection.", "accepted": 0}
{"paper_id": "iclr_2021_9MdLwggYa02", "review_text": "this submission proposes a variant of population based training pbt for hyperparameter selectionevolution, aimed at addressing drawbacks of existing variants e.g. the coupling of the choice of checkpoint with the choice of hyperparameters. reviewers generally agreed that the paper is interesting and covers an important topic, and the evaluation does show improvements over existing pbt variants. on the other hand they also raised a few important issues 1. the hoptim library is claimed as a primary contribution of the work, but it is not clear from the manuscript what benefits this library offers over existing software. when claiming a library as a main contribution, it is helpful to provide a more thorough description of the software and its benefits, andor ideally a link anonymized for review to the software. the authors did respond by providing a brief description of the benefits of the library, mitigating this issue somewhat. however its still difficult to discern howwhether to weigh the open source library as a main contribution of the paper. 2. the evaluation is not very convincing the differences are small and error margins are not provided for the neural networkbased experiments, meaning that any differences could be due to noise. the authors fairly point out that it is difficult to perform multiple runs of these experiments as the resource requirements are large, and they have done 20 runs of the rosenbrock experiment with smaller compute requirements. but the reviewers were not convinced that the rosenbrock experiment reflects the methods application to neural network hyperparameter selection; the problems are too different. the submission would be significantly stronger if it included results over multiple runs of an intermediate sized experiment on a problem involving a neural network demonstrating that romul outperforms competing approaches by a statistically significant margin. 3. the proposed approach is ultimately heuristic. this is not necessarily a problem if there are strong empirical results demonstrating the efficacy of the proposed heuristic, but in this case the empirical results didnt convince see point 2. given these concerns raised by reviewers, the submission is not quite ready for iclr. i hope the authors will consider resubmitting the paper after improving it based on the reviewers feedback.", "accepted": 0}
{"paper_id": "iclr_2021_cjk5mri_aOm", "review_text": "the paper proposes a selfsupervised method to predict the gist features of image frames during navigation of an agent supervised by depth and egomotion. the features are retargeted to train navigation policies and outperform previous methods or other pretraining schemes. the idea is related to selfsupervised by feature prediction but is employed in a zone level as opposed to isolated image level. though reasonable, in the context of the recent abundance of selfsupervised prediction papers in various level of spatial visual granularity, the paper may not be of sufficient novelty to present a sizable contribution for iclr acceptance.", "accepted": 0}
{"paper_id": "iclr_2021_6FtFPKw8aLj", "review_text": "the paper goes over a long list of proposed clustering similarity indices and attempts to provide a taxonomy of those by their different approaches and the extent by which they satisfy a list of desired properties proposed by the authors. this is very much in the spirit of earleir work on clustering similaritie by meila 2007 and on clustering quality measures ackerman, bendavid 2008, 2009. while there may be some interest in such a compendium, there is no much novelty in this paper and it relevance to practice is also unclear.", "accepted": null}
{"paper_id": "iclr_2021__bF8aOMNIdu", "review_text": "reading the paper and the reviews themselves, i found myself conflicted about this work  multiple reviewers commented that this is a rather incremental piece of work, given that its a rather straightforward combination of existing lossesmodels.  on the other hand, there is admittedly value in 1 realizing that this combination is meaningful 2 understanding the meaningful ways in which these work or do not work with ablation studies.  i am not quite satisfied that the datasets and experiments in this work represent in any meaningful way real world noise. however, it does appear that the authors ran experiments on common benchmarks using common protocols so theres only so much that they themselves can be blamed for.  tangentially, i am somewhat surprised about the relatively good imagenet performance of this method. i suspect the combination of this being done with uniform noise rather than structured noise is helping quite a bit. all in all, this work is certainly interesting enough, but the results are just not quite compelling enough to pass the bar.", "accepted": null}
{"paper_id": "iclr_2021_Kkw3shxszSd", "review_text": "this paper tests out some straightforward data augmentation strategies on the protein inputs to the transformer used in the tape paper. overall, there is insufficient intellectual merit to warrant publication at iclr. as a sidenote, the quality of the manuscript in terms of scholarliness of presentation was overall lacking.", "accepted": 1}
{"paper_id": "iclr_2021_7UyqgFhPqAd", "review_text": "the paper considers the problem of using sparse coding to create better generalization in neural networks. the new generalization bound of the neural network only depends on the l1 norm of the weight, instead of the original ell_2 version as in previous papers. well this direction is promising, the major concern about this work is that how they compare with existing generalization bounds empirically. there are definitely some handcrafted instances where this bound excel, but the authors did not provide enough evidence that this bound would actually be better than others for neural networks trained in practice for example, would adding a relatively large ell_1 regularizer resulted in a drastic decrement in test accuracy? how does the bound compare with compression based approach such as vc dimension  weight pruning since the weights are somewhat sparse, so the vc dimension is lower  one might argue that those pruning techniques do not have theoretical guarantees that they can work  well this technique does not have theoretical guarantees either whether this objective can be minimized efficiently the theorem, at least in the current form, seems to only apply to networks that are the global optimals of some nonconvex training objective mseloss involving a nonlinear neural network  ell_1 regularizer on its weights. it is also unclear whether such global optimals can be found efficiently in practice. at very least, the authors should devote some effort demonstrating the superiority of their bound empirically.", "accepted": 0}
{"paper_id": "iclr_2021_C1VUD8RZ5wq", "review_text": "this paper proposes a novel and interesting approach called codistillation for distributed training. the main idea is to add a regularizer in order to encourage local models to be consistent with the global objective. although the idea is a promising alternative to localupdate sgd, the approach is mostly empirical. the claim that codistillation helps reduce overfitting could be better justified by theoretical analysis, in addition to the experimental results. i hope that the reviewers constructive comments will help improve the paper for future resubmission.", "accepted": 0}
{"paper_id": "iclr_2021_cy0jU8F60Hy", "review_text": "the paper proposes a new measure of difference between two distributions using conditional transport. the paper considers an important problem. however, some major concerns remain after the discussion among the reviewers. in particular, the paper focuses on the evaluation on a toy dataset. it is unclear whether the claim carries over to large real datasets. the presentation of the paper also needs substantial improvement.", "accepted": null}
{"paper_id": "iclr_2021_jnRqf0CzBK", "review_text": "the focus of the submission is blind source separation bss. the authors propose a loglinear model based formulation to tackle the task and to relax assumptionrestrictions linear mixing, nonconvex objective, ... present in previous techniques. they use the maximum likelihood approach eq. 3 with natural gradient descent for optimization, and illustrate the efficiency of the approach in two toy examples separation of mixed images and that of sinsignsawtooth signals. bss is an important task in machine learning with various applications. as assessed by the reviewers, however, the submission is in a quite preliminary stage i section 1 is rather long, still it lacks providing relevant context to the work. ii the introduction of the main ideasmotivation, the assumptions imposed, and the explanation of the notations are missing. iii the usefulness of the proposed approach is questionable; the demos focus on artificial toy examples. more work and significant revision are needed before publication.", "accepted": 0}
{"paper_id": "iclr_2021_clyAUUnldg", "review_text": "the main goal of this paper is to develop a new adaptive strategy to remove the need of hyperparameter fine tuning, which hinders the performance of dgs zhang et al., 2020 method. this paper applies a linesearch of the stepsize parameter of dgs to reduce tuning. a heuristic update rule of the smooth parameter in dgs zhang et al., 2020 is also used. pros  the topic of learning hyperparameters on the fly is wellmotivated and an important direction to improve many existing methods.  a wide range of tasks are considered in the experiments are interesting, with a wide range of tasks considered. cons  reviewers have found the contribution to be incremental and marginal. specifically, the reviewers have expressed concerns on the impact of the work since it verifies improvement upon dgs only. the paper could be stronger by showing evidence of improving other algorithms.  in the work of zhang et. al., 2020, there are two parameters that require careful selection  the learning rate and smoothing radius. however, the proposed approach still relies on some hyperparameters. although the authors claim that the method is not sensitive to these hyperparameters, it could be better justified.  the initial version lacks evaluation of the adaptive mechanism, although the authors added the comparison in the revised version.  the paper could be further improved by comparing against other hyperparameter optimization methods. we acknowledge the detailed response and the modifications of the manuscript. we believe the paper will make a more profound contribution and impact after addressing some of the major concerns raised by the reviewers.", "accepted": 0}
{"paper_id": "iclr_2021_uUAuBTcIIwq", "review_text": "this paper aims at learning disentangled representation at different level without the supervision signal of group information. to achieve this, the proposed ugvae model uses both global variable beta to represent common information shared across all data, as well as a mixture of gaussian prior for the local latent variable pz  int pzdpdd where d represents the assignment of the group for a particular datapoint. experiments considered evaluation on unsupervised global factor learning, domain alignment and a downstream application task on batch classification. reviewers agreed that the proposed model seems interesting and novel, however some reviewers raised clarity concerns on how to interpret the learned representation by ugvae. revision has addressed this clarity issue to some extent, although some doubts from some reviewers still exists. also reviewers raised concerns on less competitive experimental results, and the authors have updated the manuscript with improved results. to me the main issues of the experimental section are 1 no quantitative result is provided regarding global factor learning and domain alignment, and 2 there is no other benchmark being studied in the experimental section. in my view, at least some other vae representation learning baselines can be included in the batch classification section in order to demonstrate the real benefit of learning global factor based representations in downstream tasks.", "accepted": null}
{"paper_id": "iclr_2021_IW-EI6BCxy", "review_text": "for metalearning with variable shot, this paper proposes a method for adapting the learning rate by a function of the number of training examples. the functional form is theoretically derived, and the method is simple and effective. however, metalearning methods that adapt learning rates have been proposed, and the novelty is not high enough.", "accepted": null}
{"paper_id": "iclr_2021_n5yBuzpqqw", "review_text": "the majority of the reviewers believe that this paper is not ready for publication. among their concerns is that the paper has limited novelty, especially in relation to existing work that use the kl constraint. some of the reviewers also believe that the arguments are sometimes handwavy and not rigorous. for example, in the discussion period after nov. 24th, it is mentioned that the argument by the authors that kl constraintdecrease the approximation errorincrease performance is not precise enough. i encourage the authors to take these comments into account and improve their paper.", "accepted": 0}
{"paper_id": "iclr_2021_ZglaBL5inu", "review_text": "reviewers generally appreciate the contributions of the paper, namely the horocycle neuron, poisson neuron, and the universal approximation properties. however, there are concerns, especially by r4 and r5, that the presentation is confusing, lacks clarity, and should be substantially improved. note theorem 1.7 in helgason, 1970 is proved explicitly for the case n2, not for general n as claimed in 9. thus the laplacian eigenspace motivation needs to be rewrittenreexamined.", "accepted": 1}
{"paper_id": "iclr_2021_PdauS7wZBfC", "review_text": "this paper extends recent work whittington  bogacz, 2017, neural computation, 295, 12291262 by showing that predictive coding rao  ballard, 1999, nature neuroscience 21, 7987 as an implementation of backpropagation can be extended to arbitrary network structures. specifically, the original paper by whittington  bogacz 2017 demonstrated that for mlps, predictive coding converges to backpropagation using local learning rules. these results were importantinteresting as predictive coding has been shown to match a number of experimental results in neuroscience and locality is an important feature of biologically plausible learning algorithms. the reviews were mixed. three out of four reviews were above threshold for acceptance, but two of those were just above. meanwhile, the fourth review gave a score of clear reject. there was general agreement that the paper was interesting and technically valid. but, the central criticisms of the paper were 1 lack of biological plausibility the reviewers pointed to a few biologically implausible components to this work. for example, the algorithm uses local learning rules in the same sense that backpropagation does, i.e., if we assume that there exist feedback pathways with symmetric weights to feedforward pathways then the algorithm is local. similarly, it is assumed that there paired error neurons, which is biologically questionable. 2 speed of convergence the reviewers noted that this model requires many more iterations to converge on the correct errors, and questioned the utility of a model that involves this much additional computational overhead. the authors included some new text regarding biological plausibility and speed of convergence. they also included some new results to address some of the other concerns. however, there is still a core concern about the importance of this work relative to the original whittington  bogacz 2017 paper. it is nice to see those original results extended to arbitrary graphs, but is that enough of a major contribution for acceptance at iclr? given that there are still major issues related to 1 in the model, it is not clear that this extension to arbitrary graphs is a major contribution for neuroscience. and, given the issues related to 2 above, it is not clear that this contribution is important for ml. altogether, given these considerations, and the high bar for acceptance at iclr, a reject decision was recommended. however, the ac notes that this was a borderline case.", "accepted": 0}
{"paper_id": "iclr_2021_ryUprTOv7q0", "review_text": "a quantum deformed generalization of a probabilistic binary neural network is introduced, which can be either run on a quantum computer or simulated with a classical computer. reviewers agreed that the paper is well written, introduces some new ideas merging quantum computing with a variational bayesian framework, and the reported numbers on mnist and fashion mnist outperform prior qnn approachers. however, reviewers questioned how useful the proposed ideas are, noting that the reported gains could be attributed to increased parameterization this was not carefully ablated with baseline approaches. additionally, while the quantum supremacy experiments seem technically correct, there was no clear motivation for empirically demonstrating quantum supremacy when no theoretical guarantees are provided. taken together, there was no clear path to practical improvements of real systems from the proposed ideas.", "accepted": 0}
{"paper_id": "iclr_2021_Ew0zR07CYRd", "review_text": "most reviewers are positive about this work, though they believe it is somewhat incremental, and its theoretical contributions are minor. none of the reviewers are very excited about this work. overall, the pc believes this is a borderline paper. minor note during the discussions, the paper by xiao et al., characterizing attacks on deep reinforcement learning 2019 was brought up. the authors claimed that they did not compare with that paper because the best attack there obsfgsmwb had already been studied. in a later stage of discussions, one of the reviewers stated that the method obsnnwb in that paper performed better in some domains. even though this is not a major issue, it is advisable to the authors to make sure that this is indeed the case, and if it is, provide proper comparison with that paper. we encourage the authors to consider the reviewers comments to improve the paper and resubmit to a future venue.", "accepted": null}
{"paper_id": "iclr_2021_Jr8XGtK04Pw", "review_text": "this paper analyses a recurrent neural network model trained to perform a simple maze task, and reports that the network exhibits multiple hallmarks of neural selectivity reported in neurophysiological recordings from the hippocampus in particular, they find place cells which also are tuned to taskrelevant locations, cells which anticipate possible future paths, and a high proportion of neurons tuned to task variables. the reviewers appreciated the interesting empirical analysis, and the demonstration that multiple such features could arise in the same neural network to the best of my knowledge, this had not been demonstrated explicitly before. however, there were also multiple concerns, which lead to this paper beeing discussed extensively and controversially. in particular, it is not clear which features arise from which learning objective, for example, for place cells to arise, do we just need sensory prediction, or do we need qlearning? in addition, there were some points in which the tightness of the analogy between model and biology is questionable in particular, this refers to the comprising between hippocampal recordings and the evaluation of the network. finally, it is also clear that some of these observations reported in the paper are, indeed, empirical observations rather than explanations. because of these shortcomings, there was no consensus and strong support from the reviewers for acceptance of the paper. after extensive discussion between both the reviewers, the ac and the program chair, the final decision was to not accept the paper. we do hope that the reviews will help you in improving the study and its presentation. it clearly has potential to be a valuable contribution to the literature.", "accepted": 0}
{"paper_id": "iclr_2021_pOHW7EwFbo9", "review_text": "considering reviewers comments and comparing with similar papers recently published or submitted, this is a good paper but hasnt reached the bar of iclr. we believe that the paper is not ready for publication yet, and strongly encourage the authors to use the reviewers feedback to improve the work and resubmit to one of the upcoming conferences.", "accepted": 0}
{"paper_id": "iclr_2021_mgVbI13p96", "review_text": "the authors provided a comprehensive rebuttal to the reviewers feedback that addressed most of the concerns. anonreviewer3 raised some major concerns that were partially resolved in a revision. the paper has received a split recommendation from the reviewers but within the review and discussion periods, there was no strong support towards accepting the paper. although the paper has received some positive feedback, some of the reviewers concerns were not fully addressed. id recommend the authors to address all the comments and add clarifying notes to the paper to avoid such misunderstandings if they decide to resubmit the paper to another venue.", "accepted": 0}
{"paper_id": "iclr_2021_IeuEO1TccZn", "review_text": "this paper aims to present a new representation learning framework for supervised learning based on finding a representation such that the input is conditionally independent given the representation, the components of the representations are independent and the representation is rotationinvariant. while there were both positive and negative assessments of this paper by the reviewers, there are 3 major concerns that lead me to recommend rejecting this paper 1. most importantly, experiments do not seem to be conclusive as they do not properly ablate the specific aspects of this method. more specifically, the authors compare their deep learning based approach with nondeep learning approaches but do not compare against deep learning baselines. this makes it impossible to assess the merit of the proposed approach which also appears to be complicated over much simpler baselines. 2. the required properties of the representations do not seem to be properly motivated. 3. the paper refers to their produced representations as disentangled representations. as pointed out by anonreviewer4, this appears not to be consistent with prior uses of that word in the community.", "accepted": 0}
{"paper_id": "iclr_2021_LT0KSFnQDWF", "review_text": "we thank the authors for their detailed responses and the revised version, which addresses several of the questions raised by the reviewers. the paper is correct and clearly written. all reviewers agree that the idea to add structural features in the message passing of graph neural networks is sensible. while different from previous work, the novelty is a bit incremental though, particularly given the previous work on colored graph neural network. the significance of the work is weak, given 1 the need to select by hand structural features that are passed as information, 2 the increased time complexity to compute the structural features compared to other gcnn, and 3 the experimental results that suggest that the benefit of the new approach is limited, particularly on challenging task. to summarize, this is not a bad paper, but we consider it below the standard of iclr in terms of originality and significance.", "accepted": 0}
{"paper_id": "iclr_2021_rx19UMFbC9u", "review_text": "this paper introduces allalive pruning, an approach which checks for and removes the connections to and from units with zero gradient dead units. the method is shown to improve performance of imp at extreme 128x compression ratios on mnist, cifar10, and tiny imagenet. all reviewers felt that the problem the authors study  how to identify and remove dead units  is an interesting one. however, there were concerns about the practical utility of the method, given that aap only improves performance for extreme compression ratios in which performance is already substantially degraded relative to unpruned models. i share these concerns, which mute the practical impact of this work. there were also concerns about a lack of proper baseline comparisons to more simple approaches to removing dead units. as mentioned by r1, given that the problem the study is an interesting one, the paper could make up for the lack of practical utility by providing detailed analyses of the settings in which dead units emerge, differences among pruning approaches, etc., but analyses provided here are limited. i would encourage the authors to explore these areas in a future revision of the paper, but recommend that the paper be rejected in its current form.", "accepted": 0}
{"paper_id": "iclr_2021_TlS3LBoDj3Z", "review_text": "this paper proposes practical improvements to theoretically well founded qtran, which is a stateoftheart technique of cooperative multiagent reinforcement learning. the improvements include new designs of loss function and actionvalue estimator, which might be widely applicable beyond qtran. however, it is not obvious if the proposed improvements actually improves the performance of qtran, and experimental evaluation is essential to this work. after the discussion, there remain some major concerns about the experimental results. in particular, the performance of baselines in the experiments is not consistent with those reported in the prior work.", "accepted": 0}
{"paper_id": "iclr_2021_DGIXvEAJVd", "review_text": "i thank the authors for their submission and very active participation in the author response period. world state tracking is an important problem that encompasses existing problems like coreference resolution. i agree with r2 and r3 that proposing a novel environment in which we can investigate to what extend transformers can tackle world state tracking should be interesting to the community. the majority of the reviewers agree that this paper presents an interesting benchmark r2,r3,r4 with good thorough experimental work r1,r2,r4. however, r1 is confused about the positioning of the work and r4 finds the work narrow. r2, despite positive review, agrees with this assessment. i agree with this assessment as well and, after discussion with the program chairs, came to the decision that this paper is not ready for publication in its current state. i strongly encourage the authors to incorporate r1s and r4s feedback, in particular with respect to positioning this environment in comparison to textworld, and resubmit to the next venue.", "accepted": null}
{"paper_id": "iclr_2021_--rcOeCKRh", "review_text": "after the rebuttal phase, all scores are borderline 6 or negative 4. among the most confident reviewers confidence 5, one gives 6 and one gives 4. the reviewer with confidence 4 gives overall score 6 but states they cannot support the paper. there were several concerns about the novelty of the task and method, the challenge of the experimental settings, missing comparisons to recent prior work in the original paper, etc. while the reviewers see merit, the paper can benefit from another revision before being accepted, including to better position the novelty of its method and perhaps reduce claims of novelty of the task.", "accepted": 0}
{"paper_id": "iclr_2021_pVwU-8cdjQQ", "review_text": "the reviewers appreciate the spatiotemporal formulation of amortised iterative inference. however, the paper does not clearly state what is the end goal if the end goal is video object segmentation, it should compared against other unsupervised object segmentation methods. if the goal is representation learning, it should evaluate the merit of the recovered representations, e.g. by finetuning them on some downstream task.", "accepted": 0}
{"paper_id": "iclr_2021_-NEXDKk8gZ", "review_text": "this paper arose a number of questions and concerns among reviewers that made it get belowaverage scores unfortunately, reviewers did not provide further feedback on the rebuttal. after discussion between the program chairs, calibrating decisions across all submissions and, given the drawbacks mentioned below, it is decided that this paper does not meet the bar for this years iclr. therefore, the final decision is to reject the paper. as a brief summary, i highlight below some pros and cons that arose during the review and metareview processes. pros  further developing on a simplification of previous approaches learning diffusion sigmas.  proposal of a new noise schedule.  improving the loglikelihood of diffusionbased generative models.  improving generation time. cons  similar fids as nonimproved approaches in some cases.  focus on loglikelihood may not be of paramount importance for a generative task.  dichotomy between better fid and better nll could be further discussed.  more comparison with other approaches and further data sets could be done.  a bit adhoc noise schedule.", "accepted": null}
{"paper_id": "iclr_2021_gSJTgko59MC", "review_text": "this paper develops an interesting new angle on the behavior of largewidth neural networks by elucidating the connection between the nngp and noisy gradient descent and by examining finitewidth corrections through an edgeworth expansion. while these contributions are important, the paper would better serve the community if its presentation were significantly improved before publication. the main issue is not one of presentation style  papers with physicsstyle prose are welcomed and appreciated at iclr  but rather one of presentation substance. in addition to the various specific points raised by the reviewers, i would add that the figures and captions are difficult to interpret, the experiments need a more indepth discussion, and the notations should all be defined at the time of their introduction, among other things. for these reasons, i cannot recommend accepting the paper in its current form, but i hope to see a more polished version of the manuscript at a subsequent conference.", "accepted": 0}
{"paper_id": "iclr_2021_ePh9bvqIgKL", "review_text": "the paper proposes the idea of searching parameterized activation functions, in contrast to the previous handcraft or learnable ones. it may be a counterpart of neural architecture search. pros 1. the idea is very interesting. 2. the paper is well written. 3. the experiments show improvements over baseline activation functions. cons 1. the ac fully agreed with reviewer 4 that the whole literature of learnable activation function is neglected reviewer 2 also alluded to this issue. although the authors added experiments with learnable baseline activation functionss, the literature review on learnable activation function was not included accordingly. 2. although the idea of searching activation functions is interesting, the ac doubted the necessity. since the rich literature of learnable activation functions is already there note that it is more than introducing parameters to handcrafted ones, can we simply learn piecewise linear activation functions with more pieces so that it can approximate complex enough functions? this can be much more easily implemented can go along with weight training on the standard deep learning platform and the computation cost will be much lower. such a comparison is absolutely necessary. 3. the ac was actually worried about the activation functions founded as they may be too complex, so the generalization issue even numerical stability issue may be a concern. more thorough testing is necessary currently only tested on cifar100 and three cnns; and reviewers 3 and 2 also concerned about this issue. although reviewer 2 raised hisher score, the final average score is still below threshold. so the ac decided to reject the paper.", "accepted": null}
{"paper_id": "iclr_2021_qClL9hRDSMZ", "review_text": "this paper analyzes the implicit bias of gradient descent of infinite width 2layer neural networks with relu activation. it is shown that the dynamics of gradient descent to optimize the 2layer nn converges to the optimization dynamics on the random feature model in the infinite width limit. then, it is shown that the gradient descent converges the minimal l2 norm solution from the initial parameters which yields regularization on a weighted integration of second order differentiation. although this type of analysis has been given in the existing work, this paper gives its explicit form in 1dimension input setting. this paper reveals an interesting fact about the implicit regularization that would be educationally valuable. on the other hand, i should mention that there is room for improvement in its theoretical contribution and moreover its novelty is rather limited. 1. although the explicit formulation of the implicit regularization is informative, the minimum norm bias itself is already pointed out by existing work and this work follows the line. especially, regularization on the second order derivative has been already pointed out by previous work although they are 1norm regularization. 2. the logical jump from the original data to the adjusted data is still not convincing. it is explained that some numerical experiments show the linear term is negligibly small, which means the problems 15 and 17 are very close. however, this excuse does not make sense for this kind of theoretical work. the logic used here should be clarified to make the theoretical framework complete. the evaluations by the reviewers indicate that this paper is on the borderline, and i also feel that some more additional strong point would be required so that this paper is accepted. i encourage the authors to go in this direction and make the analysis more detailed so that the theoretical framework would get more completed. minor comment theorem 4 overlaps the result given by the following paper r1. it is recommended that the relation and novelty compared with that paper is discussed. r1 e, w., ma, c.  wu, l. a comparative analysis of optimization and generalization properties of twolayer neural network and random feature models under gradient descent dynamics. sci. china math. 63, 12351258 2020.", "accepted": null}
{"paper_id": "iclr_2021_E9W0QPxtZ_u", "review_text": "the reviewers brought up significant concerns that were not resolved by the authors responses. the concerns are too significant for the paper to be accepted at this time.", "accepted": null}
{"paper_id": "iclr_2021_uFk038O5wZ", "review_text": "the authors address the important task of improving dialogue summarization using conversation structure and factual knowledge. pros 1 clearly written and well motivated as acknowledge by all reviewers 2 technically sound the proposed architecture is clearly in line with the problem that the authors are trying to solve 3 significant upgrades to the paper after the reviewer comments in particular the authors have added detailed ablation studies and results on nondialogue datasets cons 1 there is a significant difference between the results in the ablation studies in the original version and in the new version. originally, the differences between kgedcg and kgedcgge and kgedcgfkg were very minor, but now the margins are as large as 7 pts. i would request the authors to explain this in the final version the reviewing team felt that while many qs were sufficiently addressed by the authors, the large difference in the numbers reported for the ablation study in the initial and final version of the paper raises some new qs which need to be addressed before the paper can be accepted.", "accepted": null}
{"paper_id": "iclr_2021_nhIsVl2UoMt", "review_text": "this paper proposes a method for modeling higherorder interactions in poisson processes. unfortunately, the reviewers do not feel that the paper, in its current state, meets the bar for iclr. in particular, reviewers found the descriptions unclear and the justifications lacking. while the responses did aid the reviewers understanding, the paper would benefit from rewriting and more careful thought given to the experimental design.", "accepted": 0}
{"paper_id": "iclr_2021_7qmQNB6Wn_B", "review_text": "first, id like to thank both the authors and the reviewers for extensive and constructive discussion. the paper proposes a generalization of sac, which considers the entropy of both the current policy and the action samples in the replay pool. the method is motivated by better sample complexity, as it avoids retaking actions that already appear in the pool. the paper formulates a theoretical algorithm and proves its convergence, as well as a practical algorithm that is compared to sac and sacdiv in continuous sparsereward tasks. generally, the reviewers found the method interesting. after rounds of discussion and revisions, the reviewers identified two remaining issues. theoretical analysis still requires improvement and the positioning of the paper is not clear. particularly, the method is motivated as an exploration method, and it should be evaluated as such, for example, by comparing to a more representative set of baseline methods. therefore, im recommending rejection, but encourage the authors to improve the work bases on the reviews, and submit to a future conference.", "accepted": null}
{"paper_id": "iclr_2021_kmBFHJ5pr0o", "review_text": "in this paper, the authors claim to propose a distributed largebatch adversarial training framework to robustify dnn. although the authors made efforts to clarify reviewers concerns, it is clear that the authors still cannot convince some reviewers in several points after several rounds of discussion between reviewers and authors. the reviewers were not in consensus on acceptance and some concerns were still not clearly addressed in the rebuttal phase. hence, i recommend acceptance only if there is a room.", "accepted": 1}
{"paper_id": "iclr_2021_Mwuc0Plt_x2", "review_text": "this paper proposes a hierarchical flowbased generative model to learn disentangled features at different levels of abstractions. the key technical contribution is a combination of renormalization group and flowbased models. the reviewers do find the idea interesting. however, the merit of the work with respect to stylegan and styleflow has not been well established. ar3 made the following comment specially, compared with the stylebased generator1,2, , i dont find superiorities of the proposed method. the authors responded to the comment briefly but not convincingly in their rebuttal. there is no mention of it in the revised paper. a proper account of the issue would require major revision to the paper.", "accepted": null}
{"paper_id": "iclr_2021_a2rFihIU7i", "review_text": "this paper is solid. it is correct, the text and author response demonstrate good knowledge of the area, the results are significant and solid, the experiments are strengthened by many independent runs refreshing to see, the ablation study is well done, and the proposed distributed hyperparameter and nas alg is simple and practical. the paper is well written and reasonably polished. the main drawback of the work in the eyes of the reviewers is that the paper is well described as a combination of existing ideas and a significant engineering effort with good but not stellar results. the reviewers found they did not gain any substantial technical insights from the work. as a result no reviewer was willing to champion the paper. however, the discussion, reviews, and author response made it clear that 1 the paper is enjoyable to read and informative, 2 the method is actually useful and performant, and 3 the combination of implementation details and methods is worth documenting. in balance, the paper is just below the bar. the program was extremely competitive this year.", "accepted": null}
{"paper_id": "iclr_2021_hbzCPZEIUU", "review_text": "this paper introduces a method for hierarchical classification with deep networks. the idea is interesting, and as far as i know novel namely, the authors add a regularizer to the last layer in order to enforce a hierarchical structure onto the classifiers. the idea of placing spheres with a fixed radius around each classifier and forcing the childclassifiers to lie on these spheres is quite clever. the reviewers have pointed out some concerns with this paper. some had to do with terminology which the authors should fix but which is no big deal, but the main weakness are the experimental results and the ablation study. the reviewers were not convinced that the optimization in the euclidean space wouldnt be sufficient. a more thorough ablation study could help here. this is the kind of paper that i really want to see published eventually, but right now isnt quite ready yet. if you make one more iteration in particular adding a stronger ablation study it should be a strong submission to the next conference. good luck!", "accepted": null}
{"paper_id": "iclr_2021_VCAXR34cp59", "review_text": "this paper evaluates the extent to which disentangled representations can be recovered from pretrained gans with stylebased generators by finding an orthogonal basis in the space of style vectors, and then training an encoder to map images to coordinates in the resulting latent space. to construct the orthogonal basis, the authors consider 3 recently proposed methods for controllable generation, along with a newly developed generalization of one of these methods. the authors evaluate metrics for disentanglement for 4 datasets, consider an abstract visual reasoning task, and compute unfairness scores. reviewers expressed diverging opinions on this paper. r2 is in support of acceptance, r3 finds the paper borderline but is leaning towards acceptance, whereas r4 is critical. r2 and r4 engaged in a relatively detailed discussion, but maintained their scores. having read the paper, the metareviewer feels this submission indeed has strengths and weaknesses. on the one hand, the main results are notable; it is worth reporting that disentangled representations can be recovered from pretrained gans is a relatively straightforward manner. in this context, the metareviewer feels that some comments by r4 are more critical than is warranted. the authors do not necessarily have to show that ganbased methods uniformly improve upon vaebased methods, either in terms of disentanglement metrics or in terms of sensitivity to hyperparameters. the main claim in this submission is that ganbased methods are mostly comparable to vaebased methods, and this claim is both sufficiently notable and sufficiently supported by experimental results. at the same time, this submission is not without flaws. the writing is on the rough side, and as r4 notes the authors have removed all white space between paragraphs. the metareviewer also feels it is not satisfactory to show a box plot for ganbased methods in figure 2 and ask the reader to compare these plots to the violin plots for vaebased methods in the locatello paper. the authors need to find a way to make a more direct comparison here. r4s comments about the comparison in the abstractreasoning setting are also welltaken  here the baseline employs standard entangled models, so it is unclear what conclusions we should draw from this experiment. similarly the unfairness results once again appeal to an indirect comparison to results in the locatello paper on this topic. on balance, the metareviewer is inclined to say that this submission, in its current form, falls just below the threshold for acceptance. these results are clearly of note to the community and worth reporting, but the presentation has enough flaws that another round of reviews is warranted based on a revised manuscript. the metareviewer hopes to see this paper appear a conference in the near future.", "accepted": 0}
{"paper_id": "iclr_2021_Uqu9yHvqlRf", "review_text": "the authors present a study on what maintains the stability of emerged communication protocols. to study this question the authors design experiments in bargaining communities of agents in 3 setups, a no punishment of restriction of liar agents b allowing individual agents to refuse bargaining with liar agents and c introducing a global punishment system for liar agents. overall the reviewers agree that the design of the study is interesting, but also point that motivation and takehome messages of this study are unclear. having read the paper, i share the same opinion. the authors discuss on a very abstract level about the implications of this study for the field of ai, but this study is quite specific and clearly does not capture all the complexities or real societies. from the scale of results and study, i think it would be more valuable to draw some concrete proposalsimplications about perhaps multiagent modelling or environment design in general. all in all, this is an interesting study but some more work needs to be done around research framing.", "accepted": 0}
{"paper_id": "iclr_2021_DdGCxq9C_Gr", "review_text": "dropouts dream land generalization from learned simulators to reality this work explores the use of dropout inside a learned dynamics model, so that when an agent is trained inside an environment generated by this model rather than the actual environment, the policy learned would do better in the actual environment. in a sense, this is a form of domain randomization in a learned simulator. they show that their approach has better transfer capabilities compared to the baseline world models method, where an entropy injection via temperature adjustment is used to make transfer more effective, and this is particularly evident in the carracing learn in latent simulation experiment. while this work is interesting to me, and i believe it has something to offer to the iclr community, after reading the reviews and also the detailed author  reviewer discussion and after understanding and clarifying all of the nuanced points in the experiments, the reviewers and myself believe that this paper needs more work before meeting the bar of iclr conference acceptance. i believe the author clarified many issues and misunderstandings with the reviewers, and we have made sure the reviewers took that into consideration. based on the reviews, i have summarized recommendations below to help the authors improve this work. there are 2 dimensions of the work that can be improved 1 novelty and connections with prior works that used dropout in rl while this work is not using mc dropout it applies dropout inside the lstm m, there has been sufficient work as listed by r2 using mc dropout with rl, and the reviewers impression that while they may not be exactly the same, it does bring to question of the novelty in this work. it is recommended to discuss not only that the approach is not mc dropout, but also connections with previous work that used mc dropout or even other forms of dropout dynamics models to prevent overfitting, and also discuss why this particular approach is needed or is better, via experiments, over mc dropout, if that can also be used to generate novel dream environments. as r3 mentioned, the idea of applying dropout of whichever form on a learned dynamics model is new, and this point should be emphasized very clearly to the reader, so i recommend the authors improve the writing to incorporate discussions and relations to previous work r2 provides a good list, and emphasize what is considered novel in this particular work. 2 experimental design the authors show that the proposed method offers a clear advantage over the baseline wm approach for carracing, where they show that ddl can do the train in dream  deploy in real env transfer much better than the original baseline can. for the doom experiment, im less concerned about the exact score compared to the baseline as noted by r1, given the high variance of these results, and also the high randomness in the process used to collect data via a random policy, and see that their result is within the margin of statistical error. just noting the replication effort that went in as a footnote and citing existing results, noting the high variance, should suffice. what would really improve the work, imo, is to compare to gamegan on pacman environment. would ddl offer improvements vs gamegan on pacman? this will be a strong data point for the proposed methods effectiveness, compared to more trivial tasks such as doomtakecover. r3 also brings out a good point that the paper offers better simtosim transfer, rather than simtoreal transfer. that is another avenue to explore, if this work is to be improved. the authors have cited planet  dreamer  simple papers, but mentioned that these works dont deal with the issue of reality gap, but i would argue that the iterative learning data gathering  retraining aspect of these algorithms is actually one method to address the gap. the tasks studied in these more recent papers, such as dm control from pixels, or atari, have more datapoints, or leaderboard participants, using this papers terminology, so they can also be considered if the authors wish to try ddl to see if this method can help improve the performance of these newer approaches which are based on world models with iterative training and data collection. one can explore whether this approach can lead to better sample efficiency gains, in addition to absolute performance after training, when combined with iterative training in planet  dreamer type approaches. overall, the work in its current form would make a good workshop paper, but i look forward to seeing more work done in the experiments to see better convincing results, in addition to clarifying the writing on related approaches and making contributions  novelty more clear, which i believe will really improve the work for a future submission.", "accepted": 0}
{"paper_id": "iclr_2021_D3TNqCspFpM", "review_text": "the reviewers noted that this is an important, interesting but difficult topic. they appreciated that the authors clarified their assumptions in the theorem statements. nevertheless, they recommend the authors to detail in depth when the method work better than the method where only the covariates are adjusted. they still think that the paper would require major modifications to be considered for publication hence the decision is rejection the paper.", "accepted": 0}
{"paper_id": "iclr_2021_lvXLfNeCQdK", "review_text": "the authors argue that tighter relaxations for certified robustness suffer from a worse loss landscape and thus are outperformed by the much simpler and less tight ibp relaxation and come up with a new relaxation to overcome this problem. after the rebuttal there still remain doubts about the reasoning regarding the loss landscape even though i acknowledge that the authors have invested significant amount of work to support their hypothesis. moreover, the differences to existing certified training methods is small or the proposed method performs worse while being significantly more expensive in particular if one takes into account the results which are reported on the ibpcrown github page where the reported numbers are significantly lower than reported in the present paper so that the benefit is unclear. thus the majority of the reviewers still suggests rejection and i agree with that even though i think that the paper has its merits and i encourage the authors to continue this line of work. for a next version, the authors should evaluate all the methods ideally with an exact verification method resp. use the best relaxation for all methods. otherwise the differences can come just from the weaker relaxation but not from a difference in real robustness.", "accepted": 0}
{"paper_id": "iclr_2021_4mkxyuPcFt", "review_text": "in this paper, the authors theoretically analyzed the attacking mechanisms of the two kinds of adversarial examples in the gaussian mixture data model case and proved that adversarial robustness can be disentangled in directions of the data manifold. the reviewers commonly felt that the idea and theoretical analysis in this paper are interesting, but experiments are not satisfactory. at the current status, they still have a main concern regarding the correctness of comparison between the results of theorem 4 and corollary 3 which is the heart of their theoretical claims, the main message of the paper and the main motivation for experiments. as a whole, this paper has some merits but the authors still cannot clarify some concerns raised by some reviewers.", "accepted": 0}
{"paper_id": "iclr_2021_0owsv3F-fM", "review_text": "the reviewers generally appreciated the problem statement and topic of the paper, but raised concerns across the board about the empirical evaluation. since the paper is largely experimental in nature, a compelling experimental evaluation is important. reviewer concerns generally centered around 1 the relative simplicity of the evaluated tasks; 2 somewhat questionable baselines. while the authors provided responses to some of these concerns, after discussion the reviewers generally still felt that these issues were rather severe, and preclude publication at this time. i would encourage the authors to take this feedback into account in improving the empirical evaluation in the future.", "accepted": 0}
{"paper_id": "iclr_2021_vC8hNRk9dOR", "review_text": "the initial reviews were mixed for this paper. on one hand, some of the reviewers highlighted that the proposed datasets could be useful to researchers. on the other, reviewers found a few important flaws with the current manuscript including missing baselines, issues with the proposed tasks, and possibly inaccurateimprecise statements. our discussion after the authors response focussed on whether the positives aspects of the current paper outweighed some of the perceived weaknesses of the paper. in particular, while some of the initial criticisms from the reviewers were successfully addressed by the authors including possible imprecisions and to a certain extent motivation, all the reviewers remained convinced that standard continual learning baselines could be adapted to this setting. they also conjectured that these missing baselines might not allow readers to appreciate the strength of the proposed datasets. in their response, the authors argued that adapting models would require research. the reviewers are under the impression that it would be useful to test baselines more or less asis even if the authors do not think these baselines will be competitive. for example, in the discussion, a reviewer suggested that an experience replay baseline could ... have been implemented where the replay buffer includes the hidden states of an lstm. it might also be useful to study baselines that do not strictly obey the proposed setting, again to get a better understanding of the proposed tasks including how difficult it is. overall, having some of these baselines would be one way to better connect the proposed work to the current continuallearning literature.", "accepted": 0}
{"paper_id": "iclr_2021_8FRw857AYba", "review_text": "although originally all reviewers were leaning towards rejection, the authors have done a very good job at addressing their concerns, significantly strenghtening the paper. there is now a consensus towards weak acceptance, with the exception of r3. however, i have decided to ignore r3s review for the following reasons  the original review was way too short and uninformative  r3 did not reply to the authors request for more constructive feedback  r3 did not reply to my own request private email that being said, even if other reviewers decided to increase their score after the rebuttal and discussion period, none of them was particularly enthusiastic about it this remains a borderline paper combining ideas that, although promising, are not particularly original. at this time it falls slightly short off meeting the bar for an iclr publication. i do believe that combining ideas from the rl and evolutionary research communities is a promising research direction, and i encourage the authors to take into account the reviewers remaining comments to polish their paper in particular, adding even stronger empirical results, and ensuring the key takeaways are clearly communicated.", "accepted": 0}
{"paper_id": "iclr_2021_v5WXtSXsVCJ", "review_text": "this work describes a series of strategies for optimizing the training speed of word embeddings as in word2vec and fasttext. all reviewers appreciate the convincing empirical results, which are without a doubt impressive. reviewers also mostly agree that speeding up embedding training is important, and there is no doubt that this type of paper is appropriate for iclr as clearly highlighted in the cfp. however, the specific optimization strategies deployed and described here are deemed not to bring novel insight, useful in itself to the community, beyond the software contribution described. the paper seems to mostly serve as documentation of the implementation, limiting its value and impact to further research. the pedagogic value is also limited, as the paper tackles multiple different, eclectic optimizations, a narrative strategy that does not leave room to describe a single one more generally, helping the community find other places to apply it. all in all this leads to a borderline negative assessment, and i cannot recommend acceptance.", "accepted": 0}
{"paper_id": "iclr_2021_mLtPtH2SIHX", "review_text": "this work proposes to improve mixup by using soft labels, removing the need for input mixup. the reviewers found the paper was clear and found the experiments promising. the reviewers raised concerns about the lack of experiments comparing this approach to mixuplabel smoothing, which were addressed during the rebuttal by the authors. however, the reviewers did not find the empirical evidence strong enough given that this is mostly an empirical contribution. the authors do not necessarily need to train on the full imagenet, but it would be beneficial to evaluate on more standard settings on the dataset considered to facilitate comparison to previous work.", "accepted": 0}
{"paper_id": "iclr_2021_L8BElg6Qldb", "review_text": "the paper extends results from the recent work of steinke and zakynthinou sz for the test loss of randomized learning algorithms. they provide bounds in the single draw as well as pacbayes setting. the main result is about fast rates the proof of which follows with minor modifications from the corresponding result in sz. it is unclear to me the contribution over existing work is sufficient to merit acceptance.", "accepted": null}
{"paper_id": "iclr_2021_LIOgGKRCYkG", "review_text": "i thank the authors and reviewers for the lively discussions. reviewers found the work to be interesting but some concerns were raised regarding the significance of the results. in particular, two reviewers mentioned that authors did not fully address their concerns in the rebuttal period. given all, i think the paper still needs a bit of work before being accepted. i recommend authors to address comments raised by the reviewers to improve their work. ac", "accepted": null}
{"paper_id": "iclr_2021_FsLTUzZlsgT", "review_text": "this paper studies the relationship between test error as a function of training set size and various design choices of neural network training. overall all of the reviewers are excited about the prospect of relating error curves to neural network design choices, but different reviewers complain about the rigor of empirical evaluation and the accuracy of conclusions given limited data points. i agree with reviewers on both points, i.e., the paper studies different design choices, but does not do a thorough job studying those design choices. moreover, it is not clear what aspects of the study are directly related to error curves vs. a standard correlation study done in prior work, e.g. in do better imagenet models transfer better? for usefulness of imagenet pretraining. so, overall, i believe not only the empirical evaluation needs improvement, but also the story needs refinement. i am looking forward to seeing this paper published in other ml venues.", "accepted": 0}
{"paper_id": "iclr_2021_F_txysyDFbw", "review_text": "this paper proposes a promising solution to a very interesting and challenging problem, and the authors have improved the paper during the rebuttal by adding an important missing baseline. however, all reviewers still agree that the paper currently lacks sufficient analysis that would be required to understand properly the implications of past history on the regret. more specifically, the fact that assumption a2 does not apply to the given problem raises questions that should be addressed before publication. theoretical analysis was provided for previous similar work e.g. neuralucb. providing this for the proposed method would significantly improve the impact of this work.", "accepted": 0}
{"paper_id": "iclr_2021_wTWLfuDkvKp", "review_text": "this paper studies ensemble calibration and the relationship between the calibration of individual ensemble member models with the calibration of the resulting ensemble prediction. the main theoretical result is that individual ensemble members should not be individually calibrated in order to have a wellcalibrated ensemble prediction. while other recent work has found this to be the case in empirical results, this paper substantiates the empirical results through theoretical results. pros  theoretical study of ensemble calibration with meaningful insights cons  contributions limited to theoretical study of known observation and dynamic temperature scaling.  dynamic temperature scaling is not shown to outperform baseline methods.  limited experimental validation cifar10cifar100. the authors engaged in a extensive discussion with reviewers and made changes to their paper, including adding standard deviation results over multiple runs and the skce calibration measure. overall this is solid work and could be accepted to the conference; however, reviewers agree that parts of the work are lacking, in particular 1. limited experimental evaluation one type of task, onetwo datasets only, and 2. given known literature the benefit of the derived theoretical results to practioners is not clear. the discussions have been unable to resolve this disagreement.", "accepted": 0}
{"paper_id": "iclr_2021_QKbS9KXkE_y", "review_text": "there was a fair amount of discussion about the paper. several reviewers felt that the paper would have been stronger if it tried to do less but better. the reviews describe in detail what the reviewers would have found compelling, but the key suggestion is to remove the complexity that is not essential for the approach to provide consistent improvements. doing this requires a better understanding of the algorithms behavior and a valid ablation study, a new concern raised during the discussion with the authors. the reviewers felt that the proposed approach is potentially interesting and would like to see this paper done well.", "accepted": 0}
{"paper_id": "iclr_2021_Dtahsj2FkrK", "review_text": "this paper proposes a testing procedure to determine whether a policy is better than another policy with respect to longterm treatment effects. the reviewers found the problem interesting and saw a lot of value in this work. one of the key concerns was the lack of clarity throughout the paper. the reviews helped the authors actively revise the paper, improving the papers overall readability throughout the discussion phase. however, the reviewers did not change their ratings. while i agree that this work has merits, since there are many legibility issues, i cannot recommend its acceptance at this stage.", "accepted": null}
{"paper_id": "iclr_2021_otuxSY_QDZ9", "review_text": "all reviewers recommended rejection after considering the rebuttal from the authors. the main weaknesses of the submission include poorly motivated claims and designs, and insufficient experimental comparisons. the ac did not find sufficient grounds to overturn the reviewers consensus recommendation.", "accepted": 0}
{"paper_id": "iclr_2021_5USOVm2HkfG", "review_text": "most reviewers believe that the paper is not ready for publication. among their concerns are  whether the new experiment with 10 runs are conducted correctly,  the significance of the theoretical part,  correctness of lemma 2,  generalization claims may not follow from the theoretical results,  comparison with zhang et al. 2020. given these and the lack of support from reviewers, unfortunately i cannot recommend acceptance of this paper at this stage. i encourage the authors to improve their paper according to these concerns. i copypaste some of the comments that came after nov. 24th. the authors might want to use them to improve their work.  first, given the assumptions, the theoremslemmas are not sufficient to be a solid contribution. i think what important is, can the alg 1 lead to the optimal policy? more specifically, i do not doubt lemma 2; i am concerned about if updating the representations in an online manner it should be highly nonstationary can result in the optimal policy. some twotime scale analysis may address this question. as an additional note, since f can be a manytoone mapping, policy pi_i is a multimodal distribution. i am unsure if the authors consider this during implementation. second, without the twotime scale analysis, i would give weak acceptance if the authors can persuade that the superior performances are indeed due to the proposed jointly embedding learning method. thats why i ask for a baseline that directly considers environment model learning as an auxiliary task. in the abstract, the authors state that in this work, we propose a new approach for jointly learning embeddings for states and actions ..., in fact, this is not new. almost any deep rl algorithm can be thought of as the process of learning state andor action representations. in the response, the authors say, we do not believe ..., as ... are embedded into the same space. how to do embedding is more like an implementation issue; one can encode them into different spaces and learn them by learning an environment model. it is nothing fancynovel. i consider this papers main novelty to learn the optimal policy in the embedded state and action spaces, and the embeddings are learned by environment model learning. thus, the authors need to have strong evidence to persuade people 1 using an environment model to learn the embeddings is really useful; 2 a separate process of learning the policy in the embedded spaces is essential. such evidence is necessary to make this paper a solid contribution. learning an environment model has been used as an auxiliary task in deep rl. using such a baseline is to validate that it is necessary to learn the policy in the embedded space separately. the authors should also actively design other baselines to substantiate their claims.", "accepted": 0}
{"paper_id": "iclr_2021_oj3bHNSq_2w", "review_text": "i think we did learn something new from this paper, and i think the reviewers all seem to agree with this. the observation you make about the objective seems correct and interesting though reviewers and acs do sometime miss errors, but i have the following complaints that keep me from recommending acceptance 1. the theory seems right, but in practice, all sorts of gans with all sorts of objective functions experience mode collapse, so it doesnt seem like the issue you point out w the nsgan objective can be the whole story. however, we generally dont ask of a paper that it tells the whole story all in one go... 2. i do think the experiments are somewhat poorly done compared to those for say the median paper about gans that gets accepted to one of these conferences. moreover, many people have made similar experimental claims to the ones that are in this paper that havent held up on more complicated data sets, so i tend to apply more scrutiny to such claims when theyre only evaluated on smaller tasks. 3. there have, as r3 points out, been a huge number of papers proposing tricks for training gans, and some of them work really well. what im missing from this paper is an exploration of the relationship between your observation and those mostly adhoc tricks. does your observation explain why those tricks are necessary? does it explain why some existing trick works as well as it does? if your observation is totally orthogonal to existing tricks, can you get much better performance on a challenging data set by using it? i dont feel like i got satisfactory answers to those questions. all this being said, the paper was borderline, and i think if you dealt with some of the complaints above you would have a pretty good shot of getting a revised version accepted at another major machine learning conference.", "accepted": null}
{"paper_id": "iclr_2021_mLeIhe67Li6", "review_text": "this paper gives a way to learn onehiddenlayer neural networks on when the input comes from gaussian mixture model. the main algorithm uses janzamin et al. 2014 as an initialization and then performs gradient descent. the main contribution of this paper is 1. to give a characterization of sample complexity for estimating the moment tensors when the input distribution comes from a mixture of gaussian; 2. to give a local convergence result when the samples come from a mixture of gaussian. the paper claims certain behavior in the input data would make the problem harder and slow down the convergence, although the claim is based on an upperbound and would be stronger if there is some corresponding lowerbound.", "accepted": 0}
{"paper_id": "iclr_2021_IUYthV32lbK", "review_text": "i thank the authors and reviewers for the lively discussions. although reviewers agreed the work is interesting, there are some concerns about the significance of the results and experiments. none of the reviewers were strongly supportive of the paper while majority of them suggest that the paper needs a bit more work before being accepted. also, reviewers suggest that the paper is not easy to follow and its writing should be improved. given all, i think the paper , at the current stage, is below the accept threshold. i encourage authors to edit the paper according to the suggestions by the reviewers.", "accepted": 0}
{"paper_id": "iclr_2021_jQSBcVURlpW", "review_text": "this paper was reviewed by 4 experts in the field. the reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper, while the paper clearly has merit, the decision is not to recommend acceptance. the authors are encouraged to consider the reviewers comments when revising the paper for submission elsewhere.", "accepted": null}
{"paper_id": "iclr_2021_0Hj3tFCSjUd", "review_text": "before the discussion phase nearly all reviewers had doubts about the comparison of the current work with stateoftheart works notably yan et al., 2020, retroxpert, and graphretro. the authors then compared with these works and emphasized that these works rely on handcrafted features. they argue that the fairest comparison is the one where each method uses the same sort of features during traintest time. this is because in certain real world settings we may not have accurate estimates of such features e.g., atom mappings, templates, reaction centers. however, in the revised version of the paper the authors did not adhere to this concept of fair comparison in table 4 of appendix a.4. here their method uses reaction centers as input while baselines do not. while the authors claimed that the comparison here was designed to show how reaction centers provided as input improved performance, this doesnt seem like a good way to show it to isolate the improvement due to reaction center inputs you should fix everything else, i.e., the rest of the method. apart from the above contradiction, i buy the arguments of reviewers that distinguishing between methods that use handcrafted features and those that do not is not a meaningful distinction. one can apply atommapping or reaction center discovery algorithms as data preprocessing before applying other methods. ablation studies where such preprocessing is added or removed are interesting, but it is completely fair for any method to use such preprocessing before applying their method, it is up to the modeller. i would have argued for acceptance had the authors either a just included results from sota methods one, retroxpert was published 1 month after the iclr submission deadline, andor b reran their approach with such preprocessing. however the authors ended up hurting the submission by emphasizing a difference between using handcrafted features and not, then contradicting their experimental setup in table 4. this is a good paper, but i agree it is not ready to be accepted at iclr. i recommend the authors do the following a use any preprocessing they want for their method and compare with the stateoftheart, b if they want they can run their method without any preprocessing as an interesting ablation study, c remove table 4 as b already does this type of an ablation study, d describe recent work through the lense of ebm, e resubmit to a strong ml conference. the new submission will be much stronger.", "accepted": 1}
{"paper_id": "iclr_2021_4xzY5yod28y", "review_text": "the reviewers acknowledge that the paper has some promising experiments. however, they think that the theoretical contributions are not rigorous, specifically the assumption in theorem 2. it is true that the main part of the proof relies on this assumption. the main question is whether this assumption holds or not. the way that the authors provide some numerical verification is not convincing and does not necessarily help in this situation 1 mnist data set with lenet is definitely not enough to represent all situations. 2 this assumption depends on the algorithm so the parameter choices are also very important. some reviewers and i agree that this assumption may hold in some scenarios, but assuming it without proving it would significantly reduce the contribution of the paper. the following is the suggestion to improve the paper. since this assumption is not standard and hard to verify, the authors should verify more experiments with various data sets and network architectures with difference choices of the algorithm parameters to have some sense whether this assumption may be true or not. next, please try to show that this assumption holds or come up with different analysis with more reasonable assumptions. the authors should consider to improve the theory to strengthen the paper and resubmit this paper in the future venues.", "accepted": 0}
{"paper_id": "iclr_2021_rYt0p0Um9r", "review_text": "reviewers appreciate the numerical results presented in this paper. however, the paper needs a more rigorous theoretical investigation of the empirical phenomenon, or a more comprehensive empirical exploration to pinpoint the key factors. i recommend the authors to incorporate the suggestions from the reviewers and submit the paper to the next top conference.", "accepted": null}
{"paper_id": "iclr_2021_snaT4xewUfX", "review_text": "the paper presents a stochastic variational inference method for posterior estimation in a cox process with intensity given by the solution to a diffusion stochastic differential equation. the reviewers highlight the novelty of the approach. some of the concerns with regards to clarity have been addressed by the authors satisfactorily. however, an important issue of the approach is that of estimating model parameters, which the authors do not address explicitly by simply referring to that as the task of the modeller. i believe this is an important issue and, although some of the parameters can be estimated along with the neural network parameters, this has not been shown empirically. along a similar vein, the paper only presents results on a single real dataset the bikesharing dataset, which questions the applicability of the approach and no other baseline method is presented. at the very least, the authors should have provided an objective evaluation to other doubly stochastic point process models, e.g. based on gaussian processes, where modern stochastic variational inference algorithms have been presented.", "accepted": null}
{"paper_id": "iclr_2021_B9nDuDeanHK", "review_text": "description the paper discovers interesting phenomena in training neural networks with binary weights  connection between latent weight magnitude and how important its binarized version for the network performance training dynamics, indicating that large latent weights are identified and stabilize early on  observation that amongst learned binary kernel, several specific patterns prevail, up to the bits whos reversal has very little effect. this is so regardless of the architecture, the layer considered or the dataset. the paper further demonstrates how these observations may be used to compress binary neural networks below 1 bit per weight.  review process and decision the reviewers welcomed the experimental investigation of new phenomena, but commented the overall technical quality of the work as somewhat substandard. the redundancy of consecutive affine transforms is known and not connected to binary weights investigation. the investigation itself lacks a more indepth analysis. the proposed compression results appeared not convincing to reviewers since a significant drop of accuracy occurs. the ac shares these concerns and supports rejection.  general comments from my perspective, the study undertaken is methodologically wrong. an adhoc training method is investigated, which is not even clearly defined in the paper there are many ste variants and for which it is not known what it is doing, what are the realvalued weights for and whether they are needed at all as empirically argued by helwegen et al. 2019. as such, the investigation makes impression of poking a black box the training method in this case. at the same time, there are more clear learning formulations, applicable in the setting of the paper binary weights, in particular considering the stochastic relaxation  shayer et al. 2017 learning discrete weights using the local reparameterization trick  roth et al. 2019 training discretevalued neural networks with sign activations using weight distributions  peters et al. 2018 probabilistic binary neural networks these methods are approximate, but at least the optimization is well posed and it is known what do the realvalued weights represent e.g. logits of binary weight probabilities. from this perspective, it can be seen that latent weights close to 0 correspond to bernoulli weights that are almost fully random and thus only contribute noise and are fragile to gradient steps. therefore the model can only perform well if it learns to be robust to their state or their state becomes more deterministic corresponding to large latent weight. so one would actually expect to see in these models phenomena similar to the observed in the paper and not bee too much surprised or astonished by them. furthermore, there are recent works explaining ste and its latent weights as optimizing the stochastic relaxation  meng et al. 2020 training binary neural networks using the bayesian learning rule  yanush et al. 2020 reintroducing straightthrough estimators as principled methods for stochastic binary networks. the authors are encouraged to make the observed phenomena more explainable by connecting to the mentioned works.  further details  we show that in the context of using batch normalization after convolutional layers, adapting scaling factors with either handcrafted or learnable methods brings marginal or no accuracy gain to final model. from theoretical perspective, this is obvious and known to me. practically, there could be in principle some difference due to the learning dynamics, and verifying that there is none is a useful but a weak contribution. the section devoted to this issue can be given in the appendix but is not justified in the main paper.  change of weight signs is crucial in the training of bwns the sign determines the binary weights, so this is by definition.   firstly, the training of bwns demonstrates the process of seeking primary binary subnetworks whose weight signs are determined and fixed at the early training stage, which is akin to recent findings on the lottery ticket hypothesis for efficient learning of sparse neural networks in the lottery ticket hypothesis paper it is shown explicitly that the identified sparse subnetwork changes during the learning the most rather than retains its initialization state or the state in the beginning of the training. it is therefore could be of a different nature.", "accepted": 0}
{"paper_id": "iclr_2021_L-88RyVtXGr", "review_text": "i agree with the concerns raised by the reviewers. in particular, the issues of novelty and experimental evaluation mentioned in the revision summary remain the major weak points of the paper. my impression is that the changes made in the revision represent a significant experimental addition to the paper, one which might merit a full pass through peer review, and one which in any event did not alter the reviewers scores. while i think this paper has something to contribute and the empirical results suggest the method may outperform competitors, i think it would be improved by a rewrite and possibly a restructure that makes the part that is your contribution much more clear. for example, in the abstract, its only in the sentence we show both theoretically and empirically that potential vanishingexploding gradients problems can be mitigated by enforcing orthogonality to the shared filter bases that we actually get to the part that is really novel about this contribution the enforcing orthogonality that would ideally be much earlier in the abstract.", "accepted": 0}
{"paper_id": "iclr_2021_J5LS3YJH7Zi", "review_text": "inferring latent trajectory from noisy ca time series is an important and timely problem and the current study shows some progress in the inference problem. although the proposed model has some originality, there are remaining issues rendering the manuscript not ready for publication yet. reviewers raised issues on readability, lack of details, statistics of experiments, ca time constant estimation, effect size, and lack of comparison. through an extensive discussion and revisions, im happy to see the manuscript was greatly improved in readability and additional statistics were provided. however, the gaussianlfads performance at exactly chance raises red flags, effect size is small, and the significance of the scientific findings remain weak. the model is presented as a variational ladder autoencoder system with 2 layers, but the shallow latent representation is tied to the continuous approximation of the point process likelihood. hence, i view the model as an extension of lfads rather than a flathierarchical vae. overall, this paper has a potential of becoming a solid contribution for statistical neuroscience, once the above shortcomings are addressed.", "accepted": 0}
{"paper_id": "iclr_2021_lH2ukHnGDdq", "review_text": "reviewers could not reach consensus here and legitimate concerns are raised on novelty and on empirical results, although this can be attributed to the important computation times required to run experiments on 3d mri volumes. the authors have provided a comprehensive response to the reviews, the general feedback is that the work has merit but it fails to convince on its real contribution to the stateoftheart. at this stage, i fear this work cannot be recommended for acceptance.", "accepted": 0}
{"paper_id": "iclr_2021_CHTHamtufWN", "review_text": "the authors address the problem of learning environmentinvariant representations in the case where environments are observed sequentially. this is done by using a variational bayesian and bilevel framework. the paper is borderline, with two reviewers r2 and r3 favoring slightly acceptance and two reviewrs r4 and r1 favoring rejection. r4 points out that the current experiments do not do a good job of reflecting a continual learning setup and that simple modifications on existing irm based methods could outperform the method proposed by the authors. the authors are encouraged to take into account the reviewers suggestions to improve the paper. r1 argued initially that the proposed solution is not learning at all since it has errors very close to random guessing. while the authors have improved their method in the revision, the results are still close to random guessing, which questions the practical usefulness of the proposed approach. also, in the revision, the authors managed to obtain better results when their method is combined with environment inference for invariant learning eiil, but these results are secondary and not the main part of the paper. the authors should improve the work taking into account the reviewrs comments.", "accepted": 0}
{"paper_id": "iclr_2021_sAzh_FTFDxz", "review_text": "this paper studies the effect of anomaly detection using supervised learning with nonrepresentative abnormal examples on the tpr of the anomaly detection model. experiments demonstrate that when the abnormal examples presented in the training set are not representative of the abnormal examples in the target distribution, this can lead to bias in estimating the tpr. pros  this paper considers an important issue of tuning anomaly detection models in the absence of representative anomalies.  the paper is well written and easy to read. cons  the analysis and experiments provided in the paper are not surprising, and repeat, although perhaps in more detail, known effects of learning with a nonrepresentative training sample.", "accepted": 0}
{"paper_id": "iclr_2021_cuDFRRANJ-5", "review_text": "the authors study empirically and theoretically the behavior of neural networks under l_inftyperturbations on the weight matrix. for this purpose they first derive bounds on the logitlayer of the neural networks under perturbuations of a single or all layers. then they propose to merge this bound which depends on the product of the weight matrices into a marginbased loss functioncrossentropyloss and suggest to optimize this bound while simultaneously penalizing the 1,inftynorm of the weight matrices which appears in the bound. furthermore they derive a generalization bound for the robust error under weight perturbations. there was a discussion among the reviewers over this paper. while several ones appreciated the setting, there was concern about that the bound is potentially vacuous and that the theoretical results are messy. one reviewer criticized heavily the bound as not very useful and overly pessimistic. this paper is in my point of view borderline but i argue for rejection. there are several reasons for this  the motivation for this paper remains unclear. while the authors argue that a network could be attacked by changing logical values, this seems at the moment unrealistic as if the attacker has already hacked into the system much more harm can be caused in a much easier way e.g. by directly changing the output of the network. but even if one considers adversarial bit error attacks to be realistic, then the threat model would be completely different from l_infty and would rather be like l_0 with potential unbounded changes if the network is not quantized. if the target is to study that more flat minima generalize better, then the target would not be a bound on the robust error but a bound on the normal test error which integrates an upper bound which measures flatness of the function. as the derived theorem 4 contains a term where one has to take the supremum over the product of matrices over all functions in the derived function class, this is not true for the derived bound. thus i dont see why this bound is related to either of these two motivating topics.  the bound is incomplete in the sense that it contains the sup_f of the product of 1,infty norms of the weight matrices. why did the authors not try to upper bound this term over the chosen function class? even better would clearly to derive a bound on the rademacher complexity in terms of the norms which are actually appearing in the bound. in the current way the terms in the bound and the chosen function class are misaligned.  from a practical perspective the resulting loss is not useful for training deeper models in the experiments a four layer network is used as the product of the norm of the weight matrices grows exponentially in depth. moreover, as pointed out the ibp bounds of weng et al 2020 are much tighter than the bounds derived in this paper and even ibp bounds are loose  the experiments suggests that one gets a minor improvement in robustness while having to suffer from a significant drop in test accuracy figure 1b. regarding the achieved robustness for epsilon0.01 it remains completely unclear how this noise model relates to the normal size of the model weights one gets a robust error of around 30 on mnist. this is much worse than what has been achieved for mnist with much larger input perturbations epsilon0.3  the authors are missing an assumption on the activation function. with just nonnegativity, monotonicity and 1lipschitz the upper bound in a.2.1 original version cannot be derived. i guess you are implicitly assuming that rho00 but this assumption is not stated. there are other typos in the main text. in fact in the original version theorem 4 did not contain the term depending on b_h and s_j  this term was added in the revised version but not highlighted as all other changes. in total there are too many open issues here. while i appreciate the hard work the authors put into the author rebuttal, i think that this paper needs a major revision before it can be published.", "accepted": 0}
{"paper_id": "iclr_2021_IOqr2ZyXHz1", "review_text": "the authors consider the problem of causal inference from multiple conditionally ignorable models that yield different observed data distributions. this problem is distinct from transportability which assumes some types of causal invariance across domains, and aims to move causal conclusioned learned in one context to another using this invariance. the authors adapt a machine learning approach from shalit et al, 2017. because the authors describe an algorithm rather than a model, it was a bit difficult to understand what assumptions tie the different observed data distributions together i am guessing there is a way to formulate a global model tying all datasets together in terms of the algorithm hyperparameters but the authors do not discuss this. the authors evaluate their method via a simulation study. moreover, in response to reviewer criticism, the authors uploaded additional results from semisynthetic data. some of the concerns of reviewers were about novelty and scope of evaluation in addition, some complained about writing and notation.", "accepted": 0}
{"paper_id": "iclr_2021_SzjyTIc5qMP", "review_text": "this paper has been evaluated by four expert reviewers resulting in two rejections one marginal score and one acceptance recommendation. the authors provided rebuttals to the critiques, but they did not sway the reviewers assessments. the prevailing impression is that the work is interesting but perhaps not yet mature nor organized enough to benefit the iclr audience in its current form. there is also some vagueness left at the conceptual level, e.g. regarding the actual objectives  some reviewers pointed out confusing entanglement of the concepts of simplicity and interpretability. nonetheless, the paper presents an interesting work that will benefit from incorporating the constructive feedback received here", "accepted": 0}
{"paper_id": "iclr_2021_Tq_H_EDK-wa", "review_text": "this paper studies a timely problem and consider an interesting approach, but overall there were many concerns about technical details and the validity of the framework. the positive reviewer also mentioned concerns about the experiments, which others also found to be an insular comparison with weak baselines. following the response period, in discussion there are additional concerns arising related to the lack of details, for instance related to possible unidentifiability of the model. as one reviewer discusses, the authors are attempting to use rnns to impute missing infection status labels when the missingness mechanism is assumed to be i not at random, ii playing out over time as it is unclear whether yt is assumed conditionally independent of yt with t  t, and iii subject to interference whether someone is tested is the treatment here since its a missingess problem and one persons propensity to be tested could causally affect another persons downstream infection status since apparently no markov independence is assumed. there is also consensus that the writing quality can be greatly improved. overall this work contains some ideas with potential in a thorough revision", "accepted": 0}
{"paper_id": "iclr_2021_HI0j7omXTaG", "review_text": "this paper proposed a new measure of effective gradient flow egf, and also compared sparse vs. dense networks on cifar10 and cifar100. the notion of egf would be interesting, but the paper did not present enough evidence to support this notion.", "accepted": 0}
{"paper_id": "iclr_2021_W1uVrPNO8Bw", "review_text": "this paper uses concepts from physics to make predictions about stochastic gradient descent. the reviews point to two issues. firstly, the paper was not very accessible to those without a relevant background, and this is reflected in the low confidence rating reviewers gave. more importantly, two of the reviewers consistently pointed out vague mathematics and oversimplification in the mathematical arguments. the authors feedback did not successfully address the reviewers concerns, both r3 and r4 indicated there were outstanding concerns. i should note that despite giving low confidence scores and stating that some concepts from physics are beyond their field of expertise, reviewers gave high quality reviews with detailed comments and questions, and subsequently participated in the discussion revisiting their reviews. this suggests that the low confidence is not a symptom of insufficient reviewer effort, but perhaps a consequence of an inaccessible paper.", "accepted": 0}
{"paper_id": "iclr_2021_RDiiCiIH3_B", "review_text": "the work falls under the setting of learningbased sketchingcompressive subsampling. it extends the work of indyk et al 2019 including sparsity pattern optimization and some theoretical enhancements. the reviewers agree that while the conceptual novelty including the greedy optimization step is not too much, it is nonetheless interesting and is nontrivial. however, given the highly competitive submissions at iclr, the current scores are not sufficient for acceptance.", "accepted": null}
{"paper_id": "iclr_2021_VRgITLy0l2", "review_text": "this paper aims to study the convergence of deep neural networks training via a control theoretic analysis. this is a very interesting approach to establish theoretical understanding of deep learning. however, there are several concerns raised by the reviewers 1. the contribution of this paper is limited. the results simply follow from standard optimal control. it is not clear what new insight the paper provides. 2. there are already quite a few works on control theoretic analysis of deep learning. this paper did not do a good job on presenting its novelty and difference with existing works. 3. the experimental part is weak. it only involves small data set and very simple networks. based on these, i am not able to recommend acceptance for the current manuscript. but the authors are encouraged to continue this research.", "accepted": 0}
{"paper_id": "iclr_2021_r6I3EvB9eDO", "review_text": "the reviewers were clearly excited by the novel application of group theory to the problem of composition, and think the core idea is good. however, the reviewers also expressed concern about the clarity of the paper, stating that in several places examples might help. reviewers were also interested in seeing the work tied to real world applications, and how the work expands our existing knowledge about the composition of learned representations. i hope their suggestions will help the authors to turn this into a stronger, clearer paper.", "accepted": 0}
{"paper_id": "iclr_2021_ey1XXNzcIZS", "review_text": "this paper proposes routing strategies for multilingual nmt. the motivation is to train a single mixture model that can serve the training and prediction of multiple models. several strategies are proposed tokenlevel, sentencelevel and tasklevel. this is a simple and straightforward approach which is fine. the main concerns from the reviewers regard novelty and missing comparisons. in their updated draft, the authors added comparisons to bilingual models and they added a discussion wrt related work. however, the authors response did not address enough some of other reviewers concerns regarding comparison with other approaches, and the lack of novelty persists mixture models for multitask learning have been previously proposed in the literature, which makes me lean towards rejection. i suggest the authors address these aspects in future iterations of their work.", "accepted": 0}
{"paper_id": "iclr_2021_Jf24xdaAwF9", "review_text": "the reviewers have arrived at the consensus that this is a paper with an interesting idea, both novel and wellexplained, but not quite backed up with sufficient empirical evidence. like them, i think there is a lot of potential in modular methods for continual learning, and i know these are challenging advances to demonstrate. so i encourage you to persist, iterate and submit a stronger version of this paper in the future!", "accepted": 0}
{"paper_id": "iclr_2021_aRTRjVPkm-", "review_text": "we want to acknowledge that there has been a tremendous amount of work done during the discussion period on this paper for clarifying multiple points, adding multiple new comparison methods and new analysis. this is a very different draft than what was submitted and the reviewers acknowledged that. the draft is much closer from acceptance at iclr than it was at submission time. however, despite all those additions, we do not support a publication at iclr. the main issues with the current draft are its positioning and motivations. right now the draft is inbetween a paper about knowledge base construction kbc and a paper analyzing the knowledge contained within a large language model. this inbetween came up in multiple places during the discussion and is what causes the biggest confusion around this work. and, since there is no clear choice, the draft has limitation on either side.  if the main point is around kbc to build generalpurpose kbs, then one would expect experiments on downstream tasks powered by a kb, language understanding tasks for instance. indeed, kbs are just a means to an end and the latest advances in very large language models have shown that kbs were not essential to be state of the art in language understanding tasks glue, qa datasets, etc. so we would like to see whether these enhanced kbs could be beneficial. or the kbs are studied as a way to encode commonsense like in bosselut et al., 2019 or davison et al., 2019, but this is not the point of the current draft.  if the main impact of the draft is around what the language models learn, bridging the deep language model and knowledge graph communities through enhanced model transparency, as it has been said in the discussion, then the discussion with petroni et al. 19 should be more prominent and the introduction, motivation and experiments of the draft should reflect that. thats why, even if this work is of solid quality, the current draft can not be accepted.", "accepted": 0}
{"paper_id": "iclr_2021_T0tmb7uhRhD", "review_text": "this paper proposes a modelagnostic fl method called fedkt that performs only one communication round and reduces the communication complexity of federated learning. the reviewers have the following concerns about the paper  limited novelty because the proposed method is directly based on pate  insufficient experiments the authors did a great job of responding to the reviewers comments and also added some new experimental results in the updated version. but the reviewers still recommend significant revision of the paper and resubmission to a future venue. i hope the authors will find their constructive and detailed comments below helpful!", "accepted": 0}
{"paper_id": "iclr_2021_lo7GKwmakFZ", "review_text": "this paper proposes an extension of the monotonic policy improvement approach to the average reward case. although the reviewers acknowledge that this work has merits well written, clearly organized, wellmotivated, technically sound the reviewers have raised several concerns, which have been only partially addressed by the authors responses. in particular, reviewer4 is still concerned about the discrepancy between the theorem and the implemented algorithm, and the proposed simplification used in the implementation boils down to an algorithm that is very similar to trpo, thus making the contribution quite incremental as also stressed by reviewer1. furthermore, i share the concerns raised about the fairness of comparing algorithms that optimize different objective functions. i suggest the authors take into serious consideration the suggestions provided by the reviewers in order to produce an improved version of their work. the paper is borderline and i think that it needs another round of fresh reviews before being ready for publication.", "accepted": null}
{"paper_id": "iclr_2021_bK-rJMKrOsm", "review_text": "this paper proposes an interesting collaborative multihead attention mha method to enable heads to share projections, which can reduce parameters and flops of transformerbased models without hurting performance on ende translation tasks. for pretrained language models, a tensor decomposition method is used to easily covert the original mha to its collaborative version without retraining. this paper receives 3 weak reject and 1 weak accept recommendations. on one hand, all the reviewers agree that the paper is well motivated and the proposed idea is interesting. on the other hand, all the reviewers also commented that the current empirical results and comparisons are weak, which are not enough to support the papers main claim. from the current results, it is difficult to draw a conclusion that collaborative mha is better. specifically, i from table 2, it can be seen that the proposed method is not effective for pretrained models, i.e., even if the model size is not reduced much, the performance can be dropped significantly. ii more experiments, such as qa, more translationgenerated tasks will make this paper more convincing. iii more rigorous experiments are needed to justify the practical value of the proposed method. if the authors try to emphasize that they go beyond practical realm, then probably a careful repositioning of the paper is needed, which may not be a trivial task. the rebuttal unfortunately did not fully address the reviewers main concerns. therefore, the ac regrets that the paper cannot be recommended for acceptance at this time. the authors are encouraged to consider the reviewers comments when revising the paper for submission elsewhere.", "accepted": null}
{"paper_id": "iclr_2021_Ggx8fbKZ1-D", "review_text": "the paper proposes an optimization framework that automatically adapts the learning rates at different levels of a neural network based on hypergradient descent. the ac and reviewers all found the approach interesting and promising and appreciate the author feedback. we strongly encourage the authors to incorporate the points and additional results provided in their response to the reviewers. additionally, some concerns remain to be addressed regarding initialization of hyperparameters of combination weights. specifically it would be important to further investigate the impact of such initialization on the optimization performance. furthermore, additional experiments with other network optimizers would be valuable as pointed out in the reviews.", "accepted": null}
{"paper_id": "iclr_2021_cT0jK5VvFuS", "review_text": "this paper analyzes some design choices for neural processes, paying particular attention to their smalldata performance, uncertainty, and posterior contraction. this is certainly a worthwhile project, and r3 found the analysis interesting, giving the paper a score of 8. however, r1, r2, and r4 found the experimental validation to be incomplete and insufficient to support the papers broader recommendations. as the paper is investigating the various combinations of implementations, i tend to agree with r1, r2, and r4 that this paperwhile having some interesting ideasneeds a bit more precision and breadth to its experiments.", "accepted": 1}
{"paper_id": "iclr_2021_4jXnFYaDOuD", "review_text": "the paper proposes an autoencoder framework ima, a scalable model that learns the importance of modalities along with robust multimodal representations through a novel crosscovariance based loss function, in an unsupervised manner. they have compared their approach to sota methods via multiple experiments and shown how ima gives better performance. the authors have addressed some of the reviewers feedback. however, as pointed out by the reviewers, the experimental section needs better analysis of results and comparison to other methods, and the modeling section needs to be better explained and motivated. the authors have made changes in their revision, however the iclr review process does not allow for checking the cameraready. since we cannot accept the paper in its current form or with small variations and there have been many competitive submissions, we would encourage the authors to make their revisions and resubmit to other venues.", "accepted": null}
{"paper_id": "iclr_2021_WtlM9p1bVAw", "review_text": "this paper presents a continual learning method based on a novelty detection technique. all reviewers are concerned about various issues, especially, motivation, experiment, and presentation. one of the reviewers was initially positive about this paper but downgraded hisher score due to unresolved problems in the proposed method. considering all the comments and communications with the authors, ac believes that this paper is not ready for publication yet.", "accepted": 0}
{"paper_id": "iclr_2021_cbtV7xGO9pS", "review_text": "the paper proposes a reinforcement learning algorithm that combines trust region policy optimization and entropy maximization. the starting point is the lagrangian of a constrained optimization problem that upper bounds the change in the policy and lower bounds the entropy of the policy. the paper proves that the algorithm converges, and evaluates it experimentally in mujoco domains. the main issues raised by the reviewers were related to the proofs see especially r1 and experimental evaluation r4. the authors did a great job improving the paper during the discussion phase, but some of the issues remain unresolved, and thus reviewers find the paper not to be ready for publication. thus, im recommending rejection.", "accepted": null}
{"paper_id": "iclr_2021_9_J4DrgC_db", "review_text": "unfortunately some of the reviewers reactions to the author feedback wont be visible to the authors. the reviewers highly appreciated the replies and revision of the paper pros  the paper renders generalized exploration tractable for deep rl.  the idea is applicable to many drl methods and is potentially very valuable to deal with the headaches associated to drl. cons  r2 and r4 are still concerned about whether smart exploration will always be advantageous, and whether the added complexity is a good tradeoff for the potentially better performance. a comparison to pure exploration would still be insightful.  the new sac with deep coherent exploration only partially addresses the concerns of r2 and r4, especially in terms of performance while the paper has improved drastically during the reviewing process, there are still a few too many doubts.", "accepted": 0}
{"paper_id": "iclr_2021_QB7FkNVAfxa", "review_text": "the authors provide a new analysis of learning of twolayer linear networks with gradient flow, leading to some novel optimization and generalization guarantees incorporating a notion of the imbalance in the weights. while there was some diversity of opinion, the prevailing view was that the results were not sufficiently significant for publication in iclr.", "accepted": 1}
{"paper_id": "iclr_2021_vlcVTDaufN", "review_text": "this paper received high variance in the reviews. i personally agree with anonreviewer4 that the theoretical results presented in this paper are wellknown results on the sensitivity analysis of linear programs. see for instance introduction to linear optimization by bertsimas and tsitsiklis, chapter 5. more generally, these results are a special case of danskins theorem and the envelope theorem httpsen.wikipedia.orgwikidanskin27s_theorem httpsen.wikipedia.orgwikienvelope_theorem clarkes generalized gradients are just subgradients in the case of convex functions, which is the case here. my recommentation to the authors if they want to publish their work is to focus on the applications and to stop claiming novelty on the theoretical side.", "accepted": 1}
{"paper_id": "iclr_2021_in2qzBZ-Vwr", "review_text": "the reviewers have not supported the acceptance of this paper where the key weakness is that the study of the proposal neglect effect is not sufficient see the reviews for the details. i agree with the assessment of the reviewers and recommend rejecting the paper in its current form.", "accepted": 0}
{"paper_id": "iclr_2021_PuG6vCSbrV9", "review_text": "the paper provides an interesting set of theoretical ideas to improve the estimation of normalizing flows on datasets that fail to be fully dimensional. although the method is appealing, i believe the paper falls a bit short of acceptance at the conference. too many practical issues are left out, as discussed by reviewers, and the method seems promising but not fully connected to the rest of the literature on estimating lowdimensional distributions living in high dimensional spaces. we encourage the authors to use the feedback contained in this round of reviews to improve their work.", "accepted": null}
{"paper_id": "iclr_2021_9GUTgHZgKCH", "review_text": "this is a clear reject. none of the reviewers supports publication of this work. the concerns of the reviewers are largely valid.", "accepted": 0}
{"paper_id": "iclr_2021_0NQdxInFWT_", "review_text": "the review phase was very constructive, where reviewers raised several opportunities for improvements. the authors did a very good job in their rebuttal, which led some reviewers to change their opinion in a positive direction. overall, reviewers agree that this is the borderline paper with remaining concerns about the weak experimentation. the paper was again discussed by the area chair and program chairs. due to the competitive nature of the conference and the high bar of experimental evaluations expected by empirical papers, the paper was finally rejected. we hope authors will use the feedback from the reviews and make a stronger submission in near future.", "accepted": null}
{"paper_id": "iclr_2021_u8APpiJX3u", "review_text": "all reviewers agree that the paper is well written and some of the experiments are interesting. however, the paper did not clearly highlight how this work fits in with prior research, neither did it show what the advantages of the presented homogeneous network are. the authors addressed some of these concerns in the rebuttal, but not enough to sway the reviewers. in the end all reviewers recommend rejection, and the ac sees no evidence to overturn this recommendation.", "accepted": 0}
{"paper_id": "iclr_2021_eEeyRrKVfbL", "review_text": "this paper considers the problem of pruning deep neural networks dnns during training. the key idea is to include dnn elements only if they improve the predictive mean of the saliency efficiency of the dnn elements in terms of minimizing the loss function. the objective of early pruning is to preserve the subnetwork that can maximize saliency. this optimization problem is nphard, and even approximation is very expensive. the paper proves that one can simplify the approximation by ranking the network element by predictive mean of the saliency function. the proposed approach is novel as most of the prior work on pruning has focused on either i pruning on network initialization or ii pruning after the network has been fully trained. couple of issues with the paper are 1. current approach is somewhat complicated with many hyperparameters 2. experimental results are not very compelling when compared to pruning on network initialization overall, my assessment is that the paper takes a new research direction and has the potential to inspire the community, and followup work may be able to overcome the above two issues in future. however, due to the remaining shortcomings, the paper is not judged ready for publication in its present form. i strongly encourage to resubmit the paper after addressing the above two concerns.", "accepted": null}
{"paper_id": "iclr_2021_qSeqhriWKsn", "review_text": "while this paper was received pretty well, especially after the revision, reviewers still find it borderline and request further revisions which we cannot check in this short review cycle. therefore, we encourage the authors to improve the paper and resubmit to a future venue. in particular, please take into account the reviewers comment to improve the clarity of the paper. particularly it is critical to clarify the function class you are working with essentially polynomials more clearly than what you currently do i.e., your current gradient definition. it would be helpful for future work to clearly state that this function class is a shortcoming of your work, and that an interesting direction is to extend this to natural function classes in ml e.g., logistic loss.", "accepted": null}
{"paper_id": "iclr_2021_8YFhXYe1Ps", "review_text": "all the reviewers agree that the paper presents an interesting idea, and the main concern raised by the reviewers was the clarity of the paper. i believe that the authors have improved the presentation of the paper after rebuttal, however, i still believe that the paper woudl require another round of reviews before being ready for publication, in order to properly assess its contributions.", "accepted": null}
{"paper_id": "iclr_2021_bGZtz5-Cmkz", "review_text": "the reviewers liked the overall idea presented in this paper. although the idea as well as relevant tooling for incorporating constraints in the latent space has been studied a lot in the past, the authors differentiate their work by applying it in a new interesting problem. at the same time, some confusions about relation to prior work remain after rebuttal. firstly, the theoretical additions to prior work srinivas et al. 2010 are still unclear in terms of significance  they feel more like observations made on top of an existing theorem rather than fresh significant insights. furthermore, even if prior work has not considered exactly the same setup, it would still be needed to understand what the performance would be when considering prior models or prior datasets used in similar domains e.g. suggestions by r2, r3. the latter would be desirable especially since the experimental setup used in this paper is deemed by the reviewers too simple while the motivation of the paper is to solve an issue essentially manifesting in complex scenarios.", "accepted": 0}
{"paper_id": "iclr_2021_hPDC6tBFNiV", "review_text": "this paper investigates methods for producing and evaluating interval forecasts rather than point forecasts. the authors focus on spatiotemporal forecasts whose interval accuracy is measured with the mean interval score. pros uncertainty quantification is an important topic that is often ignored in the ml literature where the focus remains on point predictions. the covid19 example the authors use is a clear example of the need for such methods the cdcsponsored covid forecast hub mandates interval forecasts. cons in the words of one reviewer this paper is almost a review paper, a statement with which i fully concur. neither the evaluation metric mis nor the methods for generating intervals are novel. there is no particular effort to argue why mis and not weighted interval score or why these methods. if it were a review paper with more comprehensive coverage of the relevant stateoftheart a deficiency mentioned by multiple reviewers, it may make a more positive impression. as is, the story is mainly incomplete. to resubmit at another venue, i would suggest the authors either a state clearly what is new here in this paper or b make it a review paper that more comprehensively evaluates and discusses current state of the art. the statements in the last paragraph of page 1 read more like the authors are shooting for b than a we conduct a systematic study, we investigate. finally, the evaluations undertaken here do not really make use of or correct for the spatiotemporal task. presumably some locationstimeperiods are more difficult than others. so simply averaging over everything as in table 1 doesnt take the structure into account. it is likely better to examine relative accuracy to strawman forecaster or perhaps use a random effects model.", "accepted": 0}
{"paper_id": "iclr_2022_qyTBxTztIpQ", "review_text": "this paper studies the problem of how to collect demonstrations via crowd sourcing for imitation and offline learning. the paper received mixed reviews initially. the reviewers had difficulty understanding empirical results, asked for some more ablations, and were little unconvinced by the proposed usefulness of the collected data. the authors provided a strong thoughtful rebuttal that addressed many of those concerns. the paper was discussed extensively with one of the reviews who increased their score from 3 to 5. reviewers generally agree that the paper is good but not all reviewers are onboard with acceptance. ac recommends accept but agrees with the reviewers and the authors are urged to look at reviewers feedback and incorporate their comments in the cameraready.", "accepted": 1}
{"paper_id": "iclr_2022_V0A5g83gdQ_", "review_text": "this paper presents a tensor diagram view of the multiheaded selfattention mhsa mechanism used in transformer architectures, and by modifying the tensor diagram, introduces a strict generalization of mhsa called the tuckerhead self attention thsa mechanism. while there is some concern regarding the incremental nature of the proposition, the identification of where to usefully add the additional parameter that converts from mhsa to thsa was nontrivial, and the experimental results on the performance benefits across multiple tasks is convincing.", "accepted": null}
{"paper_id": "iclr_2022_xNO7OEIcJc6", "review_text": "this paper presents a new benchmark task for models similar to clip for evaluating how visual word forms interfere with the visual recognition of objects in images when the former are superimposed on the latter ones. specifically, by superimposing words belonging to different categories e.g., hypernyms vs basic labels the authors study the misclassification rates of clip under different degrees of varying similarity between the original and superimposes labels. all reviewers agreed that this is a novel and interesting study which, by productively using insights from cognitive science literature on language biases, aims at shedding light on the innerworkings of a popular artificial model. the main concern raised by reviewer p83y was regarding the claims around misclassification rates. indeed, since clip was not taught e.g., by funetuning or fewshot prompting which of the two labels i.e., the written or the visual is the correct one, its not fair to assess its performance on this way. while this is strictly true, the experimental protocols presented in sections 4.345 are still a valid way to assess representational inference. moreover, the authors have followed p83y suggestions and incorporated a fewshot prompting experiment in section 4.6. all in all, i think this will make for an addition to the iclr program and thus im recommending accepting this paper. minor comment wkss rightly pointing that this paper has, at best, a loose connection to compositionality. the authors changed compositionality  representations which is a better fit, so please make sure to change the title also in openreview when prompted.", "accepted": 1}
{"paper_id": "iclr_2022_lrocYB-0ST2", "review_text": "the paper addresses hierarchical kernels and provides an analysis of their rkhs along with generalization bounds and cases where improved generalization can be obtained. the reviewers appreciated the analysis and its implications. there were multiple concerns regarding presentation clarity, which the authors should address in the camera ready version.", "accepted": 1}
{"paper_id": "iclr_2022_lzupY5zjaU9", "review_text": "this paper proposes a simple meta algorithm to speed up data thinning algorithms with good theoretical guarantees. the method is both theoretically interesting and useful for practical applications.", "accepted": 1}
{"paper_id": "iclr_2022_EAy7C1cgE1L", "review_text": "the paper proposed a novel idea of requiring users to complete a proofofwork before they can read the models prediction to prevent model extraction attacks. reviewers were excited about the paper and ideas. some misunderstanding raised by reviewers were sufficiently clarified by the authors in the rebuttal.", "accepted": 1}
{"paper_id": "iclr_2022_DNRADop4ksB", "review_text": "this work starts from the observation that maximum likelihood estimation, while consistent, has a bias on a finite sample which is likely to hurt for small sample sizes. from this, they apply firth bias reduction to the fewshot learning setting and demonstrate its empirical benefits, notably relatively to l2 regularization or label smoothing alternatives. after some discussion with the authors, all reviewers are supportive of this work being accepted. two are also suggesting this work be featured as a spotlight. the proposed method is simple, well motivated, and appears to be effective. therefore, im happy to recommend this work be accepted and receive a spotlight presentation.", "accepted": 1}
{"paper_id": "iclr_2022_f9D-5WNG4Nv", "review_text": "the authors propose three strategies for coreset selection in the context of continual learning. in particular, the authors consider classimbalance and noisy scenarios. the authors run extensive benchmarks and ablation showing that the approach can be effective in practice. all reviewers were positive about this work, but found that the methodological contributions were relatively modest. the clarifications provided by the authors were highly appreciated. i would encourage the authors to revise the paper to incorporate these additional details as there were a number of concepts that reviewers found were not sufficiently documentedexplained and lacked clarity. i would also highly encourage the authors to explain their use of online continual learning as this reads like a tautology. finally, i would like to ask the authors to reflect on their insistance with the reviewers; while we would all want engaging and long discussions about our work, the reality is that reviewing papers and discussing them is time consuming and taxing, especially in the middle of continued pandemic. the authors should be grateful of the time reviewers have spent reading their work and providing feedback, and it is not in the authors interest to ask for a revision of the scores.", "accepted": 1}
{"paper_id": "iclr_2022_UYneFzXSJWh", "review_text": "the paper provides a solid and thorough analysis to the two basic methods of finetuning, linear probing lp and finetuning ft. the authors provide an important and highly interesting observation about the performance of both in and out of domain ood setting. they validate the known phenomena that ft outperforms lp in the indomain id setting, but demonstrate that when tested on ood data, lp is in fact more performant and back this observation with a theoretical and empirical analysis. the remedy provided is also a known, yet slightly less popular technique of setting the final layer lp first, then fintetuning ftlp. the authors provide thorough experiments showing that this technique enjoys the best of both worlds, meaning id and ood. i found it worth noting that during the rebuttal period the authors provided experiments on additional larger scale datasets and models and the results of the paper carried over to these new setting. the reviews agree that the analysis provided is both interesting and novel. even though the paper does not provide a new technique, there is a consensus that the understanding it gives on known techniques is a welcome addition to iclr.", "accepted": 1}
{"paper_id": "iclr_2022_v6s3HVjPerv", "review_text": "this work presents a novel and clever experiment for interpretable vision. reviewers all agreed that it tackles an important and interesting research question via a user study design. there are some concerns around the generalization and transfer to largescale realworld settings, as well as dataset construction. with the authors responses and discussion, i think the pros seem to outweigh the cons of this work a bit.", "accepted": 1}
{"paper_id": "iclr_2022_2t7CkQXNpuq", "review_text": "the current paper presents a new method for communication and cooperation in multiagent settings. specifically, the authors propose to model other agents intentions and internal states using tom nets and using these predictions to then decide how to communicatecoordinate. the authors present experiments in two multiagent cooperation tasks multisensor multitarget coverage and cooperative navigation, compare against 4 previous methods tarmac, i2c, mappo and hitmac and perform the necessary ablations studies and find that their method achieve better rewards in both environments. all reviewers have found the present study to be novel with convincing experimental findings. reviewers have raised some concerns however a great deal of those have been addressed by the authors during the rebuttal and many of these points have now been incorporated in the paper. having read the paper and considering the reviews i agree with the reviewers that this manuscript will make a good addition to the program of iclr and as such i recommend its acceptance.", "accepted": null}
{"paper_id": "iclr_2022_4C93Qvn-tz", "review_text": "the authors set up a simple combination of an energy based model and a flow based model that corrects the flow based model with an energy based term. the merits of this relative only an energy based model is improved sampling to compute the gradient. the advantage over a only flow based model is that the kinds of transforms that can be used are less limited.", "accepted": 1}
{"paper_id": "iclr_2022_UdxJ2fJx7N0", "review_text": "the paper addresses the problem of nonconvex nonconcave minmax optimization under the perspective of application of smoothed algorithms between two opponents. the paper examines a model where the maxplayer applied a zeromemory smooth from differential perspective algorithm and minplayer sgdsnag or proximal methods providing results similar with the stateofart. convergence guarantees proposed were sound and experimental results on generative adversarial networks and adversarial training demonstrate the efficiency of the proposed algorithms.", "accepted": 1}
{"paper_id": "iclr_2022_7MV6uLzOChW", "review_text": "this paper presents a method to turn a pretrained unconditional vae into a conditional vae by training an encoder to predict the unconditional vae latents given conditional input. on a variety of image tasks, the method is shown to perform competitively with gans, yielding good sample quality and diversity, and resulting in training time that improves on direct conditional generation approaches. while the technical novelty is limited, the strong empirical results and relevance given the growing availability of pretrained unconditional models lead me to recommend accepting this paper. ethics concerns have been raised for this paper. in particular, there were concerns with respect to the application of generative models, which inherit biases from the dataset, to guide medical imaging. it would be good to discuss this issue in more depth. a second point that was raised by the ethics committee is the fact that chest xrays are usually not taken in a sequential manner. we ask the authors to either provide evidence that xrays can be taken sequentially one can think of situations where thats the case, e.g., xrays of teeth in the mouth, preferably in the context of chest xrays; if thats not possible, please highlight that the application, as described in the paper, is unrealistic at the moment, and that it only serves as an illustration. the key point we therefore ask the authors to address is to ensure that the paper clearly states how realistic the application is and what potential problems may arise when using generative models in this particular domain.", "accepted": 1}
{"paper_id": "iclr_2022_VimqQq-i_Q", "review_text": "this paper presents some insightful suggestions for researchers studying generalization in federated learning by separating two types of performance gaps between training and test performance, the participation gap due to partial client participation and the performance gap due to data heterogeneity. they suggest that federated learning researchers use a threeway split between participating clients training data, participants clients validation data, and nonparticipating clients data to measure the generalization performance of an fl model. the paper presents thorough experiments to support their conclusions. a common concern about the paper is that the authors suggestions, although relevant and reasonable, are somewhat unsurprising and have been noted in different forms in other works in federated learning. another concern is that the conclusions are purely based on experiments and are not supported by theoretical justification. despite these concerns, the reviewers commended the overall insights presented in the paper. there was a healthy postrebuttal discussion and some reviewers reevaluated the paper and raised their initial scores. therefore, i recommend acceptance of the paper. i encourage the authors to take the reviewers constructive suggestions into account when preparing the final version of the paper.", "accepted": 1}
{"paper_id": "iclr_2022_HfUyCRBeQc", "review_text": "this is a welldone job which combines a few ideas to reach means to identify problematic cases and indicate this when classifying. it has raised doubts about the applicability, though i can see that a abstention rule can have multiple uses. while the work seems to be well done, it has not largely excited the committee members. it initially missed to be placed well wrt existing work to highlight the novelty, and the demonstration that the approach can be generally useful is not complete. dealing with abstention rules always brings another facet to classification and comparisons are not trivial in many situations. not surprisingly, this has landed as a borderline case, which i place on the inside as i like the topic and i think it is interesting work but it could become an outsider depending on the overall view of the selected papers for the conference and other constraints.", "accepted": 1}
{"paper_id": "iclr_2022_S874XAIpkR-", "review_text": "the paper studies the behavior cloning based strategies of offline rl algorithms in different type of environments and reports that performance primarily depends on model size and regularization. the results contradict some of the earlier claims, and the authors conjecture that model size and regularization characteristics can explain past results. during the review period, the reviewers agreed that the paper has certain merits, and on the other hand, they also raised some concerns, regarding some missing technical details, whether the empirical finding could be trusted, the generalization of the findings to more scenarios, and the comparison with some highly related papers. the authors did a good job in their rebuttal, which removed many of the above concerns although not all and convinced the reviewers to raise their scores. as a result, we believe it is fine to accept the paper although somehow like a weak accept.", "accepted": 1}
{"paper_id": "iclr_2022_gFDFKC4gHL4", "review_text": "the paper studies real world ml apis performance shifts due to api updatesretraining and proposes a framework to efficiently estimate those shifts. the problem is very important and the presented approach definitely novel. my concern is about limited novelty of the theoretical analysis and weak experimental evaluation just two dates, limited number of systems tested, small number of ablations. as of now the paper looks like an interesting but unfinished proposal. looking forward to the discussion between the authors and the reviewers to address the concerns. in the rebuttal, the authors have addressed reviewers comments, in particular by adding additional experiments that strengthen the paper. all the reviewers recommend the paper to be accepted. it is suggested that in the cameraready version the authors will add additional details regarding the experiments, as some of the reviewers mentioned.", "accepted": 1}
{"paper_id": "iclr_2022_psh0oeMSBiF", "review_text": "the authors develop a novel framework for certifying the robustness of rl agents against data poisoning attacks. they obtain lower bounds on the cumulative reward for several benchmark tasks. reviewers had concerns about certain organizational and technical aspects of the paper, but these were addressed well in the discussion phase and author responses. hence, i recommend acceptance. however, i would urge the authors to incorporate points from the discussion phase into the revised version, in particular the discussion with reviewers xueg and rqx2.", "accepted": null}
{"paper_id": "iclr_2022_ecH2FKaARUp", "review_text": "to tackle the problem of classification under inputdependent noise, the authors proposed the posterior transition matrix ptm to achieve statistically consistent classification. specifically the information fusion approach was developed to finetune the noise transition matrix. experiments demonstrated the effectiveness of the proposed approach. i would like to thank the authors for the detailed feedback to the initial reviews and also further feedback to the reviewers additional questions. many concerns were clarified by the feedback, and the additional experiments still demonstrate the effectiveness of the proposed method. the issue of data augmentation still remains, which should be at least experimentally investigated, but the contribution of the current manuscript is still valuable to be presented as iclr2022.", "accepted": 1}
{"paper_id": "iclr_2022_n0OeTdNRG0Q", "review_text": "this paper focuses on improving the efficiency of sharpnessaware minimization method for training neural networks. the proposals are stochastic weight perturbation, namely selecting subset of the parameters at any step, and sharpnesssensitive data selection. the philosophy behind sounds quite interesting to me, namely, sharpnessaware minimizer can be approximated properly with fewer computations after analyzing the minmax procedure. this philosophy leads to a novel algorithm design i have never seen. the clarity and novelty are clearly above the bar of iclr. while the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. thus, all of us have agreed to accept this paper for publication! please include the additional experimental results in the next version.", "accepted": 1}
{"paper_id": "iclr_2022_5QhUE1qiVC6", "review_text": "the papers makes progress on the important question of implicit bias in gradient based neural learning. remarkably they derive reasonable conditions for global optimality.", "accepted": 1}
{"paper_id": "iclr_2022_fvLLcIYmXb", "review_text": "the paper proposes a mlpbased architecture that makes extensive use of the shift operation on the feature maps. the model performs well on several vision tasks and datasets. the reviews are mixed even after the authors response. main pros are that the proposed architecture is elegant and reasonable, and the experimental evaluation is thorough and strong. the main con is that the novelty is somewhat limited to some prior papers. overall, i recommend acceptance. the reviewers point out that the architecture is good and the results are strong. similarities to prior works do not seem serious enough to warrant rejection  even an author of arguably the most related concurrent works  s2mlp and s2mlpv2  confirms that there is sufficient difference. moreover, this is one of the first papers to show very strong results on detection and segmentation.", "accepted": null}
{"paper_id": "iclr_2022_srtIXtySfT4", "review_text": "the reviewers were mostly concerned about the practical impactimplications of the proposed methods. there was a long discussion across multiple threads of the benefits of the approach proposed in cnns vs larger language models, dissecting the benefits in terms of training time as opposed to memory or flops, which may have a nonlinear impact on running time. overall, the authors did a good job of putting their contribution into context and addressing the reviewer concerns.", "accepted": 1}
{"paper_id": "iclr_2022_rpxJc9j04U", "review_text": "this paper has potential impact in the theorem proving community, and demonstrated the possibility of using lms for theorem proving in lean, and is good enough to use in the real world through an interactive theorem proving tool. the reviewers wish their datamodels were public to address some concerns raised by the reviewers, but we think the community can benefit from this work.", "accepted": 1}
{"paper_id": "iclr_2022_r5qumLiYwf9", "review_text": "the paper proposes a simple method for uniform sampling from generative manifold using change of variables formula. the method works by first sampling a much larger number of samples n from uniform distribution in the latent space and then does sampling by replacement using probability proportional to change in volume to generate a smaller number of final samples k  n that are seen as approximately sampled from a uniform distribution from the generative manifold. reviewers had some questionsconcerns about the confusing language in the abstract and introduction around the use of the term uniform which the authors have addressed satisfactorily. authors have also provided results on quality fid metric of the generated samples as asked by the reviewers. while the proposed method is rather simple, has high computational cost, and novelty is marginal as noted by two of the reviewers, reviewers agree it is above the acceptance bar.", "accepted": 1}
{"paper_id": "iclr_2022_OzyXtIZAzFv", "review_text": "the paper is an interesting take on representation learning, using prior tasks to determine which information is important. the problem setting is somewhat difficult to pin down, so that that finding the correct comparisons is not obvious and opinions differ on many details of the setup. however, this is not a fault of the paper; it is a general problem the further one moves away from clean settings like classical supervised learning. there was a lengthy and detailed backandforth between the authors and reviewers, where the authors clarified most of the points raised, extended their results, resulting in one reviewer switching from reject to accept.", "accepted": null}
{"paper_id": "iclr_2022_Ek7PSN7Y77z", "review_text": "i thank the authors for their submission and active participation in the discussions. all reviewers are unanimously leaning towards acceptance of this paper. reviewers in particular liked that the paper is wellwritten and easy to follow 186e,tadh,exgo, well motivated tadh, interesting pskh, novel 186e and provides gains over baselines 186e,tadh,pskh with interesting ablations 186e,exgo. i thus recommend accepting the paper and i encourage the authors to further improve their paper based on the reviewer feedback.", "accepted": 1}
{"paper_id": "iclr_2022_CSfcOznpDY", "review_text": "this paper proposes an algorithm for achieving disentangled representations by encouraging low mutual information between features at each layer, rather than only at the encoder output, and proposes a neural architecture for learning. empirically, the proposed method achieves good disentanglement metric and likelihood reconstruction error in comparison to prior methods. the reviewers think that the methodology is natural and novel to their knowledge, and are happy with the detailed execution. the authors are encouraged to improve the presentation of the paper, by providing rigorous formulation of the markov chains to avoid confusions, justification of the independence assumptions behind them, and more indepth discussions of the learning objectives.", "accepted": null}
{"paper_id": "iclr_2022_MQ2sAGunyBP", "review_text": "this paper focuses on using reference objects for long distance estimation by introducing a novel dataset and an attention based learning framework. while the presentation flows well and the methodology is practically useful, it is only marginally significant and novel. some of the practical data augmentation aspect raise some question on whether the process wouldnt confuse the network  the authors provide empirical evidence of the contrary in their response, although i find the principled argument to be somewhat lacking.", "accepted": 1}
{"paper_id": "iclr_2022_ZBESeIUB5k", "review_text": "this paper carefully shows how all the stochastic elements in neural network training could be removed by using full batch, and a dataset with fixed augmentation and still maintain good performance, by adjusting hyperparameters and adding explicit regularization. all reviewers were eventually positive and recommended acceptance, except one reviewer, who was initially not aware of the recent theoretical interest in this question, and was therefore less surprised. there are three remaining issues with the current version 1 operations in cudnn are, by default, nondeterministic, but can be made deterministic. though i believe this will not affect the final results, without it, the title and conclusions of this paper are technically unjustified. the authors have agreed to add this to the camera ready version of the paper. 2 deterministic training was only shown on cifar10. i understand imagenet would be too heavy for this task, but there are many other small and mediumsized datasets, and i think showing that this on several such datasets would strengthen the message of this paper, and convince more readers. 3 the question of how to achieve good performance with deterministic training is still mostly unknown, as it seems to require significant hyperparameter search with unknown sensitivity, and no conclusion was reached regarding the how to properly adjust them. however, i agree that a good answer to this question might not exist. lastly, i recommend the authors to mention in the main paper the new baseline experiment, where the explicit regularization is added to sgd. i saw it in the appendix, but i didnt see it mentioned in the main paper maybe i missed it. i think without it, many readers will not be fully convinced as several reviewers requested it.", "accepted": 1}
{"paper_id": "iclr_2022_JfaWawZ8BmX", "review_text": "this paper extends recent and very active literature on analyzing learning algorithms in the simplified setting of gaussian data and model weights, with the main generalization being to allow for nonisotropic covariance matrices. the main technical results seem to be correct and slightly novel, though reviewers feel they are not innovative or unexpected enough to stand on their own. however, the main contributions of the paper are then to interpret these results to give phenomenological results regarding double descent, etc., and reviewers were unanimously happy with these. in the end, all reviewers were positive about the paper. the largest reviewer criticisms of the paper were technical issues ot31 and lack of context of recent literature 3rfg. both of these concerns were mostly addressed by the revisionrebuttal. the reviewers still had specific comments on improvement, but found no major faults.", "accepted": 1}
{"paper_id": "iclr_2022_6Tk2noBdvxt", "review_text": "this paper presents an approach to synthesize programmatic policies, utilizing a continuous relaxation of program semantics and a parameterization of the full program derivation tree, to make it possible to learn both the program parameters and program structures jointly using policy gradient without the need to imitate an oracle. the parameterization of the full program derivation tree that can represent all programs up to a certain depth is interesting and novel. in its current form this wont scale to large programs that require large tree depth, but is a promising first step in this direction. the learned programmatic policies are more structured and interpretable, and also demonstrated competitive performance against other commonly used rl algorithms. during the reviewing process the authors have actively engaged in the interaction with the reviewers and addressed all the concerns, and all reviewers unanimously recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_Fza94Y8VS4a", "review_text": "this paper examines the evolution of densities of initial conditions under the multiplicative weights update rule for learning in twoplayer zerosum games. specifically, the authors estimate the differential entropy de of a density of initial conditions as it evolves over time what they call uncertainty, and they show that a as long as the density of states assigns sufficient mass to all strategies, its de will increase; and b the density of states will get arbitrarily close to the boundary of the state space infinitely often i.e., at least one pure strategy will be employed with arbitrarily small probability infinitely often. the authors also apply these results to a populationlike model of learning as well as an optimistic variant of the mwu protocol the latter in the supplement. the paper was extensively discussed during the reviewrebuttal phase. while the reviewers appreciated the conceptual contributions of the paper, they also identified certain technical shortcomings that were only partially addressed by the authors. one of these issues concerned the possibility that the density of initial conditions may exhibit singularities, in which case the de may fail to be welldefined. as a result, one of the reviewers indicated an intent to downgrade their score from 8 to 3 due to concerns on the correctness of the results presented in the paper. after discussing with both the authors and the reviewers, my view is that the merits of the paper outweigh its flaws, so i am making an accept recommendation. at the same time, there is a number of revisions that the authors will have to undertake in the cameraready version of their paper 1. the authors need to be more careful with their assumptions and notation. the reviewers already indicated a number of glitches, most of them easily fixable so they are not of particular concern. on the other hand, the issue of whether the initial density of states becomes singular or not is more subtle and led one of the reviewers to drastically change their evaluation of the paper. the problem here is that the authors are not being precise in their assumptions for g1 and its support, and this confusion remained throughout the discussion the authors are looking at distributions that are smooth with bounded support, but this does not exclude singularities. the counterexample given to the authors was a random variable x supported on mathcalx  0,1 with density gx  12sqrtx; this density has bounded support and it is smooth on its support, but it is not itself bounded. there is an ambiguity here in whether the authors are considering the support to be closed or not. the issue for the initial density can be trivially fixed by asking that g1 be itself bounded or smooth over the closed support, or any other similar statement. however, even if this is assumed for g1, the density at some later time t could, a priori, become singular incidentally, this is a problem that arises frequently in the study of densities that evolve over time, e.g., as in optimal transport. thus, even an explicit assumption for g1 does not suffice to ensure that gt does not develop singularities in future stages. incidentally, the authors reply that the singularity has measure zero and therefore does not contribute to the integral misses the heart of the matter and raises concerns about the authors overall treatment of this question the function gx  log 2 big x log2x has infinite differential entropy over 0,12 even though it is a smooth density over 0,12. to be clear, i do not believe that blowups actuall occur in the authors model, but there is still something that needs to be shown here. however, since it is impossible to check an argument or proof at this stage and i do not think it would be fair to let this stand in the way of accepting the paper, the authors should instead revise their paper to add as an explicit assumption that gt has bounded support and is bounded over its support or clarify whether they take the support to be closed or not. 1. another concern revolves around the use of the word uncertainty to describe the basic premise of the paper. in the authors model, this does not refer to uncertainty among the learners all their observations are perfectly certain and deterministic, so it is not used in the sense that is standard in game theory and learning cf. the classic works of bertsekas, dekel, fudenberg, tsitsiklis, and many others. instead, the authors use of the word seems to refer to some outside spectator who can only partially guess the players initial conditions, and tries to guess the evolution of the players mixed strategies but still has full information about the learning model that players use, its parameters, etc.. however, this model is not fleshed out in sufficient detail by the authors, so the term uncertainty does not seem appropriate here. during the rebuttal phase, the authors argued that the goal of their paper is bringing the notion of de to machine learning audiences attention as a measure of uncertainty, explaining how the change of de is related to the jacobian of the underlying dynamical systems and they asked that the papers title remains as is. while i am sympathetic to the authors request, the fact remains that the current title and part of the discussion in the abstract is not representative of the paper. given the authors stated objective, the simplest solution would be to frame the paper as the evolution of differential entropy under... or the evolution of spectatorobserver uncertainty or something of the sort. both titles carry more information and, based on the authors input, are more appropriate for the range of ideas the authors wish to convey  but simply saying uncertainty goes against the established terminology of the field. overall, i would urge the authors to avoid vagueambiguous terminology and statements, and focus instead on exact mathematical definitions that are not open to interpretation. the ideas presented in the paper are interesting and fresh, so they deserve a likewise sharp and precise treatment.", "accepted": 1}
{"paper_id": "iclr_2022_EZNOb_uNpJk", "review_text": "this paper aims at raising awareness of climate change by ganprojecting flooding images of popular places. this is an interesting case. while all reviews agree that this is an interesting direction, they also value the contributions differently. two of them would like to see more methodological contributions, two focus more on the contribution to the psychological fight of climate change. nevertheless, the rolling discussion helped to clarify several of the issues raised by the reviewers, and in my opinion combining existing methods to realize an important model that is one little step towards making people more aware how climate change may impact their own lifes is highly creative and useful. i therefor overall suggest strongly to accept the paper. as the iclr cfp reads, iclr is not just about methodological contributions. societal considerations of representation learning are explicitly mentioned.", "accepted": null}
{"paper_id": "iclr_2022_hcoswsDHNAW", "review_text": "this paper improves the training speed and decrease the computation cost of advprop, which is a method that leverages the adversarial example to improve the image recognition accuracy. the method achieves the speedup by leveraging a collection of practical heuristics, including reusing some gradient computation during training. the paper is well written, well justified with empirical supports, and can be potentially useful in many vision tasks. on the other hand, some novelty of the method is incremental, and the issues regarding empirical results and claims pointed out by the reviewers need to be addressed in the revision.", "accepted": 1}
{"paper_id": "iclr_2022_g5ynW-jMq4M", "review_text": "the paper provides new insights about how to identify latent variable distributions, making explicit assumptions about invariances. a lot of this is studied in the literature of nonlinear ica, although the emphasis here is on dropping the i. i think more could be said about how allowing for dependencies among latents truly change the nature of the problem since any distribution can be built out of independent latents, by some more explicit contrast against the recent references given by the reviewers. in any case, the role of allowing for dependencies in the context of the invariances adopted is discussed, and despite no experimentation, the theoretical results are of general interest to the iclr community and a worthwhile contribution to be discussed among researchers in this field.", "accepted": 1}
{"paper_id": "iclr_2022_IDwN6xjHnK8", "review_text": "the paper aims to improve image and video compression keeping in mind computation cost. in this regard, authors propose variants of swintransformer for image and video coding. the experimental results shows that transformer based transforms can replace conv based transforms in image and video compression, and simultaneously achieving better ratedistortion performance at much faster decode times, i.e. resulting in a better ratedistortioncomputation tradeoff. we thank the reviewers and authors for engaging in an active discussion. the reviewers found some results to be surprising in a good way and are in a consensus that the empirical results are strong across datasets for image and video compression. for completeness, the authors should provide flops or cpu runtimes in the final version so that one can compare to methods like vtm even if cpu is not the desired hardware for proposed method.", "accepted": 1}
{"paper_id": "iclr_2022_t5EmXZ3ZLR", "review_text": "four reviewers have evaluated this submission with one score 6 and three scores 8. overall, reviewers like the work and note that a rigorous and principled approach is taken by this work. ac agrees and advocates an accept.", "accepted": 1}
{"paper_id": "iclr_2022_dTqOcTUOQO", "review_text": "the article proposes an approach to alter the approximate posterior distribution in order to remove some of the information unlearning. the approach is applicable when the approximate posterior is obtained via stochastic gradient mcmc methods. unlearning is done by shifting the approximate true posterior by some value delta, which is found via optimisation with an influence function. the approach is novel, tackles an important problem and is mathematically sound. reviewers have highlighted some of its limitations. in particular, the modified posterior is obtained by translating the original posterior, without changing its shape; many aspects of the data may therefore not be forgotten. the authors have partially addressed this concern in their response and provided additional experiments. although there is still disagreement amongst reviewers, i recommend acceptance. a minor comment i would not present mcmc merely as a machine learning algorithm p.1, nor as a sampling based bayesian inference method p.1 and p.2. mcmc is a generic approach to approximate highdimensional integrals and obtain samples approximately sampled from some target distribution, dating back from the work of metropolis 1953 and hastings 1970. its application to bayesian inferencemachine learning problems came much later, see e.g. the excellent review of c. robert and r. casella a short history of mcmc subjective recollections from incomplete data. statistical science, 2011.", "accepted": 1}
{"paper_id": "iclr_2022_9kpuB2bgnim", "review_text": "this paper proposes an adaptive sparse huber additive model for for forecasting nonstationary time series. the prior work has considered similar models for gaussian innovations which is overly restrictive for a variety of applications such as finance. the results are supported both by theory and experiments. the results are novel and are of interest to iclr and machine learning communities in general.", "accepted": 1}
{"paper_id": "iclr_2022_oU3aTsmeRQV", "review_text": "this paper proposes selfensemble adversarial training seat for yielding a robust classifier by averaging weights of history models. the solution is different from an ensemble of predictions of different adversarially trained models. the authors also provided theoretical and empirical evidence that the proposed selfensemble method yields a smoother loss landscape and better robustness than both individual models and an ensemble of predictions from different classifiers. the paper receives a mixed rating of 8665 after private discussion; initially it was 8653, and all reviewers actively engaged in discussion. from the three positive reviewers, it is in general consensus that this paper has a clear motivation, is easy to follow, and owns reasonable not exceptional novelty. the negative reviewer poses a number of concerns, citing the absence of adaptive attack evaluation, the unclear difference between vanilla ema and seat, and the proof of proposition 1. the authors provided detailed responses and the negative reviewer was partially convinced not fully after viewing other comments. ac carefully reads all discussions and feels this fall into a borderline case. the authors did solid work and there is no fatal concern as ac can see. the majority sentiment is that this is a good paper, just not an exciting one. hence, the current recommendation is a borderline acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_Azh9QBQ4tR7", "review_text": "the authors propose a simple addition to adversarial training methods that improves model performance without significantly changing the complexity of training. the initial reviews raised some questions about whether experiments were sufficiently extensive, but these issues were resolved during the rebuttal and discussion period, resulting in a strong consensus that the paper should be published.", "accepted": 1}
{"paper_id": "iclr_2022_4Muj-t_4o4", "review_text": "the paper tackles the problem of generalizing to a new environment by learning a small set up anchor policies even just 2 for the final approach which span a subspace that can be searched efficiently in a new environment. the discussion and additional experiments managed to convince most reviewers that the method indeed works as the authors had hypothesized especially regarding functional diversity. at the moment the analysis is mainly based on empirical observations, it would be good to also have a thorough theoretical analysis of the method.", "accepted": 1}
{"paper_id": "iclr_2022_01AMRlen9wJ", "review_text": "this paper presents a novel methodology for performing meta learning for gradientbased hyperparameter optimization. the approach overcomes limitations scaling, e.g. of previous methods through distilling the gradients of the hyperparameters. the paper received 4 reviews, of which all were positive 6, 6, 8, 8. the reviewers appreciated the technical clarity of the paper and found the proposed approach sensible, novel, technically sophisticated and effective. the main concerns were regarding the comprehensiveness of the experiments and technical presentation of the dataset distillation. it seems that the reviewers found the author response lots of results were added satisfactory regarding these points. thus the recommendation is to accept.", "accepted": 1}
{"paper_id": "iclr_2022_NkZq4OEYN-", "review_text": "this paper addresses audiovisual navigation tasks where a reinforcement learning agent perceives visual rgb and binaural audio inputs, rendered in a firstperson perspective 3d environment, and is tasked to navigate to the audio source. the authors propose to make the rl navigation policy robust, by training the agent with additional adversarial audio perturbations. these perturbations consist of an adversarial ghost agent attacker that emits noise perturbations volume, position and category determined by policies that are trained to maximise the negative rewards for the navigation agent in zerosum game. the agent is then evaluated on the simulated replica and matterport3d environments and compared to a few baselines. the authors conduct a large number of ablation experiments. the three reviewers were globally positive about the paper, regarding the motivation, joint training of the agent and attacker, and experimental evaluation. reviewer tlmn had questions about specific results and ablations of existing baselines, whereas reviewer i5vv had questions about random noise ablations  the authors provided responses for these questions. outstanding requests were about proofreading. after rebuttal and discussion, the scores for this paper are 6, 8 and a weak 8 or 7, i.e., an average of 7, and thus i believe that the paper meets the conference acceptance bar.", "accepted": 1}
{"paper_id": "iclr_2022_2-mkiUs9Jx7", "review_text": "this paper proposes an unsupervised learning method for gans, called slogan, which allows conditional generation of samples, by utilizing clustering structures of training data in a latent space. the main significance of the proposal over existing unconditional conditional gans is that it is capable of dealing with training data with imbalance in the latent space. the proposal consists of the use of implicit reparameterization based on the generalized stein lemma, which makes learning of the mixing coefficient parameters possible, as well as introduction of the u2c loss. the initial review score distribution is such that two of them are just above the acceptance threshold, and two others are just below it. upon reading the review comments and the author responses, as well as the paper itself, i think that the evaluations of the reviewers are more or less coherent with each other 1. the proposed method is moderately, if not significantly, novel the differences from deligan are the use of implicit reparameterization based on the generalized stein lemma, learning of the mixing coefficient parameters, and introduction of the u2c loss. 2. the experimental results, while demonstrating effectiveness of the proposed method to some extent, were not convincing enough. as for the item 2, the authors have provided results of additional experiments in their responses, as suggested by the reviewers, and two reviewers have revised their scores upward accordingly. yet another point i would like to mention is that in some numerical results summarized in tables 1 and 2, as well as in several other places, one can notice somewhat large errors, so that one might be able to question the statistical significance of the claimed bestperforming methods, shown in bold. if my guess would be correct, the authors regarded the best in the mean as the best, ignoring the standard error, and did not perform any statistical testing to confirm the significance. i would therefore appreciate additional assessment of significance of the numerical results via proper statistical testing. because of the above, i would like to recommend acceptance of this paper.", "accepted": null}
{"paper_id": "iclr_2022_hR_SMu8cxCV", "review_text": "this is a strong empirical paper that studies scaling laws for nmt in terms of several new aspects, such as the model quality as a function of the encoder and decoder sizes, and how the composition of data affects scaling, etc. the extensive empricial results offer new insights to the questions and provide valuable guidance for future research on deep nmt. the datasets used in the study are nonpublic, which may make it hard to reproduce the evaluation.", "accepted": 1}
{"paper_id": "iclr_2022_9NVd-DMtThY", "review_text": "this paper considers the problem of distributionally robust fair pca for binary sensitive variables. the main modeling contribution of the paper is the consideration of fairness and robustness of the pca simultaneously, and the main technical contribution of the paper is the provision of a riemannian subgradient descent algorithm for this problem and proof that it reaches local optima of this nonconvex optimization problem. the results will be of interest to those working at the intersection of fair and robust learning.", "accepted": null}
{"paper_id": "iclr_2022_QjOQkpzKbNk", "review_text": "this paper studies the problem of distilling the knowledge present in different ganbased image generation tasks. the paper received mixed reviews. the reviewers had difficulty understanding some details regarding the approach, and requests for ablations and clarifications on existing empirical evaluation. the authors provided a strong thoughtful rebuttal that addressed many of those concerns. the paper was discussed and two reviewers updated their reviews in the postrebuttal phase. reviewers generally agree that the paper should be accepted but still have concerns regarding contribution and writing. ac agrees with the reviewers and suggests acceptance. however, the authors are urged to look at reviewers feedback and incorporate their comments in the cameraready.", "accepted": 1}
{"paper_id": "iclr_2022_DBiQQYWykyy", "review_text": "the authors have done a good job methodologically addressing reviewer concerns. the empirical results are good, and the application impact is clear. there were some concerns about the technical heft of the approach, but theres overall agreement that the effective application to the domain is interesting and done very well. the ac is a bit concerned about the impact of the regularities of the domain used on the results, especially with regard to semantic regularities homes have very particular regularities. but even without answering this question it should be discussed in the camera ready though, this paper makes a solid contribution.", "accepted": 1}
{"paper_id": "iclr_2022_VLgmhQDVBV", "review_text": "there was a healthy discussion with all the reviewers with a consensus that the results are somewhat expected and unlikely to shed light beyond the ntk regime, yet within the confine of ntk there is a solid and nicely written technical contribution.", "accepted": 1}
{"paper_id": "iclr_2022_nZOUYEN6Wvy", "review_text": "the ac and reviewers all agree that the paper proposes a very interesting framework to extend granger causality to dag structured dynamical systems with important applications. the submission was the object of extensive discussion, and the ac and reviewers all agree that the author feedback satisfactorily addresses the vast majority of their concerns. we strongly urge the authors to incorporate all the points and revisions mentioned in their feedback. we certainly hope that the author will pursue this line of work and consider scaling their approach to tackle larger applications such as those related to social networks.", "accepted": 1}
{"paper_id": "iclr_2022_xkjqJYqRJy", "review_text": "this paper provides some empirical investigation of the choice of the prior distribution for the weights in bayesian neural networks. it shows empirically that, when trained via sgd, weights in feedforward neural networks exhibit heavytails, while weights in convolutional neural networks are spatially correlated. from this observation they show that the use of such priors leads to some improved performances compared to the iid gaussian prior in some experimental settings. reviewers have conflicting views on this paper, that have not been reconcilied after the authors response and the discussion. on the plus side, the paper is very well written, the experimental part is carefully conducted, and provides some insights on the choice of the prior in bayesian neural networks, which could lead to further developments. on the negative side, the claims made in the introduction are not fully supported by the experiments the claims have been slightly amended in the revised version, and the takehome message is not so clear. in particular, bayesian approaches with the proposed priors still underperform compared to sgd without tempering. the authors could also have considered a broader sets of experiments. overall, i think the contributions outweight the limitations of this paper, and i would recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_KSSfF5lMIAg", "review_text": "the paper studies interpretability in multi instance learning where model is trained with a label provided for a bag of instances. the author proposes modelagnostic weightsampling strategy to improve sampling in prior methods such as shap, and evaluate their performance on three datasets and authors provided results on more datasets during rebuttal. all reviewers agree the paper is well written and well motivated. the paper presents a simple but meaningful extensions to existing interpretability study and will be helpful for the community. reviewers had some concerns with the comprehensiveness of the evaluation, the strength of their proposed results, and the originalitynovelty of the paper. the authors have provided further experimental results on new datasets as well as additional baselines. given the study of mil setting in interpretability is scarce, i am leaning towards the acceptance.", "accepted": null}
{"paper_id": "iclr_2022_rzvOQrnclO0", "review_text": "the paper considers modelbased rl, and focuses on approaches that benefit from the differentiability of the model in order to compute the policy gradient. it theoretically shows that the error in the gradient of the model w.r.t. its input appears in an upper bound of the error in the policy gradient computing using the learned model. motivated by this, it suggests a mbrl approach that learns two models, one of them minimizes the nextstate prediction error as commonly done and the other minimizes a combination of prediction error and the gradient error. the paper empirically studies the method through extensive experiments. reviewers are generally positive about this work. they believe that the paper is insightful and the method is original. at first, there were some important concerns raised by the reviewers, but the authors revised their paper in the discussion period, and it appears that the reviewers are all satisfied now. i also read the paper during the rebuttal phase, and i should say that i have some concerns myself, especially on the theory part of the paper. given that the authors did not have an opportunity to answer my questions, i do not put much weight on my concerns and i believe most of them can be addressed with some clarifications. considering the positive response of reviewers and promising results, i am going to recommend acceptance of this paper. i strongly encourage the authors to consider the comments by reviewers, as well as the following ones, in the revision of their paper. comments 1 the true dynamics f is defined as a stochastic one, i.e., s_t1  fs_t, a_t, epsilon_t just before eq. 1, and similarly for the learned model. here epsilon_t is the noise causing the stochasticity of the model. but later, when the errors on the model and its gradient are introduced i.e., epsilon_f and epsilon_fg, the role of stochasticity becomes unclear. for example, we have  tildefs,a  fs,a  leq epsilon_f. what happened to the noise term? the same is true for eq. 5. the nextstate s either according to the true dynamics or the learned model is random. in that case, it is not obvious how to interpret eq. 5. is it the error of the expected gradient of the next state? or is it something else? in case the dynamics is assumed to be deterministic, this should be clarified early in the paper. 2 the upper bound in theorem 1 might be vacuous if the lipschitz constant l_f of the model is larger than 1. to see this, consider lemma 1. the constant c_0 is min depsilon_f, 1l_ft11  l_f. if l_f is larger than 1, for large enough t, the term 1l_ft11  l_f blows up and c_0 becomes depsilon_f. therefore, the upper bound of lemma 1 becomes d. here d is the diameter of the state space, which is assumed to be bounded. this carries to in the next lemmas. in lemma 4, c_5 would be of the same order as c_0 multiplied by an extra l_1 l_f  1  gamma , so the upper bound of this lemma becomes proportional to d too. the c_0s appearance continues in the proof of theorem 1, in which c_8 is proportional to c_0 and c_5. so, c_8 is also become proportional to depsilon_f. when we have c_8 epsilon_f in eq. 34, we get a constant term d. a similar dependence appears in the proof of theorem 2, where b_3 is proportional to c_8 epsilon_f, which can be as large as d. and in eq. 47, we have b_32. so the upper bound in eq. 47, which seems to the be upper bound of theorem 2, is proportional to d2. this means that if l_f is larger than one, the upper bound does not go to zero, no matter how small the model error epsilon_f is unless it is actually zero. this makes the bound meaningless. this might be unavoidable. i am not sure about it at the moment. but it definitely requires a discussion. 3 assumption 2 has a term in the form of efracs_t_2 s_t_1  i have simplified the form. the states s_t_2 and s_t_1 are vectors in general. how is the division defined here? 4 please improve the clarify of the proofs. for example, in lemma 2 it seems that a negative sign is missing in eq. 49. also how do we get eq. 50 and eq. 52? i couldnt easily verify them. 5 i believe the periodicity property used in assumption 1 should be ergodicity property. 6 the paper still has a lot of typos, e.g., to optimize the objective, one can ... p3, argument data instead of augmented p4, superpose p5, funcrion p6.", "accepted": 1}
{"paper_id": "iclr_2022_sPfB2PI87BZ", "review_text": "this paper considers the generalized target shift setting for domain adaptation and proposes an optimal transport mapbased approach to it. the considered setting for domain adaptation is rather general and of practical use. the proposed method seems sensible, as supported by the theoretical identifiability and empirical results. it is worth noting that the way to cite previous work seems to be improved. for instance, in the first paragraph of introduction, the authors reviewed various settings for domain adaptation. for model shift, the authors cited previous work. however, when discussing covariate shift, target shift, and generalized target shift, the authors did not cite the original work that provides the categorization. for completeness, the authors may want to consider including the setting of conditional shift as well, which has received a number of applications in domain adaptation in computer vision. i believe the categorization of target shift, conditional shift, and generalized target shift was provided by zhang et al. 2013. this work should also be cited when the authors give the problem definition in section 2.1. the quality of the paper will be even better if the authors cite previous work in all the right placesthis may also make the authors contribution clearer.", "accepted": 1}
{"paper_id": "iclr_2022_XLxhEjKNbXj", "review_text": "this paper proposes a labeling trick for subgraph representation learning with gnns. the proposed method, glass, improves on subgraphlevel tasks. the topic of subgraph representation learning is relatively new, and this paper makes progress in that community which would be appreciated by other researchers interested in the same problem. the paper in the original submission state raised some concerns from the reviewers about unclear writing of the motivation and potential applications, technical novelty, and comparisons with existing approaches even one that are not specifically designed for subgraph representation learning. it is good that the authors conducted additional experiments to show the effect of ssl that the approach makes improvements without ssl. this and other clarifications from the authors convinced the reviewers to recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2022_03RLpj-tc_", "review_text": "this paper introduces a model, named crystal diffusion variational autoencoder cdvae, that can learn to sample valid material structures. it accounts for known symmetries se3, permutation of the structure via se3equivariant gnns. the proposed model is a complicated combination of many existing models  modeling techniques vaes, ncsns, diffusion models but it is not entirely ad hoc; the revised paper does a reasonable job in justifying the many different modeling choices made. existing model components, often designed in the context of molecule generation, do not account for the periodicity of the crystals lattice structure; so this paper introduces modifications to account for this periodicity. the paper evaluates the model on several datasets and also introduces new benchmarks that can be used for further research. the experimental results look promising but there are a few remaining clarity issues with the metrics used cf. reviews.", "accepted": 1}
{"paper_id": "iclr_2022_Oy9WeuZD51", "review_text": "the paper considers the empirical distribution of layerchannel in cnn ,and proposes to use global null tests with simes and fisher statistics to aggregate the pvalues. this method is competitive while computationally efficient. the underlying theoretical insights are discussed in detail. the paper received mixed ratings, and the discussions werent active. so, ac carefully read the paper and inspected all reviews. reviewer a8kz comments were factually inaccurate in listing references, and lack substantial feedback on the actual content of the paper. hence, the review was downweighted. the other negative reviewer ni17, as an ood expert, unfortunately did not offer more feedback to author rebuttals. from what ac comprehends, the authors should have clarified their the theoretical guarantee and compared properly with liu et. al. 2020 energyscore es. considering the above, ac feels that the study deserves to be published.", "accepted": 1}
{"paper_id": "iclr_2022_-ngwPqanCEZ", "review_text": "this paper proposes an embedding layer in which points on a model are mapped into a feature space, trained using a reconstructionbased pretext task. then, the resulting embedding layer can be applied to shape data before using different learning architectures for modalities like meshes and point clouds. the work is particularly interesting in its attempt to derive a learned shape representation that is agnostic to modality. some questions remained about experiments e.g. baselines, but these are relatively minor and partially addressed in the rebuttal phase; also, sometimes the improvement seems to be marginal in practice. two reviewers championed this work during the discussion phase. the ac tends to agree this work is an interesting direction for future work and contains insight that the visionlearning communities might be able to use in other settings.", "accepted": null}
{"paper_id": "iclr_2022_AUGBfDIV9rL", "review_text": "this manuscript expands the range of recent work in reinforcement learning for language games to much larger and more realistic datasets. a timely and relevant contribution and one that is well evaluated. further work in stabilizing rl approaches for such largescale problems is likely to have other farreaching consequences. reviewers were unanimous that this is a strong submission after the author discussion period.", "accepted": 1}
{"paper_id": "iclr_2022_B5XahNLmna", "review_text": "this paper reveals that popular data poisoning systems, fawkes and lowkey, fail to effectively protect user privacy in facial recognition. the methods to defend against poisoning attacks are quite simpleyou can either adaptively tune the face recognition models or just wait for more advanced facial recognition systems. given these disappointed findings from the technical solution side, this paper further argues that legislation may be the only viable solution to prevent abuses of facial recognition. overall, all the reviewers highly appreciate the comprehensive and rigorous evaluations provided in this paper and enjoy reading it. the biggest concern is raised by the reviewer 6s7m, given this work fails to discusscompare to previous works on facial identity anonymizing and the technical contribution is incremental. during the discussion period, all other reviewers reach a consensus that 1 facial identity anonymizing is not relevant; and 2 this work make enough contributions and is worthy to be heard by the general community; the reviewer 6s7m still hold the opposite opinion, but is okay for accepting this paper anyway. in the final version, the authors should include all the clarification provided in the discussion period.", "accepted": 1}
{"paper_id": "iclr_2022_rTAclwH46Tb", "review_text": "the paper proposes a method to learning rate scheduling that uses information form the eigenvalues of the hessian. it shows that this scheduler obtains the minimax optimal rate on the noisy quadratic problem; and, empirically, this scheduler demonstrates faster convergence on cifar10 and imagenet, when the number of epochs is small. using hessian information in direct and indirect ways is of interest to the community, and the paper does a nice job illustrating that in a context of interest.", "accepted": 1}
{"paper_id": "iclr_2022_kezNJydWvE", "review_text": "the paper introduces an idea that was found interesting by all reviewers including gxxe who recommends a marginal reject. a majority of the reviewers also point out a few weaknesses of the paper, notably in terms of clarity of several statements that were found to be handwavy see the reviews of gxxe and ospe for more precise details. the area chair agrees with those statements, but overall, the originality of the idea introduced in this paper outweighs these weaknesses, and the experimental study is conducted in a reasonably convincing manner. even though there is room for improvements, the area chair is happy to recommend an accept, but encourages the authors to follow the constructive feedback provided by the reviewers for the cameraready version.", "accepted": 1}
{"paper_id": "iclr_2022_Jjcv9MTqhcq", "review_text": "the paper introduces a simple yet effective technique for supervised pretraining based on knn lookup from a moco memory queue . initially, the reviewers raised concerns about limited novelty with respect to neighborhood component analysis, baseline results lower than the original papers, and several other questions such as how many positive samples fall in and out of knn. the author response was strong, adequately addressing the reviewers comments with additional experiments and clarifications. after the discussion period, three reviewers recommended borderline acceptance. one reviewer maintained score 5, suggesting a more exhaustive search for hyperparameters, but indicated heshe was on the fence and would be ok if the paper is accepted. the ac considers the response of the authors regarding hyperparameter search and the small gap from other reported results is reasonable, and agrees with the majority that the paper passes the acceptance bar of iclr.", "accepted": null}
{"paper_id": "iclr_2022_7DI6op61AY", "review_text": "the authors propose to combine ideas from sdes and time series modeling with stochastic optimal control to present a framework for modeling continuoustime stochastic dynamics. the reviewers are in agreement that there are several good ideas presented here and that the interface of the perspectives the authors combine toward their proposed framework is an interesting one to explore. one referee mentions valid concerns in confusing points of the details in the presentation, and the positive reviewers echoed these concerns. in particular, more details and clearer exposition are needed for the decomposition into the subproblems and the problem of many hyper parameters. nonetheless, my overall impression after a careful read of the paper and discussion is that these concerns are addressable and are to a degree ameliorated by the author response, and that they may be viewed as limitations outweighed by the merits of the novel ideas presented here. i emphasize that all reviewers were surprisingly consistent in their assessment of the shortcomings, and i encourage the authors to take these constructive criticisms seriously in preparing a final version of this paper.", "accepted": 1}
{"paper_id": "iclr_2022_T__V3uLix7V", "review_text": "the paper proposed a new architecture called regionaltolocal attention for the vision transformers. the idea is easy to understand, the model adopts the pyramid structure and adds a regional to local attention instead of using the global attention. the architecture is wellmotivated and the paper is generally well written. the main concerns from the reviewers are mostly clarification questions. the authors did a good job addressing them. apart from those, most reviewers raise the novelty issue of such architecture, which i would think is a drawback of this paper. i am leaning towards the acceptance of this paper mainly because of its experimental results. it is the best in my batch and i think there is a significant improvement over the previous approaches.", "accepted": null}
{"paper_id": "iclr_2022_OT3mLgR8Wg8", "review_text": "the paper presents an approach for learning interobject relations. the relationships are represented in terms of scene graphs, and are processed with graph convolutional networks. all of the reviewers find the problem interesting and meaningful, which is the main strength of the paper. the approach assumes good object segmentations and state changes are provided to the agent, which the ac believes is a very dangerous assumption. object segmentation or the change detection from raw data is still an active research area and lack of endtoend training capability of the proposed approach is a limiting aspect. still, the authors were able to convince all the reviewers that the problem formulation and the proposed approach is valid. experimental results were provided to support the argument. respecting the opinions from the reviewers, the ac recommends accepting the paper, although the ac himselfherself is very reluctant to give an accept rating.", "accepted": 1}
{"paper_id": "iclr_2022_SlxSY2UZQT", "review_text": "the paper proposes using the intermediate representation learned in a denoising diffusion model for the labelefficient semantic segmentation task. the reviewers are generally positive with the submission. they like the simplicity of the proposed algorithm. they also like the effort of the paper in verifying the intermediate representation learned by a diffusion model is semantically meaningful and can be used for segmentation. initially, there was some concern about the size of the validation set, which is addressed by the rebuttal. consolidating the reviews and rebuttals, the metareviewer agrees with the assessment of the reviewers and would like to recommend acceptance of the paper.", "accepted": 1}
{"paper_id": "iclr_2022_WPI2vbkAl3Q", "review_text": "the work presented in this study gives a theoretical finitesample generalisation performance of stochastic gradient descent on linear models, for different batchsizes and feature structures. this approach enable the authors to predict the training and test losses of neural networks on real data. while there were some parts that were initially misunderstood by some reviewers in the initial version of the papers, the extensive discussions between the authors and the reviewers led to several updates, both in the reference to prior work, but also in the presentation clarity. the wide impact and relevance to iclr of this type of contribution made us recommend this work for acceptance at iclr.", "accepted": 1}
{"paper_id": "iclr_2022_SwIp410B6aQ", "review_text": "based on the previously observed neural collapse phenomenon that the features learned by overparameterized classification networks show an interesting clustering property, this paper provides an explanation for this behavior by studying the transfer learning capability of foundation models for fewshot downstream tasks. both theoretical and empirical justifications are presented to elaborate that neural collapse generalizes to new samples from the training classes, and to new classes as well. the problem that this paper delves into is important. the paper is wellmotivated, and well structured with a good flow. both theoretical and empirical analyses of the paper are solid. preliminary ratings are mixed, but during rebuttal, multiround responses and indepth discussions were carried out between authors and reviewers, and the final scores are all positive with major concerns well addressed. ac considers the paper itself and all relevant threads, and recommends the paper for acceptance. authors shall incorporate all response materials into the future version.", "accepted": 1}
{"paper_id": "iclr_2022_noaG7SrPVK0", "review_text": "the paper provides a neat idea about explaining linear predictors based on designing ways of perturbing parameters. it is focused on linear models which can still lead to nonlinear classifiers, but it is a relevant case, particularly for explainability.", "accepted": 1}
{"paper_id": "iclr_2022_9SDQB3b68K", "review_text": "summary this paper proposes a method to do a sampleefficient offline domain adaptation method where the setting requires one to have abundant amount of data from the source domain but limited amount of data available in the target domain. the proposed approach dara achieves that by accounting for the the dynamics shift between the source and the target domain via a reward penalty. the paper shows promising experimental results. final thoughts overall the paper is wellwritten, and it addresses an important problem. the reviewers are mostly positive about the paper at the end. the authors did a very good job addressing the concerns raised by the reviewers. reviewer 73ni had concerns about the novelty of the approach, it would be nice if the paper can make it more clear about the novelty of the proposed approach compared to the other existing methods in the paper. i would also recommend the authors to incorporate the feedback and the suggestions made by the reviewer into the cameraready version of the paper.", "accepted": 1}
{"paper_id": "iclr_2022_O-r8LOR-CCA", "review_text": "this paper is proposed to address a novel but practical setting that the test set consists of both seen and unseen classes of the training set. to tackle the crucial challenge of distribution mismatch between the inlier and outlier features, the authors proposed a new method named orca by grouping similar instances to enlarge the classwise margin for debiasing. the experimental results on imagenet have shown the proposed orca has significantly outperformed baselines in both inlier classification and outlier detection. the whole paper is written with clear logic and is easy to follow. moreover, such a new setting may bring more inspiration to the community.", "accepted": null}
{"paper_id": "iclr_2022_cOtBRgsf2fO", "review_text": "all reviewers concur on the fact that the paper contains solid ideas. the discussion helped clarify the case of classimbalance and no major concerns remained after discussion phase. i thank the authors for the additional details on execution time  complexity. on a separate note and perhaps to dig further in the papers ideas, 1 the validity of the gaussian assumption carried in the paper was raised e.g. erck, but i would like to point out that theorem 2 can also be derived for general exponential families given the objective in 2, with perhaps a reformulation of the trace constraint still, this would imply the knowledge of the exponential family for the kl divergence to simplify. 2 when it comes to protecting labels, the authors might want to have a look at the rich literature on learning from label proportions, which shows that the knowledge of the class is not necessary to learn a supervised model see for example patrini et al, neurips  nips 2014. thus, protecting the class could in fact be more achievable than by just considering that learning needs observed classes.", "accepted": 1}
{"paper_id": "iclr_2022_YevsQ05DEN7", "review_text": "the theory and results presented in this paper provide a new method to avoid collapse in contrastive learning. all but one reviewer recommend acceptance. the lone negative reviewer is concerned with the limited experiments, but the other reviewers, and the ac, find the experimentation convincing enough to warrant acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_Q76Y7wkiji", "review_text": "this paper is a followup paper of zhang et al. 2021, that proposed a new network architecture for adversarial robustness, l_infty distance net. although the l_infty network is provably 1lipschitz w.r.t. the l_infty distance, its training procedure exploits the l_p relaxation to overcome the nonsmoothness of the model but suffers from an unexpected large lipschitz constant at the early training stage, an issue to be solved. this paper resolves this issue by a new loss design of scaled cross entropy loss and clipped hinge loss. without using mlp on top of the l_infty distance net backbone, the proposed new training method empirically outperforms the original one in zhang et al. 2021 and improves over the stateoftheart by more than 5 for 8255 and other radiuses. moreover, the paper shows the theoretical expressive power of l_infty distance net for wellseparated data. there are some concerns about the moderate novelty and reproducibility of the results. since the empirical results are indeed impressive, the paper could be accepted conditional on that the authors release their reproducible codes to the public.", "accepted": 1}
{"paper_id": "iclr_2022_I2Hw58KHp8O", "review_text": "this paper proposes a modification of the training objective of nonautoregressive mt which claims most of the improvements that other approaches obtain only through knowledge distillation kd from an autoregressive teacher. the strategy has been largely appreciated as simple and the results suggest that its rather effective. one of us was not ready to accept certain aspects of the comparisons in the paper, and challenged the paper also from a speed of generation point of view. while i see that natmt is very much concerned with speed, removing the dependency on an autoregressive teacher is an important step in the natmt agenda as kd has various drawbacks, e.g., it corrupts the statistics of the training data, and, in my view, disentangling the two desiderata e.g., faster models, and no kd is okay at this stage. i hope the authors will not take this recommendation as a reason to ignore the comments in that review, rather, that they take it as an opportunity to address those comments as well as possible for example, by positioning the work more carefully wrt speed and the possibly negative impact of kd. on style table 1 should fit within the margins of the paper, please fix it. also, avoid boldfacing model names and avoid vertical bars please check this nice guide on making tables look nicehttpspeople.inf.ethz.chmarkuspteachingguidesguidetables.pdf.", "accepted": 1}
{"paper_id": "iclr_2022_AcrlgZ9BKed", "review_text": "summary the paper studies rl and bandits in the conservative setting where the performance of the new, learnt policy should never be significantly worse than that of a baseline. discussions the main concern of the reviewers was about novelty, and specifically what new techniques and ideas were brought in this work compared to wu et al. 2016 and garcelon et al 2020. the authors have addressed these concerns and updated their draft accordingly. the reviewers have now all reached a consensus and recommend to accept this work. recommendation accept", "accepted": 1}
{"paper_id": "iclr_2022_PzcvxEMzvQC", "review_text": "the authors focus on the conditional generation of molecular conformations i.e. 3d cartesian atom positions from a given molecular graph. they formulate the generation via diffusion probabilistic models. conformations are generated by a reverse diffusion process from isotropic gaussian noise to molecular conformations. this diffusion process is learned from data using a se3 invariant formulation of the diffusion process. the authors work directly with atomic positions i.e. a point cloud instead of interatomic distances or an intermediate bond geometry representation. experimental evaluations show stateoftheart results according to covmat metrics on geomdrugs and geomqm9 datasets. strengths  high technical novelty first generative model for molecular conformation generation based on a diffusion framework  very clearly written paper.  impressive empirical results with stateoftheart results on geomdrugs and geomqm9 datasets. weaknesses  most of the weaknesses reported by the reviewers seem to have been addressed in the rebuttal. the idea of the work is highly novel. the authors propose the first generative model for molecular conformation generation based on a diffusion framework. this paper brings together recent ideas and methods e.g. diffusion, se3 equivariance to the established task of molecular conformation generation with impressive empirical results. all the reviewers agree on acceptance with high scores. i recommend the authors to look at the reviewers comments to improve the paper for the cameraready version", "accepted": 1}
{"paper_id": "iclr_2022_v3aeIsY_vVX", "review_text": "this work proposes a hybrid autoregressive and adversarial model for sound synthesis including but not limited to speech, conditioned on various types of control signals. although recent adversarial approaches have gained favor over previously popular autoregressive approaches in this domain, because of their ability to produce audio signals much more quickly, the authors argue that these models tend to introduce certain types of artifacts which stem from an inability to learn accurate pitch and periodicity. they propose to address this by reintroducing some degree of autoregression, without compromising too much on inference speed. reviewers praised the presentation of this work, the thoroughness of the experimental evaluation, and the audio examples provided. a few concerns were also raised regarding related work and the clarity of some parts of the paper, which the authors have taken the time to address. after the discussion phase, all reviewers chose to recommend acceptance, and i will follow their recommendation.", "accepted": 1}
{"paper_id": "iclr_2022_PilZY3omXV2", "review_text": "the paper proposes to learn disentangled trends and seasonal representations of time series for forecasting tasks. it shows separating the representation learning and downstream forecasting task to be a more promising paradigm than the standard endtoend supervised training approach for timeseries forecasting. during the postrebuttal phase, there were interactions from all the reviewers, and reviewer krxv raised the score. the reviewers think the contrastive learning method is novel and the added experiments have strengthened the paper. the authors are encouraged to include more standard datasets m5 in the final version. based on the above reasons, i am recommending accepting this paper.", "accepted": 1}
{"paper_id": "iclr_2022_57PipS27Km", "review_text": "this paper addresses a continuoustime formulation of gradientbased metalearning comln where the adaptation is the solution of a differential equations. in general, outer loop optimization requires backpropagating over trajectories involving gradient updates in the inner loop optimization. it is claimed that one of main advantages of comln is able to compute the exact metagradients in a memoryefficient way, regardless of the length of adaptation trajectory. to this end, the forwardmode differentiation is used, with exploiting the jacobian matrix decomposition. all the reviewers agree that the derivation of memoryefficient forwardmode differentiation is a significant contribution in the fewshot learning. the paper is well written and has interesting contributions. authors did a good job in responding to reviewers comments during the discussion period. what is missing in this paper is the discussion of some limitations of the proposed method. this can be improved in the final version. all reviewers agree to champion this paper. congratulations on a nice work.", "accepted": 1}
{"paper_id": "iclr_2022_vrW3tvDfOJQ", "review_text": "the reviewers unanimously appreciated the clarity of the work as well as the framing of the proposed method. congratulations.", "accepted": 1}
{"paper_id": "iclr_2022_VFBjuF8HEp", "review_text": "the paper tackles a very interesting problem in the context of diffusionbased generative models and provides empirical improvements. prerebuttal, reviewers main concerns lie in the motivation and clarification of the method, while after rebuttal, all reviewers satisfied the response and gave positive scores. the authors should include the additional results to well address the reviewers concerns in the final version.", "accepted": 1}
{"paper_id": "iclr_2022_CAjxVodl_v", "review_text": "the paper describes a framework that unifies several previous lines under hindsight information matching. within that framework, the paper also describes variants of the decision transformer dt called categorical dt and unsupervised dt. the rebuttal was quite effective and the reviewers confirmed that their concerns are addressed. the revised version of the paper is significantly improved and consists of an important contribution that should interested many researchers. well done!", "accepted": 1}
{"paper_id": "iclr_2022_nkaba3ND7B5", "review_text": "this paper formalizes the setting where an autonomous rl agent operates with zero or very few resets, and provides a novel benchmark for this setting with diverse environments ranging from simple manipulation to complex manipulationlocomotion. the paper then uses this benchmark to analyze current methods and provide insight into those crucial factors that affect performance in this setting. the insights into current methods especially are appreciated. as one reviewer stated, this paper isolates one problematic assumption in the way of progress in rl, the environment reset problem, and provides the groundwork for such progress, i.e. baselines, clear metrics, etc. i believe the community is much better off with this paper published, since prior works dont seem to have used compatible methodologies.", "accepted": 1}
{"paper_id": "iclr_2022_gJLEXy3ySpu", "review_text": "thank you for your submission to iclr. the reviewers ultimately have mixed opinions on this paper, but reading in a bit more depth i dont feel that the critical comments raised by the sole negative reviewer really raise valid points. specifically, the fact that this reviewer directly asks e.g. for comparisons to levine and feiz 2019, when the paper before its revisions contains an entire section devoted to exactly this comparison, strikes me as not sufficient for a thorough review. however, while im thus going to recommend the paper for acceptance it does present a notable, if somewhat minor, advance upon the state of the art in randomized smoothing, i also feel the paper is generally rather borderline for more straightforward reasons. specifically, given the _very_ narrow focus of the proposed improvements improvements to the bounds of randomized smoothing, for l0 perturbations, for topk accuracy, i ultimately dont think the paper presents that significant an advance in the field. the paper could go other way, thought definitely not doing so due to the issues that the sole critical reviewer takes.", "accepted": null}
{"paper_id": "iclr_2022_6u6N8WWwYSM", "review_text": "an interesting paper, with nontrivial results. the reviewers all agree that the paper is above bar with two of them indicating strong vote for acceptance. the simplicity of the proposed approach noted by some of the reviewers is in my view a positive. overall, a worthy contribution.", "accepted": 1}
{"paper_id": "iclr_2022_MSwEFaztwkE", "review_text": "the paper proposes a weakly supervised contrastive learning, using auxiliary cluster information, for representation learning. their method generates similar representations for the intracluster samples and dissimilar representations for intercluster samples via a clustering infonce objective. their approach is evaluated thoroughly on three image classification task. the reviewers agree that the paper is well written, presenting interesting theoretical analysis reviewer h3zd, a8kw and solid experimetal results reviewer rhyi, 1ziy. the core idea of the paper is relatively simple and well motivated reviewer h3zd. while the focus is using the clustering with auxiliary labels, the method can be applied without auxiliary labels with kmeans. there were some concerns from the reviewers the overlap with a concurrent work 1. the authors have provided detailed discussions on conceptual concurrent work focuses on unsupervised cases where this work focuses on weaklysupervised setting and emprical comparisons. accordingly, reviewer a8kw and 1ziy had some issues with the novelty of the paper, as it can be interpreted as slight modification from previously explored idea vanilla infonce loss. despite some overlap with existing approaches, the paper presents an interesting and well conducted study of integrating clustering information for learning representation, so i vote for acceptance. 1 weakly supervised contrastive learning. iccv 2021.", "accepted": 1}
{"paper_id": "iclr_2022_i3RI65sR7N", "review_text": "this paper presents a hierarchical memory for cross domain and few shot classification problems. the paper is well written, tackles an important topic, and the proposed approach which is an extension of vsm is interesting. reviewer yexz has some concerns regarding comparison to a more proper baseline. i believe that the authors have adequately addressed this. reviewer 2ajk and g1bf also have suggestions that the authors have incorporated in the revision. i recommend accepting this paper.", "accepted": 1}
{"paper_id": "iclr_2022_0xiJLKH-ufZ", "review_text": "this paper presents an analytic approach for estimating the optimal reverse variance schedule given a pretrained scorebased model. the experimental results demonstrated the efficacy of the proposed method on several datasets across different sampling budgets. given the recent interest in scorebased generative models, i believe that the paper will find applications in various domains. i am pleased to recommend it for acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_MsHnJPaBUZE", "review_text": "the paper extends the original work on flooding to individual instance level to prevent overfitting. even though the technique is a intuitive extension, the reviewers appreciate its simplicity and effectiveness, and consider the extension necessary. most reviewers concerns were addressed through rebuttal.", "accepted": null}
{"paper_id": "iclr_2022_xDIvIqQ3DXD", "review_text": "this paper presents a theoretical analysis of the approximation properties of linear recurrent encoderdecoder architectures, obtaining universal approximation results and subsequently showing approximation rates of targets for rnn encoderdecoders. it introduces a notion of temporal products, which helps to characterize the types of temporal relationships that can be efficiently learned in this setting. overall, the reviewers and i all agree that this paper makes important theoretical contributions to the important problem of the approximation capabilities of encoderdecoder architectures. the main weaknesses involve the rather simplified linear problem setup, but this limitation is easily forgiven in this firstofitskind rigorous analysis. i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_P7FLfMLTSEX", "review_text": "summary investigate the ntk of pnns and enhanced bias towards higher frequencies. strengths  spectral bias is a contemporary topic.  some reviewers found the paper well written. weaknesses  restricted setting twolayers  no bias  infinite width, particularly in view of the objective to provide architecture design guidance. restricted experiments introduction indicates learning spherical harmonics.  sparse discussion of related works, particularly on spectral bias. discussion during the discussion period authors made efforts to address some of the concerns of the reviewers. a late new experiment prompted knzp to raise score. tqnp found the paper good but also expected a more profound theorem addressing broader pnn families given the existing work. they found that experiments and discussion of prior work could be improved. the authors added discussion of prior works and provided an explanation for their choices, but left extensions and further analysis for future work. nfmy expressed concerns about applicability of the analysis and evidence in experiments. author responses addresses this in part. cecf points out that the main theoretical contributions have straight forward proofs based on previous works and asks about extensions. authors agree that the paper does not introduce novel techniques and that extending the analysis is an important direction, but leave this for future work. furi finds the paper provides an interesting viewpoint and raised score from 3 to 5 following the discussion improving presentation, rigor, clarity, but considers that the paper has several drawbacks oversimplification, lack of technical novelty that need to be addressed. conclusion one reviewer found this work marginally below the acceptance threshold, three marginally above, and one good. i find that the paper considers an interesting problem and makes some interesting observations and some valuable advances. i appreciate the authors efforts during the reviewing period. hence i am recommending accept. at the same time, i find that, clarity, technical and experimental contributions still can be improved and encourage the authors to carefully consider the reviewers comments when preparing the final version of the paper.", "accepted": 1}
{"paper_id": "iclr_2022_shpkpVXzo3h", "review_text": "this paper proposes adam and momentum optimizers, where the optimizer state variables are quantized to 8bit using block dynamics quantization. these modifications significantly improve the memory requirements of training models with many parameters mainly, nlp models. these are useful contributions which will enable training even larger models than possible today. all reviewers were positive.", "accepted": 1}
{"paper_id": "iclr_2022_Lm8T39vLDTE", "review_text": "this paper introduces autoregressive diffusion models ardms, which generalises orderagnostic autoregressive models and absorbing discrete diffusion. all reviewers appreciated the paper with a few also finding it very dense. the experimental section is a bit lacking in detail. this has to some degree been answered in the discussion and should also be included in the final version of the paper. acceptance is recommended.", "accepted": 1}
{"paper_id": "iclr_2022__SJ-_yyes8", "review_text": "the paper addresses various improvements in visual continuous rl, based on a previous rl algorithm drq. as the reviewers point out, the main contribution of the paper is of empirical nature, demonstrating how several different choices relative to drq significantly improve data efficiency and wallclock computation, such that several control problems of the deepmind control suite can be solved more efficiently. the average rating for the paper is above the acceptance threshold, and some reviewers increased their rating after there rebuttal. while a mostly empirically motivated papers is always a bit more controversial, the paper may nevertheless stimulated an interesting discussion at iclr that will be beneficial for the community, and should thus be accepted.", "accepted": 1}
{"paper_id": "iclr_2022_xMJWUKJnFSw", "review_text": "this paper presents a technique for compositionally constructing embeddings for nodes in knowledge graphs, hence reducing the memory requirements as well as allowing inductive learning. the reviewers find the direction promising and the approach novel and wellmotivated. there were some concerns about the experiment results  reviewer kubz suggests including more baselines, reviewer cpab suggests trying nodepiece on singlerelation graphs and reviewer 2qcd notes that nodepiece lags behind the other approaches on some tasks. most of these concerns seem to have been addressed in the author response and i tend to agree with the authors that singlerelation graphs are out of the scope of this work. reviewer x7aq also raised a concern about the claims made regarding i uniqueness of the hashes and ii sublinearity of the approach. it is good to see that claim ii has been removed, but i is still present in many places  it would be good to add a discussion about why the hashes are highly likely to be unique in the final version.", "accepted": 1}
{"paper_id": "iclr_2022_p3DKPQ7uaAi", "review_text": "this paper has been evaluated by three reviewers with 2 borderlines leaning towards the accept, and with 1 accept. the reviewers have noted that the idea of alignment is not particularly novel per se. nonetheless, they found some merit in the use of a network learning the alignment and they liked experiments. ac has however some concerns about this work. firstly, it is not clear why liftedsoftdtw and binomialsoftdtw completely fail in table 1, and in table 5, softdtw is worse by 30 than tap. is softdtw set up properly in these experiments the softmax temperature, the base distance used, the maximum number of steps away from the main path etc.? ac is also not convinced about the principled nature of the proposed alignment. eq. 3 and the residual design above seem more as heuristics than a principled ot transport as eq. 1 and 2 set out to suggest. with concatenation of distances between sequence features and positional encoding, the proposed alignment seems more similar to attention and transformers than ot.", "accepted": 1}
{"paper_id": "iclr_2022_JGO8CvG5S9", "review_text": "the paper studies an interesting question of whether neural networks can approximate the target function while keep the output in the constraint set. the constraint set is quite natural for e.g. multiclass classification, where the output has to stay on on the probability manifold. the challenge here is that traditional universal approximation theory only guarantees that hatfx approx fx, but can not guarantee that hatfx lies exactly in the same constraint set as fx. the paper made a significant contribution in the theory of deep learning  it is shown that the neural network can indeed approximate any regular functions while keep the output stay in the regular constraint set. this gives a solid backup in terms of the representation power of neural networks in practice, to represent target functions whose outputs are in certain constraint set e.g. probabilities.", "accepted": 1}
{"paper_id": "iclr_2022_5K7RRqZEjoS", "review_text": "the paper points out how set equivariant functions limit the types of functions that can be represented on multisets. they develop an new notion of multiset equivariance to address this limitation. the paper improves an existing multiset equivariant deep set prediction network through implicit differentiation, which is an area of rising interest. the reviewers and i note that the paper is well written.", "accepted": 1}
{"paper_id": "iclr_2022_wogsFPHwftY", "review_text": "the paper proposes a model for largescale image retrieval. unlike previous work that rely on local features, the proposed method aggregates local features into the socalled superfeatures to improve their discriminability and expressiveness. to do so, the method proposes an iterative attention module local feature integracion transformer, lit, that outputs an ordered set of such features. by exploiting the fact that features are ordered, the paper proposes a contrastive loss on superfeatures that match across images. the paper presents a thorough empirical evaluation on several publicly available datasets including relevant baselines. overall the paper is well written and the empirical results are strong including detailed ablations that motivate the design of the method. all reviewers and the ac appreciate the idea of applying the contrastive training at local feature level while only requiring imagelevel labels. reviewer hp4y points out that the proposed lit is not particularly novel, but previous work are properly cited. also this is not a major issue given that the motivation is very clear, it is well executed and the empirical results are strong. reviewer uoyn had initial concerns regarding inconsistencies in the mathematical formulation of the method, which were resolved in a detail and constructive discussion with the authors. all reviewers recommend accepting the paper, three of which consider the contribution to be strong. the ac agrees with this assessment and recommends accepting the paper.", "accepted": 1}
{"paper_id": "iclr_2022_h-z_zqT2yJU", "review_text": "the authors study the degradation problem observed in kd for large teacher networks and propose to address it by quantifying and adapting to a sharpness gap between the student and the teacher. the reviewers generally appreciated the proposed approach in handling larger teachers and found it effective within the scope of the numerical results provided in the paper. that said, the reviewers raised several critical issues concerning the writing and the presentation of several crucial parts of the paper, in particular those related to the sharpness measure and the proposed training method atkd. thus, given this, and the exchanges between the reviewers and the authors, in its present form, the paper cannot be recommended for acceptance. the authors are encouraged to incorporate the valuable feedback provided by the knowledgeable reviewers.", "accepted": null}
{"paper_id": "iclr_2022_CTOJRqLMsl", "review_text": "this paper theoretically studies the convergence of memorybased continual learning with stochastic gradient descent, and suggested several methods based on adaptive learning rates. the reviewers appreciated the novelty of the direction, and some of them thought the experimental results are promising. however, most reviewers 34 were negative. i think the main reason was the paper presentation and clarity, which they found lacking and i agree. one reviewer thought the experimental evaluation should be improved, but there might have been some misunderstanding there. lastly, even the positive reviewer thought the results were somewhat incremental and nonsurprising. i hope the authors improve their paper and resubmit.", "accepted": 1}
{"paper_id": "iclr_2022_ugxdsne_TlO", "review_text": "the paper introduces some interesting ideas on how use causal random forests for conditional average treatment effects cate, with respect to some baseline treatment level 0, when the treatment variable is continuous. figure 1 summarises the scope of the paper neatly. scalability issues are also considered. i think this is a paper nearly there in terms of a impactful contribution. the main issues are some presentation kinks and extra steps in the theory. i think the very low scores from the reviewers are not quite representative of the overall quality i would be more generous. however, im afraid im also inclined towards a reject. the paper neglects some other developments on ml for cate with continuous treatment e.g. bica et al.s estimating the effects of continuousvalued interventions using generative adversarial networks neurips 2020 and the references within. a focus on the theory would help to differentiate it, but im not that confident that the results are currently mature enough to claim them. although i suggest a rejection, let me make clear i strongly encourage the authors to further pursue their ideas. you are doing good work, and the next iteration might nail it. as you found out in the discussion, emphasise the continuous aspect of it. id also emphasise the fact that you have a clear setup of the problem in terms of the contrast wrt to a baseline treatment effect instead of some generic contrast function. people in ml tend to be oblivious to such a setup, but im not convinced you are properly exploiting it.", "accepted": null}
{"paper_id": "iclr_2022_ks_uMcTPyW4", "review_text": "due to the delayed rebuttal made it very hard for reviewers to react. the paper proposes a new subtype of pomdps dubbed afapomdp. the proposed approach first learns a sequential vae, then an rl approach learns control and feature acquisition policies jointly. the approach is evaluated on two tasks and shows very promising results compared to baselines. overall the setting and the approach are very interesting. the replies and revised paper managed to address some of the concerns of the reviewers. however, there remain a few open questions and doubts see updated reviews, in particular as some of the arguments of the authors remain in the hypothetical, and the reviewers are still not entirely convinced by the choice of the experimental tasks.", "accepted": null}
{"paper_id": "iclr_2022_Nfl-iXa-y7R", "review_text": "this is an intriguing work that introduces a novel sparse training technique. the core insight is a novel reparametrization or sparsity pattern based on the socalled butterfly matrices that enables fast training and good generalization. the theory is solid and useful. most importantly, the method is novel and is likely to become impactful. understanding better what contributes to the excellent performance is an interesting question for future work. in agreement with all the reviewers, it is my pleasure to accept the work.", "accepted": 1}
{"paper_id": "iclr_2022_Ivku4TZgEly", "review_text": "this paper analyzes analyze the fairness of integrated gradient based attribution methods. the authors exploit shap and bshap, two approaches based on the theory of shapley values, as the reference of fair methods. specifically, they present an attribution transfer phenomenon in which the integrated gradients are affected by some sharply fluctuated area across the integration path, thereby deviating from the fair attribution methods. to avoid the attribution transfer issue, they further propose integrated certainty gradients icg method, where the integration path does not pass through the original fluctuated input space. experiments are performed to demonstrate the advantages of icg in avoiding attribution transfer. while the basic premise of the work is interesting, many conceptual details remain unclear and experimental evaluation can also be improved please see detailed reviewer comments below. given this, we are unable to recommend acceptance at this time. we hope the authors find the reviews helpful.", "accepted": null}
{"paper_id": "iclr_2022_R2aCiGQ9Qc", "review_text": "this paper focuses on investigating the relations between the heterophily and oversmoothness problem. however, the relationship is not clear. the oversmoothness problem considers the features and the adjacency matrix, while the heterophily incorporates the adjacency matrix and the labels. they have different views on the graph. it may not be treated as the same coin. besides, the stacked aggregations lead to indistinguishable node representations and poor performance in the oversmoothing problem. the same phenomenon appears in the heterophily problem because the features in different classes are falsely mixed, leading to indistinguishable nodes 2. they have the same phenomenon but different origins. it may be not a necessity to combine these two problems. besides, madgap1 is proposed to evaluate the oversmoothness problem. it is unreliable to use the accuracy and the degree to measure this problem. therefore, in section 3, the relations between node degrees and the homophily ratio cannot infer the relations between the heterophily and oversmoothness problem. as a result, the authors should carefully reorganize their paper and results. a suggestion is to pack the submission as a new method to learn from heterophily instead of trying to make such a close relationship with oversmoothing.  1 measuring and relieving the oversmoothing problem for graph neural networks from the topological view. aaai 2020  2 beyond homophily in graph neural networks current limitations and effective designs. neurips 2020", "accepted": 0}
{"paper_id": "iclr_2022_tG8QrhMwEqS", "review_text": "the paper proposes a strategy for incrementally pruning deep learning models based on activation values. the approach can satisfy different kinds of requirements, trading off between accuracy and sparsity. the approach seems promising and seems to have competitive performance. however, the method is described by reviewers as a combination of ideas that have been proposed in the literature, and the experimental evaluation relies too much on a dataset considered too small to be reliable in such experiments  cifar10. we do not expect substantial experiments within the rebuttal period such comparisons with relevant sota methods should have been present in the submission. moreover, the strategy proposed for selecting a threshold seems to rely on some doubtful assumptions, and there are no benchmarks on actual runtime. the writing has improved based on reviewer input, and the reviewers are satisfied with this aspect. i would still add that i would prefer some clarity in the method presentation is there a quantity being optimized? is there a value we can monitor to ensure our reimplementation is correct? etc. in addition i would like to ask authors in the next revision to be mindful to the difference between citet and citep in authoryear citations  see e.g. the first two ones in 3.1.", "accepted": 0}
{"paper_id": "iclr_2022_saNgDizIODl", "review_text": "the paper proposes a simple approach to quantify uncertainty in deterministic neural networks, not unlike the works of sngp, ddu, and due, where one only performs one forward pass rather than in an ensemble or monte carlo sample. in particular, they propose a kernelbased method on a networks logits to estimate uncertainty, obtaining data and model uncertainty estimates separately using a bound on bayes risk. while i agree with the relevance of the problem, theres a shared concern among reviewers across both technical novelty and experimental validationparticularly compared to prior work that can be difficult to understand the key distinguishing factor. i recommend the authors use the reviewers feedback to enhance their preprint should they aim to submit to a later venue.", "accepted": null}
{"paper_id": "iclr_2022_in1ynkrXyMH", "review_text": "the reviewers all appreciated the novel concept behind the work. i agree with this, i think the principles behind the work are novel and interesting, and i would encourage the authors to improve the validation of this method and publish it in the future. however, reviewers also raised a number of issues with the current paper 1 the evaluation appears a bit preliminary, and could be improved significantly with additional datasets and more ablationscomparisons; 2 its not clear if the improvements from the method are especially significant; 3 the writing could be improved i do see that the authors made a significant number of changes and improved parts of the paper in response to reviewer concerns to a degree. probably the writing issues could be fixed, but the skepticism about the experiment results seems harder to address, and while i recognize that the authors made an effort to point some existing ablations in the paper that do address parts of what the reviewers raised, i do think that in the balance the experimental results leave the validation of the work as somewhat borderline. while less important for the decision, i found that the paper is somewhat overselling the contribution in the opening  while the particular concept of using gradients as features in this way is interesting, similar ideas have been proposed in the past, and the paper would probably be better if it was more clearly positioned in the context of prior work rather than trying to present a new framework like this. it kind of feels like its biting off too much in the opening, and then delivering a comparatively more modest but novel and interesting! technical component.", "accepted": 0}
{"paper_id": "iclr_2022_fJIrkNKGBNI", "review_text": "this paper proposes to apply a piecewise polynomial filter on the spectral corresponding to the graph convolution to enhance the model expressivity of graph neural networks. the effectiveness of the proposed model is investigated through numerical experiments and it was shown that the method achieves fairly nice performances. this paper gives a natural extension to the usual adaptive generalized pagerank approaches to more expressive piecewise polynomial filters. however, the reviewers are not enthusiastic on this paper. this is mainly because of the following concerns 1 since it requires diagonalization of the aggregation operator, it requires much more computational burden than the usual polynomial filters, which prevents the method from being applied to data with much more large size. 2 the choice of the filter could be more investigated, in particular, the complexityexpressivity tradeoff in other words, biasvariance tradeoff could be discussed more, for example, by theoretical work. in summary, the paper seems not to be well matured for being published in iclr conference.", "accepted": null}
{"paper_id": "iclr_2022_86sEVRfeGYS", "review_text": "the paper studies an important newly identified problem in continual learning of rapid adaptation, and proposes the use of a generateandtest method to continually inject random features alongside sgd, enabling better learning on nonstationary data streams. unfortunately the paper remained borderline in the discussions. while reviewers liked the overall research direction and contributions, they also agreed the paper in current form still would benefit from deeper insights into the proposed method, stronger empirical evidence. experiments cover broad applications including rl, but dont seem to give very clear advantages over other weight regularization schemes, and other metrics of quality could be added. we appreciate the authors have added additional experiments testing it both for the two important regimes of under and overparameterized networks, though those can be expanded. we are sorry that this good paper remained narrowly below the bar in this case, and hope the detailed feedback helps to strengthen the paper for a future occasion.", "accepted": 0}
{"paper_id": "iclr_2022_WKWAkkXGpWN", "review_text": "this paper has conflicting reviews with no strong advocate. one of the positive reviewers states the caveat that paper is very dense to read and needs to be improved. having looked at the paper myself i would agree with this criticism. one of the negative reviewers states that the paper gives an incremental variant of the nlm model. i am less confident in this judgement. however, i find the density of the paper and the use of synthetic data to be significant drawbacks. with the lack of any real champions for the paper i do not see a path to acceptance.", "accepted": 0}
{"paper_id": "iclr_2022_0Kj5mhn6sw", "review_text": "paper this paper describes a method to generate visual gestures by learning an intermediate representation based on gesture sequences. this proposed method builds from previous work on vae and vector quantized vae. discussion the reviewers wrote some detailed reviews about the paper, bringing some valid concerns and asking some questions to the authors. unfortunately, no responses were posted from the authors. summary after looking at all reviews, there was a general consensus among the reviewers that this paper was not ready for publication. we hope that the reviews will be helpful for the authors in revising their work for future submission.", "accepted": 0}
{"paper_id": "iclr_2022_4V4TZG7i7L_", "review_text": "paper this paper presents a multimodal autoencoder architecture built on the premise that unimodal variations can be best generated when taking advantage of a shared latent space. this is operationalized by defining a hierarchical model with two primary levels a shared structure space and unimodal variations which could be multilayer. discussion the reviewers and followup discussion brought many questions and issues. the authors submitted a significantly revised version of their paper which clarified many issues and added a few extra results. while many of the reviewers questions were addressed by the authors, it seems that reviewers ended up not changing significantly their review scores. one fundamental concern is if the basic assumption about the shared structure is effectively the proper way to approach such generative modeling task. the experimental for image generation did not seem to support this hypothesis. summary while the revised version was an improvement over the original submission, improving clarity and adding some experimental measures, the experimental results did not seem to always support the main hypothesis. human evaluation results may help in this direction.", "accepted": null}
{"paper_id": "iclr_2022_IsHQmuOqRAG", "review_text": "this paper tackles the difficult problem of learning to segment objects from an image using no supervision during training. the paper is clearly written and a new synthetic dataset is made available. unfortunately, the reviewers raised a number of issues with the submission missing citations and comparison to relevant related work  additional baselines  ablation studies  missing empirical evaluation of the proposed method on standard dataset beyond the toy dataset proposed by the authors. the paper received 1 reject, 2 marginal rejects and 1 accept but even the positive reviewer agreed that these were limitations. the authors also conceded to these limitations and initiated experiments that are starting to address the reviewers comments. at this time, the results of these experiments remain incomplete and hence most reviewers agree that the paper should go through another round of reviews before it is publishable. i thus recommend this paper be rejected in the hope that a subsequent revision will make it a much stronger contribution.", "accepted": 1}
{"paper_id": "iclr_2022_YxWU4YZ4Cr", "review_text": "this paper studies different inductive biases that would improve ood generalization and in particular under translation, rotation and scaling for image tasks. the study is focused on a toy dataset which allows authors to have more control over the data generation process and the transformations. authors further show that iterative training using an autoencoder and presenting data in logpolar space helps with rotation and scaling transformations on their toy dataset. strong points  the paper is well written and easy to follow.  the data generation process and the resulting toy dataset are novel and interesting.  the experiment design and evaluations are solid. weak points  no natural image datasets while using a toy dataset has several benefits it does not grant that the conclusions would generalize to realistic settings. reviewers have suggested several realistic datasets and i encourage authors to evaluate their findings on some of these datasets.  limited baselines as reviewers have pointed, comparison with baselines can be improved by including stronger baselines as well as more clear discussion about other techniques such as augmenting the data with the transformations.  related work a proper discussion of related work to set the context and highlight the contributions of this paper is missing. in particular, reviewers have pointed to prior work on the benefits of presenting the image in logpolar space. unfortunately, authors did not engage with reviewers during the discussion period. given the prior work and lack of any natural image dataset, i think the novelty and significance of this work is limited. therefore, i recommend rejecting the paper. however, i encourage authors to improve the paper by addressing the points raised by the reviewers.", "accepted": 0}
{"paper_id": "iclr_2022_fHPdmN3I0tY", "review_text": "this paper proposes decoupled kernel neural processes dknps, a new neural stochastic process, which learns a separate mean and kernel function to directly model the covariance between output variables. numerical experiments on 1d regression and 2d image completion are provided. there was a concern that the original version of the proposed model was not a valid stochastic process, since the consistency condition of kolmogorov extension theorem might not be satisfied. the authors fixed this by replacing multihead mixed attention mma in the covariance path with multihead crossattention. overall, reviewers find the work interesting, but there remain concerns that the novelty is limited and that the current work lacks sufficient experimental evaluation.", "accepted": 1}
{"paper_id": "iclr_2022_UF5cHSBycOt", "review_text": "this paper focuses on the extrapolation ability of graph neural networks and proposes a new pooling function based on vector norm. the proposed method can be applied to replace the commonly used pooling function like maxmeansum, and is proved able for extrapolation in a simple example. overall, all reviewers tend to reject this submission due to the following reasons  the contribution to this paper is incremental. it builds on top of the wellknown lp norm pooing function and extends it to allow negative values of p and an additional learnable parameter q.  however, this simple extension is not a wellbehaved function for gradientbased optimization, which leads to unconvinced experiments, i.e., diverse performance compared with minmax.  more recent baselines should be compared with and it would be better to see how gnp works on stateoftheart model architectures on realworld applications", "accepted": 0}
{"paper_id": "iclr_2022_W3-hiLnUYl", "review_text": "the paper performs an empirical evaluation of deterministic methods for the quantification of epistemic uncertainty. there is no new algorithm. the main contribution is the empirical evaluation. this empirical evaluation will be useful for the community. it is an independent evaluation that casts some doubts on the calibration of several existing deterministic techniques, which will spur additional research. however, the paper is not well written. as pointed out by the reviewers, the paper does not provide much background. it refers to many concepts without defining them. the concepts are not new references are provided for each concept, but since the paper does not describe any new technique it should do a good job at explaining those concepts. the authors added some explanations in the supplementary material, but some of those explanations should really be in the main paper. the most important issue with the paper is that it does not explain why the deterministic techniques do not seem to be well calibrated. the authors added a theoretical justification in section 6.1, but it amounts to saying that deterministic methods make a point estimate, which is too general to explain much. an important factor for proper generalization and calibration is the inductive bias of the model. at the end of the day, if we generate data from a model, then that model will be better calibrated than the other models. so a discussion of the inductive bias of each model and how this inductive bias relates to the properties of each dataset would have been much more insightful.", "accepted": 1}
{"paper_id": "iclr_2022_i1ogYhs0ByT", "review_text": "the paper proposes using a mixture of gaussian models for transformer keys mgk so that the posterior distribution of key given query matches the attention scores in the transformer architecture under some assumption. similarly but in reverse, the query given key under a mog also matches transformer attention score. the paper proposes that this formulation can learn more diverse attentionheads and reduce the computation by replacing some heads with the gaussian mixture heads which are easier to compute. moreover, the authors show that it is straightforward to transfer their formulation to linear transformers. the authors perform experiments on the long range arena benchmark and on wikitext103 where they show some improvements with less attention heads, while using up to 20 less flops for softmax transformers and around mostly 20 but up to 80 less parameters for the worse linear transformer. reviewers generally find that the interpretation of transformers as mixture of gaussian interesting, although it is based on normalization assumptions. the main objections from both positive and negative reviewers is the weakness of the empirical results. in summary, all the improvements in perplexity are less than 0.5, and accuracy improvements are less than 1. in the discussions, the authors claim that their comparisons are using fewer heads vs baselines with more heads, which is how the tables are presented, but upon closer look i find this unconvincing since the same head count comparisons can be reconstructed and still show very weak results. for example in table 1, accmgk461.85 vs. accbaseline461.23, or in table 2 acclinearmg455.7 vs acclk4baseline55.61. to begin with, it would be better to compare the same number of heads for more headtohead comparisons, so i find the whole argument to be misleading. the claims about flops and memory has a similar flavor where the differences are small but the claims were big. a visual evaluation of figure 3 show that most reductions are around 10 in flops or parameters for the better softmax model note the axis does not start at 0. the retrieval task for the worse linear transformer is the only case where the flops reduction seems significant. given that em is now required during training, id interpret the results as a negative for mgk given the marginal improvements achieved. several reviewers are unhappy with the strength of the empirical results while most reviewers gave favorable scores after the discussion where the authors insisted that some valid points are not valid. the authors requested the ac to look further into the one negative review due to the lack of a reviewers response. after taking some time to look at the substance of the paper and the review, i recommend rejection due to the weakness of the results, the misleading presentation and discussions.", "accepted": 1}
{"paper_id": "iclr_2022_kxARp2zoqAk", "review_text": "this paper presents a method which selects feasible data augmentations suitable for contrastive time series representation learning. the topic in this paper is timely and interesting. one of 4 reviewers did not complete the review, not responding to a few reminders. so, one emergency reviewer, who is an expert in metalearning was added. while there is one review that strongly supports this work, two reviews remained unsupportive after the discussion period ended. i appreciate the authors for making efforts in responding to reviewers comments. however, after the discussion period, most of reviewers had concerns in this work, pointing out that the technical correctness needs further justification and experiments should be improved. while the idea is interesting, the paper is not ready for the publication at the current stage. i encourage to resubmit the paper after addressing these concerns.", "accepted": 0}
{"paper_id": "iclr_2022_4tOrvK-fFOR", "review_text": "this work studies the task of sound source localization from multichannel audio. an approach to design a waveletlike filter bank for audio feature extraction is proposed. after discussion, all reviewers have given this work borderline ratings. concerns were raised about the quality of the writing, missing related work, experimental methodology, and especially with regards to confounding factors in the experimental results, which make it difficult to assess the individual merit of the different components in the proposed system i.e. features vs. transformer model. the authors have addressed this to some extent with additional experiments in the updated version of the manuscript, but this revealed that the gains obtained by the proposed feature extraction method in isolation are actually quite modest. this is in contrast with how the paper is written, with much more emphasis on this particular contribution than seems to be warranted by the empirical results. additionally, i believe the manuscript would benefit from a more careful and thorough revision to improve clarity and accessibility, beyond what is possible within a single review cycle. therefore i am recommending rejection.", "accepted": null}
{"paper_id": "iclr_2022_k32ZY1CmE0", "review_text": "this paper analyzes the gradient behavior of rnns in terms of the lyaponuv exponents of its trajectoryorbit, showing that rnns with cyclic or stable equilibrium dynamics have bounded gradients, but if the dynamics are chaotic the gradients will explode. from these insights, the authors propose an algorithmic remedy for this pathology, which is essentially a teacherforcing method that periodically projects the observation onto the hidden state during training. a thorough empirical investigation is performed showing the utility of the proposed approach for modeling chaotic data. the reviewers had split opinions on this paper. some reviewers found value in the theoretical contributions and the connection between lyapunov exponents and behaviors of the dynamics of recurrent neural networks, while others thought the theoretical framework may have limited practical utility. several reviewers found the initial experiments to be lacking, though many of their concerns were alleviated after the substantial additions the authors provided during the discussion phase. i believe the observation that exploding gradients are unavoidable when modeling chaotic data is important and would be of significant interest to the broader iclr community. however, the practical implications of this observation have not been thoroughly described or investigated, and without this perspective, the theoretical results by themselves are much less impactful. in practice, it is usually the case that the groundtruth function is not learned exactly, the time horizon is finite, the gradients are noisy, the datagenerating process is opaque, etc. do these caveats have any bearing on the conclusions? the experiments address some of these questions, but only indirectly, and a more explicit discussion of the practical implications would broaden the impact of the paper. along the same lines, the practical utility of the theoretical framework could be further supported if there were some analysis of more varied or additional rnn usecases. as one reviewer mentioned, i think the iclr community in particular would appreciate any theoretical or algorithmic insights that might yield improvements on a standard baseline task like seqmnist, which has served as a pointofcomparison for many alternative methods and which would facilitate comparisons to prior work. overall, this paper does make some nice and potentially important theoretical insights about training rnns on chaotic data, and it does include an extensive battery of empirical evaluations, however the practical implications remain largely unconvincing, and i believe the paper falls just short of the bar for acceptance.", "accepted": null}
{"paper_id": "iclr_2022_UECzHrGio7i", "review_text": "this submission addressed offline imitation learning problem with nonoptimal demonstrations. the ac went through the draft, reviews, and replies. the ac agrees with all reviewers that the mathematical analysis, empirical evaluation, and general quality of writing havent reached the bar of iclr papers.", "accepted": 0}
{"paper_id": "iclr_2022_zLb9oSWy933", "review_text": "this is an interesting and carefullypresented work which discusses how to implement finitewidth ntks more efficiently. overall, the reviews were slightly tending positive, though with a variety of concerns, including some concern that the contribution is not sufficiently substantial. in my own perusal of the paper, personally i feel it could be made more compelling if a more speedups could be considered, including ones with various tradeoffs, for instance via randomized linear algebra, b explicit consequences on various prediction tasks, rather than plotting wallclock times e.g., as this paper cites many works which tried to use finitewidth ntk, and as this paper claims massive speedups, then it will be able to repeat some of those experiments at much larger sizes, which should lead to interesting and valuable largerscale experiments which ideally have some new phenomena, but are even interesting if they simply confirm the smallerscale phenomena. as a separate concern, i second the comments of one reviewer, that part of this papers contribution is to a single software package, which is moreover listed in the abstract and not just part of the standard code release, e.g., as a footnote; this feels a little strange, like an announcement of a code release, and further limits the impact to general machine learning researchers for instance, i feel completing some of my preceding suggestions could result in, say, researchers who use other software feeling eager to reimplement this. overall, i urge the authors to continue with their interesting work and aim to resolve these concerns and those of the reviewers.", "accepted": 0}
{"paper_id": "iclr_2022_qNcedShvOs4", "review_text": "the paper aims to integrate stein variational inference methods into the existing probabilistic programming language numpyro. the implemented methods include variantions of stein variational gradient descent with different types of kernel functions, nonlinear scaling of update terms, and matrixvalued kernels. the paper includes empirical results with a comparsion with existing baselines in realworld problems. using this framework, the authors developed a new stein mixture algorithm for deep markov models, which shows better performance than existing methods. strengths  the paper is overall wellwritten and the method is clearly explained.  the literature review is thorough.  integration of steinvi into numpyro seems useful. users can easily take advantage of the stateoftheart steinvi algorithms for their own bayesian modelings.  extending the stein mixture method to deep markov models is a novel application. weaknesses  the originality is low the authors propose algorithms that are very similar to previous work and there is a lack of experiments to verify the usefulness of the proposed method, for example, to verify the decreased variance of the gradient estimates claimed by the authors.  efforts are required to illustrate why elbowithinstein is preferred over the existing work.  some important stein vi methods seem lacking.  no experiments to support the usefulness of einsteinvi for nonlinear stein vi, matrixvalued kernel stein vi, and message passing stein vi. all reviewers vote for rejection. i recommend the authors to addrss the limitatoins mentioned above and improve the paper before its resubmission to another venue.", "accepted": 0}
{"paper_id": "iclr_2022_ajOSNLwqssu", "review_text": "reviewers agreed that taking into account the secondary structure in addition to the amino acid sequence, although not new in bioinformatics, may be a good idea in the context of deep generative models of peptides. on the other hand, all reviewers also agreed that the experimental results do not allow concluding about the potential benefit of the method, i.e., whether it is likely to produce potent amps and whether it does it better than existing methods. indeed, the proposed computational criteria can not replace a proper experimental validation, and it is not clear whether a better method on the computational criteria will be better in the real world. second, the results on the computational criteria are not convincing regarding the physical properties, it remains debatable to claim that a method is good if it outputs many amps that fulfill the criterion, while less than 7 of the true amps do; and regarding the computational prediction of being an amp, the proposed method is outperformed by existing ones. in conclusion, we consider that the paper is not ready for publication at iclr, since there is no significant methodological novelty nor significant experimental results if this is an application paper, and we encourage the authors to consider a publication with wet lab experiments to demonstrate the relevance of the method.", "accepted": null}
{"paper_id": "iclr_2022_Ck_iw4jMC4l", "review_text": "the paper is well written and deals with a simple yet interesting introduction of approxithe paper is well written and deals with a simple yet interesting introduction of approxithe paper is well written and deals with a simple yet interesting introduction of approxithe paper is well written and deals with a simple yet interesting introduction of approxithe paper is well written and deals with a simple yet interesting introduction of approxithe paper is well written and deals with a simple yet interesting introduction of approxithe paper is well written and deals with a simple yet interesting introduction of approximate boolean logic activation functions. a number of comparison experiments showed intriguing differences for problems with potential logical structures. authors suggest a probabilistic rationalmotivation, i.e., computation in logitspace, however, more theoretical investigation is critically needed to answer why they perform the way they do. there are a lot of activations in the literature, so perhaps it is not easy to make a distinct contribution in performance. despite the large number of experiments the reviewers were not convinced on how they support authors claims and contributions. the reviewers and ac strongly encourage the authors to keep the direction and improve the paper for another conference.", "accepted": 0}
{"paper_id": "iclr_2022_qEGBB9YB31", "review_text": "for this paper initially the reviews were 6,8,5,5. all the reviewers have provided constructive and substantial feedback. the authors have incorporated changes to address some of these comments and some of the comments could not be addressed. the main criticism of the reviewers have been that the reviewer tkqp finds two clear limitations in the paper, reviewer 3o7z finds that the proposed idea is similar to the parameterspace adversarial attacks and reviewer scew questions the generalisability of the method to other tasks. after the rebuttal the reviewers have reached the consensus that the paper may not be above the acceptance threshold final scores 6,6,5,5. following the reviewers recommendations, the meta reviewer recommends rejection.", "accepted": null}
{"paper_id": "iclr_2022_Cm08egNmrl3", "review_text": "the paper studies the problem of ood classification the test data and training data distribution can have different spurious featureclass dependencies. the reviewers have stated that the proposed procedure is a natural choice, with simple implementation. another positive point is that it could easily be incorporated in many offtheshelf machine learning training algorithms. yet, the technical novelty was mentioned to be limited. the bilevel optimization point of view and the connection with minmax optimization problems raised some concerns, as the vocabulary used could be misleading. it was also raised that the paper lacks theoretical supports no formal analysis, most explanations are ad hoc, etc.", "accepted": 0}
{"paper_id": "iclr_2022_Uy6YEI9-6v", "review_text": "all three reviewers recommend borderline rejection based on limited novelty, missing comparisons with other methods, and runtime inefficiency. the authors response helped clarify other questions but did not eliminate the main concerns about the paper. the ac agrees with the reviewers that, in its current form, the paper does not pass the acceptance bar of iclr. the reviews have detailed comments and suggestions that should help the authors to improve the work for another conference.", "accepted": null}
{"paper_id": "iclr_2022_r88Isj2alz", "review_text": "the paper proposes an energy consumption attack to neural ode models. there are two complains from the reviewers  although this is a new application to energy consumption attack, most of the attack techniques are simple extensions to the previous attack papers, so the novelty is questioned by some of the reviewers.  the paper is poorly written. we therefore decide to reject the paper and encourage the authors to address the concerns in their next submission. reviewers also think a careful discussion about the defense or detection mechanism against the proposed attack will be a good thing to add.", "accepted": 0}
{"paper_id": "iclr_2022_bxiDvWZm6zU", "review_text": "at this time, this work is not yet ready for publication. the core ideainfluence functionswas poorly explained in the initial submission, and although major changes to the paper were made to rectify this, at least some reviewers of the remain unconvinced and it is unclear that the paper has been fully evaluated with this confusion resolved. there are a sufficient number of other concerns around the paper, that having rectified these more fully and outside the tight time constraints of the rebuttal period, i hope for an interesting resubmission in future.", "accepted": 0}
{"paper_id": "iclr_2022_mqIeP6qPvta", "review_text": "this paper introduces an architecture that uses pooling regions and eye movements to sequentially build up an object representation. a confidence threshold is used to allow recognition in less time for easier images. there was a lot of disagreement on this paper. those in favor argued that it is a worthy endeavor to explore new biologically motivated architectures and foveated eye movements are an important aspect of human vision that is worth exploring for computer vision. another pro was the improved robustness to some adversarial attacks. those arguing for not accepting the paper, argued that classification performance is not improved over sota and that more ablation studies should be done to better understand the role and importance of the various aspects of the model and how they differ from other architectural designs with dilated convolutions instead of the foveation module. i agree that more ablation studies would be useful to better understand the role of the different model components. while i feel that this novel sequential processing algorithm is worth publishing to increase activity in this area, i feel it would be best received after further studies help clarify the importance of different aspects of the model. i recommend resubmission after further analysis.", "accepted": 1}
{"paper_id": "iclr_2022_e6MWIbNeW1", "review_text": "this paper considers an important problem, graph partitioning, from a transductive viewpoint assuming that the graphs are generated by independent draws from an unknown distribution, learn some parameters in an offline phase, and use these in the online phase much as in pac learning. the authors have also answered many of the reviewer questions. in particular, the comparison with existing work is substantial. while i laud the positives of this work and the importance of the transductive approach, i see an issue as a reviewer points out and as agreed by the authors, the paper does not provide a theoretical guarantee of the quality of the generalization to unseen graphs. it would have been useful, e.g., to consider this on erdosrenyi gn,p models, stochastic block models etc.", "accepted": 0}
{"paper_id": "iclr_2022_9rKTy4oZAQt", "review_text": "this paper introduces a new approach for risk sensitive rl by using an objective that depends on the full distribution and can apply a weight to the resulting trajectory. the reviewers thought that focusing on more general and expressive objectives for rl is well motivated. however, they had a number of concerns of the current paper state, including its clarity in a number of sections and its relation to other work in risksensitive rl. the authors provided thoughtful responses but some concerns lingered around the prior concerns.", "accepted": null}
{"paper_id": "iclr_2022_kHNKTO2sYH", "review_text": "this is a borderline paper with 2 marginally above and a marginally below acceptance recommendations. while the authors provided valid responses to some of the criticism, i still find some of the motivation and assumptions not sufficiently clear, theoretical and practical issues are mixed, and the validation on only synthetic data raises practical questions.", "accepted": null}
{"paper_id": "iclr_2022_JvPopr9skL0", "review_text": "the paper adopts cvae to generate ood samples for training an outliner detector. it consists of two phases that train an ood detector by leveraging the generated ood data and shows it outperform other methods. according to reviewers discussion, there is a concern from the discussion why cvae works but other variants or cgan doesnt. the paper needs more motivation or evidence or ablations to support the generality of the work.", "accepted": null}
{"paper_id": "iclr_2022_QKEkEFpKBBv", "review_text": "all reviewers concur that the paper has promise, but fails to deliver on that promise. the idea of learning potentials based on dnns is appreciated, but the evaluation of the contribution is considered lacking by all reviewers. in addition, reviewers note that the training is not differentiable, which the rebuttal acknowledges is future work. i do not reject the paper simply for failing to beat a deep learning baseline, but for having chosen applications which do not even test the papers hypotheses reviewers note that the models are tree structured, so loopy bp is not tested, despite the revised papers claim that the inference strategy is compatible with graphs containing cycles.", "accepted": 0}
{"paper_id": "iclr_2022_oaKw-GmBZZ", "review_text": "thank you for your submission to iclr. while all reviewers felt that there were some interesting aspects to the proposed work, the consensus was also that the work didnt properly situate itself within the existing literature on related methods. in particular, i agree with reviewer klfd that a numerical comparison to pfaff et al., is notably missing here; while the authors did provide qualitative comparisons in their discussion, its not clear to me that these differences are ultimately that significant, and the methods need to be compared directly if a case is to be made for the advantages of the proposed approach.", "accepted": 0}
{"paper_id": "iclr_2022_CES-KyrKcTM", "review_text": "the paper investigates weighted empirical risk minimization where the weights on an example in the training set is given by a polynomial function evaluated on the loss on the given example. authors show that the choice of the weighting function induces a datadependent variance penalization in the training objective. authors present an algorithm for weighted erm and empirical results to support their claims. while the problem setting is broadly relevant and the approach the authors take in this paper is interesting, several questions remain unanswered. first, the authors argue that variance penalization helps but do not compare with other regularized erm approaches. second, it is not clear if the proposed algorithm is indeed gradient descent on the weighted erm objective as pointed out by one of the reviewers. finally, the writing can be improved with more emphasis on the novelty and significance of the contributions. i believe the initial comments from the reviewers has already helped improve the quality of the paper. i encourage the authors to further incorporate the feedback and work towards a stronger submission.", "accepted": 0}
{"paper_id": "iclr_2022_yV4_fWe4nM", "review_text": "this paper received a majority voting of rejection. in the internal discussion, no reviewer would like to change the score according to the author response. i have read all the materials of this paper including manuscript, appendix, comments and response. based on collected information from all reviewers and my personal judgement, i can make the recommendation on this paper, rejection. here are the comments that i summarized, which include my opinion and evidence. motivation the motivation of this paper is not strong. in this paper, the authors claimed that the fairness level of deep clustering methods is relatively poorly compared with the traditional fair clustering methods. the traditional fair clustering methods employ the hard constraints to achieve fairness by scarifying the cluster utility. instead, deep fair clustering methods seek the tradeoff balance between fairness level and cluster utility; therefore, the deep fair clustering can be regarded to use the soft constraints. there is no necessary to compare two different fairness constraints. even the proposed method is a tradeoff balance between fairness level and cluster utility. selfaugmented training the relationship between selfaugmented learning and fairness learning is unclear. i guess that the authors added this modular simply to enhance the cluster utility. however, such a loss or an operator can be also applied to other fair clustering algorithms. the experimental comparisons in section 5 is unfair. no ablation study on this is provided. novelty one reviewer pointed out there existed some work that plugs integer linear programming into a probabilistic discriminative clustering model proposed in 2017. experiments 1 scfc and dfcv release their codes; no results of these two methods were reported on har. 2 no standard deviation. 3 the initial ilp results ours and ours result in table 1 on har dataset is 0.653 and 0.468, both higher than the ground truth optimal 0.458. presentation a few statements are not wellsupported, or require small changes to be made correct. no objection from reviewers was raised to again this recommendation.", "accepted": 0}
{"paper_id": "iclr_2022_HiHWMiLP035", "review_text": "this paper proposes an early exit method that uses class means of samples that is gradient free and is aimed for low compute cases such as mobile and edge data. the idea is novel in this setting though class means have been used for other settings such as few shot classification and empirical results show that it works well. there are two main concerns from reviewer concerns that were not addressed by the author rebuttal. first, applicability of the model in real world due to its memory requirements and two, experiments that show performance on more realistic datasets such as imagenet. the reason the latter is required is the promise of mobile application for the proposed method. i suggest the authors explain the first concern more and add the requested experiments in the upcoming version of the paper.", "accepted": 0}
{"paper_id": "iclr_2022_mQDpmgFKu1P", "review_text": "the paper proposes a language modeling architecture based on the rnn cells leveraging legendre memory units. the proposal is interesting, but as all the reviewers notice, the paper is not ready for the presentation in the top ml conference for several reasons comparison with weak baselines, shallow or weak analysis of the presented results, insufficient discussion of the related work, etc. looking forward for all the comments to be addressed by the authors. in the rebuttal the authors addressed some of the questions but all the reviewers think that the paper is not ready for acceptance and careful rewriting is needed. recent research on the improved rnn mechanisms suggests that legendre memory units and related mechanisms might be a gateway to solving several standard issues of training regular rnns so the topic is definitely of great importance. thus the authors are highly encouraged to resubmit the paper after making all suggested corrections.", "accepted": 0}
{"paper_id": "iclr_2022_UGINpaICVOt", "review_text": "the paper proposed a new kind of activation function called matrix activation function that can be learnt jointly with the weights and biases. the paper got 2 strong rejects and 3 rejects. the major challenges include unclear motivation, limited novelty, incomplete related work, weak experiments, and poor paper writing. the author rebuttals did not convince the reviewers. the ac also read through the paper and agreed that the paper is below the bar of iclr. in particular, the authors neglected a large literature of learning activation functions in the original version, two more examples  xiaojie jin, chunyan xu, jiashi feng, yunchao wei, junjun xiong, shuicheng yan deep learning with sshaped rectified linear activation units. aaai 2016 17371743.  yan yang, jian sun, huibin li, zongben xu admmnet a deep learning approach for compressive sensing mri. nips 2017.  making them unable to compare with existing learnable activation functions thoroughly in the revised version in order to justify the necessity of using matrix activation functions. so the ac recommended rejection.", "accepted": 0}
{"paper_id": "iclr_2022_UvNXZgJAOAP", "review_text": "the paper proposed a sharp attention mechanism in the context of image to sequence modeling. it seeks to build a clear alignment from the attention in order to improve the performance of the task. i dont think there is a general consensus in the research community that the clear or hard attention performs better than the vanilla soft attention. therefore, experiments become the key in justifying such motivation and the model. however, as all the reviewers point out, the experiments in this paper are not satisfying. the numbers are far from the current mainstream results reviewer mesb. the experiments are done on relatively small datasetstasks reviewer 1prc and the comparisons arent strictly speaking fair reviewer 5hay. i think this alone is enough reason for the rejection of this paper. additionally, on the algorithmic side, the novelty of this mechanism is not that high, given the existence of work such as the hard attention xu et al, 2015 and variational attention deng et al, 2018. it is also an unanswered question how such a mechanism can be introduced into the modern architectures that use attention e.g. the multihead attention. the authors did not respond to the questions of the reviewers.", "accepted": 0}
{"paper_id": "iclr_2022_Ew4hVmrrqJE", "review_text": "while authors have updated the draft to address reviewers concerns, some parts are still not clear enough and the presentation needs further improvements. i encourage authors to revise the draft accordingly and resubmit in the future venues.", "accepted": 1}
{"paper_id": "iclr_2022_-qg9k1ftTc", "review_text": "the work proposes a simple neural network framework for detecting anomalies on sequential data. the manuscript is quite rough. the paper needs significant editing. the authors should take the reviewers recommendations to heart and make deeper changes to the paper.", "accepted": 0}
{"paper_id": "iclr_2022_4XtpgPsvxE8", "review_text": "the premise of this paper is that the development of time series forecasting methods has traditionally focused on accuracy rather than other criteria such as training time or latency. this work presents a new benchmark, evaluating classical and deep learningbased methods on a number of public datasets. they also propose a technique, paretoselect that is able to select models from the pareto front that can efficiently select models in a multiobjective setting. reviewer xsbl liked the observation that classical methods do not always beat deep learning methods even for very small datasets. they thought that the empirical contribution was valuable and mythbreaking. they also commented that the evaluation was robust. their main concerns were inadequate description of hyperparameters, lack of evaluations on really small datasets, missing confidence measures for latency results. they also made some suggestions for improving clarity. the reviewers responded, pointing to a description of the hyperparameters in table 2 of the appendix. they also responded to the reviewers comment about very small datasets and explained the advantage of the ranking loss. they made some small adjustments to the paper based on the clarity comments. reviewer pz2f also thought that the large scale comparison was valuable for the community. overall they thought it was well written though could be improved w.r.t. notation and writing style. they even inspected the code. their primary concern was that the paper lacked focus and tries to do too much and too little. is this a benchmarking effort of previous methods, or is the main contribution the paretoselect algorithm? this reviewer thought that due to its superficial coverage of too many things, and it wasnt ready yet for publication at iclr. the authors provided quite a comprehensive response to reviewer pz2f and pointed to some minor improvements in the manuscript. reviewer rqb3, like the others, viewed the benchmark analysis as valuable. they thought that the paretoselect approach was natural and that it was shown to be effective over baselines. like pz2f they had some structural criticisms and pressed for more insights. reviewers xsbl and rqb3 continued to engage in discussion through the acreviewer discussion phase. xsbl said that the authors response addressed some concerns yet raised others w.r.t. hyperparameter selection. rqb3 updated their review after considering the authors response, feeling that minor concerns were addressed but the paper could still use further development. overall, after considering the discussion i think that its been difficult for the authors to provide any patterns regarding which model performs best for which datasets. to me, a benchmark paper should provide some deeper insight and the paper appears to be struggling to do that. on the other hand, the study is comprehensive. the authors have argued in their response to all reviews, that their evaluation is at quite a different scale compared to other published time series model evaluations. i think that this benchmark paper can provide value to the community yet it could use further work specifically the authors need to focus the paper and communicate clearer insights from the study.", "accepted": null}
{"paper_id": "iclr_2022_ZeE81SFTsl", "review_text": "dear authors, i apologize to the authors for insufficient discussion in the discussion period. thanks for carefully responding to reviewers. nevertheless, i have read the paper as well, and the situation is clear to me even without further discussion. i will not summarize what the paper is about, but will instead mention some of the key issues. 1 the proposed idea is simple, and in fact, it has been known to me for a number of years. i did not think it was worth publishing. this on its own is not a reason for rejection, but i wanted to mention this anyway to convey the idea that i consider this work very incremental. 2 the idea is not supported by any convergence theory. hence, it remains a heuristic, which the authors admit. in such a case, the paper should be judged by its practical performance, novelty and efficacy of ideas, and the strength of the empirical results, rather than on the theory. however, these parts of the paper remain lacking compared to the standard one would expect from an iclr paper. 3 several elements of the ideas behind this work existed in the literature already e.g., adaptive quantization, timevarying quantization, .... reviewers have noticed this. 4 the authors compare to fixed  nonadaptive quantization strategies which have already been surpassed in subsequent work. indeed, qsgd was developed 4 years ago. the quantizers of horvath et al in the natural compressionnatural dithering family have exponentially better variance for any given number of levels. this baseline, which does not use any adaptivity, should be better, i believe, to what the author propose. if not, a comparison is needed. 5 fedavg is not the theoretical nor practical sota method for the problem the authors are solving. faster and more communication efficient methods exist. for example, method based on error feedback e.g., the works of stich, koloskova and others, marina method gorbunov et al, scaffold karimireddy et al and so on. all can be combined with quantization. 6 the reviewer who assigned this paper score 8 was least confident. i did not find any comments in the review of this reviewer that would sufficiently justify the high score. the review was brief and not very informative to me as the ac. all other reviewers were inclined to reject the paper. 7 there are issues in the mathematics  although the mathematics is simple and not the key of the paper. this needs to be thoroughly revised. some answers were given in author response. 8 why should expected variance be a good measure? did you try to break this measure? that is, did you try to construct problems for which this measure would work worse than the worst case variance? because of the above, and additional reasons mentioned in the reviewers, i have no other option but to reject the paper. area chair", "accepted": 1}
{"paper_id": "iclr_2022_dvl241Sbrda", "review_text": "the paper proposes to learn embeddings into complex hyperbolic space. this is an extension of the popular hyperbolicspace embeddings which have shown success on graphlike and treelike data. reviews and discussion mostly centered around the lack of clear motivation for the work why complex hyperbolic spaces? and the lack of a clear advantage over other manifold embedding methods that have varying curvature. the reviewers mentioned many questions and points that they thought the work should cover. there was also concern about the baselines against which the method was compared. there was not a consensus that the paper should be accepted, and no reviewer argued strongly for acceptance, even after the author response. as a result, i recommend that this paper not be accepted at this time. i expect a new version of this paper, incorporating this reviewer feedback and especially improving the explanation of the motivation, will be a good submission for a future conference.", "accepted": null}
{"paper_id": "iclr_2022_1Zxv7TdLquI", "review_text": "this paper proposes a method to train autoregressive model that takes advantage of a welldesigned energybased learning objective model. with the importance sampling, the model can be trained efficiently without requiring an mcmc sampling. experiments are conducted to verify the effectiveness of the proposed method. the idea is interesting and wellmotivated, but the experiments need to be improved. reviewer fnwes major concerns include limited novelty, lack of discussion with closely related works, and insufficient experiments, and recommend rejecting the paper by assigning a rating of 3. rebuttal doesnt address hisher concerns. reviewer in11 is concerned with the computational cost and training instability due to the extra ebm module and has a few unclear technical details that need to be clarified. the authors reply along with additional experiments during rebuttal partially addresses the concerns of reviewer in11, who eventually increases the rating to 6. reviewer aqxns major concern is also about the lack of sufficient comparison with other relevant energybased models. reviewer dzsj pointed out that the more insightful analysis about the model is missing in the experiments. even though the authors provide additional experiments for reviewer dzsj, they are not satisfied with the feedback because the additional results are not supportive of the claims made in the paper, and end up with a rating of 6. reviewer sjxns concerns include the lack of comparison with relevant works and the unclear motivation of the design of the joint distribution. after the rebuttal, reviewer sjxns concerns remain and assign a rating of 5 to the paper. the overall rating of the paper after rebuttal is marginally below the acceptance rate. even though this paper proposes an interesting idea, the reviewers comments are not well addressed. as a result, ac cannot recommend accepting the paper. the ac urges the authors to revise their paper according to the comments from the reviewers, and resubmit their work in a future venue.", "accepted": 0}
{"paper_id": "iclr_2022_8rpv8g3zfF", "review_text": "this paper proposes an idea to address the noniid issue that is wellknown in federated learning. after the discussion with the authors, there are still some concerns remained about the proposed approach. first and foremost, the training of local gan at each client can be demanding computationally and statistically, which limits the practicality of their approach. secondly, there has been other work that aims to study the noniid issues in federated learning, as suggested by the reviewers. the authors should consider citing some of the work in this literature and compare the prior approaches with their ganbased approach. thirdly, there is a lack of a formal statement about the privacy guarantee in this paper. in particular, it seems that the privacy guarantee would only make sense in the crosssilo setting, in which each client has many users data. if each client corresponds to a single user, it does not make sense to train a local gan. the authors should consider elaborating on the privacy guarantee in the next revision.", "accepted": 0}
{"paper_id": "iclr_2022_4JlwgTbmzXQ", "review_text": "this paper proposes to learn a latent space representation such that some linear equivariance and symmetry constraints are respected in the latent space, with the goal to improve sample efficiency. one core idea is that the latent space is also the same as the space of linear transformation used in the constraints, which is shown to simplify some of the mathematical derivations. experiments on the atari 100k benchmark demonstrate a statistical improvement over the spr baseline when using the se2 group of linear transformations as latent space. following the discussion period, most reviewers were in favor of acceptance. however, one reviewer remained unconvinced, and after carefully reading the paper, i actually share the same concerns, i.e., that it is unclear under which conditions the proposed approach actually works, and what makes it work. i believe that, as a research community, we should value understanding over moving the needle on benchmarks, especially when proposing such a complex method as this one see fig. 5. more specifically 1. the method is only evaluated on atari games, showing some improvements when using se2, and arguing that there are corresponding symmetries in such games. there is however no analysis demonstrating or even hinting at the fact that the proposed technique is actually learning to take advantage of such symmetries nb i had a quick look at the animation added by the authors in the supplementary material, but i do not see ifhow they help on this point. even if analyzing representations on atari may be tricky, i believe that given the motivation of this new algorithm, it must be evaluated on some toy example e.g., the pendulum mentioned throughout the paper to validate that it is learning what we want it to learn although i also agree with the authors that experimenting on a more complex benchmark like atari is equally important. 2. the idea of embedding states into the same space as transformations is interesting, and brings some advantages when writing down equations, as demonstrated by the authors. however, there is no justification besides mathematical convenience, and it doesnt seem intuitive to me at all that why this should be a good idea, considering that it ties the state representation to the mathematical representation of group transformations. for instance, what does the spcial group element e mean for a state? and this coupling makes it difficult to interpret the effect of using a different group of transformations for instance when moving from gl2 to se2, is the observed benefit because we are using only specific transformations, or simply because we are reducing the dimensionality of the state embedding? note that in fig. 4c the mlp variant has similar performance to gl2, and based on my understanding they use the same embedding dimensionality  i believe it would be important to check what would happen with an mlp variant using the same dimensionality as se2 3. the effect of the l_get loss is not convincing, as pointed out by several reviewers. i think it would have been an opportunity for the authors to investigate why, especially since it seems to work in some games and not others. but just focusing on here are the 1726 games where it works better doesnt really bring added value here. do these games have some specific properties that make them better candidates to take advantage of l_get? this could have been a very interesting insight if that was the case, but as it is now, i am not sure what we can learn from that. 4. there are several implementation details, some moving the final algorithm farther from its theoretical justification, that are not ablated, making it difficult to understand their impact ex using target networks, the choice of the value of m, using projections onto the unit sphere of some arbitrary dimensionality, how the s state is chosen in l_get as a result, we have here an algorithm with some interesting theoretical background, but with a lot of moving components which  when properly tweaked  can lead to a statistically meaningful improvement on atari 100k  without really understanding why. i believe this is not quite enough for publication at iclr, and i would encourage the authors to delve deeper into the understanding of their algorithm, which i hope will bring useful insights to the research community working on representation learning.", "accepted": 1}
{"paper_id": "iclr_2022_qTTccuW4dja", "review_text": "the paper proposes an entropic coding approach for sentence embedding. reviewers have spent good efforts in reviewing. they generally feel the problem is importantinteresting, but also found it difficult to understand the paper. thus, the authors are encouraged to thoroughly revise the paper according to the reviews provided, and another round of review is needed to better determine the merits of this paper.", "accepted": 0}
{"paper_id": "iclr_2022_LNmNWds-q-J", "review_text": "this paper aims to use pretraining to bridge the gap in performance between 2d gnn and 3d gnn. specifically, during pretraining, it trains both 2d gnns and 3d gnns on data equipped with 3d geometry to maximize the mutual information between the 2d gnn representation with the 3d gnn representation. the proposed approach is interesting and novel and the paper presents some promising results showing that the pretraining does provide some benefits for downstream tasks where 3d geometry information is not available in comparison to several other baseline pretraining methods. while the reviewers agree that property prediction without only 2d graph is a practically important setting for high throughput screening, there are concerns about whether the current set of results paint a clear picture on the benefits and superiority of the proposed methods to alternatives e.g., vs confgen even after the revision. this is not due to lacking of results, but more of a presentation issue where results are not organized and discussed clearly to provide a coherent story. we do see clear and strong potential for this paper but it needs a careful rewritereorganization to tease out the key messages and how the experiments support them.", "accepted": 1}
{"paper_id": "iclr_2022_LhObGCkxj4", "review_text": "this paper proves a global convergence rate of a newly proposed algorithm finite sum problem under some assumptions. while the proposed algorithm provides some interesting ideas to solving the finite sum problem using intermediate proxy solver, the current assumptions are too strong and im afraid that this can make the result essentially trivial for example, assumption 3 assumes that  h_i  v approx nabla_zphi_ihw, x_i for every i. this simply implies that  nabla_wphi_ihw, x_i _2  is as large as nabla_zphi_ihw, x_i_22  as long as the norm of v is small, since nabla_wphi_ihw, x_i  nabla_zphi_ihw, x_i h_i . hence the assumption simply assumes that if the loss is not small, then the gradient of the objective is not small using the convexity of phi, so nabla_zphi_ihw, x_i has to be large  this would imply that gradient descent can also work and arguably having the same convergence rate under this assumption. note that the smallest movement that can decrease the objective the most is indeed following gradient descent direction  so gradient descent would not move the weights more than this algorithm as well. therefore, i am not sure that there is a clear benefit to using this algorithm compared to the standard stochastic gradient descent. in particular, i would suggest the authors at least show one example where under the current set of assumptions, gradient descent does not work as efficiently compared to the proposed algorithm  this will make the proposed algorithm much more justified.", "accepted": null}
{"paper_id": "iclr_2022_C5Q04gnc4f", "review_text": "this paper finally received divergent and borderline reviews with two positive 6 and two negative 3 rates. after the thorough reviews by acs ourselves, we would like to decide to reject this work at this time, even though this submission has a lot of potentials including intensive analyses on instance segmentation frameworks and architectures. we first would like to appreciate comprehensive authors responses and additional empirical results. they should be extremely helpful to make this submission stronger. here are some of our suggested points for improvement i the novelty, significance, and practical implications of this work compared to previous analysis work may need to be better presented in a more persuasive way. ii nuance of stylization transformation can be better explained compared to other types of perturbations or transformations. iii empirical fairness can be better justified. iv since the paper is written in a highly condensed way, some of reduction may improve the readability. v finally, given that this paper focuses on empirical study about instance segmentation, it may be more appreciated in a computer vision venue.", "accepted": 0}
{"paper_id": "iclr_2022_j30wC0JM39Q", "review_text": "the authors of this work introduced new metrics for node embedding that can measure the evolution of the embeddings, and compare them with existing graph embedding approaches, and experimented on real datasets. all reviewers agreed that the work addresses interesting problem and that the proposed measures are nove, but there are too many flaws in the initial version of the paper, and despite the thorough responses of the authors, it is believed that there are still too many open questions for this paper to be accepted this year iclr.", "accepted": 0}
{"paper_id": "iclr_2022_OBwsUF4nFye", "review_text": "many problems in machine learning rely on multitask learning mtl, in which the goal is to solve multiple related machine learning tasks simultaneously. in this work, authors formalize notions of tasklevel privacy for mtl via joint differential privacy jdp. they propose an algorithm for meanregularized mtl, an objective commonly used for applications in personalized federated learning, subject to jdp. then analyze objective and solver, providing certifiable guarantees on both privacy and utility. the main results, namely the convergence rate results, are hard to parse and hard to interpret. for example, as one reviewer pointed out, it is bounded below by a constant which is not properly explained. further, comparisons to the literature in userlevel privacy which is equivalent as the tasklevel privacy is not provided enough. significant improvement in the presentation of the main results, along with an interpretable explanation of the contribution, is necessary for this manuscript.", "accepted": 0}
{"paper_id": "iclr_2022_gI7KCy4UDN9", "review_text": "the paper considers quantization issues for learned neuralnetworkbased image compression methods. many works on the topic incorporate quantization into the training of the method. the paper provides evidence that posttraining quantization is effective. specifically, the paper demonstrates that stateoftheart learned image compression methods can be quantized posttraining and retain a very similar level of compression performance. the paper argues that this is important in particular for crossplatform applications, where an image is decompressed on different architectures. finally, the paper proposes an approach to discretize entropy parameters. the reviewers raised the following concerns.  reviewer 2xdr is concerned about the application of posttraining quantization being a contribution, since posttraining quantization has already been studied in 1 and in the recent paper 2 that can be considered as concurrent work. the authors response is that the methods in 1 has extra overhead and clarify how the prior work is in fact different. this addressed the reviewers concern, and the reviewer raised their score.  reviewer eyvf finds the comparisons with previous methods to be insufficient, and in general find the value of the research unclear, as the goals are not sufficiently specified. the authors clarified, and the reviewer was satisfied with the response and raised the rating to marginal above the threshold.  reviewer l7dn tends towards acceptance, but has concerns about the technical novelty, that are unspecified unfortunately.  reviewer ov3r argues that the solution is marginal relative to prior work, and votes to reject the paper. the authors responded why they think it isnt, and also wrote a private letter to the area chair in which they explain why they think that reviewers ov3r should not be taken into account. i agree with the authors that the paper under review provides a step relative to balle et al 2019, and that the writing of the paper is not an issue; however, the reviewers overarching point is that the overall contribution is marginally significant when taking the prior work by balle et al 2019 into account and this is the sentiment of other reviewers as well.  reviewer grps, an expert on image compression, leans towards acceptance and argues that the results are strong as they show little to no loss due to the quantization technique, but also rates the contribution to only be marginally significant and novel, and raises a few questions and issues, to which the reviewers responded. this paper is really borderline. four out of five reviewers rate this paper as marginally above the acceptance threshold. the consensus is that while the experiments and claims are correct, the contribution is only marginally significant or novel, in particular, relative to prior work, and therefore i recommend rejecting the paper. i would, however, not be upset if it would be accepted.", "accepted": 0}
{"paper_id": "iclr_2022_Qu_XudmGajz", "review_text": "in order to evaluate the evidence lower bound elbo, vaes typically use a parametric distributionbased decoder pxz. if the data is continuous, one often considers a gaussian vae, where the canonical setting is to assume a diagonal covariance matrix pxz  nx; muz, sigma2 mathbfi. in this paper, the authors suggest replacing the diagonal covariance matrix with a structured covariance matrix lowrank  diagonal. as this only amounts to a minor change to a canonical gaussian vae, strong empirical results are expected to justify its acceptance. however, the image generation results presented in the paper are not comparable to the stateoftheart vae results e.g., arash vahdat, and jan kautz. nvae a deep hierarchical variational autoencoder. neural information processing systems neurips, 2020.", "accepted": 0}
{"paper_id": "iclr_2022_psNSQsmd4JI", "review_text": "this paper proposes a distributed containerized multiagent reinforcement learningcmarl framework that addresses three challenges in marl 1 demanding data transfer. 2 interprocess communication. 3 effective exploration. using a container that collects environment experiences from parallel actors into buffers and learns local policies, cmarl demonstrates notable performance improvements with respect to time as compared to stateoftheart benchmarks. although the reviewers acknowledge that the paper addresses a relevant topic, proposes an effective method, and is well written, after reading the authors feedback and discussing their concerns, the reviewers reached a consensus about rejecting this paper in its current form. they feel that the contribution is too incremental and that the experimental comparisons are somehow unfair. i suggest the authors take into consideration the reviewers suggestions while preparing an updated version of their paper for one of the forthcoming machine learning conferences.", "accepted": 0}
{"paper_id": "iclr_2022_Ctjb37IOldV", "review_text": "the authors make an experimental case that dropout aids generalization by promoting flatter minima. the reviewers felt that the work reported in this paper makes a useful step forward on a question of central interest. the consensus view was that the total weight of evidence presented was not sufficient for publication in iclr. the paper could be strengthened was more extensive and varied experiments andor theoretical analysis.", "accepted": null}
{"paper_id": "iclr_2022_YTtMaJUN_uc", "review_text": "ultimately somewhat below the threshold based on the scores. the reviewers raise issues of the overall contribution, as well as issues with the designstructure of the modelpaper and issues with the experiments. while there are some positive aspects, collectively the issues put the paper below the bar for acceptance.", "accepted": null}
{"paper_id": "iclr_2022_6NT1a56mNim", "review_text": "this manuscript presents a method to refine highlevel task descriptions into midlevel executable steps. the idea of using language models to generate steps for a robot to follow is very interesting. reviewer concerns focused on the general applicability of the approach and the evaluation. reviewers pointed out that the method is tied to virtualhome which has various properties that are in general not true the action space is small, the action space is very sparse, and objects tend to be unique. first, the method enumerates a sentence for every possible action and object combination in the environment. the fact that virtualhome has few verbs and few objects and that neither of these has complex additional structure adjectives, adverbs, etc. means that this is practical. but in any other practical setting this will be impossible. the manuscript mentions this limitation and hints at possible ways to resolve it. second, the method requires that the action space must be incredibly sparse. moreover, a set of common sense rules are needed which are environment specific and must be hand curated. virtualhome disallows microwaving a cup for example. it also disallows opening the tv. both of these are valid actions that happen all the time. third, the method requires that objects be unique. if multiple plates, vacuum cleaners, lotions, etc. existed and had to be manipulated, e.g., there is no mechanism to refer to any one plate consistently. the model could generate something like the first plate but how to actually execute such an action is far from clear. this third issue is related to the problem of grounding. normally, grounding means connecting an abstract concept to something concrete in the environment. all of the grounding that is performed here is by virtue of virtualhome having unique objects in its environments and the actions not requiring multiple instances of the same object. this is not addressing the problem of grounding. reviewers requested that grounding be removed from the manuscript. this would significantly enhance it, as the model is inherently incapable of grounding as the authors say indeed, one limitation of our approach is that we do not condition on environment state reviewers took issue with details of the evaluation, which are largely a consequence of the choice of virtualhome. sometimes this manifested as strange results like models outperforming humans in terms of correctness. as reviewers pointed out, this is worrisome. reviewers were also concerned about the title. it implies that language models are zeroshot planners, but this is not the case. they are instead able to decompose actions into midlevel steps. reviewers suggested that it would be better to focus the title and tone of the manuscript on extracting tasksubtask structures from language models. the idea presented here, that language models can break tasks into subtasks is interesting. but the manuscript goes a step further and discusses embodied agents which to reviewers appeared to be a reach there is no grounding and in no sense is the output of the language model any different if the agent is embodied. even the most positive reviewers felt that discussing embodied agents is unhelpful it would be better to focus on tasksubtask structures. and indeed, this would be more general. all of the concerns that reviewers had around the evaluation would be alleviated by focusing on a language task instead. and the effect of a narrow space of actions, constraints on those actions, and multiple objects of the same class, could be evaluated and reported. even if the authors had to collect such a corpus, given the difficulties they describe in evaluating on virtualhome, this would be less of a burden. this could be a strong submission in the future.", "accepted": 1}
{"paper_id": "iclr_2022_8IXBbFjkMat", "review_text": "the paper addresses unsupervised conditional text generation extending emb2emb mai et al, 2020 with bagofvectors antoencoders. reviewers shared several concerns about the clarity of this paper and empirical results.", "accepted": null}
{"paper_id": "iclr_2022__qc3iqcq-ps", "review_text": "understanding neural networks once they have been trained is a big open problem for machine learning. this manuscript designed graph theoretic and information theoretic measures aimed at helping us understand community structure and function in trained networks. in particular, they measure community structure modularity and entropy for trained networks and related these to the performance of the networks. the manuscript runs experiments with fully connected networks on problems such as mnist and cifar. both community structure and entropy measures are shown to correlate spearman and pearson correlation coefficients with performance metrics in the networks studied. reviewers tended to agree that the paper was well written and motivated by an interesting and timely question understanding trained networks. however, on the whole, most of the reviewers believe that the manuscript is too preliminary for publication at iclr and i agree. a central issue cited by most of the reviewers is that the experiments are performed on smalltoy models for small tasks and under particular hyperparameter regimes. it is therefore unclear to what extent the results would generalize to other situations. e.g. would the results hold for larger dataset or for convolutional neural networks? connected to this complaint, reviewers worry that there is not enough connection to the literature and baseline methods that could be used to predict performance given measures of trained network activity. even allowing that the observed correlations are true and generalizable, are these measures better than those covered elsewhere in the literature? additionally problematic, the measures are not theoretically justified either. thus, we are missing both reasoned arguments for the metrics and robust quantification beyond a limitted experimental setting. one reviewer, xmnm, is compelled by the work and recommends acceptance. however, they do not present a compelling case for acceptance, and even repeat several of the concerns raised by other reviewers. in sum, the work is on an interesting subject and timely, but needs further work to be ready for publication.", "accepted": 1}
{"paper_id": "iclr_2022_RuC5ilX2m6O", "review_text": "the paper proposed an approach to search image augmentation policies. the paper formulates this problem as a cooperative multiagent decisionmaking problem, which is interesting. the paper received 3 borderline accept and 1 borderline reject ratings. the reviewers originally had multiple concerns regarding the necessity of rlbased approach, lacking references, and additional experiments, and the authors responded to some of the concerns of the reviewers reasonably. however, none of the reviewers ended up strongly supporting the paper, staying with their ratings. the rl formulation of the problem is interesting, but it requires multiple rounds of the target network training due to its nature i.e., it is not an endtoend approach. the paper misses some details on how exactly the patchwise rlbased augmentation works and it requires additional hyperparameters for the selection of patch size and shape. it is also unclear how this rlbased method is conceptually superior to previous augmentation approaches and the empirical results are not strong enough, as some of the reviewers also pointed out. although the paper has interesting ideas and the ac also think the paper has some merit, the senior ac finds the technical contribution of the paper weaker than the others. we unfortunately need to recommend the rejection of the paper.", "accepted": null}
{"paper_id": "iclr_2022_8QE3pwEVc8P", "review_text": "this paper received scores of 5,5,6,8. the reviewer giving a score of 8 stated that they wouldve given a 7, but that that is not an option in the system. the other reviewer giving an acceptance scores mentioned that they would also be ok with a rejection. the details of the assessment are thus less enthusiastic than could be assumed with an overall average score of 6. i am therefore weakly recommending rejection. the main criticisms of the reviewers are lack of novelty, lack of deeper analyses that really provide insights into why zerocost operation scoring works, and lack of the number of nas benchmarks tested. out of these, personally, i would not criticize a lack of novelty, since it is not trivial to put together zerocost and oneshot methods and the results appear promising. however, even the most positive reviewer criticized that the work focuses on nasbench201 heavily which is particularly problematic given that nasbench201 uses a fixed wiring and only allows the choice of operations; this may make the proposed method particularly applicable. during the rebuttal, the authors added nasbench1shot, which is a very good step, but the proposed technique does not actually work as well there. while this may be due to the special nature of operations in the nodes rather than in the edges for nasbench1shot1, for a revision, it would be good to add additional experiments on further nas benchmarks in order to allow for a better understanding under which circumstances the proposed method works well. in particular, it would be interesting how well the method works on a quite different search space, such as the one of mobilenet.", "accepted": 1}
{"paper_id": "iclr_2022_vxlAHR9AyZ6", "review_text": "this manuscript proposes and analyses a weighting approach to improve the conformance of adversarial training in federated learning. the authors observe that adversarial training seems to degrade during the late stages of training, and suggest that this degradation is a consequence of exacerbated crossdevice bias in federated averaging. they suggest and analyze a weighted scheme to fix this issue. during the review, the main concerns are related to the novelty of the work compared to existing work, the clarity of the technical contributions, and unclear technical statements. the authors respond to these concerns and partially satisfy the reviewers. after discussion, reviewers remain mixed, with multiple weak rejects and one strong accept. no fatal flaws are noted. the opinion of the area chair is that while there are no fatal flaws, there is very limited enthusiasm for this paper. this limited enthusiasm seems to be a result of intuition for observed phenomena that seem incorrect or insufficient to reviewers. overall, i think this paper outlines and addresses an interesting issue of real concern. flaws in the intuition buildingexplanation, and issues with clarity of presentation need to be improved for this work to have some impact.", "accepted": 1}
{"paper_id": "iclr_2022_ybsh6zEzIKA", "review_text": "this paper proposes a mixup type of data augmentation for graphs that accounts for the difficulty of mixing graphs of different number of nodes. the authors show that the mixed graphs are invertible functions of the original graphs. reviewer d3ri liked the simplicity and effectiveness of the technique. they called it a healthy and useful contribution for the field. reviewer n1dk thought that the paper explored an important problem and thought the paper was clear, though some of the math could have been simplified. this reviewer was concerned that a central claim of the paper, that the method avoids manifold intrusion was unsubstantiated. specifically that it could not be deduced from the fact that edge connectivity could be recovered from the mixed graphs. the reviewer claimed that node features of the individual graphs were unrecoverable. the authors responded in detail to the reviewers criticism, adding two new lemmas which purportedly guaranteed node feature vectors could be uniquely recovered. the authors admitted to some conversion between manifold intrusion and invertibility and added a theorem and its proof that invertibility guarantees no manifold intrusion. the authors also responded to reviewer n1dks concern about the significance of the reported improvements. reviewer n1dk responded to the author rebuttal with concerns about the strong and unrealistic assumption of linear independence of the feature matrix. they had further concerns that for the case of weighted edges the intrusionfree property could not be enforced. discussion ensued, with the authors arguing that the independence assumption was not as strong as the reviewer claimed and that the intrusionfree property was only every for graphs with binary edge weights. reviewer 7hbs and q8bs were both on the fence. 7hbs also raised some concern with the case of nonbinary weighted edges. they also raised the same issue with respect to the connection bw invertibility and the intrusionfree property, which the authors addressed. reviewer q8bs also thought the problem was interesting, the paper was clear, yet like n1dk thought the performance improvement was marginal and had concerns with technical novelty of the work. this was a tough call, so i engaged the reviewers in further discussion. 7hbs agreed with n1dks opinion that the central claim of the paper the method being intrusionfree was not presented with strong evidence. they also raised another concern, which was that the paper didnt evaluate on node classification like most other graph mixupstyle models. q8bs agreed with n1dks concerns and felt that post discussion the technical novelty of the work was limited. without strong support from the reviewers, i think that this paper could use further development, either lightening the intrusionfree claim or presenting evidence for it in other settings.", "accepted": 1}
{"paper_id": "iclr_2022_aMaQjwz5IXI", "review_text": "this work aims to improve style transfer in the unsupervised nonparallel case. it does this by proposing a style equalization approach to prevent content leakage and assuming that content information is timedependent whereas style information is timedependent. this is an important problem to solve and lots of prior work in the area exists. the work is wellorganised with good experimental results. however, there are strong claims in the paper and there is insufficient experimental comparison to similar related work such as hsu et al. 2019 and ma et al. 2018 to back that up. if theres no comparison with the current state of the art e.g. due to a private implementation or dataset then its hard to justify calling a new work a new state of the art. even though an implementation may be private, it can be worth spending time to reproduce a paper or asking the authors for an implementation. finally task and metric selection could be improved to better highlight the performance of the approach. the reviewers thank the authors for the rebuttal but it was insufficient to change their decision.", "accepted": 1}
{"paper_id": "iclr_2022_TKrlyiqKWB", "review_text": "this paper extends prototypical classification networks to handle class hierarchies and fairness. new neural architecture is proposed and experimental results in support of it are presented. unfortunately, reviewers found that paper in its current for is not sufficiently strong to be accepted at iclr. authors have made a significant attempt to clarify and improve the paper in their response. however, reviewers believe that contributions and motivation can be clarified further. we encourage authors to improve their work according to the specific suggestions made by the reviewers and resubmit.", "accepted": 0}
{"paper_id": "iclr_2022_Zk3TwMJNj7", "review_text": "this is an interesting paper, aiming to separate the generalization properties of sgd and gd. unfortunately, the reviewers had many significant concerns, primarily on the topic of the relationship to prior work by wu et al. which has a similar setting and similar proof techniques, but also regarding presentation and interpretation of results in general. as such, i recommend the authors continue with this line of valuable work, aiming in particular to further separate it from existing results.", "accepted": 0}
{"paper_id": "iclr_2022_MWQCPYSJRN", "review_text": "this paper suggests a new technique to utilize generative replay for continual learning. specifically, the authors claim that even though the generated samples are imperfect thus cannot be used as positive samples for old classes, they can still be used as negative samples for the current class. 3 reviewers are negative and 1 reviewer is positive. the main concerns of negative reviewers are a nonablated effects of baseline and proposed components, b insufficient analysis of negative replay, and c no assessment of generated data quality. the rebuttal provides an additional experiment to address the issue a, but the reviewers and ac think the experiments should be better polished. also, ac believes the issues b and c should be better analyzed. the rebuttal claims that issue c is not applicable as they generate samples on the latent space. however, the main motivation of the paper is the low quality of generated samples, and the paper should provide a quality measure to support their claim. for example, an update of the feature extractor may move the latent space generative replay to the wrong class i.e., low quality, and thus one should not use it as positive but only as negative, as suggested in this paper. here, the negative replay would increase the margin of current and old classes, enhancing the accuracy of the current class. to analyze the source of benefits old vs. current classes, the authors could report the taskwise accuracy trends, not only the overall accuracy. it would be a nice addition to the issue b. due to these unresolved concerns, ac tends to recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2022_G0CuTynjgQa", "review_text": "this paper proposes to analyze the generalization error of deep learning models and gans using the lipschitz coefficient of the model. there was significant discrepancies in the evaluation of the paper among reviewers. while all reviewers acknowledged the interesting theoretical approach to understand generalization and the relevance to iclr of the problem, they disagreed about the readiness level of the paper. some concerns were expressed in terms of clarity and the ac agrees with these, but most importantly, reviewer wkt9 pointed an important flaw in the current analysis that was not properly responded to by the reviewers see below. in discussion, other reviewers were also concerned by this flaw, and so the ac decided to recommend a major revision of the paper taking the reviewers comments in consideration.  important flaw in the paper analysis from wkt9 basically, theorem 1 assumes that a loss fh,x is llipschitz w.r.t. input x in some compact set of diameter b for any h. the author shows that the sup_h in h e_p fh,x  e_hatp fh,x is upperbounded by l b  c sqrttextstuffm. the concern of wkt9 is that the lhs is upperbounded trivially and deterministically by the tighter l b see proof sketch next for any distribution p and hatp just because of the compactness of the input set and that f is llipschitz; one does not even need to include the number of samples m in the analysis thanks to the very strong assumption on f. the reviewer also was concerned that later theorem 3, the authors study ways that we can make l exponentially small which is interesting, but this has both the issues that 1 it tells you nothing about the absolute performance of your network, as this only bounds the variation between any two distributions indeed including the empirical and true distribution; but the fact that it also contains all distributions should indicate how loose this bound is!, and so perhaps the best empirical error one can obtained is still big 2 the current version of theorem 3 uses a loose bound with a dependence on m which was not even needed as per the result above. while its true that empirically one can observe small empirical error, and thus combining this with a small lipschitz constant would indicate good absolute performance; but the current presentation of the theory is rendered quite problematic by the above refinement, and should be corrected in a revision.  proof sketch for simplicity, ill prove it for p being a discrete distribution and hatp being the empirical; but im pretty sure you can extend it to continuous distributions as well. note that we have fh,x  fh,x leq l b for all x, x in the compact set of diameter b and for all h. now e_p fh,x  e_hatp fh,x  sum_j pi_j fh, x_j  frac1m sum_i fh,x_i for each x_i, associate several x_js so that the total sum of their probabilities is 1m split some pi_j in multiple pieces if necessary  we can augment the index set for these new pieces, to obtain new probabilities pi_j and call i_i the set of associated indices to x_i. we have sum_j in i_i pi_j  1m we thus have e_p fh,x  e_hatp fh,x  sum_i sum_j in i_i pi_j left fh, x_j  fh,x_i right thus e_p fh,x  e_hatp fh,x leq sum_i sum_j in i_i pi_j left fh, x_j  fh,x_i right leq l b this is true for any h, so this is also true for the sup, deterministically! qed", "accepted": 1}
{"paper_id": "iclr_2022_9W2KnHqm_xN", "review_text": "despite some positive points, the criticisms and overall scores put this paper below the bar. the reviewers raise issues of novelty, as well as problems with the experiments and argue that some claims are unsupported.", "accepted": null}
{"paper_id": "iclr_2022_G-7GlfTneYg", "review_text": "paper this paper addresses the problem of learning methods for general speech restoration which generalizes across at least 4 tasks additive noise, room reverberation, lowresolution and clipping distortion. the proposed approach is based on a twostage process, which includes both analysis and synthesis stages. discussion the reviewers wrote very detailed reviews which ask some important questions and point to some potential issues. the authors responded to all reviews, but only addressed a subset of the issues and questions mentioned by the reviewers. novelty and comparison with previous approaches was one of the issues mentioned by reviewers. summary while reviewers are supportive of this line of research, reviewers were also concerned with the novelty of the proposed approach and details of the experiments. in its current form, the paper may not be ready for publication.", "accepted": 0}
{"paper_id": "iclr_2022_qWhajfmKEUt", "review_text": "based on the observation that the eigenvectors with smaller eigenvalues are more nonrobust i.e., adversary adds more components along such directions, the authors propose a method called feature spectral regularization fsr to penalize the largest eigenvalue, and as a result, the other smaller eigenvalues get increased relatively. in this paper, in addition to fsr, theoretical analysis along with experimental results on different datasets and models were presented. although the proposed fsr has some merits, the major concerns from the reviewers include 1 impractical use on largescale datasets and 2 lack of significant improvement over sota. compared with other submissions im handling, i have to reject this manuscript.", "accepted": 0}
{"paper_id": "iclr_2022_QDDVxweQJy0", "review_text": "four reviewers acknowledged the authors response and did not change their largely negative scores. the one enthusiastic reviewer did not respond to the more negative reviewers and has not worked in the theorem proving area. the main problem with the paper seems to be that the reviewers were not convinced by the empirical results. they felt that results should have been presented on more widely used benchmark datasets.", "accepted": 1}
{"paper_id": "iclr_2022_Bel1Do_eZC", "review_text": "this paper studies the pruning problem of graph neural networks, i.e. finding lottery tickets for gnn. in particular, it generalizes ugs by chen et al. 2021 from transductive setting to inductive setting where prediction on unseen graphs is possible. the main idea is 1 learn a mask network to assign importance scores for edges using the embedding features of the nodes connected, that avoids the double parameter memory costs in ugs; 2 prune the edges according to the importance score and weights of gcn according to their magnitudes. main concerns from reviewers are about the novelty, evaluation, and scalability. despite that generalization to unseen graphs using the mask functions on embedding features is a new aspect, the evaluation is compared with relatively weak baselines and inference time scalability of is still an issue.", "accepted": null}
{"paper_id": "iclr_2022_Is5Hpwg2R-h", "review_text": "this paper proposes a new approach, which combines offline reinforcement learning with learning in simulation. there were different views on the paper among the reviewers and we had quite a lot of discussions. as a consequence, there were still serious concerns remaining, e.g., whether the results are significant enough, whether there are clear advantages of the proposed mehtod over directly using offline rl methods. it is not justified whether the proposed framework can use offline data more efficiently or better reduce the gap between mismatched simulators and offline data. the reviewer who gave the highest score decided not to champion the paper. considering all the discussions, we believe the paper is not ready for publication at icir yet.", "accepted": 1}
{"paper_id": "iclr_2022_SVey0ddzC4", "review_text": "the paper presents several related results. the initial main result consists in relating gpca to gcn, showing that gpca can be understood as a first order approximation of some specific instance of gcn where the w matrix is directly defined on data. this result is then exploited to define a supervised version of gpca. as a followup the authors propose a novel gpcabased network gpcanet and a gpcanet initialisation for gnns. the paper is well written and easy to read. empirical results are reported to verify the above mentioned connection between gpca and gcn, as well as the performances of gpcanet and the proposed initialisation for gnns. overall, while the mentioned connection was never explicitly reported in the literature, its existence is not surprising and thus its significance seems to be limited. also the performances of gpcanet do not seem to be significant from a statistical point of view. the novel initialisation procedure for gnns seems to be interesting and promising, although the used datasets may not make evident its full power. authors rebuttal and discussion did not change the reviewers initial assessment.", "accepted": null}
{"paper_id": "iclr_2022_3pZTPQjeQDR", "review_text": "this paper investigates the role of bpe and vocabulary sizes in memorization in transformer models. through a series of experiments on random label prediction, training data recovery and membership inference attacks, the paper shows that larger vocabulary sizes lead to improved memorization. the reviewers all agree that the paper investigates an important question and does so thoroughly. the main concerns were about 1 the validity of the conclusion that it is sequence length indeed which affects memorization; and 2 the lack of more tasks to validate the findings. for 1 the authors added another set of experiments which further rule out frequency effects as a factor, but i agree with reviewer kazc that more evidence is needed which directly shows that sequence length is responsible e.g. are shorter paq questions memorized better?. for 2 the authors shared a google drive link with additional results on nmt after the deadline, which the reviewers appreciated. overall, however, the paper needs more work in order to unify all these results in a single draft.", "accepted": null}
{"paper_id": "iclr_2022_PlFtf_pnkZu", "review_text": "this paper has conducted extensive experiments to examine the scaling and transferring laws of lms for machine translation and has concluded several interesting findings which could be inspiring to the future work. the main concerns from reviewers are that the novelty of this paper is not enough. in addition, the experiments are not welldesigned and the clarity of this paper can be further improved. we hope the reviews can help authors improve their paper.", "accepted": 0}
{"paper_id": "iclr_2022_AgDwZa1AiJt", "review_text": "this paper a distillation framework where a lightweight student model is trained to handle easy frequent instances, while the large teacher model is still used to handle the more difficult rare inputs. the models are trained to perform well in this twostage inference setting. experiments are conducted on computer vision and nlp tasks. while the idea is potentially interesting, the experimental results are fairly weak and not very convincing.", "accepted": 0}
{"paper_id": "iclr_2022_agBJ7SYcUVb", "review_text": "this paper presents a package for dynamic finegrained structured sparse attention mechanism dfssatten, which aims to improve the computational efficiency of attention mechanisms by leveraging the specific sparse pattern supported by sparse tensor cores of nvidia a100. dfssatten shows theoretical and empirical advantage in terms of performance and speedup compared to various baselines, with 1.271.89x speedup over the vanilla attention network across different sequence lengths. reviewers praised the simplicity of the method and the clean code implementation. speeding up attention mechanisms is an important problem is leveraging sparse tensor cores for attention speedup is a sensible idea. the practical speedups are significant 1.271.89x over the vanilla attention across different sequence lengths. however, they also pointed out some weaknesses the fact that the proposed method is very specific to the particular sparse pattern offered by nvidia a100, and not easily generalizable to other future hardware; the fact that the method focuses on inference acceleration and not training from scratch not completely clear in the paper, which limits its scope; and the fact that the method still has on2 complexity it still requires the computation of qkt, which has quadratic memory and computation cost, and therefore it does not really address the quadratic bottleneck of transformers, unlike other existing work in efficient transformers for long sequences. i tend to agree with the reviewers and, even though the package can be potentially useful to other researchers, the scope seems limited and the paper seems a bit thin to deserve publication at iclr. other comments and suggestions  when talking about linear transformers, you should cite 1, which predates performers  it is not clear to me why 12 and 24 are called finegrained structured sparsity  citations for the systems in tab 4 are missing  when comparing to other methods, it would be include to include their pareto curves since those methods have tradeoffs in terms of sparsity  approximation error or downstream accuracy. 1 transformers are rnns fast autoregressive transformers with linear attention. angelos katharopoulos, apoorv vyas, nikolaos pappas, fran\u00e7ois fleuret httpsarxiv.orgabs2006.16236", "accepted": 1}
{"paper_id": "iclr_2022_oZe7Zdia1H5", "review_text": "summary this work demonstrates that it is possible to identify lottery tickets with some manner of structural sparsity. the work finds success through refilling perhaps better termed infilling and regrouping, two techniques that have found previous homes in other parts of the literature.  discussion  strengths  tackles an interesting problem.  at face value, i believe the paper achieves its claimed goal, though not with full clarity as written.  weaknesses  there is room to clarify the atypical form of structure here for readers. the suggested title change would appropriately set expectations. however, refinements in the text as well would be welcomed.  as i discuss below, the claims should be settled with respect to the strongest baselines.  random reinitialization should play a primary role in the presentation of the text. as the authors note, the original lottery ticket paper did not require besting the performance of random reinitialization. however, the random reinitialization results are central to the main figures of the original paper and demonstrate that the result is not merely happenstance. this paper should follow that practice.  recommendation i recommend reject and i do not do so lightly, given the scores. the work here is promising because finding a path to betterperforming lottery tickets remains an open challenge. however, reviewer ywuh has voiced reasonable concerns about the evaluation methodology. ive read the detailed authors responses and agree with the authors that the presented results may depend on choices in the hyperparameters and training strategy. having said that, it is critically important for the paper to include in the primary text the strongest baselines available, despite this dependence. on such baselines, the results are worse than originally reported and hence, the primary claims must either be revised to be these results or revised to include these results, with the primary claims providing a range of results. though the authors offer to make revisions for the cameraready, the required revisions here are substantial enough to require additional review, which is out of the scope of the current process. an additional oversight in this methodology  at least as reported  is that the primary target of the lottery ticket hypothesis is sparse training, not sparse inference. evaluating solely the inference performance  rather than training performance, which includes both the forward and backward pass  is, therefore, inconsistent with the purpose of lottery tickets. this methodological error will too need to be repaired in the final version of this work.", "accepted": 1}
{"paper_id": "iclr_2022_xUdEO_yE-GV", "review_text": "the paper proposes a method for segmentation of thin structures in 2d and 3d, based on persistent homology and using a new filtration. the method performs similarly to stateoftheart methods on 2d datasets and outperforms some baselines in 3d. after considering the authors response and discussing, the reviewers have not arrived at a consensus. pros include  simple and reasonable approach  fairly strong experimental results some cons are  missing theoretical contributions  experimental results on 2d datasets are not that strong, while on 3d datasets important baselines are missing  at times unclearunconventional presentation overall, at this point i recommend rejection. the paper is promising, but since the main claim is good performance on 3d data, it is important to have a thorough empirical evaluation there, with the relevant baselines as mentioned by the reviewers. i very much encourage the authors to polish the paper and submit to a different venue.", "accepted": 1}
{"paper_id": "iclr_2022_ZgrmzzYjMc4", "review_text": "this paper studies the problem of choosing the best cloud provider for a task. the problem is formulated as a bandit and solved using algorithm cloudbandit. the algorithm is compared to several baselines, such as smac, and performs well. the evaluation is done on 60 different multicloud configuration tasks across 3 public cloud providers, which the authors want to share with the public. this paper has four borderline reject reviews. all reviewers agree that it studies an important problem and that the promised multicloud optimization dataset could spark more research in the area of cloud optimization. the weaknesses of the paper are that it is not technically strong and that the quality of the new dataset is not clear from its description. at the end, the scores of this paper are not good enough for acceptance. therefore, it is rejected.", "accepted": null}
{"paper_id": "iclr_2022_I1dg7let3Q", "review_text": "this submission has generated sufficient debate, including some messages that, in my viewpoint, have the wrong tone. it may well be that different colleagues see the work in different ways. it is very hard to evaluate submissions in a short time and mistakes can happen. in this case, i think there were and still are misunderstandings and unclarity wrt very crucial points of the paper. this does not mean that the work overall is weak not that there is no contribution. if the content is so interesting as discussed by authors and multiple reviewers in some way which it seems to be, then a better presentation and argumentation will lead to a publication elsewhere soon, but based on all the data that i have here, i recommend rejection. i see to reason to list details about the content and possible concerns, as they should be clear from the multiple messages among authors and reviewers. best of luck.", "accepted": 1}
{"paper_id": "iclr_2022_Le8fg2ppDSv", "review_text": "the paper is well motivated and tackles a hard and longstanding problem with seq2seq models diversity and controllability. the authors propose simple architecture for controllable text summarization. they use multiple decoders controlled by a gating mechanism which can be learnt or controlled manually. they control mostly the abstractiveness and specificity properties of the model. pros  the proposed approach is somewhat novel several earlier work have proposed multiple decoder models to control the generation  as pointed by the reviewer team  the proposed modifications are motivated well, the approach is simple and easy to understand.  the paper is well written and easy to read.  the authors made an effort to address most of the reviewers comments even added human evaluation scores which was asked by reviewers  it seems a highly flexible way of enriching existing models in a simple way for additional control behavior in output summary generation of documents. cons  during discussions, reviewers have circled around the novelty and continued to raise concerns about the weaknesses of benchmarks and comparison to related work and the fact that the proposed model has more parameters is potential advantage over other models that might contribute to the performance gains. thus, the paper could be made stronger with further evaluations that could possibly make it stand out.", "accepted": null}
{"paper_id": "iclr_2022__MRiKN8-sw", "review_text": "this paper presents a study of methods for tabular data imputation. in particular, the authors compare deep learning methods with knn based approaches. experiment results demonstrate knn to be competitive with deep learning methods. reviewers have concerns about the characteristics of datasets used in the experiments and hyperparameters used for evaluation, which i agree with. they also think that the main contribution of comparing knn and deep learning methods is not strong enough for acceptance to iclr. i recommend rejecting this paper.", "accepted": 0}
{"paper_id": "iclr_2022_PC8u74o7xc2", "review_text": "the paper proposes a new framework to express and analyze embedding methods based on the stable coloring problem. reviewers highlighted as strengths that the paper provides an interesting perspective for understanding one of the central approaches in nlp, graph learning, and other fields  and as such could inspire promising research directions. however, reviewers raised concerns regarding the significance of contributions theoretical insights and analysis, relation to prior work, missing empirical evaluation etc. as well as the clarity of presentation also with regard to correctness and scope. all reviewers and the ac agree that the paper is not yet ready for publication at iclr and would require an additional revision to address the aforementioned issues.", "accepted": 0}
{"paper_id": "iclr_2022_S3qhbZwzq3H", "review_text": "the paper propose a valueaware transformer for sparse multivariate time series data. while to approach is well motivated and the problem wellmotivated from a clinical viewpoint, the comparison with related work brought up by reviewer qfri and reviewer ph4x would really make it clear where this paper stands. the authors attempt to diffuse this issue in their replies, but empirical comparisons in the paper would guide practitioners more. this is especially important as the paper is motivated by a realworld problem.", "accepted": 0}
{"paper_id": "iclr_2022_o8iGesI9HN-", "review_text": "the paper introduces a convolutionallike operator called optimized separable convolution, which scales well in the number of channels, c. the paper studies the classification performance of resnet with the optimized separable convolution, and with other choices of the convolutional operators. the paper finds the introduced convolutional operator to be more parameter efficient than competing operators, however only by a relatively small margin, and this also comes at the cost of computational performance. the reviewers appreciated that the proposed convolutional operation is more parameter efficient and that any improvement in convolutional networks potentially benefits a large array of methods and tools. the reviewers criticize that the operation only offers marginal gains at the cost of slower runtimes, and agree that the contribution is only marginally significant and therefore below the acceptance threshold. i agree with the reasoning of the reviewers that the extra computational cost is not worth the marginal improvement, and therefore recommend to reject the paper. also, the authors didnt respond to the comments of the reviewers and their reasonable questions.", "accepted": null}
{"paper_id": "iclr_2022_yjsA8Uin-Y", "review_text": "this paper proposed a trainingfree method to detect noisy labels based a given pretrained representation. the author suggested two methods based on either voting or ranking to filter noisy instances with corrupted labels without training. reviewers generally agree that the technical novelty and contributions are only limited or marginally significant. also experiments are not very convincing as there lacks of extensive comparisons with many existing methods for either noisy label removal or learning with noisy labels and the datasets are somewhat simple and not complex enough.", "accepted": null}
{"paper_id": "iclr_2022_i--G7mhB19P", "review_text": "summary study inductive bias of natural gradient flow. strengths  some reviewers found the invariance to reparametrization insightful, a good way to better understand the interaction of the learning rule and parametrization.  experiments support the theory. weaknesses  unclear takeaway message.  comparison with euclidean case not comprehensive.  insufficient distinction between reparametrization invertible and different parametrization. no experiments on actual datasetarchitecture.  some reviewers found the the cases considered in the paper are already well understood. discussion 2yhk found that although the author responses and other reviews clarified some of their concerns, particularly about reparametrization conditions, the result provided in the paper is not strong enough and could be further clarified. the authors found that this reviewer might have misunderstood the paper. following the discussion period, the reviewer raised hisher score and lowered hisher confidence. in response to cbdn the authors added demonstration of ngd being worse than egd on matrix completion. in one of the responses, the authors summarize their contribution as replacing egd with ngd  disturbs the second mechanism dynamics and gd trajectories. i find the question really is what kind of quantitative conclusions can be made. gwo5 pointed out important related work that was not discussed in the initial submission. authors added discussion. vpsx misses applications to less well understood settings. authors however only offer to keep this in mind for future work. vpsx also asks to emphasize the insights into the inductive bias of ngd. authors added some discussion, however mostly pertaining previous works and not specific enough in my opinion. conclusion one reviewer found this work marginally above the acceptance threshold and four other reviewers found that it does not reach the bar for acceptance. i find the topic worthwhile and that it deserves a thorough investigation. however, i concur with the reviewers that some concepts require a clearer presentation and that it would be desirable to see more general results and more comprehensive discussions. several suggestions were made by the reviewers and acknowledged by the authors, but many of these were left for future work. in summary, i think that the article makes a good start but needs more work. therefore i am recommending a rejection at this time. i encourage the authors to revise and resubmit.", "accepted": null}
{"paper_id": "iclr_2022_wu5yYUutDGW", "review_text": "this paper presents work on video scene segmentation. the reviewers appreciated the introduction of a boundaryaware pretraining method. however, concerns were raised regarding limited novelty, empirical effectiveness, and generic applicability. the reviewers engaged in significant discussion based on the other reviews and authors responses. based on these discussions the reviewers concluded that while the proposed method does have differences with respect to bsp, the overall contributions were not sufficient for inclusion in iclr.", "accepted": null}
{"paper_id": "iclr_2022_reFFte7mA0F", "review_text": "this paper extends a recent approximate dynamic programming method i.e., dp with neural networks for a ride sharing problem. an elegant trick is proposed to obtain a more expressive function approximation without suffering a combinatorial explosion of the action space. while the idea is somewhat ad hoc in its implementation, and limited in novelty w.r.t. the adp work that the paper builds on, the empirical performance improvement on the ride sharing problem is clear. initially, the reviewers also raised several clarity and presentation issues, but the authors did a good job in addressing them in their rebuttal. the reviewers gave scores of 5,8,5. the main critique is limited novelty. during the discussion, we focused on the novelty of the approach, whether the ideas can be generalized beyond the very specific ride sharing problem, and whether the work is strong enough if viewed as an application paper. the conclusion, which my final decision is based on, is that currently, the contribution is very specific to the ride sharing problem, and it is not clear whether this idea can be extended to more general optimization problems. this means that the scope of the algorithmic approach, taken with respect to the iclr audience, is rather narrow. on the other hand, the current presentation does not meet the bar of a strong application paper, as there is not enough novelty in the problem and data. my advice to the authors is to broaden their investigation and evaluation. another option would be to target a venue that is more focused on the ride sharing problem.", "accepted": 1}
{"paper_id": "iclr_2022_vDa28vlSBCP", "review_text": "the authors propose a methodprototrexthat incorporates prototype networks into text classification architectures to facilitate model explanations via presentation of similar training examples. there was agreement that the direction here is promising and the work contains some nice ideas and a good initial set of evaluations. however, the presentation can be improved substantially to better situate the contribution with respect to related work and clarify the specific contributions on offer here, and to clarify the key technical details of the proposed approach.", "accepted": 0}
{"paper_id": "iclr_2022_T8BnDXDTcFZ", "review_text": "the paper derives a new parameter initialization for deep spiking neural networks to overcome the vanishing gradient problem. during the review, concerns were expressed about how well the method would scale to larger neural networks. it was also questioned how this parameter initialization technique compares with a recently proposed batch normalization technique, especially when training larger neural network on more challenging datasets. there were also concerns raised about the readability of the paper. i commend the authors for improving the readability of their paper in their revision. i also commend them for taking the time to implement the comparisons requested by the reviewers. these new comparisons revealed that batch normalization and its recently proposed variant were superior to the initialization method on its own, and that the initialization proposed in the paper did not significantly improve performance when paired with batch norm 1httpsopenreview.netforum?idt8bndxdtcfznoteidyiapcsbuaq0. the authors also acknowledged based on the new results, that their proposed parameter initialization scheme appears to fail to scale to more complex datasets and networks, especially relative to competing methods, which invalidates a key claim that their approach can accelerate training and get better accuracy compared with existing methods 2httpsopenreview.netforum?idt8bndxdtcfznoteidj12fwayweb. the recommendation is to reject the paper in its current form.", "accepted": null}
{"paper_id": "iclr_2022_4GBHVfEcmoS", "review_text": "the paper discusses propagating input uncertainty through nonlinear layers by a simple local linearization approach. this is a straightforward idea and the authors explain how this is an optimal approximation of the propagated distribution for total variation and relu nonlinearity for a single layer. this is an interesting if quite limited theoretical result. what is not clearly stated is that this result only holds for a single layer. it does not mean that the local approach is the best way to approximate in the total variation sense a distribution passed through multiple relu layers. by repeating the procedure, the authors are able to define closed form objective functions for noiserobust training of deep networks. the reviewers found this an interesting paper and there was a good effort by the authors to improve the results. however, technical innovation is modest and reviewer doubts still remain. for that reason, clarity of presentation is critically important. the overall numerical score isnt convincing and with one reviewer remaining very unconvinced. i agree with the reviewers that the technical contribution is quite limited and i would argue is not particularly well explained. for example, a simple alternative would be to use a global linearization in which one can consider the network function f as a whole, and then simply linearize this rather than linearizing each layer. indeed, the way that the paper is written, this would be a natural interpretation since f is defined in the introduction as the network function, but is used later differently eg section 3.2 as the transfer function. the approach is to recursively compute a new mean and covariance for each layer, propagating these through the network similar to moment matching approaches. it would have helped if the authors had made pseudocode for their approach. it would be natural and interesting to compare to the simple global linearization approach which is computationally faster. the presentation of results and experiments could be improved. for example, in figure 3, it is not clear nor is it explained in the text what the definition of robust accuracy is. given the modest technical innovation, i also feel that the clarity of presentation isnt yet at the level that would merit acceptance. the paper would also benefit from some deeper insight into why the approach might perform better than other approaches such as local moment matching at the network level rather than a single layer.", "accepted": 0}
{"paper_id": "iclr_2022_9dn7CjyTFoS", "review_text": "in this manuacript, the authors develop featurefool attacks with featurelevel adversarial perturbations using deep image generators and a novel optimization objective. they further show that the featurefool attacks are versatile and can generate targeted featurelevel attacks at the imagenet scale that are simultaneously interpretable, universal to any source image, and physicallyrealizable. the reviewers agree that the paper is wellmotivated and the authors have addressed some concerns. however, the reviewers still do not satisfy with some concerns so as to keep the initial scores. in comparison with the manuscripts im handling, i have to recommend to reject it!", "accepted": 0}
{"paper_id": "iclr_2022_4lLyoISm9M", "review_text": "the reviewers were fairly consistent in agreeing that this is a reasonable paper with an interesting idea. however, the usecase is fairly narrow, as the main benefit is less intermediate storage and only significant for very rectangular matrices but compared to alternatives it require many passes over the data usually 5 or so. so its a narrow usecase and many of the comparisons are not applestoapples since the accuracy, time, spacecomplexity and number of passes differ from algorithm to algorithm. so while acknowledging the potential benefits of the method, there are downsides too, and thus a clear presentation is very essential. the reviewers mention that presentation listing the algorithm, clear experiments could be improved. on my own reading, i noted that the choice of sketchysvd as the dominant baseline is misleading. sketchysvd is for streaming data more restrictive than singlepass so this is an unfair comparison. the appendix does a better job of including other baselines block lanczos, though it mischaracterizes them it says blocklanczos requires persistence presence of the data matrix x in memory, but this is not true, the method could easily be implemented in a matrixfree fashion. another method to compare with is the singlepass algorithm randsvd in yu et al., who show how to implement one of the halko et al. 2011 2pass methods in just onepass. other reviewers mention baseline algorithm issues too. i do acknowledge the improved accuracy of your method over all these baselines for some matrices, in terms of the frobenius norm or tail error; however, im not sure the differences in spectral norm are are great, and see remark 2.1 in martinsson and tropp 20 for arguments about why frobenius norm guarantees are often not as desirable as spectral norm guarantees. another issue is related to the left vs right singular vectors. a reviewer noted it is not fair to compare rangenet with sketchsvd, rangenet just produces the right singular vectors while sketchsvd produces both left and right singular vectors. the authors respond rangenet computes both left and right singular vectors but does not consume main memory to store left singular vectors at runtime. however, if we allow another pass over the matrix to find the left singular vectors, this postprocessing can be applied to any technique that approximates the singular values and right singular vectors, hence pca methods are applicable, including deterministic methods like the frequent directions method ghashami et al. 16. in summary, this method is highaccuracy and lowmemory, yet it also has downsides compared to other methods, and the paper could use some improvement. i dont think the paper is ready at this time for acceptance, but given the advantages of the method, i encourage the authors to make changes and resubmit an improved version to iclr next year or other similar venue. references yu, gu, li, liu, li, singlepass pca of large highdimensional data. ijcai 17, httpsdoi.org10.24963ijcai.2017468 ghashami, liberty, phillips, woodruff, frequent directions simple and deterministic matrix sketching. siam journal on computing. 2016;455176292. martinsson, tropp. randomized numerical linear algebra foundations and algorithms. acta numerica. 2020 may;29403572.", "accepted": 0}
{"paper_id": "iclr_2022_JLbXkHkLCG6", "review_text": "learning policies from video demonstrations alone without paired action data is a promising paradigm for scaling up imitation learning. as such the paper is wellmotivated. two approaches psil and pdac train rewards for rl training, based on learning sinkhorn distances between trajectory embeddings and an adversarial approach. the reviews brought up lack of clarity in presentation and experimental results and ablation studies falling short of convincingly demonstrating value of distance functions used and other design tradeoffs. as such the paper does not meet the bar for acceptance at iclr.", "accepted": 0}
{"paper_id": "iclr_2022_IR-V6-aP-mv", "review_text": "this paper studies the method to achieve the batch sizeinvariant for policy gradient algorithms ppo, ppg. the paper achieves this by decoupling the proximal policy from the behavior policy. empirical results show that the methods are somewhat effective at providing batch size invariance. after reading the authors feedback, the reviewer discussed the paper and they did not reach a consensus. on the one hand, the rebuttal made some reviewers change their minds who appreciated the explanations provided by the authors and the new figure that better highlights the batch size invariance property. on the other hand, some reviewers think that there is still significant work to be done to get this paper ready for publication. in particular, it is necessary to improve the theoretical analysis and the evaluation of the empirical results. i encourage the authors to follow the reviewers suggestions while they will update their paper for a new submission.", "accepted": 1}
{"paper_id": "iclr_2022_rvost-n5X4G", "review_text": "the paper presents a rl planning algorithms where a policy selects a reachable state. the empirical evaluation shows promising results in some environments. while all the reviewers agreed that the state planning rl is a relevant and promising direction, the reviewers expressed concerns with the rigor, significance of the results, and incremental novelty. to improve them paper the authors should  bring the theoretical foundation in the main text, and add more rigorous analysis, including the limitations of the method.  the readability of the figures needs to be improved. the legend on the figures is too small and colors are too similar that renders the figures unreadable and confusing.  if the authors goal is to develop a method for interpretable rl, then some results and analysis need to address the interpretability of method.", "accepted": 1}
{"paper_id": "iclr_2022_AFH3FnBksHT", "review_text": "while fusing multiple heterogeneous neural networks into a single network looks like an interesting exploration, there are many major concerns raised by the reviewers 1 the motivation why the proposed method works is not convincing. in other words, under what conditions the proposed would work or would not work is not clear. 2 the authors failed to provide either theoretical analysis or convincing empirical studies of the proposed method. in the rebuttal, the authors did not address the critical issues raised by the reviewers. 3 there are many other detailed problems about the proposed method as well as the experimental setup. therefore, by considering the above concerns, this submission does not meet the standard of publication at iclr.", "accepted": 0}
{"paper_id": "iclr_2022_9vsRT9mc7U", "review_text": "a brief summary recent works in deep learning have shown that it is possible to solve combinatorial optimization problems cop with neural networks. however, generalization beyond the examples seen in the training set is still challenging, e.g., generalizing to tsp with more cities than the ones seen in the training set. this paper proposes the ganco approach, where a separate generative neural network based on gan generates new hardtosolve training instances for the optimizer. the optimizer and the generative network are trained in an alternating fashion. the authors have run experiments with the attention model am and pomo with their ganbased data augmentation approach. the authors provide experimental results on several wellknown cops, such as the traveling salesman problem.  reviewers feedback below, i will summarize some reviewers feedback and would like the authors to address the cons noted below.  reviewer seud pros  paper is wellwritten.  task is important and wellmotivated.  good experimental results. cons  the papers core contribution on the necessity of adversarial entities is not wellmotivated.  missing baselines  rl agent trained on all target distributions. to figure out how far ganco is from the optimal policy.  the performance of an agent trained on a curriculum.  figure 2 is unnecessaryredundant in the paper.  reviewer tjch cons  the paper is reasonably written. however, it would be much easier to follow with a few changes. for example, section 3.1 explains the architecture and, in related works, a more comprehensive overview of the methods to improve the robustness of rl methods.  it is widely known that data augmentation helps in deep learning. the papers claims would be more convincing if it provided some crucial baselines, such as comparing different data augmentations methods and carefully ablating them.  reviewer n945 pros  wellwritten  good evaluation  simple model with good results cons  missing citation to the paired paper.  how important are the adversarial entities generated? is it possible to achieve similar results by just training on more samples?  missing baselines instead of training in stages, alternate optimizer and generator network per step basis.  reviewer mumn pros  the proposed approach is novel.  comprehensive and extensive experiments.  figure 1 provides a good summary of the approach. cons  motivation is for the ganco is not very convincing.  concerns about the capacity of the neural nets used in the paper.  concerns on forgetting the original distribution.  concerns about experimental evaluation protocol.  including experiments on routing problems to show the generality of the proposed approach.  request for improvements in the writing and the formatting of the paper.  key takeaways and thoughts i think this paper attacks an interesting problem. as far as i am aware of the approach is novel. however, generative adversarial networks have been used in the machine learning literature for data augmentation and rl for augmenting the environment see the paired paper. gan type of approaches hasnt been used to improve the generalization of the deep learning approaches for cop. the results look promising. however, as pointed out by reviewer mumn and tjch, this paper would benefit more from further ablations, particularly the necessity of adversarial generation part to make the arguments more convincing. as it stands now, it is not clear where exactly the improvements are coming from. reviewer mumn also raised some concerns about the poorly configured lhk3 baseline in the discussion period. furthermore, i agree with the reviewer mumn and tjch that this paper would benefit from restructuring to make it flow better. i do think that this paper needs another round of reviews. i would recommend the authors go over the feedback provided here and address them for future submission. references", "accepted": 0}
{"paper_id": "iclr_2022_nT0GS37Clr", "review_text": "the paper brings the supermask idea used in neural architecture search to the application of federated learning, here represented by a single mask of a larger network. the method can be seen as pruningbeforetraining, or more precisely pruninginsteadoftraining. it is a simplified version of the related works lotteryfl, prunefl or fedmask, with the difference that here no personalization and no training of the weights is performed, only learning of a global mask. related work discussion should be improved. while the communication efficiency impact of the method seems minor but positive, the interesting point is that authors here argue that masking will improve robustness to adversarial participants during training. unfortunately no theoretical evidence is provided for success of training, in the sense of byzantine robustness. it is known that robust training can be attacked with small perturbations correlated over time e.g. little is enough, so also over layers, two important aspects which are ignored here  as voting here is only analysed static at a single timepoint. as pointed out by reviewer jjjz, the considered attack inverting ranking is far from being formally proven to be the strongest one, and we would have wished for a more precise discussion of these issues as the target of the paper seems to be mainly robustness. concerns on the paper also remained on the level of novelty, as it only uses existing building blocks which are more or less directly applicable from the centralized setting, and on the limited contributions towards formal robustness, and on the limited discussion of related work mentioned by several reviewers, only some of which we were able to address in the discussion phase. we hope the detailed feedback helps to strengthen the paper in the future.", "accepted": 0}
{"paper_id": "iclr_2022_OCgCYv7KGZe", "review_text": "this work addresses the problem of learning representations from noisy expert demonstrations in in adversarial imitation learning. the authors build on top of gail, which utilizes a discriminator to model a pseudoreward from demonstrations. in this work, the discriminator is replaced with an autoencoder. the authors hypothesis is that using an autoencoder helps in 2 ways 1 denoising expert trajectories for more robust learning; 2 using the reconstruction error instead of binary classification loss to distinguis experts from samples provides more informative signal for reward learning. strengths on a global perspective this work is well motivated a novel algorithmic variant of gail is proposed thorough experimental evaluation weaknesses the manuscript doesnt clearly distinguish between adversarial imitation learning algorithms like gail and true inverse reinforcement learning algorithms. this makes it unclear what the real goal of the proposed method is. the ultimate goal of adversarial il is to learn a policy by inferring a pseudoreward at train time which is then never used again, while the primary goal of irl is to learn a reward function at train time, which can then be used at test time. the manuscript motivates the algorithm by saying it will have a more informative signal for learning reward functions, but the algorithm itself is an adversarial il algorithm which primary goal is to learn a policy from demonstrations. overall, makes the evaluation and analysis confusing. ideally, the authors would have focussed on the question does the reconstruction error lead to better policies? through better pseudoreward modeling  or would have extended an irl method. second, the motivation is that the autoencoder helps with more robust learning, but its unclear to me that the evaluation really shows that learning is more robust also because robustness is not clearly defined the experimental evaluation is a bit of a mixed bag, and its a unclear why the new algorithm performs better on nonnoisy data when compared to baselines, but not less so on the noisy data. summary overall, this work provides a promising direction, however in its current form the manuscript is not yet ready for publication.", "accepted": 1}
{"paper_id": "iclr_2022_gdWQMQVJST", "review_text": "this paper proposes a novel federated learning fl framework that leverages the neural tangent kernel ntk, to replace the gradientdescent algorithm for optimization. specifically, the workers upload the labels and the jacobian matrices to the server, and the server uses the tools from the ntk to obtain a trained neural network. however since this could lead to increased communication cost and compromise of data privacy, the authors propose data sampling and random projection techniques to alleviate the problem. the authors provide a theoretical analysis that the proposed scheme has a faster convergence than fedavg under specific assumptions, and experimentally validate that it significantly outperforms previous fl algorithms, achieving similar test accuracy to ideal centralized cases. pros  the idea of using ntk for model optimization without gradient descent and use of it in the fl setting is both interesting and novel.  the paper properly discusses and tackles the new challenges posed by the introduction of the new method.  the paper is wellorganized and clearly written, with sufficient discussion of related works and backgrounds. cons  the proposed method puts heavy computational burdens on the serverside.  the method violates the privacy preserving feature of fl by its nature, and while the proposed compression shuffling alleviates the concern, more discussion is necessary.  missing comparison against popular baselines such as fedprox and scaffold.  the faster convergence of the proposed method in comparison to fedavg depends on the learning rate and is not always true.  there is a gap between the theory and practice, which makes the practicality of the algorithm still questionable. although the reviewers found the idea as novel, the proposed techniques for alleviating communication cost and privacy concerns convincing, and considered both the theoretical analysis and experimental validation thorough, all reviewers leaned toward rejection due to critical concerns unanswered. during the discussion period, the authors alleviate many of the minor concerns from the reviewers, but there were still remaining concerns on the gap between the theory and practice on its convergence behavior, and insufficient discussion of the privacypreserving feature of the proposed method, as well as shifting of computation burdens to the server. thus, the reviewers reached a consensus that the paper is not yet ready for publication. despite the low average score, the novelty of the idea and the quality of the paper is much higher than those of the accepted papers in my batch, and i strongly believe that this will become a high impact paper, if remaining concerns from the reviewers are properly resolved.", "accepted": null}
{"paper_id": "iclr_2022_30nbp1eV0dJ", "review_text": "this paper gives sample complexity lower bounds for differentially private empirical risk minimization erm. while the reviewers agreed that the results are nontrivial, the general consensus was that the proofs are tweaks of previously developed techniques and that the main result is actually new in a rather narrow setting specifically, for unconstrained erm and subconstant error parameter. another concern was that one of the proofs the one on pure differential privacy was incorrect in the submission; a different proof was provided subsequently which also closely follows prior work. finally, the reviewers pointed out several issues with the clarity of the presentation and comparison to prior work. given the above, this work is below the acceptance threshold.", "accepted": 1}
{"paper_id": "iclr_2022_HmFBdvBkUUY", "review_text": "this paper proposes a novel approach to include graph information into transformers. reviewers expressed concerns on 2 main issues  1 the exact architecture proposed in the paper is not well motivated. in words of one of the reviewers i still do not understand why the authors learn the spectral gcn filter weights from the attention matrix of the transformer, which can have a completely different sparsity pattern than the input graph, instead of learning the filter weights from the graph itself, e.g., by using a gnn. . authors tried to provide an explanation in the response however i think it needs to be made much more rigorous for it to be well motivated. 2 the interplay between existing position encoding schemes and the proposed method. this point also confused couple of reviewers as the empirical results seem to be strongly influenced by the choices of position encoding. authors, i think did a great job in addressing this concern by providing additional results during the response period. given the weak experimental results and lack of clear motivations i think the paper is not currently ready for acceptance.", "accepted": 0}
{"paper_id": "iclr_2022_MAYipnUpHHD", "review_text": "this work formulates the adaptive mesh refinement amr problem used in solving finite element method fem as an mdp, and suggests an rlbased solution for it. most reviewers agree that this is a novel problem and the solution is promising. there are, however, several issues raised by our reviewers, who have expertise ranging from ml to computational methods to solve pdes. some of the concerns are  as this is not a theoretical work, the burden of proof is on the empirical evaluations. some reviewers found the experiments very small and not convincing enough.  the paper does not compare with the state of the art amr methods.  the detail of how the problem is formulated as an mdp can be improved. given that four out of five reviewers are on the negative side, unfortunately i cannot recommend acceptance of this paper in its current form. nevertheless, i believe this is a promising application of rl. id like to encourage the authors to consider the reviews in order to improve their work, and resubmit it to another venue.", "accepted": null}
{"paper_id": "iclr_2022_KmNHWX9H7Kf", "review_text": "the paper provides a uniform generalization bound for overparameterized neural networks using the notion of maximal information gain. the analysis relies on the eigendecay of the eigenvalues of the ntk, which has recently been the object of a lot of work in the literature, including the work of bietti and bach the proof actually uses one of their key lemma. the paper originally received a set of reviews with a large disagreement between the reviewers including two reviewers with a negative opinion and three reviewers being more positive. after the discussion period, two reviewers kept a very negative opinion, while other reviewers slightly lowered their score. some of the problems raised by the reviewers include the restrictions imposed on the data, a missing proof which was eventually added by the authors, the discussion of prior work being inadequate including for instance the differences with more classical generalization bounds, and the novelty of the analysis. overall, the paper clearly has some merits but some of the concerns above are too important at this stage to accept the paper. i recommend the authors address the concerns mentioned in the reviews before resubmission.", "accepted": 0}
{"paper_id": "iclr_2022_Bc4fwa76mRp", "review_text": "the paper explores the usefulness of intermediate layers for linear probing, aiming at improving outofdistribution transfer with significantly less cost than finetuning. two reviewers recommended borderline acceptance, while two others recommended borderline rejection as final rating. the main concerns raised by the reviewers were the limited novelty of the proposed method e.g., compared to elmo, unconvincing results in the natural and structure categories of vtab, and lack of experiments to justify the claims, as well as the demonstration of the method in other tasks beyond image classification. the rebuttal has clarified several other questions. the ac really likes the simplicity of the approach, and also finds the problem of improving the efficiency of transfer learning very important. in addition, the paper is very wellwritten and easy to follow, as acknowledged by all reviewers. however, the ac agrees with r2 and r3 that the paper, in its current form, does not pass the bar of iclr, unfortunately. first, the novelty is limited, as pointed out by r1, r2, and r3. in addition to the related works mentioned in the reviews like elmo, note that the idea of selecting intermediate features, concatenating them, and running a linear classifier for ood transfer has also been explored in yunhui guo et al, a broader study of crossdomain fewshot learning, eccv 2020. second, while the approach has advantages in terms of efficiency, the accuracy drop compared to finetuning for indomain tasks limits its applicability. finally, even though the ac agrees with the authors this is not a requirement, a more comprehensive set of experiments on more tasks would make the paper stronger, especially given that the novelty is incremental. the authors are encouraged to improve the paper for another top conference.", "accepted": null}
{"paper_id": "iclr_2022_v-f7ifhKYps", "review_text": "this paper has several issues 1 the empirical results were incomplete and hard to interpret. 1.a the paper uses nonstandard benchmark domains making comparisons with results in the literature very difficult. the paper does not use the same environments as related baselines they build on. some progress on this last point was made during the discussion periodwell done authors! 1.b the experiments did not sweep key hyperparameters of the trajedi baseline, and generally did not comment on nor ablate several other potentially important hyperparameters and design choices 2 the proposed method is very similar to another called trajedi and the paper and results dont clearly show why the modification of trajedi is significant see 1. the paper initially claimed the trajedi was a concurrent submission but one reviewer pointed out the work was published in may 3 writing and structure could be improved. in addition some inaccurate statements could be cleaned up 4 the algorithm is more generally applicable beyond humanai coordination and the reviewers found it odd the paper did not focus on this in addition, the authors did not respond to several of the reviewers responses. this made it difficult for the reviewers to increase their scores. several reviewers found the work intriguing, but its not ready yet", "accepted": 0}
{"paper_id": "iclr_2022_VjoSeYLAiZN", "review_text": "the paper proposes a new neural network architecture for hyperspectral image reconstruction. the paper received borderlinenegative reviews. significant concerns were raised about the novelty and significance of the contribution. unfortunately, the authors did not upload a rebuttal, preventing the reviewers from changing their opinion about the paper. there is therefore no reason to overturn their recommendation.", "accepted": 0}
{"paper_id": "iclr_2022_MmujBClawFo", "review_text": "this paper points out connections between the selfattention module in transformers and some prior art, including kernel regression, the nonlocal mean algorithm, locally linear embeddings, and the selfexpression algorithm for subspace clustering. based on these observations, the authors argue that the innovation of selfattention is not modeling the longrange relation, which is also proposed in prior work, but the learnable parameters and the multihead design. the authors also suggest several directions for future work, such as using selfattention for manifold clustering. reviewers pointed out several weaknesses with this paper that some connections e.g. connection to kernel regression had been pointed out before, that the relation between selfattention and locally linear embedding and selfexpression in subspace clustering is a bit nuanced, as pointed out by one of the reviewers, and that while some speculative future directions might be interesting, the paper falls short in actually trying some of them out empirically, or building a proofofconcept. in the discussion period, the authors pointed out that this is a position paper which unfortunately was not expressed so assertively in their submission, which according to their view liberates them from digging deeper and test empirically some of these connections and speculative directions. according to the authors, a core contribution of their position paper is that it expresses the opinion that the original attention paper failed to cite and acknowledge that attention mechanisms build upon a series of prior works in sparse coding, subspace clustering, and locally linear embedding. there are no specific guidelines to review position papers at iclr that i know of, but i will base my assessment on the assumption that a good position paper should  provide a good historical perspective of a subject  connect previously unrelated lines of work in nonobvious ways  inspire the research community to look at new directions. while a good position paper can be extremely valuable and enlightening, i am not convinced that this particular paper achieves either of the goals above, and therefore it is my opinion that it does not deserve publication at iclr. as pointed out both by the authors and the reviewers, the connection between selfattention and kernel regression and nonlocal mean denoising is not new, and so it is not an original contribution of this paper. the relation between selfattention and locally linear embedding and selfexpression in subspace clustering appears to be new, but this relation is a bit nuanced, as pointed out by one of the reviewers. the tone of this position paper is that some of these connections were missing in the original attention paper  the authors say attention did not properly acknowledge prior art in one of their responses it is not clear if they are referring to bahdanau et al.s attention paper or to vaswani et al.s transformer paper. however, the historical perspective of how attention mechanisms came to be seems to be missing from this position paper  attention has been proposed by bahdanau et al. for machine translation, inspired by the idea of word alignment that has been prevalent in machine translation for decades. later, in the transformer paper, selfattention was suggested as an alternative to recurrent and convolutional models for machine translation note that selfattention has been used before the transformer paper, see e.g. 1. while a theoretical connection with kernel regression etc. exists, this was not related to the original motivation of these works. there are many ways of arriving at the same construction! and given the simplicity of attention mechanisms it doesnt surprise me that connections with other lines of research exist. had they been noticed, they would probably be a parenthesis in the original papers, because attention is derived there in a much more direct way this doesnt mean that the connections arent interesting, but that they are not _essencial_ to the construction. in their response, the authors dismissed a constructive suggestion from one of the reviewers which in my opinion would have strengthen this paper  the connection with graph neural networks. if the point of the paper is to point out past research that connects fundamentally to the idea of attention mechanisms, why leaving this out? in sum, in my view this paper lacks the rigor, the insight, and the historical perspective that should characterize a strong position paper, and as such i cannot recommend acceptance. i strongly suggest that the authors take into account some of the insightful suggestions given by the reviewers in future iterations of their work. 1 ankur parikh, oscar t\u00e4ckstr\u00f6m, dipanjan das, and jakob uszkoreit. a decomposable attention model. in empirical methods in natural language processing, 2016.", "accepted": null}
{"paper_id": "iclr_2022_FpnQMmnsE8Y", "review_text": "meta review for recurrent parameter generators this work investigates a method for reducing the parameters of a deep cnn by having a recurrent parameter generator rpg produce the weights, in effect achieving this compression via parameter sharing across layers similar to earlier works, such as the 2016 hypernetworks paper, as discussed in between xuep and the author during the review period. but unlike previous work, this work conducts extensive empirical experiments on classification and even pose estimate tasks, and proposes an additional method, such as the use of pseudorandom seed to perform elementwise random sign reflection in the weight sharing. the novelty and experimental results are clearly displayed in this work, and shows a lot of promise, but after much discussion, i currently cannot recommend acceptance for iclr 2022. in my assessment, and also looking at reviewers and discussion, i believe this work is a great workshop paper at present, but there are a few items that would make it much stronger. there are outstanding issues in the paper that need to be improved. in particular, during discussions, reviewers noted that the paper has a problem with the design and presentation of the experiments. it somehow shifts the readers focus to the compression task 3 of the 4 reviewers raised concerns about the compression performance and questioned the baselines. in their rebuttal, the authors emphasized that their contribution is not limited to compression but is rather more fundamental, and the authors propose an approach for understanding the relationship between the model dof and the network performance. but if thats the main narrative of the paper, rather than the compression aspects, the authors need to clearly articulate why decoupling the dof from the underlying architecture is advantageous and also make the narrative more clear in the writing. while there are novel innovations in the method proposed, the authors also need to explain clearly why their method works well, why the even weight assignment and random sign flipping are so effective? there is discussion between the authors and reviewers about what constitutes vector quantization, and i believe the author has clarified their position effectively with regard to cgcss review, and i believe this will be explained in great clarity in future revisions. but even with that disagreement out of the way, we still believe that this work needs improvement to meet the bar of iclr 2022. reviewers, including myself, do acknowledge the novelty and are excited about the method proposed, and we look forward to seeing an updated version of this work published or presented at a future journal or conference. good luck!", "accepted": null}
{"paper_id": "iclr_2022_CxebB5Psl1", "review_text": "reviewers unanimously vote for rejection for several reasons. first, the draft is incomplete and difficult to read. second, one of the proposed methods contextual sentence encoder appears the same as past work, while the other proposed method graph encoding is difficult to interpret from what is written. third, the draft is missing comparisons with recent work, and some included comparisons may be unfair due to data conditions. no author response was provided. the reviewer consensus is that this draft is underdeveloped, and not yet ready for submission or publication.", "accepted": 0}
{"paper_id": "iclr_2022_-29uFS4FiDZ", "review_text": "this paper investigates a technique for projecting contextual embeddings into static embeddings. neither the technique is ver novel, nor are the empirical results very strong. while the reviewers did not engage in a discussion, the area chair does not see this paper reaching the quality bar of the conference.", "accepted": 0}
{"paper_id": "iclr_2022_qiukmqxQF6", "review_text": "this paper proposes a new autoregressive flow model with autoencoders to learn latent embeddings from time series. the authors conducted extensive comparative experiments, and the experimental results are very encouraging. however, the proposed method, as a combination of the encoderdecoder structure and autoregressive flows on the latent space, does not seem novel enough.", "accepted": 0}
{"paper_id": "iclr_2022_jZQOWas0Lo3", "review_text": "in this paper propose a novel approach for semisupervised domain adaptation based on the cyclic monotonicity property of optimal transport map. the main idea is to adapt perturbed wrt a source classifier the labeled source samples toward the target samples while preserving the known labels via the cyclical monotonicity. then these perturbed samples can be used to perform classical ot domain adaptation. this preprocessing of the data has been shown in the numerical experiments to lead to better performance in average. the proposed method has been found intriguing by all reviewers but the writing of the paper has been found clearly lacking and several suggestions were proposed by the reviewers. the choice of the authors to call the perturbed samples adversaries for instance made the paper harder to understand and the antiadversarial is also not a good choice of words. another concern was that despite encouraging numerical results lack more baselines semisupervised domain adaptation methods discussed by the reviewers were not compared with or without ot. the authors provided a short but clear response that was appreciated by the reviewers. but the clarifications promised by the authors were not done in the pdf during the editing period which means that the paper clearly needs a new round of reviews. for this reason the consensus during the discussions was that this paper should be rejected. the ac believes that this is an interesting research direction that should be investigated but that the paper needs some more work before reaching the threshold for acceptance in selective ml venues. the authors are strongly encouraged to take into account the comments form the reviewers before resubmitting their work.", "accepted": 0}
{"paper_id": "iclr_2022_5_zwnS5oJDp", "review_text": "in this paper, the authors leverage information gain in conjunction with bayesian neural networks in order to to improve the robustness of bayesian neural networks. however, as pointed out by reviwers, there are several mistakes in theier derivations and evaluations. moreover, the authors failed to crrectly refer to the exisiting work proposing similar methods.", "accepted": 0}
{"paper_id": "iclr_2022_VDdDvnwFoyM", "review_text": "the authors propose a vaebased architecture for generating multivariate time series. the base version of timevae models a distribution over a fixedlength sequences of observations using a latent vector of fixed dimensionality and a convolutional encoder and decoder. the interpretable timevae model incorporates additional features from traditional time series models such as explicit modelling of trends and seasonality. timevae is compared to several baselines such as timegan on four small times series dataset and seems to perform competitively according to two custom evaluation metrics and a visualization. the reviewers thought that the paper was interesting but not ready for publication due to the following the papers contributions and their significance are not clear interpretable vae was not used in the experiments and its interpretability has not been verified coverage of related work is insufficient", "accepted": 0}
{"paper_id": "iclr_2022_ffS_Y258dZs", "review_text": "all reviewers have agreed that the topic of evaluating compositional skills of agents is an important one and cast it as compositional learning as metareinforcement learning is an interesting approach. at the same time, reviewers have raised concerns with respect to the benchmark itself, the exposition and clarify of the ideas as well as the experimental evidence used to support some of the claims. the authors have not provided an author response but have acknowledged the reviewers feedback. as this paper stands i cannot recommend acceptance for the current manuscript.", "accepted": 0}
{"paper_id": "iclr_2022_NK5hHymegzo", "review_text": "this paper studies the convergence of adamtype algorithms two variants of amsgrad in particular in minmax problems that satisfy a onesided minty variational inequality condition. the reviewers identified several weaknesses in the paper and the authors did not provide a rebuttal to these concerns so there was consensus to reject the paper.", "accepted": 0}
{"paper_id": "iclr_2022_KDAEc2nai83", "review_text": "this paper introduces a variant of dqn optimized for desktop environments to make large scale experiments more feasible for anyone. this paper was close. the reviewers appreciated the effort and motivation, but in the end the reviewers all seemed to think that the paper was not ready. the main contribution is framed as making dqn training more feasible, but the reviewers expected the paper to show examples of what the workflow for another architecture would look like and ideally present results for domains beyond atari. in addition, several reviewers thought the paper could be more precise about 1 ruling out speed differences due to hardware and lowlevel software, and 2 contextualizing the speedups reporteddoes 3x matter, what should we expect, etc. this is certainly an interesting direction. the ac personally thinks that if the authors take some steps to address the points above this will be a great and potentially high impact paper.", "accepted": 0}
{"paper_id": "iclr_2022_1QxveKM654", "review_text": "the paper demonstrates that one phase of de novo assembly, specifically the layout phase, can be replaced with graphneuralnetwork based methods. the paper clarifies in the rebuttal that it focuses on building a method for assembling highquality long reads. all four reviewers rated the paper as below the acceptance threshold. the reviewers largely agree that the idea of using gnns to assemble a genome from reads is novel, interesting, and has the potential to be very useful. the reviewers raise the following concerns the paper only considers synthetic data, and the synthetic reads used in the simulations are errorfree. in practice, reads are not errorfree, and thus simulations on real data or at the very least on reads with errors are needed. the authors acknowledge that, and state that theyll provide such experiments in future work. in summary, the reviewers found the experiments to be insufficient to support the claims, even though it is understood by the reviewers and me that the paper only presents a proofofconcept idea. i agree with the reviewers that simulations on erroneous reads, ideally real data, would be needed for acceptance. i recommend to reject the paper, since the paper provides insufficient experiments to understand the merits of the proposed approach.", "accepted": 0}
{"paper_id": "iclr_2022_Y3cm4HJ3Ncs", "review_text": "even though reviewers found some responses by the authors satisfactory, several concerns regarding the paper still remain. the authors are strongly encouraged to 1 explore how dataset size impacts accuracy. 2 reason about annotation costs via empirical experiments. 3 including benchmark datasets in experimental evaluations.", "accepted": 0}
{"paper_id": "iclr_2022_bVkRc9NDHcK", "review_text": "this paper presents a steganographic approach called variable length variable quality audio steganography vlvq that encodes variable length audio data inside images with varying quality tradeoffs. however, according to the reviewers, the proposal made in this paper is not novel enough, there are many details missing in the paper, and the experimental study is far from comprehensive and conclusive. afte the reviewers provided their comments, the authors did not submit their rebuttals. therefore, as a result, we do not think the paper is ready for publication at iclr.", "accepted": 0}
{"paper_id": "iclr_2022_Lwclw6u3Pcw", "review_text": "summary compare neural networks and tasks using tda, particularly persistence diagrams. strengths  some reviewers found this a fresh perspective.  distance calculation using tda can offer advantages and a theoretical basis. weaknesses  insufficient motivation and experimental evidence for utility of the proposed approach.  computational cost and hyperparameter choices in pd computation.  difficulty of interpreting proposed distance matrices. discussion zggm found the paper interesting and that it offered a fresh perspective, but that the purpose of the comparison was not sufficiently well motivated. the authors provide some explanations, particularly about the method allowing to compare networks of different sizes, but zggm found their comments were not adequately addressed. rtbj found that even though the authors made efforts to address their comments, the paper still requires substantial improvements. hwgx appreciated the authors responses but considers that the paper needs to be improved with additional validation. they expressed doubts about the adequacy of the approach and found that although it improves upon certain methods, it is insufficiently verified. conclusion all reviewers agree that this work has some strengths but also significant weaknesses and does not reach the acceptance bar for this conference. main weaknesses are insufficient motivation and experimental evidence. the reviewers made several suggestions on how the paper could be improved. i agree with the reviewers and hence i must reject this article.", "accepted": 0}
{"paper_id": "iclr_2022_ezbMFmQY7L", "review_text": "this work proposes to use a transformer model and language model inspired selfsupervised training techniques to generate local modifications of organic molecules. the use of iupac names coupled with language inspired pretraining is indeed an interesting idea worthy of exploration. the paper has a lot of promises in this regard but needs more work to deliver it through the finish line. in the rebuttal, the authors have provided strong arguments toward the advantages of using iupac representation. while these arguments make sense, they are more or less conceptual and better and more clear empirical evidences are required to back them up.", "accepted": 0}
{"paper_id": "iclr_2022_fwJWhOxuzV9", "review_text": "this paper explored pretraining for deep offline reinforcement learning, developing a method that first pretrained decision transformers on trajectories without rewards, and then finetuned on limited data with rewards. the reviewers were pleased with the overall research questions and directions, but found that they were substantial shortcomings in the experimental setup and results that make this paper not yet suitable for inclusion. the approach is relatively simple and straightforward, which is actually a good thing, but that means that it must be correspondingly investigated and developed with convincing empirical results. unfortunately, there are a number of open questions about the experimental set up, and the results are not convincing that the method is effective against alternatives, as detailed in the reviews. there was no author rebuttal.", "accepted": 0}
{"paper_id": "iclr_2022_f9AIc3mEprf", "review_text": "this paper introduces an imagenetscale benchmark uimnet for uncertainty estimation of deep image classifiers and evaluates prior works under the proposed benchmark. two reviewers suggest reject, and one reviewer does acceptance. in the discussion period, the authors did not provide any response for many concerns of reviewers, e.g., weak baselines, weak novelty, and lack of justification for the current design. hence, given the current status, ac recommends reject.", "accepted": 0}
{"paper_id": "iclr_2022_bq7smM1OJIX", "review_text": "this paper proposes to use longstanding statistical learning techniques to identify the nationality of the author of a text. reviewers agreed that this work is a poor fit for iclr, as there is nothing here that advances our understanding of representation learning. reviewers were further concerned about the soundness of the claims, raising issues about data contamination and comparison with prior work. finally, reviewers pointed out correctly in my view that work that aims to infer protected identity characteristics of nonuser human subjects should be held to an especially high ethical standard, and needs a highly persuasive costbenefit analysis that defends why the problem is ethical to study at all. the available discussion of ethics is not up to this standard.", "accepted": 0}
{"paper_id": "iclr_2022_HdnUQk9jbUO", "review_text": "this paper shows sgd enjoys linear convergence for shallow neural networks under certain assumptions. however, reviewers reach the consensus that this paper lacks technical novelty. the meta reviewer agrees and thus decides to reject the paper.", "accepted": 0}
{"paper_id": "iclr_2022_SCn0mgEIwh", "review_text": "the problem studied in this paper is interesting and the highlevel motivation of the proposed research is reasonable. however, as pointed out by reviewers, it is not convincing that the developed components in the proposed method are able to address the issues mentioned in the highlevel motivation. furthermore, the experimental results are not convincing to verify the motivations either. though the authors provided some clarifications in the rebuttal, reviewers major concerns still remain. the authors are encouraged to take reviewers concerns into consideration to revise the proposed method to make it a stronger work for future submission. based on its current form, this work is not ready for publication at iclr.", "accepted": null}
{"paper_id": "iclr_2022_bUAdXW8wN6", "review_text": "the paper describes an adversarial training approach that, in addition to the commonly used robustness loss, requires the network to extract similar representation distributions for clean and attacked data. the proposed method is inspired by domain adaptation approaches that require a model to extract domain invariantagnostic features from two domains. although the experimental results are solid and technically sound, the novelty of the methodology is not enough, as the domain classifier and the gradient reversal layer are the same with those methods in domain adaptation such as unsupervised domain adaptation by backpropagation. on the other hand, more recent sota methods are missing and only smaller scale datasets are used for evaluation. during the discussions, the major concerns from three reviewers are novelty. i totally agree that the simplicity of the method should be a virtue. however, the idea of domaininvariant representation learning is already established well, and its application to adversarial training is quite intuitive to the community. also, the similar methodology already exists in domain adaptation. according to the toptier conference culture in the ml community, what most valuable is the novelty and insight, not the performance. in the end, i think that this paper may not be ready for publication at iclr, but the next version must be a strong paper.", "accepted": 1}
{"paper_id": "iclr_2022_lsQCDXjOl3k", "review_text": "this paper modifies the conditional diffusion model guided by a classifier, as introduced by dhariwal  nichol 2021, by replacing the explicit classifier with an implicit classifier. this implicit classifier is derived under bayes rule and combined with the conditional diffusion model. this combination can be realized by mixing the score estimates of a conditional diffusion model and an unconditional diffusion model. a tradeoff between sample quality and diversity, in terms of the is and fid scores, can be achieved by adjusting the mixing weight. the paper is clearly written and easy to follow. however, the reviewers do not consider the modification to be that significant in practice, as it still requires label guidance and also increases the computational complexity. from the acs perspective, the practical significance could be enhanced if the authors can generalize their technique beyond assisting conditional diffusion models.", "accepted": null}
{"paper_id": "iclr_2022_bM45i3LQBdl", "review_text": "the paper considers the natural class of algorithms, namely aggregators with gaussian noise for distributed sgd with differential privacy dp and byzantine resilience br. previous results shows vnbr convergence of sgd. the authors first show that aggregators with gaussian noise algorithms satisfy dp but violates vn necessarily, so approximate vn is proposed. theorem 2 shows approximate vnconvergence. proposition 2 shows the above algorithms satisfies approximate vn with certain parameters. with the combined bound corollary 1, the authors observe and then verify by experiments that larger batch size is beneficial and in particular more beneficial than when dp or br is enforced alone. in the formulation, an important baseline of robust mean aggregation diakonikolas,kamath,kane,li,moitra,,stewart2016 and even more relevant baseline of robust and dp mean aggregationliu,kong,kakade,oh,21 are somehow missing. one would assume that directly applying these wellknown techniques might give the desired dp and robust sgd. the field at the intersection of differential privacy and robustness has evolved quite a bit recently and tremendous technical innovations are happening. given the relveance of the proposed problem to this line of work, one should make the connections precise and explain the differences.", "accepted": 0}
{"paper_id": "iclr_2022_0GhVG1de-Iv", "review_text": "in this paper, the authors studied algorithmic stability of batch reinforcement learning algorithms, as well as its connection to certain generalization bounds motivated by the prior work hardt et al developed for sgd on nonconvex optimization problems. while understanding the stability and generalization of batch rl is certainly an interesting and important direction, the paper in its current form is not yet ready to be published. as the reviewers pointed out, both the analyses and the claims need to be polished in fact, important details and definitions are missing; and the theoretical contributions are only made in a limited setting.", "accepted": 0}
{"paper_id": "iclr_2022_Eot1M5o2Zy", "review_text": "the paper proposes a new neural network, the aestheticnet, for a biasfree facial beauty prediction. all the reviewers agree that the work is not suitable for publication as it raised some serious ethic concerns  prediction of beauty aesthetic scores is a potential harmful application. wellintended as it may be, a research along these lines might be harmful.  nonanonoymity issue writing revealsimplies authors identity with reference to previous work  research integrity issues e.g., plagiarism, dual submission, a figure is copied from previous work. there is also a concern that the work is not novel and not interesting as such. the authors did not respond to the concerns. i suggest rejection.", "accepted": null}
{"paper_id": "iclr_2022_bpUHBc9HCU8", "review_text": "the paper studies a robust gnn against adversarial attacks on both graph structure and node features. the reviewers agree that the paper need to improve in terms of novelty and more technical details to meet iclr standard.", "accepted": 0}
{"paper_id": "iclr_2022_jGmNTfiXwGb", "review_text": "a brief summary this paper uses offline algorithms that can see the entire timeseries to approximate the online algorithms that can only view the past timeseries. the way this is done is basically, the offline algorithm is used to provide discrete class targets to train the online algorithm. the paper presents results on synthetic and historical stock market data.  reviewer s1h9 strengths  practical problem.  novel approach.  clear presentation. weaknesses  no other baselines.  no theoretical guarantees behind the approach.  writing could be improved.  reviewer egw9 strengths  clear writing.  interesting research direction. weaknesses  the primary claim seems incorrect and unclear.  due to the unclarity about the primary claim of this paper, it is difficult to evaluate the paper.  lack of baselines.  the lack of discussions of the related works.  reviewer gii5 strengths  interesting and novel approach. weaknesses  difficult to evaluate, with no empirical baselines or theoretical evidence.  the datasets used in the paper are not used in the literature before. authors should provide experimental results on datasets from the literature as well.  the paper needs to compare against the other baselines discussed in the related works.  more ablations and analysis on the proposed algorithm is required.  unsubstantiated claims regarding being sota on the task, since the paper doesnt compare against any other baselines on these datasets.  the paper can be restructured to improve the flow and clarity.  reviewer zokr strengths  novel and interesting research topic.  bridging classical algorithms and ml.  clearly written. weaknesses  lack of motivation for the problem.  the approach only works with offline algorithms that work on timesegmented data.  reviewer aafn strengths  novel algorithm. weaknesses  potentially overfitting to the offline data.  data hungry approach.  confusion related to the occurrence moments of predicted future actions.  section 2 is difficult to understand.  key takeaways and thoughts overall, i think the problem setup is very interesting. however, as pointed out by reviewers gii5 and egw5, due to the lack of baselines, it is tough to compare the proposed algorithm against other approaches, and this papers evaluation is challenging. i would recommend the authors include more ablations in the future version of the paper and baselines and address the other issues pointed out above by the reviewers.", "accepted": 0}
{"paper_id": "iclr_2022_uHq5rHHektz", "review_text": "this manuscript proposes an information fusion approach to improve adversarial robustness. reviewers agree that the problem studied is timely and the approach is interesting. however, note concerns about the novelty compared to closely related work, the quality of the presentation, the strength of the evaluated attacks compared to the state of the art, among other concerns. there is no rebuttal.", "accepted": 0}
{"paper_id": "iclr_2022_e_D6AmszH4P", "review_text": "a method for efficient exact computation of the generalized gaussnewton matrix is given. using this method the authors provide several empirical observations of first and second order statistics of neural networks during training. additionally the authors use to tool to propose a new damping technique that some reviewers found particularly interesting. reviewers noted that the low rank decomposition the authors provide is not new, and has been used in prior work, although the trick may not be widely known within the deep learning community. as such novelty is not a strength of the work, and reviewers suggested the authors could strengthen the work with a convincing demonstration that the method can be made to work at scale, as well as providing more detailed run time and memory comparisons with other approaches to calculating the ggn matrix. although the authors agreed with reviewer suggestions, the paper was not updated during the rebuttal period. as such i recommend the authors resubmit with the proposed revisions.", "accepted": null}
{"paper_id": "iclr_2022_auLXcGlEOZ7", "review_text": "this paper studies margin maximization in linear and relu networks. the reviewers appreciate the technical contributions of this paper, especially the simple counterexamples. however, reviewers also found the new results seem not to give enough conceptual insights or an important main result. the meta reviewer agrees and thus decides to reject this paper.", "accepted": null}
{"paper_id": "iclr_2022_8qWazUd8Jm", "review_text": "the authors propose a new set of metrics for evaluation of generative models based on the wellestablished precisionrecall framework, and an additional dimension quantifying the degree of memorization. the authors evaluated the proposed approach in several settings and compared it to a subset of the classic evaluation measures in this space. the reviewers agreed that this is an important and challenging problem relevant to the generative modeling community at large. the paper is wellwritten and the proposed method and motivation are clearly explained. the initial reviews were borderline, and after the discussion phase we have 2 borderline accepts, one strong accept, and one strong reject. after reading the manuscript, the rebuttal, and the discussion, i feel that the work should not be accepted on the grounds of insufficient empirical validation. establishing a new evaluation metric is a very challenging task  one needs to demonstrate the pitfalls of existing metrics, as well as how the new metric is capturing the missing dimensions in a thorough empirical validation. while the former was somewhat shown in this work and in many other works, the latter was not fully demonstrated. the primary reason is the use of a nonstandard benchmark to evaluate the utility of the proposed metrics. i agree that covering a broader set of tasks and models makes sense in general, but it shouldnt be done at the cost of existing, wellunderstood benchmarks. i expected to see a thorough comparison with 1, one of the most practical metrics used today which can be easily extended to all settings considered in this work notwithstanding the drawbacks outlined in 2. what are the additional insights? what is 1 failing to capture in practical instances? does the rank correlation change with respect to modern models across classic datasets beyond mnist and cifar10? this would remove confounding variables and significantly strengthen the paper. my final assessment is that this work is borderline, but below the acceptance bar for iclr. i strongly suggest the authors to showcase the additional improvements over methods such as 1 in practical and wellunderstood settings commonly used to benchmark generative models e.g. on images. the experiments suggested by the reviewers are a step in the right direction, but not sufficient. 1 improved precision and recall metric for assessing generative models. tuomas kynk\u00e4\u00e4nniemi, tero karras, samuli laine, jaakko lehtinen, timo aila. neurips 19 2 evaluating generative models using divergence frontiers. josip djolonga, mario lu\u010di\u0107, marco cuturi, olivier frederic bachem, olivier bousquet, sylvain gelly. aistats 20", "accepted": 1}
{"paper_id": "iclr_2022_8kpSWDgzsh0", "review_text": "in this paper, the authors consider linear quadratic network games also known as graphical games and they discuss a number of conditions and procedures to learn the underlying graph of the game from observations of bestresponse trajectories or possibly infinite sets thereof in the game. the reviewers initial assessment was overall negative, with two reviewers recommending rejection and one giving a borderline positive recommendation. the authors rebuttal did not address the concerns of the reviewers recommending rejection, and the authors did not provide a revised paper for the reviewers to see how the authors would implement the suggested changes, so the overall negative assessment remained. after my own reading of the paper, i concur with the majority view that the paper has several weaknesses that do not make it a good fit for iclr especially regarding the lack of precision in the theorems and the statement of the relevant assumptions, so i am recommending rejection.", "accepted": 0}
{"paper_id": "iclr_2022_DFYtZFo_1u", "review_text": "the paper proposes to compute local representations on device, which are then shared between clients using an alignment mechanism. reviewers did appreciate the value of the topic and several contributions, but unfortunately consensus is that it remains below the bar, even after the discussion phase. concerns remained on privacy and motivational positioning with fl, and lack of simpler baselines, even after the author feedback. we hope the detailed feedback helps to strengthen the paper for a future occasion.", "accepted": 0}
{"paper_id": "iclr_2022_m4BAEB_Imy", "review_text": "this paper deals with a problem of significant practical relevance memory efficient neural networks. the authors propose some pruning methods for binary networks. however, several weaknesses were identified by the reviewers novelty, lack of extensive experiments, problems with the presentation of the paper, and several valid points of concern were raised. these points of criticism were not adequately addressed, hence the paper in its current form cannot be recommended for publication.", "accepted": 0}
{"paper_id": "iclr_2022_QCeFEThVn3", "review_text": "this paper proposes to use an energybased model for a multiobjective molecular generation. the energy function is parameterized by relational graph convolutional network rgcn so that it has a permutation invariance property. the model is trained by contrastive divergence and the generation is performed by langevin dynamics. experiments on single and multiobjective molecule generation are conducted to verify the effectiveness of the proposed framework. the paper is wellwritten, and the experiments are comprehensive. the major shortcoming of the paper is its limited novelty, since using ebm for graph generation is a straightforward application of the existing deep ebm framework. the contribution is marginal. during the discussion, two of the reviewers pointed out that the contribution is limited and marginal. two reviewers pointed out that the performance gain obtained by the proposed model is marginal and not significant. one reviewer has a concern about the computational cost of mcmc. however, the authors didnt provide a rebuttal to address the concerns raised by the reviewers. given the fact that all the concerns from the reviewers remain, and the contribution and performance gain of the work are marginal, the ac recommends rejecting the paper.", "accepted": 0}
{"paper_id": "iclr_2022_3iH9ewU_KJT", "review_text": "this paper proposes a multitask version of gradient boosted machines gbms. the paper proposes a learning algorithm that adaptively adjusts the learning rate per task. empirical evaluation is carried out on two datasets with the method implemented in the lightgbm framework. the reviewers thought that the paper is not very clear. they were not ready to accept the paper claims based on the current version. in particular, the algorithms are hard to follow, the empirical evaluation is not easy to follow and there are missing comparisons to related work. the authors did not offer a response to the reviews.", "accepted": 0}
{"paper_id": "iclr_2022_oLYTo-pL0Be", "review_text": "this work describes an interesting approach of using a reinforcement learning algorithm for federated learning. the paper is well organized and the usecase of performing federated learning while preserving patient privacy is also important. however, the paper has room for improvement. important baselines used for client selection are missing and so the deep reinforcement learning approach is not wellmotivated. many important technical details are missing such as hyperparameters and distributions for mnist and cifar. the approach is also lacking novelty, drl has been used for neural scheduling before and the authors do not suggest improvements to that. finally, the experiments showing robustness to backdoor attacks is unconvincing and can benefit from more analysis.", "accepted": 0}
{"paper_id": "iclr_2022_gX9Ub6AwAd", "review_text": "this paper handles anomaly detection in surveillance videos. the authors proposed to use a framegroup attention method for handling this task. however, all the reviewers have concerns about the novelty, clarity, and experimental evaluations of this work. moreover, no rebuttal is provided by the authors.", "accepted": 0}
{"paper_id": "iclr_2022_Aot3sKdraW", "review_text": "all four reviewers agree that the paper should be rejected in its current form, but make numerous suggestions for improving it. the main points of concern were the motivation of the proposed method, novelty and the quality of the presentation of the work. the authors did not provide a response. the ac agrees with the reviewers and recommends rejecting the paper.", "accepted": 0}
{"paper_id": "iclr_2022_MXrIVw-F_a4", "review_text": "this paper presents fast learnable onceforall adversarial training float which transforms the weight tensors without using extra layers, thereby incurring no significant increase in parameter count, training time, or network latency compared to a standard adversarial training. compared to existing sota, float is better in many metrics including training time, training parameters and hyperparameters, storage cost, potential inference latency, speed, and task accuracy. this paper received highly mixed scores 8653. during the private discussion, reviewer dn3 stated that shehe was willing to raise score from 3 to 5. although i am not sure why the reviewer did not actually make the change, im consider the rating increase as happened i.e., factually 8655. after reading this paper, ac agrees that float solved an important limitation of the previous stateoftheart method oat reducing the film overhead by using a more efficient model conditioning method, i.e. adding configurable scaled noise on model weights. the lukewarm part is, the method is no doubt heavily based on the oat paper. even one can argue the method as a whole is novel, the contributions despite interesting remain slightly incremental. in view of the above, ac currently places this paper as a borderline rejection.", "accepted": 1}
{"paper_id": "iclr_2022_27aftiBeius", "review_text": "all reviewers agreed on rejection. unfortunately, there was no author response so there was nothing to drive further discussion on the paper. the reviewers gave very detailed advice on improving the work.", "accepted": 0}
{"paper_id": "iclr_2022_ZC1s7bdR9bD", "review_text": "the paper explores a method to identify features in an input that can explain uncertainties in the model prediction. the proposed approach is similar to integrated gradients ig,with a different explanation target and integration path. overall, the idea seems fairly incremental and the experimental evaluation is lacking and does not sufficiently demonstrate the advantages of the proposed approach. evaluation metrics could be improved see suggestions by reviewer n3ei to strengthen the paper.", "accepted": 0}
{"paper_id": "nips_2021_AIQOddM5Xm", "review_text": "this paper studies the problem of 3d human pose estimation by combining information from labeled sources and unlabeled data in the wild. the paper received mixed reviews, in the beginning, tending near borderline. the reviewers major concern was regarding the presentation clarity. multiple reviewers found it hard to follow. the authors provided a rebuttal that addressed some of the reviewers concerns and promise to improve the presentation. the paper was discussed and most reviewers responded to the rebuttal. one of the reviewers raised their rating but one reviewer did not. the main complaint is still around the writing and presentation. ac agrees with the reviewers that the paper has a good contribution to be accepted and urges the authors to look at the reviewers feedback, incorporate their comments and clarify the writing as promised in the cameraready.", "accepted": 0}
{"paper_id": "nips_2021_WtmMyno9Tq2", "review_text": "this work presents a proofofconcept for multimodal fewshot training of frozen language models  where the parameters of a pretrained language model are kept fixed while an image encoder is trained to prompt it to generate text. despite its simplicity, the approach demonstrates surprising generalization capabilities of the model when presented unseen visual concepts, although as pointed out by the authors themselves, the results are still far from what current models can achieve on these tasks and more work is needed to refine these ideas further.", "accepted": 1}
{"paper_id": "nips_2021_Yx1OzVU_SRi", "review_text": "three of four reviewers generally agree to recommend this paper for acceptance, and i agree with the strengths that they highlight, especially as reviewer y1dq summarizes them. reviewer hgar, who recommends rejection, makes a valid point as well the experimental section and the discussion of assumptions leaves room for improvement. i encourage the authors to take the committees suggestions about these parts into consideration when revising the draft. for instance, two reviewers would have liked to see an empirical signal for whether the conditional independence assumption holds in a real data setting not simulation. even if the authors choose not to do this, it is natural to ask and it is lacking from the experiments, and so it is ultimately a limitation. its best to be clear about whether the experiments are meant to closely verify theory including its assumptions, or whether they are meant only to show that the analysis is at least not at odds with empirical observations. the latter seems to better describe this papers experiments, but it is a weaker point than the former, and discussing limitations ultimately helps with clarity. i will add that there are other ways to support an assumption than by experiment. one is to identify example mathematical constructions that satisfy them e.g. topic models, hmms. another is to point to similar assumptions made in related prior work. the paper does the latter reasonably well. if it is easy enough to do, perhaps the authors can make use of an example construction or two as well. overall i second y1dqs remarks in discussion that this is a field with little theoretical analysis, and that what constitutes a reasonable abstraction and assumption is not yet clear. making assumptions for progress is fine, especially if similar ones have been make in related work. the validation and discussion of assumptions in this paper could be improved as hgar emphasizes, but overall the support from the remaining reviewers leads me to recommend acceptance.", "accepted": 1}
{"paper_id": "nips_2021_LcSfRundgwI", "review_text": "the paper proposes a new class of priors for variational autoencoders vaes. the main idea is to use noise contrastive estimation and a twostage training method. the reviewers highlighted that  the paper is clearly written.  the idea is simple in the positive sense! and easy to implement.  the proposed prior could be plugged in any vae. however, the reviewers raised some concerns  sampling from the vae with the suggested parameters is very slow.  the experimental results are not overly convincing.  the organization of the paper could be improved.  the method cannot reliably be used to evaluate a bound on the loglikelihood.  the only performance metric used is the fid, which arguably favors the quality of samples overgeneralization. moreover, i find it rather surprising that the authors do not compare their approach to at least one of the following two methods  tomczak, j.,  welling, m. 2018. vae with a vampprior. in international conference on artificial intelligence and statistics pp. 12141223. pmlr.  norouzi, s., fleet, d. j.,  norouzi, m. 2020. exemplar vae linking generative models, nearest neighbor retrieval, and data augmentation. arxiv preprint arxiv2004.04795. it seems rather natural to focus on a thorough comparison against other priors for vae. instead, the authors have a limited comparison in table 2. moreover, the authors decided to compare their method to various deep generative models. i totally agree that it is also important, however, without a proper comparison against various priors, it is hard to properly evaluate their idea. the authors provided a thorough rebuttal and they promised to improve the paper. overall, the paper is solid and the idea is neat, therefore, i tend to accept the paper.", "accepted": null}
{"paper_id": "nips_2021_2GapPLFKvA", "review_text": "thank you for your submission to neurips. there is broad consensus among the reviewers that the paper presents a significant step forward on a difficult problem. the reviewers found the paper wellwritten and easy to read. three of the four reviewers felt that the experimental aspects of the paper could have been improved. one reviewer raised concerns regarding the bernoulligaussian assumption on x. while the assumption is standard in the area, it nevertheless appears to be removed from practical applications. this reviewer suggested that more thorough simulations could have better covered this weakness. given the interesting and nontrivial nature of the theoretical contributions, the authors are encouraged to followup by submitting a long version of the paper to a traditional math journal.", "accepted": 1}
{"paper_id": "nips_2021_yWd42CWN3c", "review_text": "the authors propose a continuous time model combining recurrent and convolutional structures. overall, reviewers are supportive of the paper. the main remaining concerns, after discussion, are mostly with respect to the presentation. it was felt that the paper is dense, heavily relying on the appendix, and could be more clearly communicated in the main text. there was also a perception that the main contributions were in engineering. while the authors push back this notion, there is no need for this to be perceived as a drawback, and could be reasonable to highlight engineering contributions in revisions. there were also some concerns that it was hard to derive clear takeaways from some of the experiments details in the reviews. please try to be receptive of reviewer comments in preparing revisions. but generally, this is great work!", "accepted": 1}
{"paper_id": "nips_2021__idcJrecij", "review_text": "this paper proposes an approach for performing conditional density estimation of the form px_ux_o for any arbitrary choice of the observed x_o and unobserved x_u variables by using an energy function parametrized by a neural network to estimate the 1d distributions. three reviewers recommend acceptance, and one reviewer believes the paper is below the acceptance threshold. after reading the reviews and the rebuttal, i recommend this paper for acceptance. i also encourage the authors to improve the paper by considering the reviewers comments. in particular  add an analysis of the number of samples needed for the importance sampling approximations.  as noted by reviewer aeg1, the imputation method may not be appropriate for certain applications because the imputation is performed using the marginal means, e.g., the means of px_2x_1x_1 and px_3x_1x_1, therefore, ignoring the dependences between x_2 and x_3. this should be mentioneddiscussed in section 4.4.2 or 5.1.2.  add a sentence motivating importance sampling over the grid.  add some discussion on the skipconnection and multiple latent vectors.  include a derivation or discussion of the stochastic approximation of the autoregressive score in the appendix.", "accepted": null}
{"paper_id": "nips_2021_oepSB9bsoCF", "review_text": "the paper focuses on an interesting problem, goalconditioned policy generalization in block mdps a kind of pomdps where the current state is uniquely identifiable from the current observation, even though a state can emit many different observations. this is a mostly theoretical work, but its experiments are convincing. the reviewers have closely examined the papers theory and, on the one hand, didnt find errors in it but, on the other hand, found theory gaps in it that could be explained either by the paper being not fully clear or not sufficiently rigorous. please see the discussions with reviewers wvnf and s2fl, especially reviewer s2fls thanks for your response comment. nonetheless, due to this work being one of the early ones on this topic and being likely to become a stepping stone for further exploration of this area, the metareviewer recommends acceptance, trusting that the authors incorporate the points that came up in the discussion into the final version its hard to think of a reason not to do this.", "accepted": null}
{"paper_id": "nips_2021_slvWAZohje", "review_text": "the authors study the fuzzy kmeans problem with oracle queries and present algorithm that outperform previous work. the reviewers find the result interesting and novel and so, after the discussion phase, we think that the paper should be accepted. one limitation of the paper is that it is a bit hard to read but the authors should be able to address the reviewers concerns for the final version of the paper. one important point raised by the reviewer is that lemma 2 in the current form is very confusing and it should be rewritten and probably moved to the end of the paper.", "accepted": null}
{"paper_id": "nips_2021_pbAmqUUHsQ", "review_text": "the paper considers the problem of regret minimization on bandits with continuous arms. the key idea is to use the correlation between arms. it shows matching upper and lower regret bounds for full information and semibandit settings, and upper bound for the full bandit setting. three expert reviewers considered the strengths and weaknesses of the paper. in the author rebuttal, the authors additionally provided evidence on a nonsynthetic dataset, in response to two reviewers comments. the authors engaged with reviewer questions, and the reviewers appreciated this engagement, with two scores increasing. a brief discussion post rebuttal confirmed that all reviewers thought that the paper is novel and provides interesting results. therefore it is great pleasure for me to recommend that the paper is accepted for publication at neurips.", "accepted": null}
{"paper_id": "nips_2021_7PkfLkyLMRM", "review_text": "the reviewers in most part agree that paper is wellwritten, is tackling an interesting problem and also the authors have engaged constructively during the reviewer feedback period and addressed the main concerns and questions. please address the minor points raised by the reviewers in the final version.", "accepted": null}
{"paper_id": "nips_2021_bqGK5PyI6-N", "review_text": "this paper address efficient finetuning of pertrained models and investigate applications in nlp domains. the idea is to insert a parameter compact layer called compacter in the pretrained models. by further parametering the compacter layers with lowrank representation, its achieves a better tradeoff between model performance, trained parameters and memory footprint compared with existing methods. the idea is quite simple and the techniques are well explored in different scenarios by existing works. thus most reviewers consider the technical contribution to be incremental. however, it demonstrates very good results in nlp pretrained models. some reviewers were not satisfied with the limited experiments provided in the original submission. in the rebuttal, the authors was able to provide more results in a more complex data and model. and the results are in consistent with the results provided in the paper. the reviewers are generally satisfied with the rebuttal. considering the technical limitation and the support from the reviewers, i recommend a weak accept and the authors to incorporate the new results and other comments from the reviewers into the revision.", "accepted": 1}
{"paper_id": "nips_2021_tCYjE8Pf2Zg", "review_text": "two of the reviewers agreed that the paper is novel and can result in cheaper, higherperforming object detectors. the third reviewer misunderstood the term spatial transformer and was confused by its use, not responding to the authors clarification. given the strong experimental result and interesting approach, i support the acceptance of this paper.", "accepted": 0}
{"paper_id": "nips_2021_vIRFiA658rh", "review_text": "this paper considers sketching and random feature methods to accelerate learning with the neural tangent kernel ntk. ntk based methods are promising linearized approximations of nonconvex neural network models, however, they suffer from the high computational complexity of kernel matrix operations. scaling ntk methods via randomized approximations is a promising direction towards understanding deep neural networks and matching their performance using simpler architectures such as linear kernel machines. the reviews all agreed that the paper contains interesting theoretical and experimental results, and expressed minor concerns and suggestions. one reviewer pointed out that a discussion of the regime in which ntk is not fully descriptive would be useful. another reviewer commented that the complexity notions are hard to digest. overall, the paper received generally positive reviews, while some of them pointed out certain minor problems and gave above borderline scores, they all recommended accepting the paper. please take into account the updated reviews when preparing the final version to accommodate the requested changes.", "accepted": null}
{"paper_id": "nips_2021_aExAsh1UHZo", "review_text": "the paper studies a phenomenon, referred to by as gradient starvation e.g., httpsarxiv.orgabs1809.06848, in which only a subset of features relevant for the task is captured during the training, despite the presence of other predictive features. the reviewers found the explicit study of this phenomenon interesting and of practical relevance, and generally appreciated the numerical results provided in the paper. however, whereas some of the reviewers felt that the focus on the ntk regime is fair, others raised several significant concerns regarding the technical contributions of the work, in part but not only due to the by now wellestablished discrepancy between ntk and neural network models.", "accepted": 0}
{"paper_id": "nips_2021_jh1lAmTMOJp", "review_text": "the reviewers agreed that the paper contains compelling ideas and should be accepted. some initial concerns about the generality of the claims were well addressed by the authors.", "accepted": null}
{"paper_id": "nips_2021_IoEnnwAP7aP", "review_text": "this paper proposes two algorithms corresponding to bernstein and hoeffding bonuses for rewardfree exploration in linear mixture mdps and analyzes their sample complexity. the authors also provide a lower bound for this setting. the proposed algorithms involve a computationally difficult subproblem, which is replaced by an lp relaxation in the implementation. overall, the results and techniques are not very surprising, leaving most reviewers ambivalent. however, given that the paper is executed well and fills a gap in the literature, i recommend acceptance.", "accepted": null}
{"paper_id": "nips_2021_0yMGEUQKd2D", "review_text": "3 of 4 ratings were accept. a key concern shared by several reviewers was about comprehensive experimental baselines, and the authors responded by performing requested experiments which supported their results. because the one reviewer who gave the lowest rating a 5 suggested this experiment, the authors performed it and it was favorable, and the reviewer didnt respond, im inclined to treat the reviewers rating as a 6.", "accepted": 1}
{"paper_id": "nips_2021_bSgieZ8-be", "review_text": "this paper uses admm to induce structured weight sparsity during neural net finetuning. it is specifically targeting a very local sparsity pattern n nonzeros in every block of m weights which is supported in recent nvidia hardware and libraries with nearlinear speedups. this is a different constraint scenario than most structured sparsity approaches, and admm does seem like a good fit. as such, reviewers find the contribution valuable but also highlight some weaknesses. the experimental methodology seems susceptible to noise; multiple runs would help; however, simply getting comparable performance seems sufficient when sparsity is the goal. reviewers and possibly all readers are left wanting speed comparisons; authors should place more emphasis on the libraryhardware support for nxmsparsity to better clarify that this is established independent of this work. the writing and presentation should be polished as well.", "accepted": null}
{"paper_id": "nips_2021_oyHWvdvkZDv", "review_text": "this paper studies highdimensional linear regression in a truncated setting, where only examples with labels in a given set are observed. learning from truncated samples is a classical topic in statistics that has received renewed interest in the last few years. prior work had given an efficient algorithm for this problem under the assumption that the additive observation noise has known variance. this paper gives an efficient algorithm for the broader, more realistic, setting that the variance of the noise is unknown. handing unknown variance turns out to require nontrivial new ideas. after extensive discussion internally and with the authors, the reviewers agreed that this contribution merits acceptance to neurips.", "accepted": null}
{"paper_id": "nips_2021_Efqe8E4Bww", "review_text": "most reviewers appreciated the novelty of the class of the problems studied and found it interesting. there are concerns about the expressivity of hcc games in practical settings that has not been fully addressed by the authors. there were also questions about the contributions w.r.t. reference 66. however, as authors mentioned, the invertibility assumption is one of the key differences of their work with reference 66. another comment was about defining various concept needed to understand the paper. it would help the reader a lot if the authors can define all necessary concepts to understand the paper e.g. la salles principle vo neumann solution, etc.", "accepted": 0}
{"paper_id": "nips_2021_Y2OaOLYQYA", "review_text": "all reviewers are favorable after the author responses. clearly, this is a fairly significant contribution. reducing the number of ci tests within the fci family of algorithms handling both selection bias and confounders is very important. i am impressed by the significant reduction in number of ci tests and significant improvement in orientation accuracy which is non trivial empirically. further the anytime guarantees and associated theoretical results make it a solid paper. the authors even clarified key technical concerns after which reviewers felt more positive. please do add experimental results quoted in the review response about fprfnr vs other fci based algorithms and please do make changes as recommended by reviewers regarding rephrasing of lemma 1.", "accepted": null}
{"paper_id": "nips_2021_ErivP29kYnx", "review_text": "this paper proposes a more nuanced take on contrastive selfsupervised learning ssl. rather than frame the objective of ssl as a binary similarnot similar target, the authors propose to use multiple sampled augmentations as a similarity distribution. the goal of ssl becomes to embed the training data in a way that agrees with the distribution. the result can be viewed either as a relaxed form of contrastive ssl, in which multiple views of the same instance only need to be similar, not the same, oras one reviewer pointed outa kernelbased approach that operates on the similarities among instances. while the idea is intuitively appealing, the authors show that simply plugging it into existing ssl setups gives very poor performance, so the authors provide a recipe for this alternative approach. in particular, only weak augmentations as opposed to changing the instances heavily leads to better results, in contrast with existing contrastive learning approaches where the augmentation must be strong to make the pretext task challenging. experimental results show that the learned representations lead to significant gains on object classification tasks. the reviewers generally agree that this paper is well written and makes a significant contribution. during the discussion phase, the authors introduced several follow up experiments, which the reviewers encourage them to include in the final version.", "accepted": null}
{"paper_id": "nips_2021_SMU_hbhhEQ", "review_text": "there is general consensus among the reviewers that the paper should be accepted. the authors made extensive effort to answer reviewer comments and provide additional empirical results. we expect the authors to implements all clarity improvements, as requested by the reviewers. figure 1 looks pixelized, it should be converted to vector graphics.", "accepted": null}
{"paper_id": "nips_2021_4CrjylrL9vM", "review_text": "the reviewers were overall positive regarding the paper, and they were satisfied by the rebuttal. after the discussion period, the reviewers requested for the following things to be included in the paper 1. more description of the optimization problem in algorithm 1. 2. a table about the capacity of the attacker in the federated learning setting. 3. include the additional empirical evaluations mentioned mentioned by the authors during the rebuttal period.", "accepted": null}
{"paper_id": "nips_2021_vCthaJ4ywT", "review_text": "this work introduces a novel capsule architecture that learns motion patterns in an unsupervised manner. evaluation on synthetic and realwork skeletonbased human action datasets demonstrate the model good performance when compared to previous stateofart methods. using the formulation of capsule autoencoders to explicitly learn transformable trajectory templates through selfsupervised learning is novel and an interesting addition to the capsule network literature.", "accepted": null}
{"paper_id": "nips_2021_2rR3aBnhCaP", "review_text": "the paper explores when predictions relying on the posterior predictive distribution of a bayesian neural network can be poor in the presence of covariate shift and provides a plausible explanation. weakly informative likelihoods cause a lack of posterior contraction, causing the posterior to revert to the prior for a subset of the model parameters. the resulting posteriorpredictive reverts to the prior predictive and can be detrimental in covariate shift. the paper convincingly demonstrates the empirical phenomenon of bayesian neural networks struggling under certain types of covariate shifts and proposes a plausible explanation for the observed phenomenon. it is solid in terms of its empirical analysis. furthermore, it presents a new prior that does not suffer from the downsides of the conventional priors commonly used. initial doubts about a lack of samples to evaluate the posterior predictive distribution were successfully addressed in the discussion period. likewise, initial concerns about a lack of novelty with respect to izmailov et al., icml 2021 were also clarified. another concern was that the proposed phenomenon might not be the only explanation for bnns failing in ood situations, but the reviewers didnt consider this a significant weakness of the paper.", "accepted": null}
{"paper_id": "nips_2021_YDepgWDUDXx", "review_text": "this paper leverages an analysis of deep gaussian processes to argue that an excess increase in the width of a neural network can degrade performance. the question of whether or not neural networks benefit from increased width is an important and unresolved question in the literature, and the reviewers and i agree that this paper provides an additional and important perspective on the topic that will be of interest to the neurips community. while some reviewers found the experimental results unconvincing and the conclusions somewhat speculative, others found the framework to be illuminating and to provide new perspectives on this important problem. overall, i do not expect this or any paper to fully and unambiguously resolve all questions relating to the benefitsdrawbacks of large width networks, and i believe this paper provides novel and useful insights that shed light on this important problem. therefore, i recommend acceptance.", "accepted": null}
{"paper_id": "nips_2021_Alr5_kKmLBX", "review_text": "all three reviewers recommend acceptance 1 rating of 7, 2 ratings of 6. reviewer mc1w raises the initial rating of 6 to 7, because of the convincing additional results included in the author response. however, the reviewer still recommends revising a claim and clarifying the contribution in the abstract and title. the experimental setup should be corrected to make the objective of the work clearer. reviewer taqs asked several clarifying questions, which were well addressed in the author response. the rating was thus increased to 6. finally, based on authors feedback, reviewer 3etu acknowledges the memory reduction as an important improvement and raises the rating to 6. the acs concur with the acceptance recommendation made by the reviewers.", "accepted": null}
{"paper_id": "nips_2021_aSjbPcve-b", "review_text": "all four reviewers advocate acceptance. i also recommend accepting the paper for its contributions to the emerging field of bayesian causal inference.", "accepted": null}
{"paper_id": "nips_2021__4VxORHq-0g", "review_text": "the paper received mixed ratings, with three reviewers recommending acceptance and one rejection. the reviewers main concerns include the novelty of the method, its motivation, missing comparison to some baselines, and the clarity of some parts of the paper. the paper went through several rounds of questionsanswers, and the authors feedback addressed the concerns of most reviewers. the most negative reviewer did not update their score, but we believe that the latest feedback from the authors, on which the reviewer did not comment, better addressed some of the concerns raised by the reviewer. as such, we believe that the paper can be accepted to neurips but strongly recommend the authors to incorporate their feedback in the final version.", "accepted": 0}
{"paper_id": "nips_2021_0zXJRJecC_", "review_text": "the paper tackles a challenging problem of unsupervised domain adaptation uda without access to source data while assuming access to a pretrained source model. the proposed method takes inspiration from contrastive losses such as infonce and tailors these for the uda setting. reviewers appreciated the significance of the problem tackled by the paper sourcefree uda and the experiments on a variety of vision tasks that show clear improvement over existing methods. authors should revise the paper taking into account the reviewers comments, including a discussion of the prior work pointed out in the reviews on the sourcefree uda and universal da problems.", "accepted": 1}
{"paper_id": "nips_2021_HCrp4pdk2i", "review_text": "the reviews for this paper were mostly borderline, with one review strong negative. while the reviewers thought that the paper addresses an important problem in a novel way, and provides interesting insights, the author response did not lead them to change their scores. despite this lukewarm assessment, i believe the paper makes contributions that likely merit acceptance 1 tackling an important problem in generating and evaluating explanations in nlp and ml more broadly the oodness of feature importance attribution methods that are based on masking, ablation, etc., since they produce counterfactualood examples; 2 providing a clear approach to mitigate this problem adding counterfactuals at training time, which can work well little drop compared to standard models; 3 evaluating various methods to create explanations and finding that some work well in the new counterfactually trained models achieve good sufficiencycomprehensiveness scores, which are standard. given these contributions, and considering that no paper is perfect, i recommend acceptance. since my recommendation goes somewhat against the general sense of the reviews, i provide here a detailed discussion of the main issues raised by the reviewers, how the authors responses, and my opinion of the issue. in any case, i urge the authors to take these issues into account in their next revision. comments by reviewer nmtf  argument on social alignment doesnt add much. authors its another perspective on why fi oodness is problematic, and may help. ac natural application of the social alignment argument from jacovi and goldberg to another class of explanations.  using accuracy to assess distribution shift is misleading, can conflate oodness and information removal. authors remove the same information in all conditions, difference is due to oodness. ac agree that accuracy change between iid and ood data is a reasonable metric of oodness.  not convinced about sufficiency and comprehensiveness metrics. authors these are standard in the literature. ac agree, these are standard faithfulness metrics. whats missing here is an evaluation of plausibility, for example by comparing with humanannotated explanations highlights, rationales.  baseliens do terrible. authors lime does well, ig is likely a poor method. ac its tricky to compare when the benchmark is not common. it could have been better to use standard datasets from explanations in nlp, where prior results are available. but, lime results are indeed quite good. reviewer lhz2  the contributions do not form a consistent story line. authors disagree and explain that the part on local attributions depends on fixing the ood problem of explanations. ac agree, the first part is a nice contribution in itself and it makes the rest possible.  counterfactual is an incorrect term here, since a counterfactual explanation means something different. authors did not use counterfactual explanation. ac best to clarify that counterfactual input is not to be confused with the technical term of counterfactual explanation.  questions the sufficiencycomprehensiveness metrics and prefers more meaningful metrics like shap. here i agree with the authors that it is useful to apply these empirical metrics, which are standard in the literature, even if they do not have some nice theoretical properties like shap.  changing the training distribution is questionable. authors to use fi explanations, have to align training and test distributions, so if we want to keep test time metrics, must change training distribution. ac indeed, i was also concerned by changing the training distribution and hence the model, but was pleased with the minor drop in accuracy, at least for some of the methods. however, its possible that a model trained on the original distribution arrives at decisions in a different way than a model trained on the counterfactuallymodified distribution, but they have similar performance. this is a hard problem to get around, but one that should be discussed.  search methods provide binary importance while existing methods provide scalar scores. authors argue that their method still brings useful rankings 15 out of 24 vs line and that for selecting features it works best. ac not convinced by the author response, as 1524 seems weak, and also as scalar weights can indeed be useful, both for interpretation users might prefer them and for downstream tasks training with explanations, etc..  sufficiency and comprehensiveness are often worse when using the proposed counterfactual training. authors explain they recommend using ct regardless of these metrics, because using ct guards again oodness. ac while i agree with the idea in principle, im unconvinced by the argument. if using a standard model leads to better explanations according to the metrics in question, then the argument for using ct grows much weaker. reviewer 2jsj  had some questions that seemed to have been answered.  not clear that the explanations will truly be more socially aligned. ac did not see a clear answer to this. the answer to the issue of social alignment and how different models react to ood counterfactuals is part of the story, but probably not all. reviewer abdp  there are existing method to deal with the problem of estimating effects in the case of counterfactuals mentioning specific studies causalm, inlp, cace. authors dont see how these particular studies lessen the contribution, since they dont discuss how oodness affects social alignment. also say that the mentioned methods are not meant to use with individual data points. ac agree. those should probably be discussed, but are not at the core of the current work.  counterfactual training can be misleading in terms of evaluation. authors argue that in their case it makes sense, because the ct models work better on ood data, and theyre not interested in different kinds of oodness. ac agree, this actually seems reasonable for the present use case of evaluating explanations, if not for robustness more broadly. the authors should clarify their goals in this evaluation and distinguish from general model robustness.", "accepted": 0}
{"paper_id": "nips_2021_Z4ry59PVMq8", "review_text": "the paper proposed a new method for debiasing vqa models. a consensus for acceptance emerged very quickly in the discussion phase, helped by a swift rebuttal by the authors and additional experiments on new datasets. the ac concurs.", "accepted": null}
{"paper_id": "nips_2021_R6nFQy2vwQq", "review_text": "this paper addresses a new image descriptor, referred to as deep self dissimilarity dsd which measures the dissimilarity between deepfeatures from the same image presented at different scales. reviewers agree that the paper presents a novel idea which suggests a measure of scalewise dissimilarity of deep features as a descriptive measure of images. the paper is well written and the key idea is appreciated. the rebuttal addressed most of concerns raised by reviewers, leading that two of reviewers raised the score during the discussion period. i believe that the paper is deserved to be presented in the conference.", "accepted": 1}
{"paper_id": "nips_2021_Aeo-xqtb5p", "review_text": "this paper received very positive reviews initially. the reviewers liked the originality of the work, its theoretical soundness and the good performance of the algorithm. they had some concerns and wanted to see more about generalization capacities, stronger baselines and a larger set of test environments. the discussion was rich enough to convince the reviewers to raise their scores. the authors made additional experiments and addressed all the concerns of the reviewers.", "accepted": 1}
{"paper_id": "nips_2021_DMkdzO--w24", "review_text": "this paper considers the problem of adapting to noise variance in linear bandits and reinforcement learning linear mixture mdps. the reviewers agree that the main technical result in the paper a new confidence set construction which is adaptive to variance is interesting and highly nontrivial. this is a timely result, and i am confident this technique will find broader use. in addition, there are many interesting directions e.g., improving computational efficiency for future work. while the reviewers felt that the paper is generally wellwritten, the authors are encouraged to incorporate their suggestions to improve the clarity and organization of the main body and appendix.", "accepted": null}
{"paper_id": "nips_2021_NrEwQwhPODl", "review_text": "this paper describes a method mixacm for transferring robustness from a teacher model to a student mode using activated channel map matching and mixup, obviating the need for expensive adversarial training. a key concern raised by reviewers was that, while the individual components of the method are well motivated and the overall combination of them appears to be highly effective, these individual components are already known, and their simple combination is not as novel or theoretically motivated as one might hope. reviewers also expressed concerns about quality of writing and the overall clarity of the paper. this is something of a borderline case, but given the strong results produced by this method, i nonetheless recommend that the work be accepted.", "accepted": null}
{"paper_id": "nips_2021_swur4c3YSyF", "review_text": "the paper proposes a new marked temporal point process model for continuoustime event sequences. while the standard neural stochastic point process models e.g., neural hawkes processes parameterized the timevarying intensity functions using recurrent neural networks, these models are often black boxes and dont allow to estimate the influence inhibition or excitation that event types have on future ones. in contrast, the proposed work uses interpretable gaussian processes and achieves better performance while maintaining interpretability and scalability. in this sense, the approach achieves to both flexibly and transparently estimate various decaying influences between every pair of events. with few exceptions on the model training algorithm that could be presented more clearly, the paper is overall wellwritten and mathematically sound. despite its sophisticated inference scheme, it provides a valuable contribution to machine learning and statistics. please run a grammar check over the paper for the final version since many small mistakes are present.", "accepted": 1}
{"paper_id": "nips_2021_A9HVNx1J8Pc", "review_text": "the paper presents a learningaugmented algorithm for online facility location. the competitive ratio of the algorithm depends on the quality of the prediction, and the size of the advice; it is never worse than the competitive ratio of the best worstcase algorithm. there was a substantial debate about i the advice model  how naturalrealistic it is and ii the approximation guarantees, esp. the possible discrepancy between opt and optp,s. overall, however, the reviewers felt that these issues do not significantly reduce the value of the paper.", "accepted": 1}
{"paper_id": "nips_2021_JXREUkyHi7u", "review_text": "the paper studies the robustness verification problem for neural networks over simplex inputs. unlike the linf case where a tight bound requires exponential complexity, the paper proposes an efficient method to propagate the simplex through relu networks where the size of relaxation remains linear in the number of neurons and shows the algorithm can successfully compute tight bounds on several benchmarking datasets. after discussions, the reviewers think that although the method is an extension of tjandraatmadja et al., the derived closedform expression is new and it is interesting to see that such a nice form can be obtained in the l1 case. therefore we recommend acceptance of the paper. the reviewers also think the paper is not well written and hope the authors improve the presentation of the paper based on the review comments.", "accepted": null}
{"paper_id": "nips_2021_xwGeq7I4Opv", "review_text": "the reviewers were unanimous in their appreciation of the paper and hence i recommend a clear accept. i request the authors to look into improving the notation in the paper and reassessing the paper in terms of clarity of presentation of the proofs etc to improve readability. suggestions of this form have been laid out in multiple reviews.", "accepted": 1}
{"paper_id": "nips_2021_wLsA3nurh9W", "review_text": "this paper considers the limited singular value distribution of a random feature model, which consists of an entrywise nonlinearity and a product of two i.i.d. matrices. in particular, the generalization performance of a single hidden layer neural network where the input data and hidden layer weights are i.i.d. random can be determined through this analysis. the authors show that the resolvent method can be used to simplify and generalize the existing techniques for this problem. the reviews all agreed that the paper contains strong theoretical results, only expressed minor concerns and suggestions, and recommended acceptance. as reviewer o1s8 noted, adding a discussion of the advantage of the resolvent approach and clarifying the novelty of the results w.r.t. existing work e.g., dealing with additive bias will improve the paper. please take into account the updated reviews when preparing the final version to accommodate the requested changes. thank you for your submission to neurips.", "accepted": 1}
{"paper_id": "nips_2021_4-Py8BiJwHI", "review_text": "the authors have written a very convincing rebuttal that leans me toward acceptance of this paper.", "accepted": null}
{"paper_id": "nips_2021_4XOrn_Y-dqp", "review_text": "the paper considers the problem of understanding if sgd can be seen as a solution to implicit regularized empirical risk minimizer for the problem of linear regression. the authors compare the excess risk guarantees of sgd solution to the one obtained by solving ridge regression exactly. the authors show that sgd solution is always almost as good as the one obtained by ridge regression. but on the other hand, there exists a problem setting where the excess risk of the ridge regression is substantially worse than the sgd solution. this shows that sgd cannot be seen as implicitly regularized ridge regression. this paper generated a large number of discussions between the reviewers and also between reviewers and authors. while for some points there isnt total agreements, given the reviews and the discussion, it seems clear to me that the paper should be accepted first because despite some of the mentioned possible shortcomings the paper still is interesting and second, the subtler discussion points are exactly why we need to accept the paper so these issues are more widely discussed.", "accepted": null}
{"paper_id": "nips_2021_iLn-bhP-kKH", "review_text": "this paper had very split reviews and required some thought when coming to a recommendation. the authors provided very thorough responses to each reviewers critiques. while the two positive reviews responded saying they were satisfied with the author responses, neither of the negative reviewers responded to the author feedback. one negative reviewers only objection was a lack of comparison against you et al. 2020. the authors provided such a comparison in the response, showing good results for their method relative to the baseline. the other reviewer had more detailed critiques, but from my estimation of the authors response, those critiques were well rebutted. unfortunately i do not know if the reviewer themself thought the critiques were answered well. i am inclined to recommend acceptance for this paper, on the basis of the very thorough responses which i believe answered most of the reviewer critiques, and the failure of the reviewers themselves to answer the authors.", "accepted": null}
{"paper_id": "nips_2021_DKRcikndMGC", "review_text": "visualization with umap has become standard practice in multiple fields in recent years, originally presented as being derived from topological construction as an alternative to the popular tsne visualization. however, over the past few years several studies have investigated its relation with tsne and other visualizations and raised questions regarding the matching or rather discrepancy between its theoretical derivation and the implementation used in practice. this work provides an interesting in depth analysis on this topic, constructively providing a closedform formulation of the loss function that is implemented by the umap algorithm which is different from the loss it was purportedly supposed to optimize. in doing so, it provides concise insights into understanding of previous observations, and more generally the behavior of umap. after discussion, the reviewers unanimously agree the paper should be accepted. most concerns that have been raised by initial reviews have been addressed in the responses authors please follow up on these in revision of the manuscript itself. therefore, i recommend accepting the paper and i am positively certain it will be of great interest to a significant portion of the neurips community.", "accepted": 1}
{"paper_id": "nips_2021_Rx9dBZaV_IP", "review_text": "the paper extends the conformal prediction methodology to the time series setting, with exchangeability assumed between time series. in doing so, the proposed approach can provide uncertainty estimates with coverage guarantees. the reviewers agree that the paper tackles a relevant problem, proposes a reasonable solution, is well written and theoretically sound. while the reviewers pointed out some areas for improvements additional related work, improvements to the empirical evaluation, the authors were able to alleviate all major concerns during the discussion period, making this a solid contribution with the potential to spark interesting followup work.", "accepted": null}
{"paper_id": "nips_2021_yTXtUSV-gk4", "review_text": "the authors in this paper study the learning of a boltzmann machine, aka the inverse ising model, a classical problem in graphical models and markov random fields. they propose an analysis of the performance of the l1regularised linear regression estimator in finding the underlying nonzero coefficients. the analysis is performed using a nonrigourous but powerful heuristic approach from statistical mechanics, the replica method, that has been used in many other machine learning problems, in idealistic situations. the author confirmed the theoretical result by running a large number of numerical simulations. they show the sample complexity is such that olog n samples are enough to correctly identify the right neighbourhood of a generic variable. the theory presented seems to be powerful enough to predict both the precision and the recall rates for finite dimension, and seems to give fairly good prediction for graphs with many loops. interestingly, the methodology of the present work can be apparently generalised and extended also to other estimators. the review process was intense, with six reviewer and a large number of forum replies. all reviewers testified of the quality of presented results. from a technical point of view, computation contained in this manuscript was found to involved and complex, yet it was found provides a nontrivial analytical result. the comparison with the numerics however strongly supports the claim that the replica computation is giving the correct answer, even though it is not rigorous. this paper was thus judged to represent a nontrivial contribution from statistical physics to machine learning. however, the use of the nonrigorous replica method and its acrobatic nonrigorous mathematics was judged to be sometimes dazzling. the rebuttal saw a number of discussions, and most reviewers agreed that the paper was impressive, and score were increased during the process. a criticism that remained, though, from a minority of reviewers, was that the derivation is likely inaccessible for nonexperts. while this is true, it can be said of many theoretical at neurips, including rigorous ones, and there is a long standing tradition of welcoming such nonrigorous paper at neurips. given the agreement on the quality of the results, the very good ranking  grading of the paper, the extensive numerical simulations that confirm the validity of theory, we believe the paper to be largely worthy of a publication at a venue like neurips, and recommend acceptance.", "accepted": 1}
{"paper_id": "nips_2021_7Da3azsjjlh", "review_text": "this paper introduces a selfinstantiated recurrent unit that is related to the lstm but with additional capabilities for soft recursive. the authors evaluate their method on a range of tasks including image classification, logical inference, sorting, tree traversal, music modeling, semantic parsing, and code generation. the reviewers and i agree that the evaluations are quite extensive. its also clear the method performs well. the model presentation is mostly clear, but there were still a number of queries and points of confusion that popped up in the backandforth relation to stacked lstm, why only hidden and output gates determined recursively, which parameters are shared, etc.. the author rebuttal helped in this regard, but these points werent completely resolved. i agree with rvosb that the ablation analysis isnt as revealing as it should have been in illuminating the architecture choices. also, i found that section 3.7 wasnt very developed and didnt add much understanding. i wish it was clearer why and how the architecture works. that said, i recommend acceptance, based on the strength of the experiments and the cleverness of the architecture. i hope the exposition will be further improved in the final version, as there are many helpful comments from the reviewers.", "accepted": null}
{"paper_id": "nips_2021_HS_sOaxS9K-", "review_text": "this paper was generally well received. reviewers unanimously agreed that the paper was interesting and impactful, especially given the widespread interest in message passing neural networks to model atomistic systems. it seems clear that neurips is an appropriate venue for this publication. the authors provided a number of clarifications and further experiments in response to the reviews that helped to address several concerns in particular regarding baselines against existing models such as nequip and ablation studies. the main issues that could not be addressed during the rebuttal period were the clarity of exposition which several reviewers commented on. i would encourage the authors to work on simplifying the paper as much as possible in the time leading up to the camera ready.", "accepted": null}
{"paper_id": "nips_2021_Xci6vUAGeJ", "review_text": "the paper studies structural gnns in the large graph limit where they converge towards their continuous counterpart csgnn. the paper presents a variety of theoretical results showing that csgnns are provably superior to the continuous version of vanilla gnns namely, cgnns. in particular the paper shows that csgnns can recover communities in a stochastic block models in regimes where cgnns fail. all the reviewers agreed that the paper presents a novel and interesting set of theoretical results. however, the reviewers also felt that the paper can do a better job of discussing and comparing with existing literature. i recommend the paper for acceptance with a strong recommendation to the authors that they take into account the reviewer comments about existing work when preparing the camera ready version.", "accepted": 1}
{"paper_id": "nips_2021_1G6jPa9SKYG", "review_text": "this paper presents a method for this paper has four favorable reviews, describing beneficial methodology for semisupervised learning for imbalanced data, and good experimental results for real data sets. in contrast, a reviewer pointed out that the combination of the existing classimbalance learning cil and semisupervised learning is straightforward, and therefore the novelty is limited. the authors argue that it is not straightforward to utilize unlabeled data in cil and that there are no such studies in the past. however, related research is not entirely absent; for example, t. iwata, a. fujino, n. ueda, semisupervised learning for maximizing the partial auc, proc. of aaai 2020 has already been proposed. the authors should compare the proposed algorithm not only with cil and naive combined methods, but also with other existing related works.", "accepted": null}
{"paper_id": "nips_2021_vqzAfN-BoA_", "review_text": "this paper addresses the problem of longtailed visual recognition. they proposed two complementary methods to address the imbalance problem i a data augmentation scheme based on the mixup and ii an approach to compensate for bias in class prior. the final proposed pipeline is 1 stage, as opposed to 2stage which has featured prominently in recent work. results are presented across several different image classification datasets, showing superior performance compared to existing work in most cases. during the review process there was a lot of discussion regarding the relative performance compared to the recent cvpr21 paper, improving calibration for longtailed recognition aka mislas. this paper was published after the neurips submission deadline, and thus the relationship between the two works did not factor into the final decision. however, the authors are strongly encouraged to cite it, and include the new 2stage comparison to mislas from the response for completeness. in addition, they should more clearly clarify the relationship to the logit adjustment work from iclr21. the reviewers noted that while the technical contributions were not overly significant, the main insight of the paper that naive mixup introduces head to head class bias, i.e. a bias towards head majority pseudo data is well characterized and will be of interest to the longtail community. there were also concerns about the clarity of the exposition and some missing discussion. many of these issues were cleared up in the discussion, but the authors are strongly encouraged to address these issues with the writing for the final version. this is very important as it will hopefully significantly improve the clarity of the paper. the paper should also be proofread for grammar e.g. line 129. in the end, three of the reviewers were broadly supportive of the paper, but one recommended rejection based mostly on the relationship to mislas  see above. this ac, agrees with the consensus of the reviewers and supports the paper being accepted.", "accepted": 0}
{"paper_id": "nips_2021_Fj6kQJbHwM9", "review_text": "congratulations, the paper is accepted to neurips 2021! please make an honest effort making this paper more accessible to general ml audience nonexperts in tda. clarify the barcode construction. include persistent homology literature background. elaborate limitations. please include other clarifications, edits and additions as discussed in rebuttal and reviews.", "accepted": 1}
{"paper_id": "nips_2021_TJOQw_vMlAj", "review_text": "the expert reviewers all appreciated the paper and agree it provides useful results and that the paper should be accepted. the authors are to be congratulated for an interesting contribution that nicely handles a very timely topic. the authors are expected to address the points raised by reviewers in a final version, including as they outlined in their response. in my opinion the most important thing to address is to make very clear early on the setting where the method is appropriate, namely wellspecified parametric models, and that this excludes, for example, a constant model for the mean reward of an arm or policy. this is not to detract from the work  rather, making clear what the paper sets out to do and what it does not will significantly improve clarity for the reader and therefore the papers impact. a discussion regarding what would actually happen under misspecification and whether slight misspecification results in only slight aberrations from the results would potentially add a lot too i believe the target estimand would then depend on the logging policies, if i understand correctly.", "accepted": null}
{"paper_id": "nips_2021_MTs2adH_Qq", "review_text": "the idea of the tasks scheduler for metalearning is interesting. the solution is technically sound and intuitive. the theoretical analysis is also interesting, although it requires more discussions on when the assumption can hold. the experiments are quite exhaustive. the discussion on the computational cost is nontrivial, and more detailed analysis is needed", "accepted": null}
{"paper_id": "nips_2021_vwgsqRorzz", "review_text": "this paper proposes edge representation learning framework in graphs based on a hyper graph transformation. the reviewers agreed that the problem motivation has some merits. however, all the reviewers had hard time discerning whats the novelty of the proposed work, and the significance of the contribution. in future submission, it would help to have a better description on the novelty of the significance of the contribution, and contrast it with existing work more clearly.", "accepted": 0}
{"paper_id": "nips_2021_HEzEy_V7LF3", "review_text": "this paper proposes sadga, a method for improving crossdomain texttosql, where the model has to generalize to unseen database schemas. the core method proposes to parse both natural language query and the database schema and then compute an alignment between both graphs which is then passed to the module producing sql output. the assumption is that learning the alignment between query graph and database schema is more transferrablerobust across novel database schemas. the paper shows improvement on crossdomain texttosql benchmark spider.  reviewers are positive about this paper. although query and schema graphs have already been used in the literature for texttosql, the central idea of aligning query and schema graphs via localglobal aggregation is considered novel and promising. overall, the model achieves strong empirical performance. one reviewer noted how sagda outperforms other models without any pretrained component, which strengthens the claim of the paper. the main criticisms revolve around clarity of exposition and lack of interpretation. authors promised in the rebuttal to add interpretable examples of how sagda solves the crossdomain texttosql benchmark. i additionally suggest the authors to strengthen the main motivation e.g. matching in graphspace leads to more transferrable models behaviour. figure 1 needs some work e.g. longer and more explanatory caption as well as the intro needs proofreading. authors responded to reviewers concerns about clarity issues and i believe all the concerns can be easily addressed in the revised version. overall, this paper provides a wellexecuted empirical contribution to crossdomain texttosql problem with positive empirical results. the graphmatching architecture proposed here can possibly inform and be extended to other tasks. therefore, i recommend this paper for acceptance.", "accepted": null}
{"paper_id": "nips_2021_l4DQWgjbZg", "review_text": "this paper proposes forster decomposition, a new analysis tool for learning linear classifiers and applies it to derive an algorithm that learns halfspaces with massart noise without the dependence on bit complexity which was necessary for previous algorithms. the contributions is very strong technically and addresses one of the classical problems in ml. while it has downsides, such as being technically rather complex and likely to be fully appreciated by a handful of experts, i recommend acceptance.", "accepted": null}
{"paper_id": "nips_2021_aXbuWbta0V8", "review_text": "all reviewers are positive about this work, and they believe the paper provides novel insights to the value equivalency principle. some issues are brought up in the reviews, most of which are satisfactorily answered in the authors responses we had private discussions. please consult their reviews for the details. some of them are  the paper is sometimes too rushed in its explanations. perhaps the authors can remove some of the derivations e.g., eqs. 1318 to an appendix, so that they can expand their discussions elsewhere.  some reviewers found the detail insufficient for reproducibility.  more clear discussion on the relation between bisimulation and value equivalence can be helpful. the revisions required to improve the paper are minor enough that another round of reviews is not required. therefore, i recommend the acceptance of this paper.  in addition to these comments from reviewers, i have some questions and comment that i would appreciate if the authors answer them in their revisions. these are not critical, so they are inconsequential to the decision.  the loss function in eq. 8 and 9 have summation over policies and value functions. but pve requires the exact match for all policies and values, for ve. requiring an exact match suggests that we need to have a loss that encourages the error to be small uniformly over values and policies. this means that we may need to consider the supremum over v and pi, instead of summation over them. the summation can be just too relaxed to impose the required equivalency. this might be more of an issue when the value and policy space is very large, e.g., infinite number of elements. in that case, a large error in a small say, zero measure subset of the valuepolicy space does not affect the loss at all, but it violates the value equivalency. if we agree that this is the rightbetter way to write the loss function, then the losses 8 and 9 might be written as 8 sup_pi in pi sup_v in v  t_pik v  tildet_pik v and 9 sup_pi in pi  v_pi  tildet_pik v. these show some similarities with vaml. vaml uses the bellman optimality operator, but we can easily have a version for bellman operator for a policy pi. in that case, with k  1, the inner optimization sup_v in v  t_pik v  tildet_pik v would be the original vamls, and for k  1, it would be a multistep extension of vaml which was not introduced in that paper though. 2 is there any typo in proposition 1ii? currently, we have mathbbmk there, but shouldnt it be mathcalmk? the same comment for proposition 3.", "accepted": 1}
{"paper_id": "nips_2021_FKCTeO1fsvH", "review_text": "this paper consider the sparse tensor pca problem, an interesting generalisation of the spikematrix model. it is a theoretical paper focusing on computational complexity, and the main results theorem 1, 2 discussed the performance of a family of algorithms that smoothly interpolates between polynomialtime and the exponentialtime exhaustive search algorithm. overall, there is a clear agreement on the reviewers side to accept the paper for publication at neurips. the consensus is that this is a good and solid paper, in terms of results and clarity. while the paper adapt existing algorithmic and lower bound techniques to the sparse tensor pca problem, it has been judged well written, and enjoyable to read. some of the reviewers actually increased their score after the rebuttal, acknowledging that the authors successfully answered their comments.", "accepted": 1}
{"paper_id": "nips_2021_dJcUhDVu1G", "review_text": "overall the majority of reviewers liked the paper and found the contributions interesting for neurips. i also went over the paper and i think it provides an interesting and worthy contribution to distributed algorithms, studying saddlepoint optimization problems under datasimilarity mostly motivated from statistical settings and providing lower bounds on communication and matching upperbounds. i recommend acceptance.", "accepted": null}
{"paper_id": "nips_2021_9Jsop0faZtU", "review_text": "the reviewers were generally positive about this paper  they liked the svd based approach to simplifying or initializing deep neural networks for graph representation learning, and appreciated the positive empirical results. thus, the paper is recommended for acceptance. connections to prior work should be clarified in the revise submission  the abstract of the paper seems to imply that the idea of computing a partial svd of an implicit matrix is new. there is significant work in the numerical linear algebra community on matrixfree svd methods  in fact this is a critically important feature of nearly all krylov based and sketching methods for partial svd. see this link for a number of references to background literature httpsstats.stackexchange.comquestions159325whatfastalgorithmsexistforcomputingtruncatedsvd", "accepted": 1}
{"paper_id": "nips_2021_T3_AJr9-R5g", "review_text": "the paper suggests a network compression scheme which alternates, in training, between dense uncompressed phases and sparse compressed phases. the compressed phases work along the lines of stochastic iht. the papers merits are mostly in the experimental side. there is also a theoretical part which in my opinion does not directly explain the experimental success but is still somewhat relevant to the main idea. the reviewers seem to have done a thorough review and seem to slightly lean toward acceptance. given the importance of the field of network compression together with the merits of this paper, i am inclined to accept.", "accepted": null}
{"paper_id": "nips_2021_lS_rOGT9lfG", "review_text": "the expert reviewers for the most part appreciated the paper and were guardedly positive. the paper is commended for posing an interesting new question. at the same time there were concerns about the relevance of the estimand targeted as well as formal guarantees. the authors suggested possible ways to address this that should be incorporated into the paper. crucially, the permutation inference result would add an important aspect to the paper that would merit its acceptance. moreover, the authors should give a detailed discussion about their atet estimand and explain carefully its limitations in the absence of homogeneity, which is the practically common setting, and how possibly slight violations might affect the interpretation of the results.", "accepted": 1}
{"paper_id": "nips_2021_RSc-kfiLMNn", "review_text": "all four knowledgeable reviewers recommend accepting this submission. i agree. this submission makes a valuable contribution by demonstrating the invariance of image classifiers to diffeomorphism.", "accepted": null}
{"paper_id": "nips_2021_qpdc7sCpbi", "review_text": "this paper proposes a novel bandit algorithm for sampling neighbourhoods for gnns. the reviewers agreed that the work was of good quality, significance and originality. some reviewers noted, and the authors are encouraged to take on board, that the current write up is quite dense and exposition could be improved by addressing this.", "accepted": null}
{"paper_id": "nips_2021_iVL-2vJsy4e", "review_text": "the paper presents a refinement of nash equilibrium for cases where one player is rational and the other is not. some nice properties have been presented about the onesided quasiperfect equilibrium qpe. the paper is wellwritten with very valuable insights. most concerns raised in the reviews have been addressed by the authors. the contents are solid enough to justify for a publication, although the significance of the work highly relies on whether we can find practical scenarios of onesided qpe.", "accepted": 1}
{"paper_id": "nips_2021_WSykyaty6Q", "review_text": "this paper presents several interesting results concerning stochastic gradient methods on highdimensional leastsquares problems. the characterization of the role played by the momentum coefficient in terms of convergence is sharp. the new taskdependent algorithm matches the optimal complexity.", "accepted": 1}
{"paper_id": "nips_2021_KrAVI2AhNJh", "review_text": "three reviewers indicated acceptance, only one reviewer had concerns about the novelty with respect to reference 1. it turned out, however, that 1 is basically an unpublished preprint of this paper, and in their rebuttal, the authors could convincingly show that even with respect to 1, the paper contains some important extensions. so i recommend acceptance of this paper.", "accepted": 0}
{"paper_id": "nips_2021_5BVsfC0goqI", "review_text": "the paper introduces parameter and scale free algorithms for convex concave saddle point problems, based on the blackwell approachability algorithm. the resulting algorithmic schemes exhibit standard 1sqrtt regret, but in comparison to existing algorithms do not require the knowledge of the problem parameters. the techniques for proving convergence rely on results from conic optimization. numerical results on real and synthetic data demonstrate the efficacy of the introduced algorithms. overall, the reviews appreciated the novelty and the simplicity of the framework. however, there were several issues raised by the reviews relating to relationship with related work and the overall clarity. the authors should take these comments into account when preparing the revised version of the paper.", "accepted": 0}
{"paper_id": "nips_2021_-_D-ss8su3", "review_text": "the authors present two approaches for bounding constrained most probable explanation cmpe tasks in graphical models, in which we seek an mpe solution to one model, constrained to a subset of configurations by another model. both methods are simple in a good way, relaxing the cmpe to an unconstrained mpe problem, or to a multichoice knapsack problem, then using lagrangian optimization to tighten the resulting bounds. reviewers were unanimously positive, highlighting the novelty of the work as a strength. several reviewers did bring up points that should be addressed in a final version, however, including comparison to lpbased techniques, and some issues with the presentation see individual reviews for details.", "accepted": 1}
{"paper_id": "nips_2021_AcoMwAU5c0s", "review_text": "after the discussion and rebuttal process, all reviewers now recommend acceptance, albeit with several borderline ratings. several reviewers flagged limitations of the experimental evaluation, which the authors have responded to by running additional experiments. the discussion also cleared up some confusion about memory savings, which seems to have stemmed from a misinterpretation of figure 5. overall, reviewers seem to agree that the combination of implicit computation and implicit representations is interesting and creates some nice synergies, and also that this idea deserves the attention of the research community. i am therefore inclined to recommend acceptance, though this is conditional on the authors adding the new results to the paper and addressing the various other remarks and recommendations made by the reviewers e.g. with respect to figure 5.", "accepted": null}
{"paper_id": "nips_2021_Ir-WwGboFN-", "review_text": "the paper studies the connection between the architecture of deep neural networks and their robustness to noisy labels, which hasnt been studied yet. a takeaway message is that a welldesigned architecture can help learn good representations even the training sample have label noise. theoretical and empirical analyses are provided. although reviewers rrs5 and nwj7 concern that the theoretical analysis is limited, the paper has certain merits in empirical contributions. all the reviewers agree that the paper is interesting. one reviewer commented that the work can be potentially very useful in several areas, such as network architecture search, training with noisy data, and representation learning, to which the metareviewer also agrees. since the paper brings a unique spark that may potentially enlighten other researchers and benefit the machine learning society, the metareviewer is happy to recommend an accept. we ask the authors to carefully take the useful comments from reviewers in the final version, e.g., some writings that look overconfident should be revised. we also suggest the authors review an important topic in learning with noisy labels, i.e., modelling the label noise r1, which has been exploited to correct loss r2, r3 and should be useful in designing and validating the architecture of deep models by only exploiting the noisy data. r1 yao et al. dual t reducing estimation error for transition matrix in labelnoise learning. in neurips 2020. r2 natarajan et al. learning with noisy labels. in neurips 2013. r3 liu et al. classification with noisy labels by importance reweighting. ieee transactions on pattern analysis and machine intelligence 38.3 447461, 2015.", "accepted": 1}
{"paper_id": "nips_2021_S9dwZk0EXB", "review_text": "the reviewers are all in agreement that this work ought to be accepted, and the largelyminor concerns were all resolved following the author feedback. there were no major points of discussion that need highlighting in this metareview, but i encourage the authors to carefully incorporate the reviewer feedback in the final version. as one specific point, i do agree that the word adversarial could potentially cause confusion, especially in the title. although the word has many meanings, it could serve as a disadvantage if the title gives potential readers the wrong impression. i leave it up to the authors whether to change the title, but ask that they consider the possibility carefully.", "accepted": null}
{"paper_id": "nips_2021_Ke9lCi1vGF", "review_text": "the paper generated a fair bit of discussion, and the reviewers agree that the paper has interesting approach and combination of arguments of escaping saddle point with compressed gradient is very elegant.", "accepted": null}
{"paper_id": "nips_2021_UlSjqPEkI1V", "review_text": "the paper presents a formal theoretical analysis of the utility of observing multimodal information over unimodal information from the perspective of empirical risk minimisation. the analysis is followed up with some empirical evidence seeking to confirm theoretical findings. the reviewers all agreed that the theoretical contributions were both sound and principled. there were a few concerns about related work eg. cpmnets and making things clear and precise that the authors should address. for example the characterisation of 35 as widelyused is odd to say the least. the main concern with this paper is with the experimentsspecifically the design and the observed results. it appears to be the case, as the authors themselves point out l265267, that the complexity of the function class has a pretty strong effect on the _significance_ of the results cf. table 3. the bold values are only significant when the ratio of sample size is 1.0; in all other cases, theyre within the reported standard deviations, and cannot be claimed to particularly significant. moreover, as reviewer vzfz points out with relevant citations, in practice, multimodal learning tends to perform worse. in the discussion, this is somewhat handwaved away as having to do with optimisation, but it would seem to be a pretty serious issue affecting the theory if it is something typically not observed in practice! a proper discussion of what the cause for this apparent contradiction is and how it might relate to the complexity of the function classsample size ratio in such cases would strengthen the paper. i would also urge the authors to incorporate the clarifications and edits in discussion with all the reviewers, especially reviewer esgn into the updated manuscript. if there such a thing as a conditional accept, this paper would be a prime candidate as it does have relevant contributions, but also leaves a few things to be desired before it can be classed a complete piece of publishable work. it would probably be reasonable to accept the paper on the merits of its theoretical contributions, but strongly encourage the authors to address the question of applicability, relation to contradictory results in practice, how this relates to the functionclass complexity vs empirical observations tradeoff they discuss esp sec 5.1.", "accepted": 0}
{"paper_id": "nips_2021_IUjt25DtqC4", "review_text": "this paper addresses the goal of modular information processing in deep networks in an interesting way, inspired by the architecture of function application in a pl interpreter. everyone agrees that this is an interesting approach, and the analogy to interpreters is useful. the results, especially after the response phase, support the ability of this architecture to generalize in interesting ways. it would be useful in revision to make sure to clarify some key points raised by reviewers such as what encourages the ni to learn sparse types.", "accepted": 1}
{"paper_id": "nips_2021_oAxm0Wz7Bv", "review_text": "all reviewers unanimously acknowledged the technical contributions in providing mathematical guarantees on neural networks with probabilistic specifications. the authors responses also addressed the reviewers concerns. its a clear accept.", "accepted": null}
{"paper_id": "nips_2021_NhbFhfM960", "review_text": "this paper proves on32 overparameterization condition for training twolayer neural networks when the activation function is smooth and the loss function is square loss. the majority of the reviewers are in support of accepting this paper. thus, i recommend acceptance. in the cameraready, the authors should make it clear that the comparison in table 1 is under different assumptions on the activation functions e.g., smooth activation functions vs. relu.", "accepted": 1}
{"paper_id": "nips_2021_Ua9Vi0QqwD4", "review_text": "the four reviewers thought this paper was above threshold for acceptance. they all found the idea useful and interesting. the author response also helped to clarify some issues raised by the reviewer.", "accepted": null}
{"paper_id": "nips_2021_GERI2kZ84V", "review_text": "the problem studies inverse problems with a partially unknown forward operator. the idea is to use a general variational expectationmaximization framework aided by a normalizing flow generative network. the paper is focused on blind seismic tomographic and shows very good performance. the comparisons to baselines was a bit confusing but we understand that there are limited prior works on this problem and that the proposed method will easily outperform methods that do not use deep generative models. there was a debate on this paper solving blind inversion versus that mismatch. in any case there is good novelty in the paper which contains novel ideas, is solid in a real and useful application and is very well written.", "accepted": 1}
{"paper_id": "nips_2021_xZvuqfT6Otj", "review_text": "three reviewers gave favorable scores, one borderline, and one negative arbj. the last reviewer engaged in a productive discussion with the authors, so we can expect the final paper to be improved. the paper addresses a important question and has been refined a lot since it was first submitted to icml 2021, so it deserves to be published now. the paper addresses a problem that is important and will be of interest to a broad audience. the issue is that in a system that uses multiple machine learning models, improving the accuracy of some component models may lead to worse results for the system as a whole. the paper explains the problem and some formalizations of it. although solutions are nor provided, illustrative examples and cases from the real world are described.", "accepted": 0}
{"paper_id": "nips_2021_tTeJejS8vte", "review_text": "the paper provides new geometric insights into acceleration under the euclidean squared norm based on parallelism and collinearity of query and auxiliary points. the reviewers and area chair found the conceptual value of this contribution somewhat hard to evaluate, possibly because it only covers the euclidean case and does not fully motivate the choice of lyapunov function. however, the value of the contribution is reinforced by the fact that the authors use it to provide the best known bounds for making gradient small in the proxgrad setting and to derive a number of alternative algorithms for making gradient small. this improvement testifies to the novelty and usefulness of the techniques in the paper.", "accepted": null}
{"paper_id": "nips_2021_nqutwR1WDBY", "review_text": "the reviewers are satisfied with the author responses and agreed with acceptance. the authors incorporate reviewer feedback and additional experiments presented in the rebuttal in the final manuscript.", "accepted": null}
{"paper_id": "nips_2021_0jHeZ7-ehGr", "review_text": "this seems to be a strong theoretical contribution to neurips, although both some of the reviewers and myself suspect it is less relevant for practical usage.", "accepted": null}
{"paper_id": "nips_2021_chuGnZMuye", "review_text": "this paper proposes a knowledge graph embedding method called cone, which can simultaneously model both hierarchical and nonhierarchical relations by embedding entities into hyperbolic cones with relations as transformations between cones. the reviewers agree that this is a strong paper, and the authors did an excellent job in their rebuttal in addressing any remaining concerns.", "accepted": null}
{"paper_id": "nips_2021_RcfJUrZzhoL", "review_text": "this paper considers imitation learning from demonstrations with varying optimality. the authors propose a framework to jointly learn confidence scores for demonstrations and a wellperforming policy. the reviewers find the research problem interesting. the strengths include a bilevel optimization formulation of the problem, promising theoretical results, and strong empirical results. there is some concern on the applicability of the proposed method to highdimensional complex tasks. however, there is a clear consensus among the reviewers that the paper should be accepted. it is recommended that the authors consider if there is a connection between this work and zhang, et al . causal imitation learning with unobserved confounders, neurips 2020.", "accepted": 1}
{"paper_id": "nips_2021_Y10GtvGEgR", "review_text": "this paper studies the learning dynamics in deep networks by making a novel observation regarding weight decay as mutually frozen weights and their role in generalization. the paper initially received reviews that tended towards rejection. the reviewers had difficulty understanding some details and were concerned whether the results will hold true in realworld settings. the authors provided a thoughtful rebuttal that addressed the reviewers concerns. the paper was discussed and all the reviewers updated their reviews in the postrebuttal phase. all reviewers switched their score to weak acceptance note one reviewer still has a score of 4 in the main review, but they have switched to 6 in comments. ac agrees with the reviewers and suggests acceptance. however, the authors are requested to look at reviewers feedback and incorporate their comments in the cameraready.", "accepted": 0}
{"paper_id": "nips_2021_VLQV2vqjLf3", "review_text": "this paper tackles the problem of visualinput simtoreal and tries to overcome the reality gap between images rendered in simulators and those from the real world. it presents an approach that, instead of building and running simulators that render costly highquality images, generates only lowdimensional states in simulation to learn the policy, and then learns encoders from and decoders to observations that handle sequential observations. this enables to train and transfer state dynamics from a source environment to a target environment with only highdimensional observations. it then proposes a mathematical derivation for the crossmodal unsupervised domain adaptation problem. the method is then evaluated on mujoco tasks openai gym and robot hand manipulation and compared to several gan, cyclegan and temporally stacked gan baselines, where it shows better domain adaptation and transfer performance. reviewer ndfm praised the paper and their minor comments were addressed. reviewer cbxrs most negative comment was about the limited impact of the work due to similarity with existing work, but the authors contest this review and justify their contribution as a combination of mathematical derivation of the problem as variational inference and the addition of sequential modeling of the dynamics as opposed to vaegan. reviewer 6qjy complained about missing realworld data evaluations to make this work relevant a claim disputed by the authors and reviewer ndfm, as well as missing comparisons to two recent sim2real techniques that adapt image and state to hidden state and use that to train the policy and that modeled sequences of states from observations the authors provide a clarification. review scores are 5, 5, 6, 8, average 6. reviewers cbxr score 5 did not respond to the rebuttal and did not update their scores but i believe they could promote their score. while waiting on this reviewer, i am therefore willing to challenge them and promote this paper to an acceptance.", "accepted": 1}
{"paper_id": "nips_2021_4vUZPUKZsr5", "review_text": "the reviewers unanimously believe that the paper should be accepted. the paper focuses on an important problem and some interesting ideas such as guided diffusion and the use of selfsupervised representations has been proposed. there are complaints about the quality of the baselines and the rigorousness of the evaluation. the highlighted results do show some artifacts, and the proposed approach sounds like a combination of several disjoint technique. hence, i recommend acceptance as a poster.", "accepted": null}
{"paper_id": "nips_2021_-h6Ldc0MO-", "review_text": "this paper gives a detailed study for outofdistribution generalization in kernel regression, going substantially beyond known results in this particular problem, which are both of independent interest and also may help serve as a baseline of understanding for similar results in other methods. some concerns did come up in the reviews both in terms of the framing  which seem to be generally resolved in the discussion phase, though please make sure that these are reflected in the final version of the paper  and in particular for the style of result in proposition 1, which is worth thinking a little further about. overall, though, this will be a nice contribution to the conference.", "accepted": null}
{"paper_id": "nips_2021_goEdyJ_nVQI", "review_text": "the paper proposes a practical algorithm called svag, which is shown to converge in the weak sense to the sde approximation of sgd, for even moderately large step sizes. authors demonstrate its performance via experiments on neural networks, and show that svag can diagnose if sgd and its sde approximation have similar behaviors. authors further investigate the conditions in which the sde approximation fails. the motivation of this paper is clear, and very timely. all the assumptions and the results are presented in a clear way. all reviewers agree that this paper is wellwritten and its results are interesting. authors should address reviewers suggestions in the final version of their work. specifically, authors should add a discussion on the limitations of the experiments and amend the statement about svag converging for small values of l, clarify the bound in li et al., 2019, and add more intuition about the importance of scale invariance.", "accepted": 1}
{"paper_id": "nips_2021_1dq2MVDXot-", "review_text": "this paper addresses the problem of selective rationalization, revealing that the usual cooperative rationalization paradigm suffers from model interlocking  this arises when the predictor overfits to the features selected by the generator thus reinforcing the generators selection even if the selected rationales are suboptimal. to sidestep this, the paper proposes a new rationalization framework a2r, which introduces a third component into the architecture, a predictor driven by soft attention as opposed to selection. experiments on two synthetic benchmarks and two real datasets demonstrate that a2r can significantly alleviate the interlock problem. this is a solid paper that proposes a simple strategy to solve an important practical problem with rationalizers. while reviewers pointed out some weaknesses lack of clarity and need for better motivation, no sensitivity analysis for the lambda coefficient, some of these concerns have been successfully alleviated in the rebuttal, with new results for dependency on lambda. therefore i recommend acceptance. i urge the authors to take into account the reviewers comments when preparing the final version.", "accepted": 1}
{"paper_id": "nips_2021_VeZQA9KdjMK", "review_text": "the paper proposes an approach for forecasting a single macroscopic time series which is the sum of several microscopic time series by forecasting the microscopic time series using a mixture of transformerbased seq2seq models and finally combining the forecasts. the reviewers agree that the paper proposes a pragmatic solution to a relevant problem, and demonstrates clear performance improvements in doing so. while the building blocks mixture models trained with variational inference, convtrans time series model are well know, the combination is novel and wellchosen to address the task at hand. the reviewers highlight the convincing empirical evaluation against sensible baselines, where significant performance gains were demonstrated. initial reviewer concerns around novelty and clarity were alleviated during the discussion period, leading me to recommend acceptance.", "accepted": null}
{"paper_id": "nips_2021_GitDcBlcg78", "review_text": "the paper introduces an efficient transformer architecture with a glance branch to model longrange dependencies and a gaze branch to account for local context. three reviewers recommend acceptance, highlighting that the idea is novel, the paper is wellwritten, and the results are solid. one reviewer, despite appreciating the intuitive appeal of the glance and gaze idea, recommends rejection primarily because of the discrepancy between the swin baseline numbers being compared with and the actual numbers in the swin paper referenced in the manuscript. however, the ac agrees with the other three reviewers and the authors based on their response that the comparison with swin transformers is fair, as the same setting was used to compare both models. the paper is technically sound, has an interesting idea, and the results are promising. the authors should carefully proofread the paper and add the discussion in the rebuttal to the final version.", "accepted": 0}
{"paper_id": "nips_2021_4bzanicqvy8", "review_text": "the paper proposes a new variational inference approach for goalbased reinforcement learning. the authors goal is to more directly tackle reinforcement learning problems where the goal is to reach a certain outcome, without resorting to manual reward shaping. similarly to some previous work, they define an inference objective which is maximize using techniques from variational optimization. different from such works, in their inference formulation they condition on the goal being reached, rather than using the reward function as pseudolikelihood. the reviewers consider the considered problem goalconditioned rl important and very relevant for the machine learning community. the resulting approach was considered novel by the reviewers, especially the extension of a framework similar to vice 3 to an offpolicy setting. the reviewers also considered the paper technically sound and sufficiently well supported by experiments and ablations that show the method outperforms e.g. sac, another method in this space. also, the reviewers consider the paper wellwritten.", "accepted": 1}
{"paper_id": "nips_2021_VsUQQkpEXgr", "review_text": "this paper proposes universal adaptive stochastic gradientbased methods for variational inequalities. it unifies several dual averaging algorithms, uses adagradtype stepsize to achieve universality and adaptive results, and shows asymptotic convergence of the last iterate. i agree with the reviewers that the technical contributions are interesting and novel, and i am happy to recommend acceptance.", "accepted": null}
{"paper_id": "nips_2021_9SD2Rb3NiWu", "review_text": "very interesting solid work with potential impact. please take all comments in consideration.", "accepted": 1}
{"paper_id": "nips_2021_FGAi8TP3ShV", "review_text": "this paper provided the justification of the implicit differentiation in the nonsmooth settings with conservative jacobians. the proposed method is also compatible with the standard ad. the authors also established the connection of the conservative jacobians with the clarke jacobian. the authors finally apply the results to multiple machine learning problems, e.g., deep equilibrium networks, optimization layers, and bilevel optimization. the paper is wellwritten and easytofollow. i believe this paper will be of interest to the wide range of neurips community. there are still several minor issues need to be address, especially the discussion to the related work in literature reviewer gmrk, 2ovw and more comprehensive empirical experiments reviewer ealv.", "accepted": null}
{"paper_id": "nips_2021__MQBBpJzoZd", "review_text": "all of the reviewers agree that the paper presents a novel approach to modelfree bayesian rl that distinguishes itself from previous work with a clear theoretical construction and empirical evidence to support the approach. given the potential importance and interest to the community, i recommend acceptance as a spotlight.", "accepted": 1}
{"paper_id": "nips_2021_9fHO2sjdZq8", "review_text": "there was initially some spread in the reviewer scores and perception of this paper, but the reviewer consensus did lean towards acceptance in the end. the additions proposed in the rebuttal are quite extensive but written out in sufficient detail that the reviewers also believe the paper can be accepted. please, carefully pay attention to the reviewer comments and make sure to include the promised changes in the cameraready version. in terms of additional experiments, including the results you presented during the rebuttaldiscussion phase is sufficient.", "accepted": null}
{"paper_id": "nips_2021_vIDBSGl3vzl", "review_text": "this paper presents a modelbased policy optimization method that reduces the frequency that unsafe states are visited. all three reviewers recommend acceptance, one strongly. the primary concerns of the reviewers centered around whether the proposed method would extend to interesting problem settings. after the discussion, the reviewers were all convinced that the covered settings are of interest, and the extension to even more settings stochastic transitions may be feasible as an avenue for future work. the ac recommends that the authors work to improve the clarity of these points in the paper to ensure that future readers do not run into the same points of confusion that tripped up the reviewers.", "accepted": 1}
{"paper_id": "nips_2021_sIDvIyR5I1R", "review_text": "the paper studied the generalization of adversarial attacks induced by generative methods. the authors found that, by maximizing the difference of midlevel features of neural networks between the clean and perturbed images, the generated perturbation can be better transferred to another task setting such as a different data distribution or a different network structure. the finding is interesting and useful to the community. however, there is still a gap between the attack performance under the whitebox and blackbox settings. furthermore, the proposed attack is effective against hgd and rp defenses, but not effective against pgd and featuredenoisingbased defenses. we suggest the authors discuss these in more detail in future revisions.", "accepted": null}
{"paper_id": "nips_2021_t6EL1tTI3D", "review_text": "i believe there is a strong consensus around this papers novelty and its contributions, and on that note it is also a good chance to thank the reviewers and authors for the informative dialogue. from my own reading i can add that while the problem is well motivated, the exposition in various parts and the notational conventions make the paper less accessible than it can be and this is something that can be improved with the final version i hope. it may also be worth commenting further on the nature of the histograms and the relevance to ones seen in various modeling instances, e.g., mixture models, to ensure these are not perceived as just esoteric oneoff examples.", "accepted": null}
{"paper_id": "nips_2021_21uqYo8soks", "review_text": "the paper seems strong for both theoretical and practical point of view. there were few issues with the paper, but it seems that most of them were solved during the rebuttal. the argument used for the boundedness of the iterates needs to be made more rigorous and included in the cameraready version.", "accepted": null}
{"paper_id": "nips_2021_KJ5h-yfUHa", "review_text": "the reviewers unanimously recommend an acceptance. they acknowledged that the proposed attention bottleneck module is simple and demonstrated to be effective by extensive experiments and ablation analyses. they also appreciated an empirical exploration of different fusion mechanisms in transformer models. the initial reviews raised some concerns about insufficient experiments and asked clarifying questions, and the rebuttal successfully cleared up the questions. overall, this is a nice paper addressing an important problem of modeling multimodal data and proposes a simple method based on transformer architectures, which is a timely topic for this conference.", "accepted": null}
{"paper_id": "nips_2021_M-W0asp3fD", "review_text": "this paper analyzes the step decay schedule constant and then cut for nonconvex optimization problems, showing that it can find an approximate first order stationary point in oln tsqrtt rate. most reviewers found the result interesting and that it gives a better understanding of the step decay schedule. there are some concerns that should be addressed in the revision 1. clarifying why the requirement on boundedness of f can be replaced by efx_t  fx and why the latter expectation can be bounded in natural cases; 2. detailed comparisons with previous results acknowledging that similar or even better rates were achieved by different algorithms in all the settings.", "accepted": null}
{"paper_id": "nips_2021_BKeJmkspvc", "review_text": "this paper studies communication efficient estimators for the distributed mean estimation problem. the paper presents two methods that exploit either spatial or temporal correlations of the data, with the goal to improve communication efficiency. the mse of the two estimators is studied analytically, and numerical benchmarks demonstrate that the proposed techniques can improve the communication efficiency in distributed learning. the reviewers spotted a few inaccuracies in the proofs, but following the authors response they believe that these issues can be addressed in the final version. on the one hand, the reviewers emphasized the simplicity of the method, but on the other hand, they found the contribution to be slightly incremental. at the end, the good experimental results were the deciding factor in the discussion. the reviewers believe that this work inspire future work including attempts to address current limitations regarding practicality in distributed settings and will be of interest to the community. the authors are strongly encouraged to take the reviewers feedback into account when preparing the final version, including the already proposed changes, and perhaps also to include a discussion on lower bounds from an informationtheoretic perspective as guides for followup work.", "accepted": null}
{"paper_id": "nips_2021_V5prUHOrOP4", "review_text": "the submission tackles the problem of metalearning on a multimodal task distribution. it introduces an analytical methodology inspired from recent work on transference in multitask learning and proposes a new multimodal metalearning approach called kernel modulation kml which is claimed to outperform competing approaches. reviewers found the problem tackled to be important to the research community and the transference analysis to be an interesting and valuable contribution borrowed from the multitask learning literature. they expressed reservations about the way in which the contributions were framed in that the transference analysis feels disconnected from the kml contribution and how many important details are relegated to the supplementary material. however, given that the authors were very responsive and open to incorporating their feedback, the reviewers and i feel positive about acceptance, if the changes promised are incorporated in the final version see summary here httpsopenreview.netforum?idv5pruhorop4noteidl5jmuhlsrah.", "accepted": 1}
{"paper_id": "nips_2021__6DawVPqyl", "review_text": "this work proposes reducing the computational cost of running inference on high resolution images by leveraging a multiscale attention architecture. the method progressively employs higher resolution imagery to zoom in regions of interest that aid in discriminative training, while ignoring any computation on noninformative regions. the resulting model is tested on imagenet, fmow and cub2002011 against several baseline architectures, including saccader, dram, bagnet, and efficientnet and shows favorable performance in terms of computational cost versus accuracy. the reviewers did voice some concerns about the lack of testing on a high resolution imagery, which remain unresolved. overall, the reviewers were favorable with this work, and the concerns were notable, and did not rise to a significant enough level. this paper will be accepted to the conference.", "accepted": 1}
{"paper_id": "nips_2021_njIekVo3wLP", "review_text": "this work studies the question of learning an equilibrium in routing games and provides a smooth interpolation between static results with o1t2 convergence and worstcase results with o1sqrtt convergence rates. this is a relatively narrow contribution, but the reviewers found the results interesting and technically nontrivial.", "accepted": null}
{"paper_id": "nips_2021_f5liPryFRoA", "review_text": "this paper tackles the problem of partial domain adaptation. after considering the reviews, author rebuttal, and discussions the reviewers remained split on their recommendations with one accept, one borderline accept, and one borderline reject. 2hkg found that the work contributed a brandnew idea combining reweighting and adversarial learning and ksrv increased their recommendation from borderline reject to borderline accept postrebuttal citing that the authors had given enough results to demonstrate the effectiveness of the proposed model. the ac has reviewed the weaknesses brought up by 3wdy and determined that the responses and additional experiments authors provided in the rebuttal are sufficient. the authors have in general provided many additional experiments and clarifying thoughts during the rebuttal and need to be sure to include these in the final paper.", "accepted": null}
{"paper_id": "nips_2021_9CPc4EIr2t1", "review_text": "this paper proposes a neural network classifier architecture based on neural odes against adversarial attacks. some of the reviewers have concerns on the experiments, while in the rebuttal the newly added experiments convince the reviewers. all reviewers finally give positive support to the paper. thus, i recommend accepting the paper and the authors should include the new experiments in the final version.", "accepted": 1}
{"paper_id": "nips_2021_OU4LL1qP3Dg", "review_text": "the authors formulate a new problem of predicting the probability paths of a forecaster over time as it obtains new evidence, and a new algorithm for doing so, based on the fact that forecasts are expected to satisfy a martingale property. reviewers agreed the problem setting and method were novel, interesting, and technically well executed, and gave scores of 5, 6, 6, 6. there were no major critiques raised, though reviewer mtu3 wrote a long and insightful review that raised some questions. the metareviewer is unsure about the eventual scope of application for this problem and method, but overall finds that the ideas are fresh, interesting, and well executed, so should be of interest to the neurips community.", "accepted": null}
{"paper_id": "nips_2021_DyE8hmj2dse", "review_text": "this paper presents new results for finetuning in the linear regime linear parameterization and linearized nn. while there is still a gap between the linear regime and practice, the reviewers and the ac believe this paper gives an important initial step towards building a comprehensive theory for finetuning.", "accepted": 0}
{"paper_id": "nips_2021_YCqx6zhEzRp", "review_text": "the reviewers are generally in favor of accepting the paper for its algorithmic and theoretical contributions on federated multitask learning. based on that, i recommend acceptance. however, please make sure to incorporate the reviewers comments and the rebuttal into the final version.", "accepted": 0}
{"paper_id": "nips_2021_bhCEPHQR0hB", "review_text": "according to the reviews, which are all in favor of accepting the paper although most of them only slightly, this is a solid contribution for which i recommend acceptance. the points raised in the reviews and discussed in the rebuttal e.g. concerning motivation and some of the debated technical details shall be taken into account when preparing the final version.", "accepted": 1}
{"paper_id": "nips_2021__WnGcwXLYOE", "review_text": "this paper introduces a family of multitask benchmarks for fewshot learning in nlp. it appears to be built on existing data with relatively lightweight new methodological contributions, but reviewers agreed that it represents a useful tool and a clear improvement over current standard practice. id urge the authors to add some discussion of the licences that apply to the datasets used. this issue came up only toward the end of reviewer discussion, and so wont influence our decision about the paper, but its not currently clear whether the required data for the benchmark is guaranteed to remain legally available or whether private firms can use it.", "accepted": null}
{"paper_id": "nips_2021_ZarM_uLVyGw", "review_text": "ive read through the reviews and responses as well as the paper. it seems like the reviewers found the paper very interesting the problem domain is compelling and underexplored, and the motivation of the model is clear. in other words, the reviewers generally felt the paper was wellpositioned to bring value to the field of abstract reasoning and planning, and the authors did a decent job at demonstrating this through benchmarks introduced here as well as showing some reasonable baselines fall short. however, there were some concerns, and i was borderline as far as whether these concerns are addressable through a minor revision in the final draft. it seems like a lot of the issues are on the stated limitations of the work the authors acknowledge in the discussion some of these limitations are properties of the domain they are working in. after looking carefully at the discussion and the paper, i believe the paper still has value, despite these limitations. we shouldnt detract good research in difficult domains, and the fact that theres more to say about this domain shouldnt make it weaker than a paper that purports to study reasoning in a blackbox environment like atari. however, i strongly assert that the authors make very clear the limitations of their benchmarks and conclusions draft from running models on these benchmarks in the final draft. i cannot conditionally accept the paper, so i am going to trust the authors on this.", "accepted": 0}
{"paper_id": "nips_2021_myJO35O7Gg", "review_text": "the reviewers agreed that this is a wellwritten paper that addresses a relatively unexplored facet of fair ml the impact of missing data on fairness. the setup considered in the paper is simple, yet amenable towards an interesting theoretical analysis. however, the paper also has limitations, particularly in terms of how the bounds can translate to fair imputation methods, as noted by reviewer ykyw. i agree with this concern, and wished that the authors had gone further than the results in the paper and discussed methods to ensure fairness in the presence of missing data. the reviewers also noted that the experimental results are on datasets where entries are artificially missing, raising concerns about the practical impact of the paper in setting where data missingness may be correlated with group attributes. i also found the claim our work provides the first known results on fairness guarantee in analysis of incomplete data to be a bit misleading, since there have been a few papers that at least discuss the topic e.g., missing the missing values the ugly duckling of fairness in machine learning by fernando et. al and httpsarxiv.orgabs1911.12587, albeit to a less theoretical extent. overall, the merits of this paper outweigh is limitations, and its publication will encourage more discussions on the impact of missing data on fairness.", "accepted": null}
{"paper_id": "nips_2021_wfGbrrWgXDm", "review_text": "the authors response has address concerns from the reviewers, and all reviewers agree on an acceptance. i encourage the authors to revise the draft by including the discussion and the additional experiments from the rebuttal.", "accepted": 1}
{"paper_id": "nips_2021_n-FqqWXnWW", "review_text": "although there is some variance in the reviewer assessments about the paper, most reviewers are positive and think the work nicely integrates a set of techniques for positional encoding that can capture relative positional relationships. the work evaluates these techniques in several distinct tasks, including vision, language and speech. as reviewer mldf commented it the technique seems to help on speech and vision, but not so much on machine translation. i would want to thank the authors for providing extensive additional experimental results in the rebuttal, which strengthen the paper. the reviewers have provided many great points for improving the paper, which i hope the authors can include them in the revision of the paper.", "accepted": 1}
{"paper_id": "nips_2021_iZDMbX1W8AV", "review_text": "all reviewers have agreed that the paper is nicely written and presents a proper solution to the problem examined given the fixes in the proof agreed with the authors in the discussion phase, and hence all of them recommend acceptance. on the other hand, there is also an agreement that the paper lacks proper motivation, as none of the examples mentioned by the authors including the ones in the discussion properly fit the framework provided. the paper would be an excellent fit to the conference with a proper motivation, and the authors are highly encouraged to demonstrate the existence of a realisticlooking problem where the framework is indeed applicable otherwise this will remain a purely mathematical contribution.", "accepted": null}
{"paper_id": "nips_2021_j2gshvolULz", "review_text": "this paper had thorough reviews, rebuttals, and discussion. the reviewers all agreed that the work presents a very interesting analysis and set of conclusions, along with some simple methods to improve results of mocolearned representations. while individual contributions are simple and not necessarily novel from a technical perspective, as agreed by the reviewers the set of narratives, experiments, interesting findings, and methods globally provide an interesting novel perspective to the community. analysis of invariances and especially dataset types e.g. objectscenespecific is indeed interesting and one that may exist as common knowledge but has not been thoroughly and rigorously analyzed. in other words, while the paper is largely an empirical investigation it is wellexecuted in making claimshypothesis and resulting analysis. one of the main concerns expressed by multiple reviewers is the lack of a strong connection and holistic perspective offered across the two sections of the paper. in the rebuttal, the authors provided several arguments and expanded this narrative and connections, which the reviewers were satisfied with. the authors should comprehensively incorporate this and other great suggestions made by the reviewers into the final version. while not mandatory, one of the remaining weaknesses that have not been addressed is the applicability of the findings to other selfsupervised learning methods. we encourage the authors to add this if at all possible, as it would significantly increase the impact of the paper.", "accepted": null}
{"paper_id": "nips_2021_Goz-qsH1F14", "review_text": "this paper identifies a new concern in machine unlearning, the role of adaptivity in the request sequence. specifically, if removal requests may be adaptive, then the unlearning guarantees may be violated. the authors also give a differential privacy based method to mitigate this issue. this is an interesting new phenomenon, and the paper should be accepted. the authors are suggested to pay attention to the presentation comments made by the reviewers as machine unlearning is a relatively new field, it is important to make the early papers as well written as possible. as one minor personal comment, i disagree with the authors response that a drop from 97 to 91 accuracy mnist k6 is relatively small, i would say this is significant when the error rate is so small.", "accepted": null}
{"paper_id": "nips_2021_bGVZ6_u08Jy", "review_text": "in this work, the authors theoretically study representation learning on mdps with low bellman error and lowrank mdps. the main concerns from the reviewers are about the polynomial dependence on n in the main results and the lack of detailed comparisons with the existing works. we suggest that the authors can further improve their theoretical analysis and add more detailed discussions in their next version.", "accepted": null}
{"paper_id": "nips_2021_nJUDGEc69a5", "review_text": "this paper presents an approach for optimizing the intrinsic parameters of spiking neurons, and tests it within a teacherstudent paradigm. the initial reviews were generally positive, and reviewers agreed the paper was technically sound and interesting, though there was some concern about the paradigm used for evaluation. however, after the authors rebuttals, and some discussion amongst reviewers, a consensus was reached that this paper merits acceptance at neurips.", "accepted": null}
{"paper_id": "nips_2021_NKNjbKb5dK", "review_text": "bringing multiple shooting, timeparallel methods for odes to bear on improving inference and training of deep architectures based on neuralode is a strong technical and conceptually novel contribution. as such, the paper may be expected to generate interest in both ml and numerical methods differential equations communities. the reviews do ask for improved clarity in presentation and problem formulation in the final version.", "accepted": null}
{"paper_id": "nips_2021__H7TNRQQeH8", "review_text": "the original submission was missing an essential comparison with lip2wav. this has been added in the rebuttal period, and all reviewers now recommend acceptance. the authors should include the lip2wav comparison and the missing references highlighted by the reviewers in the final version.", "accepted": null}
{"paper_id": "nips_2021_4h4oqp-ATxb", "review_text": "all ratings were accept. the reviewers raised various suggestions which the authors responded to thoroughly, with additional experiments that reinforced their results. this should be a solid contribution.", "accepted": null}
{"paper_id": "nips_2021_-t6TeG3A6Do", "review_text": "paper presents a method for intrinsic exploration based on learning a representation of observation that can ignore distractors using the dynamics bottleneck principle. optimality of the exploration bonus in the case of bandits is established. results on standard benchmarks used in the exploration literature are convincing. reviewers unanimously vote to accept the paper, which i agree with.", "accepted": 1}
{"paper_id": "nips_2021_j3fpZLKcXF", "review_text": "reviewers unanimously enjoyed this paper and thought it was a timely and impactful contribution to neurips, especially in reinforcement learning and robotics. reviewers especially highlighted the softbody model and the motion planning experiments. there were some questions about the clarity of the exposition, and so the authors are encouraged to address the issues raised by the referees before the camera ready version of the paper is due.", "accepted": null}
{"paper_id": "nips_2021_yKdYdQbo22W", "review_text": "the paper provides theoretical work on the invariance of positive definite kernels and rkhs. the obtained results are novel and of high theoretical significance. on the other hand, its practical meaning is not very clear in this paper. overall, the theoretical development on the invariance of kernelrkhs is significant enough and applicable to a variety of problems in the future. we judge the work is worth presented in neurips.", "accepted": 1}
{"paper_id": "nips_2021_a_f_NR8mMr9", "review_text": "after reading each others reviews and the authors feedback, the reviewers discussed the merits and flaws of the paper. the authors feedback was instrumental in the reviewers change of mind. the paper is still borderline, but i trust the authors promises to revise their paper according to the reviewers suggestions. and i propose to accept this paper. i will check the final version of this paper hoping to find all the additions that have been requested.", "accepted": null}
{"paper_id": "nips_2021_nehzxAdyJxF", "review_text": "the paper provides some interesting theoretical results on estimating the worstcase performance of a model over all subpopulations of a given size, and has supporting experimental evaluations. the reviews generated a detailed backandforth discussion with the authors. the main concerns raised by the reviewers are about the lack of sufficient experimental results demonstrating the efficacy of the proposed method and the lack of practical guidance for choice of the group size parameter alpha. my impression of the paper is that the results presented would be of interest to the ml fairness community, and so i would recommend accepting it. however, its very important that the authors do a thorough job of incorporating the following changes in the final version of the paper in addition to the other changes that they have promised to make in their response  simulation study analyzing the asymptotic convergence of the proposed estimator  experiments illustrating how the parameter alpha should be set i would also like to emphasize that the reviewers were chosen from diverse backgrounds within ml to assess both the theoretical and practical aspects of the work, and i think they have done a thorough job of reviewing the paper. i understand that there were some initial questions about the correctness of theorem 2 which, to be honest, i too had during my initial reading of the paper, and i am glad that the authors were able to satisfactory resolve them. i trust that the authors will use all the feedback provided and make all the changes they have promised in their response.", "accepted": null}
{"paper_id": "nips_2021_RUQ1zwZR8_", "review_text": "the paper presents a method for adaptively setting the gradient clipping threshold for dp federated learning. the proposed method is mostly a combination of known techniques, but it provides a neat and wellpresented solution to an important practical problem. the reviews are mixed some reviewers are more positive because of more emphasis on the practical relevance while others are more negative because of emphasis on limited technical novelty. the ac is in favour of accepting the paper because of its practical usefulness and potential impact. in my opinion, the authors have been able to answer the criticisms of the negative reviews in their rebuttal. unfortunately these reviewers have been unresponsive in the discussion and thus the reviews have not been updated in light of the author responses, potentially leaving the reviews unnecessarily negative. while not reflected in the reviews, this paper was discussed extensively by the ac who read it in full and championed the work and the sac, leading to the decision to accept.", "accepted": 0}
{"paper_id": "nips_2021_UJw7jgbLgS", "review_text": "this paper analyzes the reason for adversarial transferability between models, and proposes an algorithm to reduce the transferability between base models. all reviewer agree that this is an interesting topic and the paper provides theoretical insight on the transferability problem. on the other side, the experimental analyses need to be strengthened. what are the additional costs for the reduction of adversarial transferability? overall, this is an interesting paper. i recommend accept.", "accepted": null}
{"paper_id": "nips_2021_d7skOEQClK", "review_text": "the manuscript has been reviewed by four experienced reviewers, all of whom acknowledged the contributions of the submission and recommend acceptance. specifically, the reviewers agree that the manuscript is easy to follow and the proposed method is novel. since there is no attempt to reject the submission, there is no basis to overturn the consensus. the ac thus recommends an acceptance.", "accepted": null}
{"paper_id": "nips_2021_BbSPfmZqs4B", "review_text": "the paper studies model selection in contextual bandits and related problems and the main result is a lower bound showing that proper algorithms cannot obtain the model selection guarantee conjecture in fkl20. a consequence is that the second order bound conjecture by freund in a colt open problem is not achievable. the result is very interesting and after a careful reading along with discussions with the reviewers, we believe it is correct. it is clearly a very important result and represents significant progress in our understanding of online learning and bandit problems. as such the reviewers and i agree the paper should be accepted. however, the paper was very hard to read, particularly regarding the proof of the lower bounds. my thoughtssuggestions here  i do not think the calculations for wrapping up the proof are very illuminating, e.g., paragraph block labeled proof of theorem 3 and proof of theorem 4. i would prefer to see these replaced with a more qualitativeconceptual explanation of what is going on in the proof. obviously the calculations should be included in the appendix, but i feel something more conceptual might be helpful to convince a reader.  i found it helpful to think about what an algorithm might do to convince myself. for example, for theorem 3 the algorithm basically has two options 1 it can choose to play only arm ik or 2 it can essentially explore uniformly in arms k1 for n_max rounds. in the former case, the environment will not switch, which is a contradiction of the algorithms guarantee, while in 2 it incurs a lot of regret due to lemma 1. then you can do the calculation showing how you set delta to ensure that both of these cases work out to confirm the theorem. this is not a proof but i think provides more intuition than the calculations provided at present. for the stochastic lower bound i think it is worth highlighting the following steps this is also how i understand the proof  a proper learner means that there is no cherrypicking on contexts, so the statistical lower bound is a passive learning lower bound  the construction is set up so that the lower bound part is essentially full information. in particular the algorithm always knows the loss for action 3 and the realized losses for actions 1 and 2 are coupled so that when you explore you are in a full information setting.  thus we can reduce to standard fullinformation passive learning problems. the flavor of such problem is closely related to work on estimating learnability 1 or tolerant testing 2. this is a very clever reduction and highlights the role of the proper learning assumption.  the specific learning problem being reduced to is essentially the problem of estimating sparsity in linear regression see also the part on learning dictators in 2, in particular where the sparsity is just 1. in fact i think lemma 2 is very similar to theorem 4.5 in 3 although that theorempaper is also quite hard to read. it would be great to mention this, so that readers who know this work or who trust that result can quickly convince themselves that your result is correct.  actually the results in 3 demonstrate something quite interesting that i think would be worth pointing out here. if i understand it correctly, their results show that a slightly different construction does not completely work for the model selection lower bound. suppose that the means are given as in your construction, but rather than bernoulli noise, the noise is gaussian. then the statistic frac1n sum_i ell_i,1  122 actually provides a better testing sample complexity bound than the lower bound in lemma 2. the key here is that the variances under both hypothesis are the same, so the difference in mean can be detected by looking at the second moment. on the other hand, the bernoulli construction is closer to the unknown variance setting in 3 where the mean difference is washed out by changing the variance. you can see that in 3 the complexity of the known variance and unknown variance problems are actually different.  this is more or less how i convinced myself that the proof is correct. at a high level, i think it would be great to rewrite the lower bound section to capture the following two properties 1 some handwavy way to convince an expert that the result is likely correct, without them having to read the technical details e.g., by citing related results and maybe following the recipe above, and 2 some intuition based more on algorithmic considerations which i think are easier for readers to understand conceptually. finally, regarding proper vs improper algorithms. the reviewer raised a concern here on the actual definition but additionally there are many contextual bandit algorithms that are formally improper e.g., mixing uniform exploration makes an algorithm improper. additionally, maybe you can use the lower bound argument on active tolerant testing of dictators in 2 to actually get a lower bound against improper algorithms? otherwise i think it is fine to conjecture this, but i might slightly tone down the claims about solving the open problems in fkl20. if i understand it correctly, the present paper formally resolves open problem 1 and 1a, 1b in fkl20 but does not formally resolve open problem 2 due to the properimproper learning issue. i am aware of some some folklore constructions that critically rely on cherrypickingactive learningimproperness to obtain model selection guarantees in some special cases, so i personally would be more hesitant to make these claims. in addition to my suggestions, i hope the authors incorporate feedback from the reviewers, who i know spent quite a lot of time studying the paper and understanding the details. references 1. kong, valiant. estimating learnability in the sublinear data regime. httpsarxiv.orgabs1805.01626 2. balcan, blais, blum, yang. active tolerant testing. httpwww.cs.cmu.eduninamfpapersactivetesting.pdf 3. ingster, tsybakov, verzelen. detection boundary in sparse regression. httpsarxiv.orgabs1009.1706 please also follow references to find the related papers to these. i know there are others.", "accepted": 1}
{"paper_id": "nips_2021_cwWfDHYpb1z", "review_text": "this paper proposes a novel approach to distributed dataparallel training, moshpitsgd. the approach targets systems where fault tolerance and robustness important, e.g., when some worker nodes may fail, there is load imbalance, or other sources of heterogeneity in the system. this is a setting relevant to practitioners applying distributed training. the reviewers agreed that the contribution is interesting and worthy of acceptance. while there were some questions and concerns raised in the initial reviews, the authorss responses largely addressed these. in particular, the new empirical results nicely illustrate the tradeoff between messaging rounds and resilience to node failures. it is important that the cameraready revision should include the points mentioned during the rebuttal to address questions raised by the reviewers to improve the clarity of the paper, especially around experimental setup and findings. we also strongly encourage the authors to include the additional experiments in the paper and make the experiments more focused on specific contributions e.g., the proposed matchmaking scheme in addition to highlighting performance of the method as a whole. this will likely involving movingadding material to the supplementary material.", "accepted": 1}
{"paper_id": "nips_2021_RQfcckT1M_4", "review_text": "this paper proposes a novel approach to an important and popular problem, and in doing so provides a natural but novel application of another important and popular line of work irm. the work is more theoretically justified than most in the area, and has reasonably satisfying experimental results. overall, it will make a fine contribution to the conference. to maximize that, please make sure to incorporate the outcome of the reviewer discussions  some of which seemed fairly illuminating  into the final version of the paper.", "accepted": 1}
{"paper_id": "nips_2021_48LtSAkxjiX", "review_text": "all reviewers agree that this is a solid contribution to neurips it tackles a relevant problem in the context of metalearning and does so by proposing a novel and interesting strategy inspired by the matrix sensing literature. the authors provide a thorough theoretical investigation of the setting considered, which yields insights also on previous work on the topic of linear metarepresentation learning. there are a few issues with clarity that authors have addressed during the discussion period and that will need to be taken care of in the final version of the paper. in particular, including material from the appendix e.g. plots concerning the behavior of burermonteiro gradient descent and adding further insight regarding the distinction between altmin and altmingds also explaining why this was not tested in the experiments.", "accepted": null}
{"paper_id": "nips_2021_ljOg2HIBDGH", "review_text": "this paper investigates crossmodal contrastive learning for semantic representations and finds that grounded language embeddings are more semantically coherent than ungrounded ones. the reviewers generally like this direction and the paper is wellwritten. the authors should really heed the feedback from the reviewers, however, and in particular the authors should make very clear that this is not a methods paper, but an analysis paper. occasionally the paper reads too much like it is claiming a new method, which is problematic because a the comparison to other models is lacking; and b the paper is not adequately positioned in the existing literature. there are many papers that have explored visuallygrounded semantic embeddings in the past, several of them already quite old, and this work should be positioned accordingly.", "accepted": null}
{"paper_id": "nips_2021_2vubO341F_E", "review_text": "this paper proposed a new framework to train vision transformers with token level supervision. while there were some debate over the novelty of the proposed technique in the context of previous methods mixtoken, cutmix, relabel, it was agreed that the proposed method is efficient and performant, and the extensive empirical studies in this paper could benefit the community. the authors also did well by providing convincing results on extra baselines requested by the reviewers in the rebuttal.", "accepted": null}
{"paper_id": "nips_2021_Fv0DPhwB6o9", "review_text": "this paper studies the problem of learning directed acyclic graphs via a greedy scorebased algorithm. specifically their algorithm works in two stages, first by estimating the topological ordering of the nodes, and then by pruning edges that do not influence the score. compared to earlier works, the main innovation is in using the bregman information as the score function. this allows them to show their algorithm recovers the ground truth network under various assumptions. this is a nice contribution, but the main point of contention among the reviewers was whether these assumptions were natural andor properly justified. in their response, the authors related it to causal minimality, however the reviewers felt that it seemed to be stronger. as a suggestion for the authors, it would be better if assumptions 4.3 and 4.4 were discussed in more depth and made more of a focal point of the paper.", "accepted": 1}
{"paper_id": "nips_2021_blzTEKKRIcV", "review_text": "3 of 4 ratings were accept. the lowest rating a 5 was most concerned about insufficient experimental evaluation of speedaccuracy tradeoffs, which was echoed by other reviewers as well. through the rebuttaldiscussion period the authors were very responsive and caused several reviewers to raise their ratings. my view is that the original submission was somewhat confusing and incomplete in a few ways, but that the core of the method and results are solid, and through the discussion period the authors have improved the paper enough to warrant publication. the confusion could also stem from the fairly high technical complexity in the work, which id encourage the authors to explain as clearly as possible in the final text. i do agree with the concern about speedaccuracy analysis, and id encourage the authors to do anything they can to address this for cameraready if the paper is accepted, however i dont anticipate any surprises.", "accepted": null}
{"paper_id": "nips_2021_D5APl1Yixnc", "review_text": "this paper studies decentralized policy evaluation for cooperative multiagent rl. the main approach of this paper is 1 reduce the problem to a minimax optimization problem, and 2 develop new algorithms for solving this minimax problem using gradient tracking and variance reduction technique. as pointed out by reviewers, the main concerns of this paper are that a the main technique contribution is mostly in optimization, while the paper currently mainly sells as a rl paper, without detailed comparison in terms of rate with existing optimization algorithms. b the strongly concave assumption lacks justification in the rl setting. except these, most reviewers are convinced that the paper is wellwritten, the results are solid, and this paper makes interesting contributions. we hope authors can address above concerns in the final version.", "accepted": null}
{"paper_id": "nips_2021_yxHPRAqCqn", "review_text": "this paper examines the convergence of stochastic gradient descent in strongly convex minimization problems. the novelty of the analysis is that the authors do not assume that the variance of the gradient queries is finite; instead, they consider heavytailed gradient noise models with bounded moments for some pin1,2  but not necessarily for p2 or higher. this paper received almost universally positive reviews during the review phase, and the only weak reject recommendation was changed to a weak accept after the authors addressed the reviewers concerns. as a result, during the committee discussion, a consensus was reached early on to make an accept recommendation.", "accepted": null}
{"paper_id": "nips_2021_i_Q1yrOegLY", "review_text": "all reviewers have agreed on the positive aspects of the paper, and its important contributions for tabular deep learning. there were significant additions by the authors during the review process, particularly extra experimental results, method motivations, clarification of experimental setups and hyperparameter tuning. it is important to reflect these in the final version of the paper. finally, as discussed, it would be an impactful addition to opensource the benchmarking framework.", "accepted": null}
{"paper_id": "nips_2021_QgX15Mdi1E_", "review_text": "the authors propose a novel transformer architecture for video recognition whose performance scales linearly with the number of frames as opposed to the usual quadratic scaling. to achieve this the authors restrict the time attention to a local temporal window, and introduce an efficient spacetime mixing procedure. the proposed approach offers competitive results in terms of accuracyflops tradeoffs on several popular video recognition benchmarks. the paper was reviewed by 4 expert reviewers and received borderline ratings. the rebuttal managed to address points raised by most reviewers, who found it a valuable contribution to the community. one reviewer maintains the view that the work lacks novelty in terms of technical contributions, and that more ablation studies are necessary. after considering the manuscript, the reviews, and the discussion, i felt that the work should be accepted for publication and that a minor revision is sufficient to address the raised criticisms.", "accepted": 0}
{"paper_id": "nips_2021_p5rMPjrcCZq", "review_text": "the authors propose a finetuning free structured pruning method oto. the idea is to first partition the parameters into zeroinvariant groups, pruning the zero groups, and solving for a structured sparsity optimization problem with projections. the experiment results on cifar10, imagenet, and squad show the competitive performance on flops and number of parameters reduction. the paper initially received a split review. i would like to thank the time and effort the authors and reviewers spent engaging in the active discussion during the rebuttal phase. i strongly recommend the authors to revise the submission and include the ablation study and the baseline comparisons the reviewers requested. specifically, please include 1. the ablation study of initialization stage vs projection stage 2. baseline comparison with related methods 1,2,3,4,5. 1 learning structured sparsity in deep neural networks, neurips 16 2 deephoyer learning sparser neural network with differentiable scaleinvariant sparsity measures, iclr 20 3 gate decorator global filter pruning method for accelerating deep convolutional neural networks, neurips 20. 4 operationaware soft channel pruning using differentiable masks, icml2020 5 lossless cnn channel pruning via decoupling remembering and forgetting. iccv 2021.", "accepted": null}
{"paper_id": "nips_2021__IY3_4psXuf", "review_text": "this paper proposes a new way to perform graph sampling by using small neighborhoods of points for training and testing. the resulting method enables very deep networks to be used on a range of graph problems. the reviewers agree that the method is simple, and that this is a merit. while theres some discrepancy over the usefulness of the theory, there seems to be consensus around the point of view that the presence of theoretical results adds to the paper.", "accepted": 1}
{"paper_id": "nips_2021__cXX-Dr7sf0", "review_text": "the paper presents a family of approaches for adapting pretrained models to a variety of changes to the model architecture, training data or other aspects of the training setup. given the cost of training models from scratch and the generality of the presented approach, this is a highly relevant piece of work with potential for impact. after the discussion, three reviews recommend acceptance and one review recommends rejection. the negative review points to certain issues with the current version, but doesnt convince me that these issues rise to the level of rejection. after considering both positive and negative points, i lean towards recommending acceptance. comments to the authors after reading the paper, i was left with the impression that clarity could be improved, and that there are places where details are missing. i see that some of these missing details and some additional experiments were supplied during the discussion with the reviewers. i would like to see these, together with any other reviewer feedback, incorporated into the final version of the paper.", "accepted": 0}
{"paper_id": "nips_2021_qPOeyokHXT8", "review_text": "the paper presents a method to transform a multitask mdp into a multiplayer game where agents act in related but slightly environments, and share information. several reviewers commented on the quality and clarity of writing, and while there were many specific technical questions, they were mostly clarified in the responses. one point of criticism from several reviewers is the lack of technical novelty. however, in the discussion, the reviewers also agreed on the importance of multitask rl and that the theoretical results are stateoftheart. as stated by two of the reviewers, experiments in common domains could be useful to demonstrate the practical applicability. i encourage the authors to consider this point in future work.", "accepted": null}
{"paper_id": "nips_2021_UZgQhsTYe3R", "review_text": "this paper was an exemplary case of the value of discussion between the reviewers and the authors. after multiple clarifications, additional experiments and even a code snippet examples, the reviewers agreed that the paper provides a valuable contribution to a problem of weak supervision in rl. i encourage the authors to address reviewers extensive comments and update the paper for the final revision.", "accepted": null}
{"paper_id": "nips_2021_jfDaBf8PAE", "review_text": "the paper proposes a fast minimumnorm fmn attack that works with different norm perturbation models and is robust to hyperparameter choices. some reviewers had concerns regarding the novelty of the work with respect to prior works e.g. ddn. authors clarified some of these concerns in the discussion period. overall i think the paper makes good contributions. i suggest authors to take reviewers suggestions into account in the final draft of their work.", "accepted": 1}
{"paper_id": "nips_2021_ekKaTdleJVq", "review_text": "this paper proposes a nodedependent local smoothing ndls algorithm to control the smoothness of every node. ndls provides a bound to guide how to control the extent of smoothness for different nodes. extensive experiments on seven realworld graph datasets demonstrate that ndls pipeline enjoys stateoftheart performance on node classification tasks, can be combined with any gnn models, and is scalable and efficient. the proposed ndls algorithm is novel and generalizes some existing smoothing techniques. the ndls kernel can act as a building block to replace other graph kernels and be combined with some existing models. the authors also provide some theoretical analysis of the space and time complexities of the proposed ndls pipeline. the paper is very wellwritten and easy to follow, with intuitive figures for better understanding the concept. ndls shows stateoftheart performance across various datasets. the results are nicely interpreted. thus the algorithm is robust and generally reliable in practice. therefore, we recommend accepting this paper.", "accepted": null}
{"paper_id": "nips_2021_2ybxtABV2Og", "review_text": "this paper proposes an nas approach namely bns for continual learning that builds dynamic network structures such that it minimizes catastrophic forgetting but maximizes forward transfer. the experiment results on several cl tasks show the effectiveness of the proposed method. overall, all reviewers found the paper to be well written and easy to follow, and focuses on an important research problem with relatively little attention from the area. based on the reviewer comments, the authors further provided missing details pointed out by the reviewers and some reviewers increased the review score. after the rebuttal, all the reviewers remained positive. i recommend accepting this paper.", "accepted": null}
{"paper_id": "nips_2021_xfskdMFkuTS", "review_text": "metareview of meta internal learning this paper proposes a framework for single image generation from the perspective of metalearning on larger datasets. the method uses hypernetworks as the generator and discriminator so that the weights of these networks can be conditionally adapted from the single image from a set of projections, and can be trained with adversarial and reconstruction losses. they discuss capabilities of the method such as interpolation and modeling of novel images. most reviewers agree that this paper is well written, easy to follow, includes solid analysis of the proposed methods, and also generally agree with the novelty. the fast inference can find important uses in realworld applications. the weaknesses are highlighted by the authors and discussed in detail with reviewers, and the authors have included several comparisons to previous work. as suggested by reviewer fnnf who had increased their score, the authors are strongly encouraged to better reorganize the manuscript in the camera ready version, in which the authors agreed to do so. for all these reasons stated, i recommend acceptance of this work.", "accepted": null}
{"paper_id": "nips_2021_SAPEODcpNvf", "review_text": "i agree with the two of the reviewers that this contribution is novel and interesting. while the empirical evidence presented here could and was improved in the rebuttals, i think that a sufficiently revised version of the paper as suggested in the rebuttals can be accepted for a poster at neurips.", "accepted": null}
{"paper_id": "nips_2021_Gl3ADZLz9ir", "review_text": "the paper studies the revenueoptimal mechanism design when the bidder value distribution can be adversarially corrupted, and gives tight bounds on the achievable approximation and the sample complexity. this is a nice contribution and a good fit to neurips.", "accepted": 1}
{"paper_id": "nips_2021_Rw_fo_Z2vV", "review_text": "the authors consider variational inference for a hierarchical model where each observation is generated using a localobservationspecific latent variable, which in turn depends on the global latent variable. they prove that using a variational posterior with the same dependence structure as the model results in inference accuracy comparable to a general joint posterior approximation at a much lower computational cost and allows observation subsampling for an additional speedup. experiments confirm the theoretical results and show that amortizing local inference enables training on much larger datasets without affecting accuracy. the paper describes a simple but effective idea clearly and shows that it works in practice. the discussion of related work is insufficient however, and should be extended to include the neural statistician and its descendants and their relationship to this work. also, as pointed out by a reviewer, the title of the paper is too general and should really be made more specific, as the method applies only to hierarchical models with a very specific, if common structure.", "accepted": null}
{"paper_id": "nips_2021_tW7L9dKZ0OM", "review_text": "contributing a unified api and benchmark for language grounding in symbolically represented environments can potentially be valuable for the community. such a benchmark can focus efforts, spark discourse about appropriate evaluation, and avoid bespoke, overengineered models from dominating individual tasks and obscuring more general solutions. reviewers all advocate for acceptance and the ac agrees. authors are encouraged to revise the paper to address reviewer concerns  especially to clarify howwhy these environments were chosen both these specific ones and why perceptual environments were excluded. further, the details on messengers performance included in the response would be useful for future readers as well.", "accepted": null}
{"paper_id": "nips_2021_96ULbah4DC", "review_text": "the paper tackles hierarchical learning in dynamic environments. dynamic environments induce nonstationarity from the point of view of higher level policies, as the effect of highlevel actions depends on the lowlevel policies that are learned concurrently. earlier work has tried to address this by relabelling past actions. however, in dynamic environments this is not sufficient, as background processes might continue further or less far dependent on how fast the lower level policy reaches its goal. the submitted manuscript investigates the use of timed sub goals to address this, having subpolicies attain their goals at a specified time. the reviewers are mixed on this submissions, with the reviewers having shifted towards a marginal accept average. the main points by the reviewers are  the main ideas are mostly considered relevant, interesting, and original  the execution of experiments and motivation of design decisions is technically sound. one reviewer wrote that the problem definition could be more precise.  the paper is well written  a main criticism with the original manuscript by a majority of the reviewers was that the chosen experiments insufficiently support the main claims made in the paper.   one of the main points is that the paper aims to address a common problem in hierarchical learning in all kinds of dynamic environments, whereas the experiments focus on very specific problems where precise timing is very important. while these are highly illustrative, they do not make it clear whether the method also brings benefits on problems that are dynamic but do not rely that much on very precise timing. the paper does show results are competitive on static mdps, with a small gap to methods specifically for that setting as expected.   furthermore, some concerns exists regarding the paper not being tested on common benchmark, the high variance in results, and the assumptions made.   the authors have since provided additional results on more standard settings, with hits outperforming sac on 2 out of 4 environments with results being practically the same as the baselines on a third environment, ball in a cup. one concern about the additional evaluations is that it is hard to understand what factors cause the difference in performance  e.g., is it really in the timing of the subgoal, or in some other aspect of the difference between methods e.g. reward structure for lower level policy, .... unfortunately, the authors reply came close to the end of the discussion period and so these aspects could not be fully discussed. taken together, i think that although the ideas are interesting and original, currently there are still some doubt about the thoroughness of the support of the claims of the paper. as an aside, one reviewer brought up an interesting point that the performance of timed subgoals might indicate that some form of communication is needed, but that other types of communication might work as well. it was mentioned as being unclear whether timed subgoal is indeed the best type of communication needed. this seems to be an interesting point for future work, although in the absence of other communicationbased strategies of the problem its not a comparison that i would have expected in the current paper.", "accepted": null}
{"paper_id": "nips_2021_86NHK__yFDl", "review_text": "the work proposed a test time training method to address the distribution shift between training and testing using online feature alignment and contrastive loss based ssl. reviewers overall see the paper well written and are content with the empirical evaluation. the rebuttal was able to convince some reviewers to raise their scores. i do agree with reviewer qjr7 that the contrastive loss based ssl task does not specifically address test time training, but rather is a generic approach to improve representation learninggeneralization, which is well established. focusing more of the discussion around the online feature alignment might shed more light on the new contribution of the work. as suggested by another reviewer, onthefly adaptation might be the more interesting setup to further distinguish the ttt setup from other setups facing distribution shift, such as domain adaptation.", "accepted": null}
{"paper_id": "nips_2021_xRLT28nnlFV", "review_text": "most of the reviewers and the ac agree that the submission makes a worthwhile theoretical contribution in designing and analyzing new algorithms for rsot and ribp. the reviewers highlight the following main concerns  the algorithm appears to be a simple normalized version of the sinkhorn algorithm for uot.  the technical novelty appears to be somewhat incremental, as the necessary techniques already appear in the ot and uot literature. in the rebuttal, the authors explain that the obstacle to overcome in the analysis of the new algorithms. they also stress the fact that no simple, very efficient algorithms were known for rsot and ribp before this work. the ac believes that the simplicity of the algorithm and the lack of other algorithms besides standard convex solvers makes this paper close to the acceptance threshold for neurips. a number of issues spotted by the reviewers and the ac could be addressed by the authors in a final version  the discussion of the significance and novelty of the convergence analysis of rsot should appear in the paper,  the experimental section would ideally include more varied experiments. in particular, larger examples would be nice to test the scalability of the method, even if it may not be possible to run cvxpy on them to obtain the groundtruth optimum.", "accepted": 0}
{"paper_id": "nips_2021_GEKTIKvslP", "review_text": "reviewers were unanimous in their assessment of the paper they all thought the paper addresses a practically wellmotivated question i.e., fairness considerations in allocating the costs of exploration in grouped mab settings; it offers a compelling formulation using ideas from the axiomatic bargaining literature; and finally, the subsequent analysis leads to intriguing insights. reviewers offered several suggestions for further improvement, including expanding the discussion of asymptotic analysis of regret and contrasting it with finite time horizon. i am confident that reviewers can incorporate these suggestions in their next revision of the work, and for those reasons, i suggest acceptance.", "accepted": 1}
{"paper_id": "nips_2021_DfWL8kIb0eF", "review_text": "this paper considers the problem of constructing convex surrogates for supervised learning tasks, and provides lower bounds for the convex consistency dimension, which is the lowest prediction dimension with a consistent convex surrogate. reviewers felt that paper was wellwritten, and that the techniques are novel and generally interesting. in particular, the authors use their framework to resolve open questions of frongillo and kash 2015 and dearborn and frongillo 2020. however, given that the topic is somewhat niche for the neurips community, the authors are encouraged to incorporate the reviewers suggestions to better convey the significance of their results to the broader community.", "accepted": null}
{"paper_id": "nips_2021_ohfi44BZPC4", "review_text": "this work brings together a number of algorithms to provide a method to detect the 3d location of cells from data collected with new highdensity electrode arrays neuropixels. all reviewers appreciated the analysis developed by the authors in this work. the main discussion centered around the scope of neurips with respect to engineered solutions to neural imaging problems. the authors, as well as some reviewers, pointed out the historical presence of such papers at neurips, as well as the stated scope. thus i am happy to recommend this paper for acceptance at neurips.", "accepted": 1}
{"paper_id": "nips_2021_5CGPY2VeEGb", "review_text": "this paper offers an adaptive equalization strategy for semantic segmentation to improve performance of underperforming or underrepresented categories. all reviewers found positive strengths in the paper, and recommended acceptance. the reviewers rebuttal provided additional information that were compelling and confirmed that this paper is acceptable, preferably as a spotlight.", "accepted": 1}
{"paper_id": "nips_2021_HnLDt9v6Q-j", "review_text": "there was extensive discussion on this paper. perhaps the largest concern among reviewers was the significance of the model fit though i believe the author response on this issue is satisfactory. while this paper ended up as a split decision among reviewers, some strong support and comments like the following particularly stood out to me the paper shows how a different kind of model can provide a substantially different interpretation than numerous other studies and the paper could encourage other researchers to study how the brain implements distributional rl to make everyday decisions. given that this paper proposes and defends a novel perspective in an interdisciplinary area highly germane to current neurips discourse also with high potential impact, i believe the paper has passed the bar for neurips acceptance.", "accepted": 1}
{"paper_id": "nips_2021_q4Dln9kWFA0", "review_text": "this paper looks at the multiplayer multiarm bandit problem, with heterogenous rewards. this specific problem of reaching the centralized case with decentralized protocols was still open, while it was settled with homogenous rewards. this result requires combining additional, non trivial techniques from combinatorial bandits amongst others, hence hitting the acceptance bar.", "accepted": null}
{"paper_id": "nips_2021_45GfBQYtYlp", "review_text": "this paper introduces the problem of contextual recommendation, a variant of the linear contextual bandit algorithm in which a learner selects actions based on context in order to maximize reward, but rather observing the reward, observe the identity of the optimal action instead. the reviewers found that the results are novel and technically interesting, and in particular thought that formalizing the problem of regret for cutting plane algorithms and connecting this to contextual recommendation was valuable, and is likely to find further use. overall, this is a solid contribution albeit, slightly niche for the neurips community. the authors are encouraged to incorporate the reviewers suggestions and spend more time motivating the problem, as well as discussing practical aspects of their algorithms e.g., implementation.", "accepted": null}
{"paper_id": "nips_2021_V8PcLz1NoQ0", "review_text": "this paper adds learning curve information to popular nas benchmarks via a surrogate model. this is valuable to the community since it opens up current nas benchmarks to multifidelity search approaches. furthermore it demonstrates a compeling template that can be used for creating new nas benchmarks on new search spaces where it is not feasible to create tabular benchmarks due to the sheer size of the search space. during the review process a number of improvements have come about with the reviewers, especially when engaging with nyyv with respect to top architecture predictions. it is strongly recommended that the authors include this new information in the paper. in this vein, the paper should clearly mark the limitations of this benchmark and how nas researchers should consume this benchmark and interpret the outcomes of their search algorithms. this benchmark is going to be widely used by the community. so the quality of the software release and easeofusage is quite critical. please pay a lot of attention to making sure that the package can be installed and run easily via standard python package distribution channels like pipconda and the associated datasurrogate models can be installed in oneclick and there are plenty of examples to showcase how to use the benchmark.", "accepted": null}
{"paper_id": "nips_2021_pk4q0SD_r1X", "review_text": "this work improves electra language pretraining approach by introducing corrective language model task so the model can generate words and sequence contrastive learning so sentence representations are more informative. i agree with the reviewers that the authors have conducted extensive experiments to show that the approach is effective and the analyses are also very thorough and insightful, e.g., analyzing cosine similarities of sentence vectors. there are a few points that the original paper wasnt convincing, e.g., cocolm wasnt better than electra in large setup and only wordsimilarity is used to demonstrate the main point that cocolm has better language modeling capabilities. the new results added for the large and fewshot promptbased finetuning results convincingly addressed these concerns; hence, i recommend accept.", "accepted": null}
{"paper_id": "nips_2021_0Kb33DHJ1g", "review_text": "this paper proposes a method called adaptive graph diffusion convolution adc, which can adaptively decide the neighborhood size for each layer and feature channel. the proposed method can be plugged in to any gnn frameworks. the experimental studies have demonstrated the superiority of the proposed module. the reviewers were split in the beginning, with concerns about novelty etc. after receiving a strong rebuttal from authors, including additional experiments, two reviewers have raised their scores to reflect their satisfaction to the rebuttal. in the end, the reviewers have reached consensus to accept this paper.", "accepted": 1}
{"paper_id": "nips_2021_a1wQOh27zcy", "review_text": "the initial reviews raised several concerns about novelty, missing experiments, and analyses that provide insights. the rebuttal addressed most of the concerns; some reviewers raised their ratings after the rebuttal. overall, the paper shows how to combine existing ideas to tackle a challenging problem in a simple and effective manner. the reviews are generally positive, mainly due to the good empirical performance, but they are not overwhelmingly enthusiastic about the technical novelty. i carefully read all the reviews, rebuttal, and discussion, and also read the paper in detail. i tend to agree that the paper lacks novelty but must say the proposed approach is simple, effective, and performs well. i am recommending an acceptance, but wouldnt mind if it is rejected.", "accepted": null}
{"paper_id": "nips_2021_HfpNVDg3ExA", "review_text": "two reviewers advocate strongly for acceptance, one reviewer has been convinced to favor acceptance, and one reviewer recommends rejection. i agree with these first two reviewers about the novelty of the proposed method, and i shared their enthusiasm. i share the concern of reviewer jdmt about the omission of a comparison to convolutional ssms. because the authors empirical results are already fairly extensive and impressive, i do not favor rejection on these grounds, but i do encourage the authors to include a comparison to convolutional ssms in the cameraready version of their manuscript.", "accepted": 1}
{"paper_id": "nips_2021_JNSwviqJhS", "review_text": "the paper proposes a normalizingflow architecture that includes latent variables and is geared towards image modelling. the paper demonstrates good empirical performance of the proposed architecture at modest computational budgets. there is a clear consensus among reviewers about the strengths and weaknesses of the paper the main strength is the good empirical performance of the proposed architecture; the main drawback is the incremental novelty. overall, the paper seems a useful contribution for generativemodelling practitioners, so im happy to recommend acceptance.", "accepted": null}
{"paper_id": "nips_2021_REjT_c1Eejk", "review_text": "there was quite an extensive discussion between the reviewers on this paper after the author response. there is no doubt that the the paper proposes a simple in a good way fix to train gnns with ppo. so it makes a valuable contribution to the community. there was also agreement that the paper could potentially be made a lot stronger with additional work, e.g., the paper is mainly based on empirical evidence for the proposed method so evaluating freezing part by part reviewer pqsj would be very interesting, more comparisons to other work, etc. the main difference was in the weighting of these two aspects.", "accepted": null}
{"paper_id": "nips_2021_StbpmmlJbH", "review_text": "this appears to be a solid paper with strong reviews. the reviewer concerns were addressed with clarity and detail by the authors. i hope that any final version of the paper will incorporate improvements as a result of the reviewer feedback.", "accepted": 1}
{"paper_id": "nips_2021_AlD5WD2ANIQ", "review_text": "the paper considers a variant of the mab problem where there are n agents who may perceive the reward from the k arms differently. the goal considered in this paper is to obtain a fair distribution over the arms in terms of nash social welfare. this application is new, even if the algorithms and analysis do not seem to be that different from the standard mab literature. the classical lower bound is also shown to hold in this case.", "accepted": null}
{"paper_id": "nips_2021_hyJKKIhfxxT", "review_text": "the reviewers in most part recognise the noveltyoriginality of the presented approach. the experiments, through which a strong case is made for the scalability of the method to large number of agents, seem solid and competitive with state of the art. some concerns raised by reviewer t76m about the clarity of section 4.2 on metagradient subroutine which needs to be addressed in the final version.", "accepted": null}
{"paper_id": "nips_2021_AHFfYwM7WGP", "review_text": "the paper presents residual2vec, a technique for reducing structural biases for graph embedding. existing methods use random walkbased sampling, which has a strong preference towards hub nodes. the paper draws an insightful analogy of how debasing effectively happens in negative sampling in word2vec, which inspired the authors to develop residual2vec to compensate structural biases in random walks. the reviewers are overall happy with the paper, with questions and concerns mostly regarding the presentation of the work. these questions are mostly clarified during the rebuttal, which the authors should include in the final version of the paper. the work makes a useful contribution for learning effective graph node embeddings with less dependency on node degrees.", "accepted": 1}
{"paper_id": "nips_2021_0O5jpovbdHO", "review_text": "this paper proves the equivalence between multiclass logistic regression and multiclass svm, based on which it proves the benign overfitting for multiclass interpolator. the main concern from one of the reviewers is that the results derived for multiclass benign overfitting are worse than using the standard technique i.e., converting kclass classification to kk12 1 vs. 1 binary classification problems. the authors have made a great effort to address the reviewers questions and concerns. after the author response and reviewer discussion, the paper gathers enough support from the reviewers. thus, i recommend acceptance.", "accepted": null}
{"paper_id": "nips_2021_k_w-RCJ9kMw", "review_text": "all the reviewers agree that the paper has some nice and interesting ideas and it is indeed quite promising that the proposed method gives minimal accuracy loss while providing interpretability. having said that, a majority of them felt that a more thorough analysis of the method e.g., ablation studies and evaluation on at least one dataset on the scale of imagenet is necessary to properly evaluate the utility of the proposed method. this is particularly so because of the large amount of existing work in this area and the limited novelty of the proposed method. i am sharing some of the key concerns and points that arose during the discussion period.  as a problem on which there is earlier work senn, prototypednn, restricting to smallscale datasets seemed limiting. the new results on cifar100 were nice to see, but results on one larger complex dataset tinyimagenet, cub, imagenet would have made it better to see the usefulness of the work considering the method by itself has no new components, its an aggregation of existing pieces.  the paper seems to be missing analysis ablation studies in the case of this work where the results are shown with choice of different layers, or choice of different epochs where loss functions are introduced. these are important decisions in the framework, and how they impact the final result would have been nice to see even if some of the choices did not yield strong results, and the method worked only for certain choices. update the authors response dated 2nd september indeed satisfactorily addresses the two main issues of i evaluation on a larger dataset and ii analysis of the effect of different choices in the proposed framework. since the reviewerac discussion had concluded by 30th august, this last authors response was not noticed at the time of the original metareview. given that all the reviewers agreed that the paper has interesting results, and that the two main criticisms have been satisfactorily addressed, i recommend acceptance of the paper.", "accepted": null}
{"paper_id": "nips_2021_h4es0CIohF", "review_text": "in this paper the authors use temporal context invariance tci to understand temporal integration in blackbox deep neural networks. the study is carried out using the deepspeech 2 model. the authors find that integratoin windows are specialized to different time scales in different layers, the higher the layers the more sensitive to longer time scales. in addition, lower layers are more sensitive to static windows while higher layers are more adaptive to duration of structures. all reviewers find the work interesting and novel in methodology. the hierarchical structure and pattern of response revealed in the experiments, despite in line with some previously observationsspeculations, are still quite interesting and intriguing. the method investigated in the paper also can potentially provide a new tool to the community to gain insights into the blackbox machine learning models. i would recommend accept. the authors should address the questions raised by the reviewers in the revision. if possible, please also include new experimental results on other model architectures to make the paper stronger.", "accepted": null}
{"paper_id": "nips_2021_VH2og5jlrzm", "review_text": "the paper proposes a new, fullybayesian method for online metalearning vcbml. a mixture models over the metaparameters is updated dynamically. this is used as an informative prior for the upcoming tasks. the reviewers highlighted the novelty of the method with respect to previous works. the paper is theoretically sound, and vcbml improves over known metalearning algorithms in practice. therefore, i will recommend to accept the paper. please take into account all the comments of the reviewers for the cameraready version of the paper. especially, reviewer h2cd pointed out a few problems in the writing of the paper and in the comparison with jerfel et al. 2019. the reviewer updated hisher evaluation of the paper following your detailed reply and your promise to revise the paper accordingly.", "accepted": null}
{"paper_id": "nips_2021_-U9I0f2S7W", "review_text": "after some discussion, theres general consensus toward accepting the work. as reviewer cpuf notes it is the first work that uses dynamic programming to search in a nontrivial search space that includes data parallelism, tensor model parallelism, pipeline model parallelism, and activation rematerialization. there are a number of challenges in finding parallelizable configurations, and while the papers specific ideas may have overlap with prior works or may be argued as incremental, it makes an important delta in solving this important problem area. flawwise, there are several experimental limitations that would be worth resolving as reviewer bm6l and cpuf note. i highly recommend the authors address the rebuttal concerns as they further polish the paper.", "accepted": null}
{"paper_id": "nips_2021_I1GHll1Z7E", "review_text": "originally there was a slight disagreement among the reviewers, with this paper receiving 3 positive ratings and 1 negative one. the reviewers acknowledge the interest of the studied scenario and that the empirical results are convincing. they nonetheless proposed several ways to improve the paper, by clarifying several aspects of the method and of the results, as well as discussing some additional works. the authors addressed these points in their feedback and managed to convince the most negative reviewer to raise their score. we therefore believe that this paper can be accepted to neurips but strongly encourage the authors to incorporate their feedback in the paper for the final version.", "accepted": null}
{"paper_id": "nips_2021_bDdfxLQITtu", "review_text": "all reviewers feel the paper is on the borderline but leaning towards accept. the metareviewer read the paper and feel the insight obtained from this paper is interesting. therefore, the meatreviewer decides to be conformal with the rest reviewers and suggest an acceptance of this submission.", "accepted": null}
{"paper_id": "nips_2021_P7GUAXxS3ym", "review_text": "this paper borrows insights from neuroscience and psychology, exploring how symbolic approaches can be combined with a largescale neural language models to improve responses because systems like gpt3 make mistakes that humans would not. this work can be viewed in several different lights evidence that the system1system2 insight is useful for ml systems and should be explored more; further evidence that hybrid ml and symbolic approaches are promising; a way to improve increasing coherence and accuracy blackbox ml systems for deployment. indeed the paper speaks to many of these views, but the reviewers felt that the paper does none of them convincingly. the reviewers flagged several issues including speed of the inference step, that this is not really a compelling instantiation  evaluation of the sys1sys2 concept, and the consistency in the parsing process. one reviewer was concerned that the work builds on and uses a closed source entry gpt3 for both practical and ethical reasons. several reviewers flagged useful related works whose discussion will strengthen the work. in the end all four reviewersall working in related areasall agreed on rejection. the main concern being scalability and generality of the proposed approach due to the need to handdesign design world models for each domain. the reviewers found the lack of general principles  advice offered for this critical step unconvincing. the experiments presented did not do enough to convince on this critical question. further, the discussion revealed the reviewers were not satisfied with this as a first stepthat too much was being left to future workand that nontrivial revisions are needed. besides these concerns the work is polished and he author response comprehensive. on balance, this work was close but just below the bar.", "accepted": 0}
{"paper_id": "nips_2021_vERYhbX_6Y", "review_text": "the paper proposes a novel method to perform classification with high imbalanced classes. the aim of the method is to minimise the fpr at high tpr  this is suitable for many safetycritical applications where a misclassification can have dire consequences. in order to solve the problem the authors formulate the classification problem as a constrained optimisation problem and introduce a constraint that can be used with existing loss functions to enforce maximal area under the roc curve auc through prioritizing fpr reduction at high tpr. the authors propose a new method and they do a got job at explaining it through the usage of a simple example. i really liked the simple example, i think it really helps the reader in understanding what is the objective of the proposed constraints. regarding the simple example, i think the cost of the constraint should decrease by frac39delta250frac3lambdadelta25 in the first case, and by frac19delta250 frac11 lambda delta25 in the second case, and not by 39delta23lambdadelta in the first case, and by 19delta2 11 lambda delta in the second case. further the method has been tested on a variety of datasets, with different ratios between the critical and the noncritical class. regarding the experiments, i have the following concerns  in both the binary case and the multiclass case the distribution of the train set and the validation set are different. indeed, we have highly imbalanced datasets for the training sets, but balanced datasets for the test sets. would the results be the same if the authors picked test sets that are as imbalanced as the training set? which is also what we expect to happen when we apply the method in the real world in the experiments on the mri dataset, what is the reason of picking just fpr0fn and fpr1fn? i think it would have been interesting to show at least fpr2fn, and maybe fpr5fn just to see how the performance varies when allowing for more false negatives  in the multi class experiments, why do the authors report the overall accuracy and not the auc score maybe averaged in different ways?  is there any reason why the authors did not try mbaucalm? regarding the readability, the authors did a good job in explaining the intuitions behind the proposed method, however, the section about the experimental analysis can be confusing  rho is introduced only at page 7, and the reader does not really know where this hyperparameter stems from. i would encourage the authors to include algorithm 1 in appendix to the main paper.  alm_m,1 and alm_m,2 are never formally introduced, and they are first mentioned at page 8.  the authors did not specify what does the bold and underlined in the tables mean. i assume that bold means that the almmethod is able to perform better than the method alone, while the underline highlights the best method. i would encourage the authors to better explain the above, and also to bold the results of the methods that are able to outperform methodalm otherwise the tables can be misleading  is there any reason why the avg auc in table 3 has no bold or underlined results? after reading authors comments the authors addressed all my concerns. i hope they will include the results showed in the rebuttal phase in the appendix of the final paper. i changed my score from 6 to 7. yes summary this paper proposes a new optimization approach to classification problems with class imbalance, where correctly classifying the minority class is more important than correctly classifying the majority class. one specific motivation is in medical imaging diagnostics, where misclassifying the critical minority class e.g., diseased is a more severe mistake than misclassifying the majority class e.g., healthy. the authors therefore use an augmented lagrangian method alm approach to minimize the fpr for the majority while maintaining a high tpr for the critical class in a binary classification problem, as well as suggesting an optimization formulation for multiclass classification tasks. the alm approach tries to enforce a margin between the critical and noncritical samples in the output space, and is structured to penalize a critical misclassification more than a noncritical misclassification. the problem is certainly important for medical diagnostic tools, as well as other problems where errors have different costs for different classes. the proposed method is a useful approach for these problems, and the theoretical analysis of the asymmetry and required imbalance of the problem is clear and interesting. the experiments show some benefit of the almbased approach and its ability to be coupled with many different loss functions. the writing is clear overall, but with various grammatical mistakes throughout and particularly in the supplement. my major concerns are the significance of the result and the setting of the cifar experiments. major comments 1. the paper does not compare with any aucoptimizationbased approaches apart from mbauc. in section 2, the authors say that ... all aforementioned auc optimizationbased methods were applied to linear predictive models and their performance on dnns are unknown. why not compare against any of these approaches? or is it not just that the performance is unknown, but that their application to dnns is nontrivial? 2. significance of the result. statistical significance is not included in table 1 and table 2. multiple replicates should be used to test the significance. some numbers are really close in table 1,2 and still marked in bold text better performance. avg aucs in table 3 have large standard deviation, which might not indicate significant improvement. instead of using selected thresholds 98, 95 and 90, it is more informative to show a tpr fpr curve plot. 3. how to predefine critical and noncritical classes? in binary classification, it seems that the minority class is always the critical class. is this always the case in realworld applications? in the multiclass setting with many classes, classifying each class into critical and noncritical seems to be difficult and needs domain knowledge. if a critical class is misdefined as a noncritical class, this might be problematic for many applications. 4. setting of class ratios in cifar experiments. since this paper is motivated by clinical applications, the prostate cancer example makes sense to me. however, in the cifar experiments, very large ratios of 1100 and 1200 are used. this ratio seems to be unrealistic e.g., a clinical dataset with 200 healthy patients and 1 sick patient. is it because the method can only have good improvement on such a very imbalanced ratio? i suggest the author to either show improvement on cifar with smaller ratios e.g., 15, 110 or find a realworld medical application that has such a class ratio close to 1100. minor comments 1. a slightly longer description of mbauc, such as the framework it uses or other ways in which it differs from the proposed method, would be useful. 2. in section 3, the authors say that other methods ...suffer from training instability and convergence. i think you mean nonconvergence? it would be useful to include a citation for the claim in section 4.1 that satisfying the constraint would directly ensure maximal auc. 3. in figure 4.2, given the precise setup of the line plot i think that it would be useful for the grid axes to go from 0, 5, rather than 0, 1. 4. in section 5.1, what is the justification for cifar10 and cifar100 to use validation sets that have 100 samples per class? why is this not also classimbalanced? 5. i find the tables difficult to read. first, when the original loss function achieves a lower fpr  higher test auc than the almoptimization version, i think those results should be bolded for clarity. given the quantity of bolded results, it is also difficult to distinguish the underlines of best result, particularly when it is not one of the bolded entries. also, given my understanding of what bold means in these tables, i didnt understand why the entry for alm_m, 1  ldam acc. critical class 1 was not bolded in table a5. 6. the details of the multiclass classification experiment were not fully explained in the paper. specifically, i was not fully clear that there was only one critical class and multiple noncritical classes in this problem formulation until section a.4. 7. i wish the authors provided slightly more discussion about the actual results, including why the alm helped more with some methods than others. one particularly interesting result for me was the performance of bce, which in theory does not take into account class imbalance, but was one of the bestperforming baselines and, in some cases, outperformed or was very close to alm  bce particularly on the cifar data. this wasnt the case on the mri dataset  is this due to an easier problem lower fprhigher tpr to begin with? weaker requirements on tpr? less extreme class imbalances? 8. i would appreciate some kind of study on the impact of class imbalance ratios to the improvement attributed to the alm optimization. this would help me understand more details about the kind of problem that the alm optimization is most suited to, as well as potentially answer some of the questions in the point above. 9. showing the  improvement by incorporating alm would also be a useful plot to include somewhere, and might be more readable than the tables. 10. tables a.3 and a.4 have less convincing improvements than the binary classification task and particularly table a.4. a little more discussion here to connect these results with table a.5 would be useful. one specific confusion with table a.5 was the low critical class accuracy  given the papers motivation, could you increase the required tpr? currently there is much lower accuracy on the critical class than the noncritical classes, which seems counter to the main goal. is this mitigated at all with a less extreme class imbalance ratio? in figure a.2, what is delta_n set to? given the prior discussion about delta_p and delta_n, this would be useful. also, is delta_e just the delta in the text? 11. the tables and table numbers seem to be missing from section a.10. 12. the most consistent grammar mistake was ... critical class . the singular possessive of class is classs. update  i raised my score after reading the response from authors. thank you for the detailed explanation and additional results. i dont have any concern about the negative societal impact of this work. this paper proposes a very interesting approach for training dnns on binary and multiclass classification tasks with critical underrepresented classes, which is very typical in domains like healthcare. in this approach, the task is posed as a constrained optimization problem and a novel constraint is designed that emphasizes on reducing the false positive rate while maintaining a high true positive rate, which is then added to the loss function and solved using an augmented lagrangian method. the main contribution of the paper is proposing a novel constraint optimization approach that can be added to any previous work, and that improves upon the state of the art results. originality  the proposed approach is definitely very novel and interesting. i am not aware of other work in this area that pose auc optimization as a constrained optimization problem.  the authors provided a very comprehensive and well organized related work section, and its clear how this approach distinguishes itself from the others. the related work is broadly split into 3 groups the first proposes variants of common loss functions with added emphasis on minority classes, the second focuses on different sampling strategies to improve the occurrences of the minority class samples in the batches, and the third focuses on scaling and thresholding model outputs post training. in the first group, another subgroup of papers focus on auc optimization. this paper falls in that subgroup, but differs from the other papers by proposing an approach thats scalable to large datasets and model architectures, and that can be used along with other loss functions. clarity  the paper overall is very well written. its clear, understandable, and very well organized. the code is also provided in the supplementary material, and the algorithms are very well explained in the paper.  it was very helpful that the authors included a toy example in section 4.2 to explain intuitively how their approach works.  one minor comment is, in figure 1, the axis labels should be false positive rate and true positive rate instead. quality  the paper is technically very sound. the proposed constraint was given ample intuitive and mathematical explanation, and using a toy example in section 4.2 and detailed theoritical explanations in the appendix section a.8 is very helpful.  the authors conducted very comprehensive experiments including the results presented in the appendix and provided compelling evidence for the improvements obtained by their approach by comparing directly with the results of several approaches from the related work. this approach improved the results considerably in most of the cases.  the authors also extended their work to the multiclass domain and provided the relevant results. significance  the proposed approach produced considerable improvements in empirical results over several state of the art methods. the main appeal of this approach is that it can be integrated with any other approach very easily. i think this will make it very easy for other authors to include it as a baseline in their works. this is a complete piece of work with a several important details provided in the appendix, and code required to reproduce the results provided in the supplementary material.  the proposed approach provides a unique way to reduce the fprs of classifiers operating at a high tpr. this is a very wide spread problem faced especially in the medical domain, and a lot of approaches do lead to a high fpr which impacts their practical usability adversely. this approach is definitely a very welcome addition in tackling that problem. weaknesses  it would have been helpful if the authors provided results in another public medical dataset like adni, etc.  it would be helpful if the authors include some discussion on how they plan to extend this work in the conclusion section.  one main limitation that the authors discussed is that they performed their experiments on the small mri dataset, which can make the results unreliable. they addressed this by using an ensembling strategy.  there is no obvious potential negative societal impact of this work. the authors further mention that fairness is one of the motivations of this work. this paper proposes a novel approach to train deep neural networks for classification tasks that involve high class imbalance, which is ubiquitous in medical problems. the novelty of the proposed method is very high, and related works are presented comprehensively. the paper is overall very well written and structured. the potential impact of the proposed algorithm is high given that class imbalance is a common problem in many realworld applications. reviewers raised major concerns regarding various aspects of experiments, which were successfully addressed by the authors. overall, this paper constitutes an important contribution to the field and passes the bar for the acceptance to neurips.", "accepted": null}
{"paper_id": "nips_2021_9-sCrvMbL9", "review_text": "in this paper, the authors consider the offline learning for constrained multiobjective mdp cmomdp. specifically, the authors proposed an algorithm which exploits the primal dual structure with pessimistic planning. the algorithm is extended for linear kernel cmomdp. moreover, the authors also provided rigorous suboptimality and constraint violation guarantees. although as most of the reviewers pointed out, the original version of the paper is lacking some related work discussion about offline rl and constrained rl, and empirical justification of the algorithm, the authors provided these parts during the rebuttal period, therefore, making the paper more convincing. i recommend acceptance for this paper. please take the reviewerss other suggestions, especially adding the intuitionsketch of the proof into main text, to improve the paper.", "accepted": null}
{"paper_id": "nips_2021_1ANcwXQuijU", "review_text": "this paper proposes a novel method for set prediction tasks, whose goal is to predict multiple elements without consideration of their orderings. a limitation of the existing set prediction methods is that they need to solve for the matching problem between the predicted and the groundtruth set under a certain distance metric, but the choice of distance metric is critical for the convergence property of the matching algorithm. to deal with this issue, the authors propose to learn a latent space and perform set prediction tasks in this space, which they refer to as latent set prediction lsp. lsp is beneficial over existing methods as it allows us to use simple euclidean distance, which eliminates the need of selecting a specific handcrafted distance metric, and enables efficient matching with teacherforcing. however, a naive lsp may not converge well due to the instability of the matchings across the set elements, and the authors propose techniques to allow stable pairing of the elements across two sets, further showing its convergence guarantee. the authors validate lsp on semantic scene description, multimodal report generation, and the object detection task, and the results show the effectiveness of the proposed lsp over relevant baselines. the paper received split initial reviews with three leaning negative and two positive. however, despite the negative scores, most reviewers found the paper wellwritten, the tackled problem important, and the discussion on the limitations of the existing set prediction techniques insightful. also, they considered the proposed set matching technique to be sound and nontrivial, and the provided theoretical convergence guarantee to be valuable. however, the reviewers had the following common concerns there exists an obvious gap between theory and practice, which renders the convergence property of the proposed method unclear in realworld scenarios, but there is no empirical analysis of convergence. the baselines used for the experiments are weak, and the proposed method is not validated against some highly relevant baselines for set prediction e.g. tspn and dspn. there is no ablation study of the proposed losses and techniques, which are essential in verifying their effectiveness. the experimental validation part of the paper is not well organized, and comes with different baselines and evaluation protocols. during the discussion period, the authors dealt away most of the concerns by providing experimental results with relevant baselines, providing the results of the ablation studies, and by providing the learning curves. the reviewers revised their reviews and increased their scores as they found the new results and discussions satisfactory, and reached a consensus to accept the paper. i also agree with the reviewers that the paper is proposing a highly original idea and sound methods to solve a wellmotivated problem, and that the theoretical analysis is nice. the only weak part was experimental validation, but this has been satisfactorily addressed in the author responses, and i believe that the paper will be in a very good shape after incorporating all the discussions and results into the revised version of the paper.", "accepted": null}
{"paper_id": "nips_2021_Hj_PxeC8CiV", "review_text": "this paper proposes an interesting and novel view on maxent rl it observes that sac may suffer from a feedback loop since the q values of already visited states tend to be greater than that of other states because the policy update increase the entropy. this in turn encourages the policy to visit these states even more. this defect is confirmed in a noreward environment and a method to address this is presented with encouraging experimental results. while there could have been more rigorous investigation into the claim rarely visited state have low entropy, the overall contribution of the paper is appreciated by most of the reviewers and the interesting take on maxent rl warrants acceptance.", "accepted": 0}
{"paper_id": "nips_2021_DZKsFQyDB9", "review_text": "the author response addressed most of the concerns of the reviewers and all reviewers recommend to accept the paper after the author response and discussion. i recommend accept with the expectation that the authors will revise the paper for the camera ready, addressing the reviewers concerns according to the author response, including, but not limited to the following aspects 1. add the mean and standard error to table 1, 2 and figure 4, 6 2. add results on pascal voc 3. add an extra ablation study on augmentations and batch sizes 4. add ablation study for mihai et al. and discuss it in the related work 5. move the discussion on sentence length from the appendix to the main paper 6. fix typos and make other minor writing improvements", "accepted": null}
{"paper_id": "nips_2021_LBhruMnhgIB", "review_text": "the metareview was written jointly by ac and sac after discussion with the program chairs. this paper studies how qualification disparities between groups can arise without structural differences between different groups. that is when optimal machine learning systems make decisions that impact people, how can these systems cause inequality to arise and maintain inequality when it is present? furthermore, when the system is modified to enforce standard fairness constraints, how does this impact long term inequality? overall, the topic is of extreme interest to the ml community longterm fairness of systems. the reviewers found the paper to be technically clear and impactful, and all recommend acceptance, two strongly scores  8,9. the one reviewer only weakly recommending acceptance score  6 did not engage in the discussion. however, while the papers technical contributions are crisp, several sociotechnical issues were identified by the sac in collaboration with ethics chairs and an additional ac. the decision is to conditionally accept the paper. the sociotechnical issues, raised in a separate comment that serves as a sociotechnical review, must be addressed. in particular, the following changes need to be implemented for the paper to be accepted these are copypasted from the sociotechnical review 1. review and update analogies drawn from evolutionary biology, so as to minimize the risk of essentialist misinterpretation. 2. connect technical definitions to some realworld examples, and discuss how the assumptions made in the paper align or not with those realworld examples. some of the key terms that require connecting to examples are qualification and subpopulation. the best approach would be to introduce a plausible running example that readers can use to understand the implications of the results. such an example would need to pin down what precisely is meant by qualification, how the groups subpopulations are defined, and what the classifier is doing. if no plausible example can be produced, then the authors need to walk back their statements about the applicability of the results to realworld sociotechnical systems. 3. be explicit about the operationalization of fairness and explain what the various assumptions and criteria around that operationalization mean in the context of realworld dynamics. 4. discuss how the replicator dynamics might be realized in realworld examples. 5. connect the proposed intervention to a clear motivation of mitigating harms to marginalized groups, and more explicitly clarify differences between this intervention and other approaches such as equalized odds and demographic parity in service of that overall goal. update the revisions submitted by the authors have been reviewed and the paper has been officially accepted.", "accepted": 1}
{"paper_id": "nips_2021_OMNRFw1fX3a", "review_text": "after the rebuttal most reviewers agreed to accept this paper. the remaining concerns centered around connections of the proposed evaluation method and algorithm to downstream planning and driving performance. the ac agrees that the paper would be stronger making these connections, but sees enough merit in the proposed work to accept the paper without it.", "accepted": 1}
{"paper_id": "nips_2021_NvN_B_ZEY5c", "review_text": "after the rebuttal phase, the paper now has only positive reviews, and as such could be accepted. the main issues found during the review phase are the somewhat unclear novelty and the insufficient experiments. after carefully reading the paper again, i need to add another concern the independence assumption of the pvalues. while the authors try to address this issue in section 5, their approach is somewhat limited  it is assumed that sufficiently far away pvalues are independent, this is measured by the distance l in time, see 10. however, it remains unclear whether and when this assumption is satisfied, and the authors do not discuss this issue.  the only proposed method working for 10 needs to know l, which again sounds rather unrealistic. in summary, despite having a good average rating and a positive rebuttal phase, i personally see this as a true borderline case.", "accepted": 1}
{"paper_id": "nips_2021_ClwfZc4ooKM", "review_text": "strengths  novel and elegant approach  clever training procedure that incorporates humans intheloop  can potentially give insight into human decisionmaking process from visual stimuli  wellexecuted experiments involving real human subject feedback weaknesses  no clear application or realworld use case  requires a more careful discussion on algorithmic decision support and agency summary reviewers were mostly unanimous in their opinion that this is a solid contribution to the growing literature on learning to support human decisionmaking. one reviewer initially raised several concerns regarding practicality, but the authors response which included additional experimentation were helpful in increasing his score. learning with humans intheloop is challenging, and this paper does a commendable job at executing experiments in this setting. as one reviewer comments, the authors should clarify the amount of human resources time, queries, etc. required in their approach. but more importantly, i would encourage the authors to discuss in detail the potential impacts of deploying their algorithm in the real world. issues such as the role of algorithms for decisionsupport and their relation to human agency should be acknowledged  especially given that the paper does not highlight any specific usecase. the authors should also carefully consider and justify what they measure and why. all in all, paper presents a fresh approach and is likely to be the focus of interesting followup work.", "accepted": 1}
{"paper_id": "nips_2021_UMrf6F4Tg9c", "review_text": "there were two main concerns with this paper  the algorithm requires exponential space and a concern about the fairness in the experimental results. i am not worried about the worst case exponential space requirement of the exact algorithm. many algorithms have bad worst case behavior while having fast and effective approximations as several reviewers note. the one low score on this paper is due to the exponential space issue. also, the discussion of the fairness of empirical comparisons convince me that this is an issue in the clarity of the writing rather than a fundamental issue with the paper.", "accepted": 0}
{"paper_id": "nips_2021_jfd_GB546GJ", "review_text": "update the revision has been reviewed and the paper is officially accepted.  after extensive discussions among sacs, acs, and the program chairs, we have decided to conditionally accept this paper. the primary concerns were around overclaiming, the interpretation of results, and the analogy to programming which was viewed as a distraction from the main contributions of the paper. there was some debate around whether these issues constitute a fatal flaw that should prevent the paper from being published at neurips, but all agreed that these issues need to be addressed. we hope that the authors will take into account feedback from the reviewers and make the changes that were promised as part of the rebuttal. additionally, in order to be accepted, the following changes must be made in the revision introduction l52 this joint training optimizes the classifier not only to be highly performant, but constraints its activations to a manifold that the generative model is able to decode. consequently, the networks entire computation is constrained to a decodable vector space...  this is misleading and should be rephrased or removed; just because the activations at a given layer can decode the input doesnt mean that that information is being used by the downstream layers to actually make decisions. l59 this paragraph should focus on what the paper actually demonstrates empirically; essentially remove the first sentence. section 3 highlight that equation 4 doesnt require that the reconstructed images g_phih_l actually produce the same output y, so the reconstruction may not actually be highlighting the features that were important for f to make its decision in the first place. section 4 this section should be significantly rewritten to avoid making claims unsupported by the evidence. specifically l116 in practice, this theory leads us to expect useless information... this would be true if equation 4 wasnt rewarding the network for keeping the useless information to help support the decoding task. l119 this paragraph is misleading and should be removed. while it is true that you can visualize the information at each layer, you cannot actually tell which of that information is actually being relied upon to make the decision. l130 this is the only paragraph in this section that states an empirical fact; the whole section should focus more on what this experiment objectively shows, but avoid making claims not supported by the observations made in this experiment. l138 this paragraph again misrepresents what can really be inferred from the generated images. section 6. the analogy with assertions is misleading as it implies a level of guarantees that is not supported by the evidence. the section should focus more closely on what the experiments actually demonstrate for this application. the attractiveness task should also be replaced with the bald vs not bald, and bearded vs not bearded experiment presented in the rebuttal. abstract the abstract should be consistent with all the changes made to the rest of the paper.  the original metareview for this copy of the paper follows i am really torn about this paper. on the one hand, i think this paper presents an intriguing and original idea train a generative model to reconstruct the input from the intermediate layers of the network, and then feed those reconstructed inputs either to the same network or to other networks that perform related tasks, possibly even recursively. the paper then presents empirical evidence that these compositions can be engineered to have some useful properties, such as identifying when their input is out of distribution, or avoiding the use of certain types of information in a classification. on the other hand, the paper as written suffers from extreme overclaiming and interprets many of its results in a way that i dont think is warranted by the data. the gap between the clear contributions of the paper and the narrative that the authors built around them was pointed out by multiple reviewers. while the authors did significant work through the rebuttal period to address many of the reviewer concerns and made some significant promises in how they were going to rewrite the paper. i think the issues are significant enough that i really think a fresh version of this paper that incorporates all the proposed changes needs to be reviewed by a fresh panel. the issues with his paper are too big to accept it without further review. i now describe some of the issues that i found most questionable about this paper. the paper is organized around an analogy to traditional programming and program debugging. the claim is that the generative models allow us to interpret what each of the layers in the network mean so that they can be debugged probed, asserted, etc. however, the analogy doesnt really stand up to scrutiny, and in arguing for this interpretation of its results, the paper fails to explore alternative explanations for the observed behaviors. for example, consider section 4. the objective empirical observation is that for this set of images and image classification tasks, when an image is misclassified, the reconstructed images progressively look more and more like the incorrect label. this is interesting, but the authors try to claim something much bigger; they claim that these images can allow one to understand what features a neural network is attending to in making a prediction, or to understand what features make it susceptible to making an error. i find this highly unlikely. in fact, the loss function in equation 4 almost guarantees that this would not be the case. for example, suppose that in the case of the handbag, the neural network actually learns that a handful of pixels making up the handle of a handbag are what characterizes it as a handbag. an image that allows one to understand what features a neural network is attending would show just the handle of the handbag and discard everything else if thats what the network is using for its classification. however, equation 4 rewards the network for maintaining enough information at each layer to reconstruct the whole handbag, even if most of that information is not actually being used to make the prediction, only for the reconstruction. in fact, the handle of the handbag has very few pixels, so the reconstruction network would not be significantly penalized if it failed to display the handle; there is nothing in equation 4 that actually requires the reconstruction to focus on the features that are important for classification. there are similar issues with each of the subsequent sections. they are motivated and explained in terms of this analogy to programming, but the analogy just doesnt hold up to scrutiny. i think section 6 is particularly problematic because the analogy with assertions implies a level of guarantees that is simply not supported by the evidence. now, i actually think this section is the most interesting one in the paper, and i think it is unfortunate that the authors chose such a bad example to make their point. however, i think a lot more evidence is needed to show that this approach can be used to reliably prevent the network from using a protected characteristic when making a decision that would be a great paper by itself. for example, equation 8 forces the generative model to optimize three criteria faithfulness to the input image, randomness with respect to the protected characteristic, and ability to recreate the correct classification output. how these competing goals are balanced out will depend on some hyperparameter tuning that may determine the extent to which the resulting network actually satisfies the desired constraint. for example, a protected characteristic that affects a lot of pixels in the image will increase the cost of adding diversity relative to the goal of being faithful to the input image and may require different hyperparameters from one that only occupies a few pixels. overall i think there is an important and novel contribution in this paper, and i do believe the authors understand some of the issues with the paper as submitted and have demonstrated a willingness to rewrite the paper based on the criticisms in the reviews. this is reflected in the high scores given to the paper by all the reviewers. however, multiple reviewers raised the same concern highlighted above, and the area chair feels that this concern is quite substantial and requires a major revision to address.", "accepted": 1}
{"paper_id": "nips_2021_bm1Mrc3WHSe", "review_text": "there is a consensus among the reviewers that this paper presents significant results of interest to the conference.", "accepted": null}
{"paper_id": "nips_2021_d1FHmxHPEQ0", "review_text": "overall strong scores, and reviewers are mostly aligned in finding the paper to be above the bar. scores were initially more mixed, though after the discussion the weakest score was improved to borderline. reviewers praised the originality of the idea, and found the model performance through the experiments convincing enough. several issues were raised regarding clarity and other specific details, and an indepth discussion took place to resolve most of these issues. ultimately the scores are strong, and the rebuttal was fairly persuasive. the reviewers do highlight several issues and in spite of the strong scores, the reviews are not particularly gushing in the text. but the issues raised seem mostly in terms of clarity, or otherwise are adequately addressed during the rebuttal phase.", "accepted": 1}
{"paper_id": "nips_2021_AnJUTpZiiWD", "review_text": "this paper is the first to address partial label learning in the setting where the partial label depends on the feature vector. the authors introduce a principled approach to solve this problem and demonstrate some stateoftheart empirical performance. this paper will be wellcited in the pll literature and serve as an important benchmark for future research. the reviewers make a handful of comments that should be reflected in the final version. a pertinent missing reference is katzsamuels et al., decontamination of mutual contamination models, jmlr 2019.", "accepted": 1}
{"paper_id": "nips_2021_CRPNhlp4jM", "review_text": "i decided to accept the paper since it presents an interesting problem and provides a novel solution with a surprising connection to the dual averaging technique. in the final version, please make sure to clearly and explicitly discuss the bounds and their dependence on the problem parameters.", "accepted": null}
{"paper_id": "nips_2021_P9ld0c4dwUF", "review_text": "the paper provides improved analysis for both symmetric and asymmetric lowrank optimization problems via the burermonterio factorization approach. specifically, based on the assumption that the problem satisfies restricted isometry property rip, this paper provides much tighter rip constants for ensuring the following three cases 1 nonexistence of spurious secondorder critical point, 2 strict saddle property, and 3 the existence of a counterexample that has spurious secondorder critical points. on the other hand, reviewers pointed out that the current presentation of the main results like theorem 4 appears too complicated and could be simplified and improved. also, the analysis cannot be applied to other lowrank optimization problems e.g., phase retrieval that have no rip. overall, all reviewers appreciate the technical contribution of this paper and agree that the merits outweigh the pitfalls, so i recommend accept. nevertheless, please modify the paper accordingly to improve the presentation and highlight the limitation of ripbased analysis in the introduction while mentioning some practical problems e.g., quantum state tomography that obey rip.", "accepted": 1}
{"paper_id": "nips_2021_MwFdqFRxIF0", "review_text": "while there was a discrepancy between the overall scores initially, the authors rebuttal letter clarified several concerns of the reviewers, which resulted in all the scores being positive. hence, i recommend an acceptance. please implement all the changes suggested by the reviewers in the cameraready version.", "accepted": null}
{"paper_id": "nips_2021_avgEPdjT2Uz", "review_text": "i agree with the reviewers that the paper is not quite ready for publication. the paper is exploring a promising direction, and i encourage the authors to explore submitting this paper for future conferences. however, i suggest the authors address the following issues before another round of submission 1. move a big part of the introduction where you describe connections with other problems to the supplementary material to open up space for your own work. 2. provide runtime analysis for different algorithms and compare them. 3. given that the paper is mainly empirical, it is important to provide a fair comparison with existing algorithms. i am sorry that the outcome is not what the authors expected it to be and i hope this outcome does not discourage the authors from pursuing this research direction further.", "accepted": 0}
{"paper_id": "nips_2021_7EFdodSWee4", "review_text": "the paper studies impact of various dp algorithms on fairness. the paper is well written and easy to follow. i encourage authors to incorporate reviewer suggestions and make the analysis more rigorous in the final version of the paper e.g., change theorem 1 to include higher order error terms via o notation.", "accepted": null}
{"paper_id": "nips_2021_hhkyM3ib9e6", "review_text": "the reviewers all agreed that this paper should be accepted. please read through the reviews and responses and make sure to include all suggested changes in the camera ready version. one area to pay special attention to when preparing the camera ready is clarity. for example, this paper contains quite technical definitions of permutationspartitions and their relationships. it would be very useful to the more general reader to provide many more illustrative examples in the appendix, if space is limiting. as a side note, the reviewers greatly appreciated the authors enthusiastic engagement during the discussion period  well done!", "accepted": 1}
{"paper_id": "nips_2021_bvzhvNPZlqG", "review_text": "the paper proposes a novel extension to contrastive learning, using both cluster level and instance level tasks in representation learning. the bulk of the reviewers found this method interesting and wellmotivated, and from my perspective as well using this additional structure is a good direction in contrastive learning  representation learning. the authors felt their work should be viewed independently from swav, but the reviewers clearly saw the strong overlap between what the two models do. the reviewers added swav as a baseline, and their model performed favorably on their benchmark tasks. however, i encourage the authors to be generous on the connection between their work and swav in particular in the final draft, while the downstream applications are similar, i believe the motivations are very similar. finally, the reviewers requested more benchmarks, such as tinyimagenet, and this satisfied the reviewers. therefore i recommend acceptance as a poster. a note that i do believe that a clusteringfriendly representation learning via instance discrimination and feature decorrelation should be included in the final draft. i do not believe that the significance of the submitted work should be judge by a, as it is quite new and could have feasibly fallen under the radar with no ill intent. but i believe both papers are doing similar things and as such this paper could do the community a service by including results in a and providing this as additional context to their results. i strongly encourage the authors to do this.", "accepted": null}
{"paper_id": "nips_2021_SFLSOd_hv-4", "review_text": "all 4 reviewers suggested acceptance of the paper and the authors clarified all open questions and concerns in their rebuttal. therefore i am recommending acceptance of the paper.", "accepted": 1}
{"paper_id": "nips_2021_x6tV8QhHjs1", "review_text": "from the sac. this is an instance where the rebuttal and the discussion worked. while the original decision for this paper was to not accept, it is being raised to a recommended accept. the primary reason is the quality of the rebuttal, and the useful technical discussion between authors and reviewers that ensued that seems to have been revealing. to the authors i trust that you will take all reviewer feedback into account and most importantly, that all of the things in your rebuttal and discussion that were promised will be done in the next version of the paper.", "accepted": null}
{"paper_id": "nips_2021_IKz9uYkf3vZ", "review_text": "three of the four reviewers recommended accepting the paper , and one of them increased the score following the rebuttal. i am happy to accept it, i encourage the authors to include the additional material that they discussed in the rebuttal in the final version.", "accepted": 1}
{"paper_id": "nips_2021_aF60hOEwHP", "review_text": "this paper proposes a framework to reconstruct 3d garment models from 3d point cloud sequences of dressed humans. the paper raised concerns regarding robustness of the model to segmentation errors, limited real world results, similarities to recent works, requirement for body point clouds under the garment, and accuracy of the proposed method. the rebuttal submitted by the authors addressed these concerns by showing real world results and the feasibility of obtaining body pointclouds using the smpl model. though the relevance of the paper to a wide neurips audience maybe still be under question, all reviewers are positive about the paper, and it is suggested for publication.", "accepted": null}
{"paper_id": "nips_2021_Qo6kYy4SBI-", "review_text": "the paper presents a new communication protocol for decentralized learning when the nodes have significant data heterogeneity. the reviewers all agreed about several of the positives of the current draft good presentation, extensive experiments, interesting ideas. the main concern was the algorithmic novelty of the scheme similar to treereduce, and gossip with delays, and the memory overhead proportional to the number of neighbors. however, it was agreed during discussions that this work carries enough technical depth to be interesting to the related communities decentralizedfed learning.", "accepted": null}
{"paper_id": "nips_2021_Mj6MVmGyMDb", "review_text": "this paper studies a meta learning for bandits problem, using a hierarchical bayesian formulation. it proposes the adats algorithm, which can be instantiated to gaussian multiarmed, linear, combinatorial semi bandits, and more general exponential family bandits. specialized to the gaussian bandits setting, the proposed adats algorithm improves over two baselines, including 1 the prior work of metats, in its regret in learning the task prior parameter mu_ from sqrtmn2 to sqrtmn 2 thompson sampling without metalearning, in that when the task prior is sufficiently concentrated and the meta prior is sufficiently spread out. empirical results clearly support the claims. one limitation of the current work is that it does not have theoretical results yet in the more general exponential family bandits setting e.g. bernoulli bandits.", "accepted": null}
{"paper_id": "nips_2021_W9250bXDgpK", "review_text": "the paper proposes a framework that unifies regularization based techniques in cl and projection techniques. as other reviewers pointed out e.g. v6dd this is maybe one of the better formally motivated and better studied direction for addressing cl. and hence at a high level the final result might not seem surprising e.g. that regularizion methods can be seen as a trust region method, and then there is a deep connection between projection methods and these regularization methods. however i want to stress that the final algorithm and its derivation is by no means trivial. and i think the improvement at least in the theoretical understanding of these methods is of great values for those focusing on such techniques. so from my perspective, in terms of novelty and significance i think the manuscript has provided enough of both. and while there might be some scalability worries, i think there is a significant subset of the cl community that will find this result very useful. the main weakness of the work is the empirical exploration. while i believe the experiments are done carefully, and provide evidence for the efficacy of the algorithm plus i welcome the new experiments on feedforward models, the scale of the empirical section is a bit weaker compared to an average paper on this topic. however, particularly considering the new results mentioned by the authors that should be integrated in the paper, i think they might just be sufficient to reach the requirement of the conference.", "accepted": null}
{"paper_id": "nips_2021_u7Qb7pQk8tF", "review_text": "the reviewers highly appreciated the author response and the additional details. reviewer uyk4 didnt update the review but indicated in the discussion to have read the replies. we had quite an extensive discussion between the reviewers on whether the setting of this paper too narrow and whether that would actually speak against publishing it. in the end they came up with a few examples please add some motivating examples that have practical relevance to the paper and everybody was convinced that this is a sound and interesting paper.", "accepted": 1}
{"paper_id": "nips_2021_aZEgelE6kJN", "review_text": "thank you for your submission to neurips. the reviewers and i are in agreement that the proposed work presents some substantial advances to the topic of optimizing nondecomposable metrics, in this case using a nice application of unrolled smoothed lp solving. the reviewers provided several comments, and in some cases have already improved their scores after rebuttal. i would only suggest that the authors be sure to make these modifications, but i have no concerns, as these are largely easy to make. im happy to recommend the paper for acceptance.", "accepted": null}
{"paper_id": "nips_2021_ErNCn2kr1OZ", "review_text": "four reviewers recommend this paper to be accepted. the paper provides result addressing why and when pruning is possible. a concern is that the main result is based on a condition that might be too ideal. taking the discussion into consideration, i find that the manuscript makes valuable contributions outweighing its limitations. hence i am recommending the submission for publication. i ask the authors to implement the improvements promised in their responses and to carefully consider the reviewers comments in the preparation of the final manuscript.", "accepted": null}
{"paper_id": "nips_2021_AADxnPG-PR", "review_text": "the reviewers thought this was an interesting paper considering how to combine metagradient methods with hrl, and the empirical experiments were convincing. in the internal discussions two reviewers suggested the paper would be further strengthened by a more detailed discussion on the theoretical side. the authors are encouraged to consider the reviewers feedback in their revision for the camera ready.", "accepted": 1}
{"paper_id": "nips_2021_x9jS8pX3dkx", "review_text": "the reviewers all agreed that this is an interesting paper and a worthy improvement over existing gradient inversion attacks against federated learning. i agree with the reviewers the paper is clearly written and the many experimental results are worthy of publication. the paper will, at the very least, serve as an important motivation for further research on privacy in federated learning. i do caution the authors to take the suggestions made by the reviewers and the changes proposed in their rebuttal seriously and improve the manuscript in terms of related work and additional experiments. it is also essential for the authors to implement what they promised in their rebuttal to the ethics reviewers in the final version of the paper. critically, an experiment to demonstrate a potential fundamental defense mechanism and replacing humanface images in the papers quantitative evaluation should be considered as mandatory revisions. i agree with the ethics reviewers that the authors response helps assuage the ethical concerns assuming that changes will be made to the final version.", "accepted": null}
{"paper_id": "nips_2021_CyZF4CLnQ8D", "review_text": "the main task of this work is fast motion deblurring and yet the paper claims to jointly estimate 3d shape and texture along with the motion of the object. the paper received positive reviews 7, 9, 6, 6. the paper is written well with impressive results compared to previous works. however, the main limitations of the paper is the lack of novelty. as pointed out by a reviewer, the proposed approach is an extension of the defmo paper. the authors use defmo for the silhouette consistency prediction and differentiable interpolationbased render dibr and kaolin 39 for the image formation check. the approach combines these modules to solve the problem differently. while the defmo directly renders the appearance and silhouette directly from the given input image and estimated background, this work instead estimates a 3d shape from the given input image and background and then renders the appearance and silhouette. it is also worthy to note that estimating 3d shape and differentiable rendering from an image have been previously studied 23, 24, 25, 26. the fact that this work estimates it from a motionblur image doesnt necessarily make it significantly different from previous works. moreover, the 3d estimation from the motionblur image is also constrained with specific types of prototypes which makes it less generalizable to all types of fastmoving objects. the approach demonstrates a new pipeline for jointly estimating shape, texture, and motion with challenging blurred images of moving objects. i am not sure the neurips venue would be the best fit for this problem. based on the reviews and the author rebuttals, i lean toward to reject the paper.", "accepted": 1}
{"paper_id": "nips_2021_1lCZrXJBpM", "review_text": "this paper studies the problem of karmed contextual preference bandits and gives optimal efficient algorithms matching lower bound regret guarantees. empirical studies are also included. the results appear interesting and novel. despite some concerns about the motivation, the paper makes a sufficient contribution overall to warrant acceptance.", "accepted": null}
{"paper_id": "nips_2021_IaM7U4J-w3c", "review_text": "all reviewers agree that this paper applies rl qactorcritic method to learn large neighborhood search policy for integer linear programming ilp and achieves strong performance compared to default policies in popular ilp solvers scip and gurobi in multiple domains sc, mis, ca and mc that have been widely used in previous works. some of its proposed algorithm is novel e.g., training an independent policy for each variable, new network architecture design, the presentation is clear, and experiments are welldesigned and convincing, showing the final performance the community cares i.e., the quality of the solution given fixed wall clock time. the authors also address many of the reviewers concerns in the rebuttal, by including comparison with gasse et al and different setting of the ilp solvers, further solidifying the work. overall, i happily accept the work. it can be a strong contribution to the learningtooptimize community and i would highly encourage the authors to open source the code.", "accepted": 1}
{"paper_id": "nips_2021_HjFtRc83eBB", "review_text": "largescale bilevel and multilevel optimization has several emerging applications e.g., robotics where optimizers are themselves embedded in endtoend deep learning pipelines as layers. bilevel problems where the lower level subproblem is stronglyconvex and the upper level objective function is smooth is an interesting special case amenable to complexity bound analyses for reaching a stationary point. as such, the contributions of this paper should interest both numerical optimization and application communities. the reviews ask for a few clarifications in the final revision.", "accepted": null}
{"paper_id": "nips_2021_TiwPYwg3IRf", "review_text": "the reviewers deemed the paper of interest. i would like to thank the authors for providing focused discussion content for both theoretical and experiments, which contributed to explain further the papers content and its improvements with respect to its submission history. in the revised version, the authors should include the generalisation to theorem 1 proposed at discussion phase to cover asymetric noise and in experiments, find a place to report consistency and instancebased label noise, a short discussion vs wei  lius approach as developed in the discussion point 12 6kdx and complete references to be more extensive. this is extremely important given the context of the paper and the richness of the relevant literature in the past decade or so. ac.", "accepted": null}
{"paper_id": "nips_2021_SkDYNXUM4xZ", "review_text": "federated learning algorithms based on operator splitting or admm are more sophisticated than simple algorithms such as fedavg or fedprox, and they bring more flexibility and rigor in algorithm design. the proposed feddr method and its asynchronous versions extends the classical douglasrachford splitting method, combining with randomized blockcoordinate updates, to the federated learning setting with nonconvex loss functions. the idea of applying operator splitting methods to federated learning has been proposed in a few recent works, but the reviewers are supportive of the generality and technical results obtained in this paper. they make solid contribution to the federated learning literature.", "accepted": null}
{"paper_id": "nips_2021_GgS40Y04LxA", "review_text": "this work makes progress in the recently popular learningaugmented algorithms framework and argues that a regressionbased approach forms a correct learning solution for many of the problems. the authors do a complete study of this question; of particular interest is their exploration of the learning problem itself. here they show that special care must be taken in setting up the loss function for the regression problem to achieve good results. overall, a solid contribution.", "accepted": null}
{"paper_id": "nips_2022_09QFnDWPF8", "review_text": "all reviewers recommend accepting the paper. congratulations! but in your cameraready version, please revise the paper to better emphasize the relevance of this line work to a general machine learning audience. for example, during the discussion one reviewer wrote the following the authors have not thoroughly convinced me of the relevance to a ml audience, as i will explain. to me, the relevance is very likely, as they apply sgd to a regression problem, and then ml learners on top of that, which is very much in the vein of ml. my confidence is limited, however, since i am not familiar with functional regression. the paper could better appeal to an ml audience by mentioning ml applications of their method whether it be functional linear regression or otherwise early in their paper and in an appealing way. section 5.2 real data application does a good job of this, but giving motivating use cases could help pique reader interesthelp them understand application earlier. the paper will have greater influence if the final version can convince readers of its relevance to ml!", "accepted": 1}
{"paper_id": "nips_2022_7eUOC9fEIRO", "review_text": "in postrebuttal discussion, reviewers debated the merits of the paper and especially reviewer gwj8s concerns. in the end, reviewer gwj8 agreed with other reviewers that the paper should be accepted, although all reviewers would like to see the final revision reflect the points and concerns raised in the postrebuttal discussion.", "accepted": null}
{"paper_id": "nips_2022_kpSAfnHSgXR", "review_text": "this paper extends box embeddings to allow for them to represent directed graphs. in particular, the proposed binary box embeddings can represent cycles in graphs, which was not previously possible. the reviewers appreciated the introduction of binary box embeddings and felt the contribution was novel and elegant. during the discussion, the reviewers felt that the rebuttal generally answered their questions and felt that the contribution would be of interest to the neurips community. a number of clarity issues brought up in the initial reviews should be addressed for the final version of the work, for instance, the motivation of the binary code embeddings and the discussions with 3kmq on transitivity holding. please revise the paper to address the remaining comments from the reviewers and carefully incorporate the additional results presented during the author response discussion phase.", "accepted": null}
{"paper_id": "nips_2022_6dfYc2IUj4", "review_text": "this is a borderline paper studying an interesting question around generalization bounds for equivariant networks. initially there were significant concerns around presentation of the key results and related work. during the rebuttal phase authors updated the manuscript as per reviewers suggestions, resulting in a significantly better manuscript. i applaud the efforts of all the reviewers who engaged with authors leading to a better submission. one reviewer still kept their negative score, but other reviewers and i believe their concerns were addressed in the updated manuscript. overall i recommend acceptance and it is important that the authors revise the manuscript highlighting the key assumptions upfront about the fourier space following reviewer fd7rs suggestions.", "accepted": 0}
{"paper_id": "nips_2022_2S_GtHBtTUP", "review_text": "this paper implements a set of optimizations on top of the xla compiler with the explicit purpose of reducing the memory footprint of an algorithm. this is important because while a lot of optimization work in this space has traditionally focused on speed, memory is more commonly the bottleneck when running large computations. the key strengths of the paper are that it presents a genuinely useful artifact and is generally well explained except for section 4.3 which is not very well explained. the results also show that one can get some significant memory improvements with zero additional effort if your code already compiles with xla. the main limitations of the paper are limited novelty none of the transformations are particularly surprising and limited relevance to neuripsthis is really a compilers paper, and its not even evaluated on any deep learning workloads. these two limitations make this a borderline paper, but all reviewers considered it to be above the bar. minor comment in section 4.1, in the compiler literature, these matchandreplace optimizations are known as peephole optimizations. the term should be at least mentioned.", "accepted": null}
{"paper_id": "nips_2022_ft4xGJ8tIZH", "review_text": "this paper seeks to understand and explain why variational bayes seems to underperform if not fail completely in overparameterized models such as bayesian nns. the proposed explanation is an additional gap in the marginal likelihood bound that results from these invariances. the paper demonstrates the gap in detail for a linear model with translation invariance. the reviewers had a favorable opinion of the work, having only the criticisms of 1 clarity of writing and 2 questioning if the paper can say anything concrete about bayesian nns. the authors have substantially revised the work, and i and the reviewers agree that the clarity now meets the bar for publication. i think the second criticism is still valid, and the paper would be better served to be billed as a study of overparameterized models. yet, the papers merits outweigh this downside which i encourage the authors to improve upon before the camera ready. also, this paper is related enough to warrant inclusion in the related work, as it seeks to directly address the symmetries algorithmically moore, david a. symmetrized variational inference. in nips workshop on advances in approximate bayesian inference, vol. 4, p. 31. 2016.", "accepted": 0}
{"paper_id": "nips_2022_O3My0RK9s_R", "review_text": "the paper receives positive feedback after rebuttal. all reviewers agree that the idea of distilling structural knowledge for object detection is novel and worth sharing to the community. ac agrees with it and recommends accepting the paper.", "accepted": 1}
{"paper_id": "nips_2022_tX_dIvk4j-s", "review_text": "the paper originally received mixed scores, with two reviewers recommending acceptance and two rejection. while the reviewers acknowledged the soundness of the approach, the clarity of the explanations, and the good ablations of the methods components, they expressed concerns regarding the limited scope of the experiments, the lack of runtime analysis, and the fairness of the comparison to baselines. the authors feedback convincingly addressed these concerns; in the discussion, tygs agreed to raise their score to weak accept, and ubx9 to borderline accept. this led to a consensus for acceptance. we nonetheless strongly encourage the authors to incorporate elements of their feedback in the cameraready version of their paper.", "accepted": 0}
{"paper_id": "nips_2022_mamv07NQWk", "review_text": "this paper studies regret bounds for multilabel classification and derives new upper bounds for surrogate risk minimization under a lownoise assumption. although there is some concern regarding the conditions, all reviewers support accepting this paper.", "accepted": 1}
{"paper_id": "nips_2022_4wrB7Mo9_OQ", "review_text": "while the paper is not rated too high by the reviewers, and overall the endorsement for it is a bit less strong that what we would have liked to see, it seems that the paper might have results that are of interest to those working on materials science applications of ml. one common complaint the authors had was the limited experimentation, but given the authors response regarding the validation which in particular goes through an important dataset, the concern was overcome. however, to improve the impact of the paper, and to offer stronger motivation for followup works, it would be a valuable addition to the final version of the paper to have more experiments, especially those that more elaborately outline suitable ablation studies, to illustrate the theory in action, and importantly, to add in some comparisons against some of the generic methods which will be understandably, not respecting all desired invariances, still, it is good to see how much of a big deal that is, notwithstanding the comments on platon et al. one minor but potentially useful technical point to note is that what the authors call continuity theorem 4.3 is a type of lipschitz continuity, which can be further exploited for algorithm design and analysis.", "accepted": null}
{"paper_id": "nips_2022_isPnnaTZaP5", "review_text": "the paper contributes a novel training methodology based on a ladder network for a scenario of finetuning under limited memory constraints. two out of three reviewers recommended rejecting the paper. reviewer l9hl noted that the paper is related to distillation or network prunning and is not discussed sufficiently. the method is related but not the same as distillation or network prunning. both of these techniques require an additional computation step. i agree with reviewer tntu that it would strengthen the paper to make a onetoone comparison with representative distillation or network prunning methods, but i also feel it is not strictly necessary as they have slightly different use cases. hence, i feel it is more of an issue with writing, than with the core contribution. it was also brought up that side tuning is a similar method from 19. my understanding is that figure 8 and the rebuttal properly discusses this point and shows that the design choices made by the authors e.g. injecting the representation at multiple stages are crucial innovations, and do not diminish novelty in my opinion. all in all, i believe this is a solid contribution that is another tool that will help democratize largescale models. it is my pleasure to recommend acceptance. please remember to address reviewers remark, and please pay special attention to better contextualizing your work in the broader field of memory efficient training of neural networks.", "accepted": 0}
{"paper_id": "nips_2022_wKhUPzqVap6", "review_text": "the paper shows that empirical risk minimization is sufficient to obtain good worstgroup accuracies and specialized group robustness methods do not appear to provide additional benefits. the reviewers pointed out that the current work depends on dfr, which seems to require some additional data compared to group robustness methods. the reviewers also note that the nlp experiments did not use more recent models, and the authors addressed these issues. generally the reviewers think this is a wellexecuted paper on an important problem, and are unanimous in accepting it.", "accepted": null}
{"paper_id": "nips_2022_yKYCwTvl8eU", "review_text": "the paper considers the important problem of shortcut learning and the existing solutions and suggests learning unknown concepts besides the known ones prior knowledge in other to better tackle shortcut learning. they built upon concept bottleneck models cbm to also learn the unknown concepts. the proposed approach is simple and easy to understand, with the detailed analysis presented for linear models.the authors have tested the method on the cub birds dataset and edema prediction from xray images. the considered problem is important and the experiment results validate the effectiveness of the proposed approach. moreover, the authors were able to address reviewers questions and concerns during the rebuttal period. therefore, i suggest the paper to get accepted.", "accepted": null}
{"paper_id": "nips_2022_JoukmNwGgsn", "review_text": "the reviews are mostly positive and consider that the paper solves an interesting problem with nontrivial techniques. for further improvement, the reviewers mentioned some concerns about the problem setup being restricted, and gave some suggestions for improving presentation.", "accepted": null}
{"paper_id": "nips_2022_8gjwWnN5pfy", "review_text": "overall the reviews are positive, leaning towards accept. the reviewers agree that the main idea is interesting and the paper is well written and structured. also several issues raised are properly addressed and checked, and i think that the remaining issues are also taken care of. hence, i recommend the acceptance of this paper.", "accepted": 0}
{"paper_id": "nips_2022_pV7f1Rq71I5", "review_text": "this paper improves the stateoftheart in estimating the entropy of a discrete distribution under memory constraints. the reviewers agreed that the presented result is elegant and nontrivial and that the ideas are described well. there is some concern that the paper is incremental due to the absence of lower bounds, but the algorithmic contribution is strong enough to merit acceptance to neurips.", "accepted": 0}
{"paper_id": "nips_2022_JRXgTMqESS", "review_text": "all four reviewers agree that this paper demonstrates strong improvements over prior methods, and there is broad agreement among the reviewers that the model is well motivated and novel, and that the paper is clearly written. there was a good discussion between the authors and reviewers on a number of perceived weaknesses in the paper, and the authors were able to address these concerns with additional experiments and proposed revisions, prompting two reviewers to raise their scores. in the end, all reviewers recommend acceptance to some degree, and in my judgement, the most negative reviewer who recommends borderline accept has missed the point, made both in the paper and during the discussion, that the estimation of source direction from singlechannel audio depends not only on audio cues, but also on video cues.", "accepted": null}
{"paper_id": "nips_2022_zD65Zdh6ZhI", "review_text": "the paper studies the complexity of discovering probabilistic explanations for decision trees. they show that computing minimumminimal probabilistic explanations is an nphard problem. on the other hand, they also identify structural conditions that make the problem efficiently solvable. the reviewers appreciated the challenges in proving the complexity results. although the experimental results are not so convincing, we evaluated the paper mainly on its theoretical aspects and find it suitable for acceptance.", "accepted": 1}
{"paper_id": "nips_2022_TItRK4VP9X2", "review_text": "this work proposes a channel shuffling as a way to distinguish between backdoor and clean examples, based on the hypothesis that trigger features are sparsely encoded and activated in only a few channels. reviewers all agreed that is a pretty intuitive, yet effective method and that it had solid evaluations after the rebuttal. reviewer ofcu had the concern that this paper is entirely empirical and has no supporting theory. i think this is ok given the precedence of papers in this field and also the framing of security and privacy is a more practically oriented one anyway. the most critical reviewer pjhg pointed out that the benchmarks and related work contextualization were severely lacking. after the rebuttal period, these concerns were mostly alleviated. given the strength of the evaluations and the novelty of the idea, i believe this paper should be accepted. that said, please address the following for the camera ready please improve the writing as this was brought up by several reviewers. there are a lot of grammatical errors. please improve the discussion of the limitations section of this detection method as suggested by reviewer pjhg.", "accepted": null}
{"paper_id": "nips_2022_Fhty8PgFkDo", "review_text": "paper studies computing ppr in differential privacy setting. given the importance of ppr in real world applications, we recommend accepting the paper as it brings an important problem to the dp community. however, we encourage authors to incorporate the comments from the authors, make sure that all the details of the proofs are made available in the final version, and clarify any comments reviewers raised. in particular, we encourage the authors to adequately address the criticisms of reviewer mdnv in the true spirit of science.", "accepted": 0}
{"paper_id": "nips_2022_4G1Sfp_1sz7", "review_text": "the paper introduces three theoretical analyses explaining the effectiveness of the transformer in long sequences. the reviewers raise three aspects that the paper can be further improved. first is the writing of the paper, it may be too dense and hard to follow. second, the authors may want to elaborate a bit more about what deeper insights the proofs can bring to the community, except as a proof for the transformers effectiveness in long sequences. third, the authors may consider adding empirical results from the experiments to support the theory. for those reasons, i think the paper is not ready to be accepted for now.", "accepted": null}
{"paper_id": "nips_2022_xz-2eyIh7u", "review_text": "this work studies an interesting collaborative linear bandits problem and makes solid technical contributions efficient algorithms, optimal regret, and generalization to other settings. clear accept. please do address the minor issues pointed out by the reviewers in the final version.", "accepted": null}
{"paper_id": "nips_2022_zdmYnIRXvKS", "review_text": "this paper proposes a derivation of spiking networks from an efficient signal reconstruction cost function. the derivation leads to more biological features than previous ones, including fast and slow synaptic currents, and rebound currents. this is a nice addition to the growing literature in this field. the negative review below mostly focused on formatting of the paper. the ac did not see a violation of neurips formatting policy, however agrees that the presentation could be clearer.", "accepted": null}
{"paper_id": "nips_2022_4B7azgAbzda", "review_text": "reviewers agree that this is an sound and well presented contribution.", "accepted": 1}
{"paper_id": "nips_2022_L3uTDctm3s9", "review_text": "this paper proposes three different, easy to implement methods for dataaugmentation when learning models on compositional data where each feature lies in a potentially highdimensional simplex. the basic idea is that for such data, it is important to create augmentations that respect the fact that data are within the simplex. there was consensus among the reviewers that this work should be accepted. this work was simple, interesting and results in empirical improvements across various choices of models of outcomes. i think this work can have an impact for those building predictive models in applications of machine learning such as microbiome data.", "accepted": null}
{"paper_id": "nips_2022_IbBHnPyjkco", "review_text": "there were substantial discussions around this paper and its contributions. the authors did a good job of explaining and interacting with reviewers with, as a consequence, a substantial raise of scores. to prepare the cameraready version, it is strongly suggested to include the material introduced at discussion time, including the experimental results k1dy and use the intuitive explanation provided for the dynamic programming approach 7kxj to revamp a section  paragraph on a highlevel explanation of the approach.", "accepted": null}
{"paper_id": "nips_2022_ZXoSAAlBnW8", "review_text": "the paper has generated a lot of discussion, and on balance the reviewers appreciate its technical contributions but find that the paper would benefit from a more indepth discussion of its relationship to hierarchical rl.", "accepted": 0}
{"paper_id": "nips_2022_uPdS_7pdA9p", "review_text": "this paper received unanimous recommendations of acceptance from the reviewer. the authors did a good job addressing concerns from the reviewers, especially with the additional ablation studies to decouple the gains from other techniques such as supcon. the ac agrees with the reviewers regarding the contribution of this paper and recommends acceptance.", "accepted": null}
{"paper_id": "nips_2022_QLPzCpu756J", "review_text": "this paper presents comprehensive experiments studying the role of data in finding lottery tickets in the early stage of training. all reviewers liked the paper and agreed that the paper has novel and insightful results worth sharing with the community.", "accepted": 1}
{"paper_id": "nips_2022_7WvNQz9SWH2", "review_text": "the reviewers agree that the paper should be accepted albeit with a mix of the level of acceptance. i agree. the presentation of the paper could be better.", "accepted": null}
{"paper_id": "nips_2022_rJjJda5q0E", "review_text": "the paper presents an analysis of the bayesian regret of thompson sampling algorithm in contextual bandits under an adversarial context process. the authors express the regret as a function of the socalled lifted information ratio. this information theoretical quantity is a natural extension of those introduced by russo and van roy. the analysis provides new regret upper bounds for logistic bandit, and comes with simpler and elegant proofs. the reviewers easily reached a consensus on this paper. it is very well written, easy to follow, and provide very interesting contributions. the discussion phase gave the opportunity for the authors to correct the related work section, and in particular to reposition the paper compared to papers pointed out by during the review process.", "accepted": null}
{"paper_id": "nips_2022_2dgB38geVEU", "review_text": "in this paper the authors propose a socalled rnn of rnns architecture where an assembly of contractively stable rnns can still work collaboratively under contractive stability. the authors draw the motivation from neuroscience where multiple brain areas can work simultaneously for complex behaviors. conditions are investigated under which multiple interacting rnns with feedback connections will preserve contractive stability. experiments are carried out on seqmnist, permuted seqmnist and seqcifar datasets. it is shown that the proposed network of rnns can give decent results. notably, sota performance is claimed within the class of provably stable recurrent architectures. all reviewers consider the work theoretically solid and timely to the community. authors rebuttal clears most of the concerns. the paper can be accepted. one lingering concern which, however, still stands after the rebuttal and discussion is its connection and impact to neuroscience where the motivation is drawn. following the authors narrative, the work would be more convincing if the proposed rnn of rnns can show effectiveness directly on neuroscience datasets. the authors should revise the paper accordingly to reflect that.", "accepted": 1}
{"paper_id": "nips_2022_cV03Zw0V-3J", "review_text": "i went through the reviews and authors responses. the reviewers are experienced in this field and provided good quality reviews. scores are the same weak accept. i also went through the paper and considered it reached the bar of neurips.", "accepted": null}
{"paper_id": "nips_2022_9-8YT5G36g-", "review_text": "the paper tackles the multiobjective optimization moo problem using adaptive reference vectorsrvs. the authors claim two main contributions 1 the use of adaptive reference vectors rvs, instead of fixed ones as in existing gradientbased moo algorithms 2 rv adaptation is based on the gradient of the quality function. it is reasonable to learn the rvs rather than using a fixed set of rvs. the authors also provide theoretical results about the proposed method. however, the concerns raised by reviewers should be carefully considered and addressed. also, the authors also consider only 2 or 3 objectives. it would be interesting to see results with more objectives see 10 on classical numerical multiobjective optimization problems. last, figure 1 is confusing and difficult to understand.", "accepted": 0}
{"paper_id": "nips_2022_cYPja_wj9d", "review_text": "dear authors, congratulations on your paper being accepted! the reviewers unanimously recommended acceptance. the reviewers made a number of recommendations on how to improve the paper further, in particular with respect to clarity of writing and explaining the motivation behind different analyses. we strongly encourage you use this feedback to improve the paper if needs be additional clarifications can be added in the supplement. in addition, it would indeed be highly useful to make your source code publicly available, as you indicated in your response. best, your ac", "accepted": 1}
{"paper_id": "nips_2022_tz1PRT6lfLe", "review_text": "the paper focuses on two important challenges in federated learning communication efficiency and privacy of the users local data. there has been a large body of work studying each of these challenges individually, but there are not many prior works which consider both of these challenges simultaneously. the paper proposes a unified framework that enhances the communication efficiency of private federated learning with communication compression. the paper provides two types of results i it theoretically analyses the case in which a compression method is applied directly to differentiallyprivate stochastic gradient descent; using this analysis, the authors then explain some limitations of this approach. ii the authors then propose a framework, called soteriafl, which accommodates a general family of local gradient estimators which includes sgd, saga, svrg, etc and a shifted compression scheme which is one of the wellknown methods for compression. soteriafl is then analyzed theoretically and performance tradeoffs in terms of privacy, utility, and communication complexity are obtained. the benefits of soteriafl are then shown in terms of communication complexity and less privacy, utility loss compared to other methods. table i provides a good summary. all in all, the reviewers found the theoretical results novel and interesting. the reviewers had a number of concerns most of which were addressed by the authors response and through the discussions. i thank the authors for their responses and also for adding experimental results which was mentioned by all the reviewers. two of the reviewers are very positive about the paper. the third reviewer has also read the author responses and mentioned through private email communication that theyd be willing to increase their score if the experimental results are reasonable. all in all, based on the reviewers feedback and my own assessment i think this paper provides an array of novel contributions in the field of fl.", "accepted": 1}
{"paper_id": "nips_2022_UxDO_gOhxhO", "review_text": "this is quite a borderline case. three of the reviewers are in favor of acceptance with three different degrees borderlineweakclear, while one reviewer maintains clear rejection. however, the content of that review and the remarks during the discussion period are perhaps better aligned with a score of 4 borderline reject than 3. for example, the reviewer explicitly states that the paper is above the bar in terms of technical content. the main concerns were then i suitability for neurips, ii scope of contributions, and iii clarity of presentation, including technical content. regarding i, although there exist conferences that are a better match to the paper, the topic does seem to be an ok match for neurips, which often contains papers in highdimensional statistical topics, e.g., compressive sensing. and some group testing papers have appeared before, albeit very rarely. regarding ii, the combination of deterministic  nonadaptive  quantitative is a fairly specific one, but deterministic  nonadaptive is a very widely soughtafter combination in the group testing literature, so i believe the overall combination isnt farfetched by any means. it would be ideal to have a clear and specific practical setting with this motivation, but being a theory paper, this doesnt seem like a dealbreaker. moreover, the general consensus albeit with varying degrees of enthusiasm is that the results derived are valuable. perhaps the main aspect that makes the decision borderline is iii. the reviewer asked some specific questions about the sui definition and intuition, and the authors response did not address those questions. the paper is very technical and would potentially be much easier to follow if it were presented in a more accessible manner, e.g., with more aid via diagrams, discussions, intuition, additional crossreferences  hints, and so on. i agreed with this sentiment after stepping through one of the proofs  i found it to be readable but with quite a bit of difficulty e.g., pausing for a long time to check a step that was made out to be very simple. this could certainly be detrimental to how much nonexperts can gain from reading the paper. on the more positive side, based on the proof that i read and similar efforts by another reviewer, the analysis appears to be correct, at least as far as we can tell. overall, i will cautiously judge that the good technical contributions and apparent correctness should outweigh the room for improvement in accessibility. in any case, the authors should very carefully revise the paper with the reviews and the above comments in mind.", "accepted": 0}
{"paper_id": "nips_2022_0Fe7bAWmJr", "review_text": "all reviewers consider this paper to make a good contribution to multifidelity bo, it is a novel combination of known techniques, and achieves stateoftheart performance. multiple reviewers ask for releasing code which authors promise to do before cameraready deadline. the authors feedback also addresses most other concerns or questions from the reviewers. after discussion, the remaining concerns from one reviewer are that 1 authors should discuss more on related works that conduct learning curve extrapolation lce and make a comparison to one of prior works 2 lack of ablation studies to answer e.g., why does dyhpo achieve strong performance the deep kernel, or the mulitfidelity ei acquisition function? for 1, it would be useful to include the discussion on lce related work from appendix a.6 and the reply to njxe into the main body of the paper. for 2, another reviewer finds the enhanced figure 7 in section 5 and figure 8 in appendix b from rebuttal useful to showcase the impact of the learning curve. i would encourage the authors to take into consideration the remaining concerns from the reviewers and edit their final revision accordingly.", "accepted": 1}
{"paper_id": "nips_2022_LP0malvd4x", "review_text": "this paper offers an interesting perspective on the min player and the max player for contrastive learning. it also addresses how the temperature parameter influences hard sample mining. the metareviewer recommends acceptance of the paper as a poster.", "accepted": 1}
{"paper_id": "nips_2022_eYfIM88MTUE", "review_text": "the paper proposes a method for unsupervised learning of objects from videos. in particular, the proposed approach combines two existing ideas  a recurrent slotbased architecture like savi and an autoregressive image decoder like slate. the methods is thoroughly evaluated and shown to outperform the relevant baselines. after considering the authors feedback and extensive discussions, the reviewers opinions of the paper are still mixed two strongly positive, one neutral leaning positive, and one strongly negative. the key strengths and weaknesses that were pointed out are as follows strengths 1. simple and efficient model 2. convincing experimental results on 8 datasets, including 2 realworld 3. insightful analysis and ablation studies weaknesses 1. lack of novelty 2. no experiments on more complex realworld datasets 3. fgari is not a perfect evaluation metric 4. lack of insight from the experiments note that some of the mentioned weaknesses are in conflict with strengths  for instance s1 with w1, s2 with w2, s23 with w34. taking all of the above into account, i believe the paper presents a simple yet efficient combination of two existing methods and evaluated it thoroughly, in line with and actually somewhat above what is commonly done for unsupervised object learning papers  leading to good performance and interesting insights. i believe the simplicity of the method is valuable and should not be critiqued as lack of innovation, and the provided experiments are already on more complex datasets than commonly used in the field. therefore at this point i recommend acceptance, but encourage the authors to take the reviewers comments to heart whenever possible and adjust the paper accordingly.", "accepted": 1}
{"paper_id": "nips_2022_Rgz_prESe-b", "review_text": "the paper builds a deep learningbased framework for estimating states and physical parameters by embedding a differential physics simulator into an autoencoder architecture. integrating physics into neural networks is an interesting research area, and this paper proposes some interesting and novel ideas such as explicitly predicting the physical parameter of a given problem from data. the reviewers acknowledged the relevance of the proposed method and generally appreciated the results. the paper is nicely written, but the experiments are somewhat limited. also the related work section is very shallow, ignoring to discuss recent work on koopmaninspired autoencoders for scientific problems that has appeared at iclr, icml and neurips in recent years. this said, i want to thank the authors for their detailed responses that helped in answering some of the reviewers questions. the reviewers have provided detailed feedback in their reviews, and we strongly encourage the authors to incorporate this feedback when preparing a revised version of the paper. i particularly stress this, because the authors have not sufficiently incorporated the feedback of previous reviews at icml. in general, i feel that this is bad practice!! in this particular case i will have an additional look at the camera ready version of this paper. nevertheless, the overall feedback of the reviewers is positive reviewer dt3b noted that he will raise his score, and i feel that this paper has the potential to motivate future research in this area. thus, i am leaning toward suggesting a weak accept of this paper.", "accepted": 0}
{"paper_id": "nips_2022_HFkxZ_V0sBQ", "review_text": "this paper is part of a recent line of work that augments algorithms with ml predictions. it introduces an interesting new model where the quality of a prediction is measured by its accuracy, instead of its error, and gives strong competitive ratios for the online problems of caching and covering augmented with such predictions. this new model could influence future work in this area.", "accepted": null}
{"paper_id": "nips_2022_GNHyNOR8Sn", "review_text": "this paper introduces a boosting approach for rl. rl is reduced to a supervised learning problem. the proposed method implements the frankwolfe algorithm where the objective is the value function is nonconvex. the weak learners are used approximately and efficiently solve the problem. the paper gives a sample complexity bound for finding a nearoptimal policy using weak learning assumption. this paper provides an interesting novel approach for rl that is certainly of interest to neurips community and as such is a valuable contribution to the conference,", "accepted": 1}
{"paper_id": "nips_2022_-IHPcl1ZhF5", "review_text": "reviewers are on the whole positive about this paper, after detailed responses from the authors. if this paper is accepted, many readers will be interested, and it does have the potential to be useful for realworld applications in medicine and elsewhere. therefore, on balance, the paper does reach the standard required for neurips.", "accepted": null}
{"paper_id": "nips_2022_AYkBQEm5AY", "review_text": "this paper proposes an extension of the covering discovery framework from singleagent to marl. there was unanimous support for accepting this paper and agreement that it is an interesting and very useful contribution. the responses to reviewers helped to clarify a few points. when preparing a revision, we strongly encourage you to address these points which came up during the review process  consider putting more emphasis on the mujoco experiments and dl extension in the main paper, to illustrate the generality of the results  discuss limitations and future work related to communication complexity of the proposed approach  clarifyjustify details of how tabular and nnbased methods are compared in the grid world experiments  clarify the setup in the option discovery phase", "accepted": null}
{"paper_id": "nips_2022_NI6hB70ajO7", "review_text": "this paper studies an interesting setting in which the future training data and the past data are generated from different distributions, but the two distributions are close in terms of kolmogorov or wasserstein distances. the paper gives a nice set of algorithmic and impossibility results. initially, some reviewers had questions about the novelty of the framework and technical approaches; the authors have addressed most of these questions properly.", "accepted": 0}
{"paper_id": "nips_2022_eyE9Fb2AvOT", "review_text": "the paper studies the gyrovector space structure of a few matrix manifolds. this is of broader interest to practitioners who may be interested in using geometric tools in applications. as the reviewers mention that although interesting, the analysis is per se straightforward for many manifolds. having said that, this ac believes it is a worthwhile effort to compile such results in one place and show the practical benefits. this paper is a good attempt in this direction. in the final version, please include all the suggestions both in explanations and as well as the new results presented in the discussion period.", "accepted": null}
{"paper_id": "nips_2022_1BJUwgi3ed", "review_text": "the consensus was that the reviewers were not convinced that the results were significant, and did not see significant fundamental novelty in the analysis.", "accepted": null}
{"paper_id": "nips_2022_0xdH-09oGD7", "review_text": "the reviews for this paper have a high variance ratings 3,4,6,7. the reviewer who gave a 3 seems unfamiliar with the basic bandit literature and i dont find the review particularly insightful. the other three reviewers mention that the theory is useful and solid, and the setting is interesting to them. on the negative side the reviewers mention that the paper is quite dense and some motivating examples would be useful. these are both minor points and, in my opinion, the positive aspects outweigh the negatives.", "accepted": 0}
{"paper_id": "nips_2022_wgRQ1IM4g_w", "review_text": "the paper provides new techniques algorithmic as well as analytical to solve black box optimization of smooth functions with constraints. the reviewers are largely in favor of the papers contributions, and the author responses have helped to clarify several aspects of the presentation and connections to existing work. therefore, i recommend that the paper be accepted.", "accepted": null}
{"paper_id": "nips_2022_PikKk2lF6P", "review_text": "this paper addresses issues with the reliability of calibration estimators by introducing proper calibration errors  low variance estimators that are upper bounds of calibration error. the paper has a strong theoretical grounding and addressees an important issue that is relevant to the machine learning community at large. though the original draft had some missing notation and definitions, the authors promise to fix these issues in the revision. overall, this paper is well suited for publication at neurips and will be a nice addition to the program.", "accepted": 1}
{"paper_id": "nips_2022_h73nTbImOt9", "review_text": "all reviewers are positive to this paper. the authors also respond actively to address the problems raised by the reviewers. i recommend acceptance. one thing is that reviewer 4qbz has pointed out the authors should be responsible to take the comments into consideration and do necessary changes in the paper, such as adjusting the title and the introduction and adding connections to previous work.", "accepted": null}
{"paper_id": "nips_2022_88ubVLwWvGD", "review_text": "this paper proposes to fix common issues in motion generation by representing motion latents as combinations of discrete latent codewords learned by a vqvae style approach. the idea is interesting and novel, and in the response period has been shown to potentially work for more than just dance generation, including a human trajectory prediction task. while the idea might be quite wellmotivated, reviewers all agreed that the writing does not quite do it justice in the submitted draft. i would recommend that the authors work on addressing those relevant reviewer comments for the next iteration either cameraready  future submission, aside from incorporating the new results into the draft.", "accepted": 1}
{"paper_id": "nips_2022_slKVqAflN5", "review_text": "the main contribution of this work is to extend and improve previous results regarding optimization complexity of lipschitz functions. in particular, it addresses issues in previously proposed algorithms that did not make it implementable. the newly proposed algorithm does not use the strong oracle that was required in previous work, and only uses firstorder information, as is common in largescale optimization problems in ml. there is also improved analysis of the complexity of the algorithms. the reviewers had many questions for the authors, and there was a fruitful and constructive discussion between all parties. their remarks have been adressed, and all reviewers recommend to accept this work, and so do i. the comments that i would personally have is that 1 for an implementable algorithm, an implementation would have been nice 2 since the motivation is to train models in an ml setting, some discussion about the quality of these stationary points from the point of view of generalization abilities would also complete nicely this work.", "accepted": 1}
{"paper_id": "nips_2022_Y-sdZLIi9R9", "review_text": "the paper provides a theoretical pac analysis of metalearning when the task distribution is directly learned. under the assumption that the task distribution can be approximated in a lowdimensional space, this approach leads to improved bounds wrt to previous literature. the authors further illustrate how the lessons learned in the theory can be translated into a practical algorithm by integrating their kernel density estimation into the varibad algorithm. there is general consensus that the paper makes several interesting contributions. first the authors devise a new algorithmic approach for metalearning and derive pac bounds that clearly improve over current results. the results are rigorous and their interpretation is insightful. furthermore, the authors made a considerable effort in translating their method into an actual algorithm with a nontrivial empirical validation. while more work may be needed to have a full picture of the empirical merits and limitations of the proposed method, i am confident the paper can be built upon by other researchers in the area. i strongly suggest the authors to integrate their rebuttal into the final version of the paper, in particular the discussion on the dependency on the dimensionality.", "accepted": null}
{"paper_id": "nips_2022_81LQV4k7a7X", "review_text": "the paper proposes refactorgnns, a unified view on factorizationbased models fms and graph neural networks gnns. it shows how to how fms can be cast as gnns by reformulating the gradient descent procedure as messagepassing operations. the reviewers agree that the idea is intuitive and yet very clever at the same time, as one of the reviewers puts it. connecting fms just as distmul with gnns is great, and the paper is well written and presented. the one more negative reviewer expresses a low confidence and provides an informed outsider view on the paper, which is actually rather positive. i fully agree though i would also like to provide some more links to related work httpsarxiv.orgabs1509.08535 httpsforum.stanford.edueventsposterslidesmessagepassingformatrixfactorization.pdf httppfister.ee.duke.edupaperskimistc10.pdf moreover, another indication is that transformers are quite expressive, and transformers are gnns. i i would like to encourage the authors to provide a more highlevel view on the problem, and then argue that the presented approach makes this intuition very concrete. moreover, it would be great to discuss more future work. for instance, it seems possible now to learn new fm approaches automatically from data. kind of fm discovery or learning to factorize. this is all very exciting.", "accepted": null}
{"paper_id": "nips_2022_Adl-fs-8OzL", "review_text": "the paper a new algorithm for representation learning in rl based on mdp homomorphism. the paper presents a nice theory, the algorithm is well motivated and the experiments are convincing. the concerns of the reviewers such as comparisons with dbc or dm control suite with distractions have been addressed adequately by the authors. all reviewers evaluated the paper very positively and i join their decision.", "accepted": null}
{"paper_id": "nips_2022_tfkeJG9yAX", "review_text": "the paper provides novel lower bounds on function approximation, relating lp norm approximation error to combinatorial complexity measures of both the approximating and approximated functions classes. these bounds are instantiated for approximation via piecewisepolynomial neural networks. there is a consensus among the reviewers that the results of the paper are novel, solve an open problem, and follow from deep technical insights. consequently, i concur with the majority of reviewers and recommend acceptance of the paper.", "accepted": 1}
{"paper_id": "nips_2022_ckQvYXizgd1", "review_text": "this paper examines the impact of dales principal from neuroscience on neural network computation. dales principle says that neurons only release a single neurotransmitter type from their axons, which in principle, means neurons are either excitatory or inhibitory, but not both. using both spiking and ratebased recurrent networks, the authors show that networks that respect dales principle can recapitulate the same computations as those that do not while exhibiting greater robustness to noise. this provides some account for why dales principle may provide actual benefits to neural computation. the reviewers agreed that this paper is wellwritten and addresses an important question. the decision to accept was unanimous.", "accepted": 1}
{"paper_id": "nips_2022_47lpv23LDPr", "review_text": "the paper proposes an autoencoder that maps a point to a canonical point and to a group element such that the composition of the group and the canonical point reconstructs the point the invariance  equivariance is in this sense. the method is promising since it learn in an unsupervised way the group actions. experiments were on simple tasks and group actions. reviewers were overall positive and the paper improved a lot during rebuttal revision thanks to their recommendations. i have one recommendation to the authors to include the use of the representation learned in a classification task to see how the learned representation alleviates the need of large training samples. accept", "accepted": null}
{"paper_id": "nips_2022_-3cHWtrbLYq", "review_text": "ratings 655. confidence 343. discussion among reviewers no. this paper provides results on local identifiability of deep relu networks. identifiability of neural networks is an important theoretical topic, with practical implications such as reproducability, and we think the neurips community would find the material interesting. although the result is only for fairly specific assumptions that typically dont hold in practice, and only on local identifiability, the reviewers generally agree that the material is well presented. i think the result could lead as a stepping stone towards more general results, and i think that the results are worthy of presentation at neurips. my recommendation is to accept, assuming the list of promised updates to the paper, as detailed by the author, are executed.", "accepted": null}
{"paper_id": "nips_2022_COAcbu3_k4U", "review_text": "this paper presents a neural method for the graph retrieval problem. reviewers agree with the technical contribution of this paper, its empirical soundness, and the well written presentation. discussions in rebuttal and additional experiments provided useful information and made this paper stronger. we suggest the authors update their next version according to reviewers suggestions and rebuttal discussions.", "accepted": null}
{"paper_id": "nips_2022_Wtg9TUL0d81", "review_text": "this paper studies the calibration problem for gnn node classification. it identifies 5 factors contributing to miscalibration general underconfidence, diversity of distribution, distance to training, relative confidence level, and neighborhood similarity. a temperature scaling method is proposed where each node is assigned with a different temperature. all reviewers vote for accept. however, multiple reviewers have raised concerns on the validity of the 5 factors. i encourage the authors to thoroughly address them in the revised version.", "accepted": 1}
{"paper_id": "nips_2022_4d_tnQ_agHI", "review_text": "in this paper, authors provide an analytical theory of curriculum learning for an online teacherstudent setting where a subset of features are relevant and are used by the teacher while a student might get distracted and use irrelevant features. in such a setting, the difficulty of examples can be captured by the variance in the irrelevant features. the insights from this analysis help explain existing empirical observations reported in prior work for effectiveness of curriculum learning as opposed to randomordering and anticurriculum learning in computelimited regime. further interesting connections to the literature on cognitive science are discussed in the paper. given the lack of theoretical basis for curriculum learning, reviewers are in agreement that this paper is intime and impactful for the ml community and they all recommended accepting the paper. however, during the discussion period it became clear that this recommendation was based on the promise of authors to revise the paper which never happened during the rebuttal period. my final recommendation is to accept the paper based on the authors promise that the final version will include all changes promised by authors in their response to reviewers. although there is no formal conditional acceptance mechanism, the authors should view this as a conditional acceptance based on taking their word that they will revise accordingly see the list of changes below. i will check the cameraready version and callout anything less than that. list of changes promised by authors quoted from their response authors response to reviewer cgtb 1 unfortunately, implicit curricula cannot be directly studied within our framework, since we always assume the hardness information is completely disclosed. considering a pseudolabeling step before the actual learning stage would likely make the already involved computation unfeasible. empirically this procedure has been investigated in e.g. httpsarxiv.orgabs1812.05159, httpsarxiv.orgabs2012.03107 and, limiting our interest in the generalization aspect, this heuristic does not seem to induce a sizeable improvement. we will include this discussion in the revision. 2 we mean an algorithm that explicitly depends on the curriculum, for instance by changing its objective function when example difficulty changes. that is, a curriculum aware algorithm would adapt the learning process in order to account for different levels of difficulties in the data. a simple way of implementing this is to modify the training loss, as proposed in this paper. other approaches may involve adapting the optimization algorithm as proposed in httpsproceedings.mlr.pressv139ruizgarcia21a.html, or possibly modifying the architecture httpswww.sciencedirect.comsciencearticlepii0010027793900584. a key message emerging from our work is that standard algorithms do not dramatically benefit from curriculum, and we believe curriculumaware algorithms may be the way forward. we will include a clear definition and discussion in the revision. authors response to reviewer s1ip 1 the first point raised by the reviewer is the object of current investigations see answer below. we will add more details on the cifar experiments in the revised version. 2 we agree that this appears counterintuitive. we also have hoped for greater intuition on this point, but we do not have a simple explanation for this phenomenon. this result is what appears from solving the equations and it was checked in the numerical simulations. a possible intuition could be that, in some settings, the large amount of noise contained in the hard data will always be too disruptive for effective learning. thus, leaving the clean data for last could allow the model to better exploit the easy data. we will add this possibility to the revision. even without a clear intuition, our contribution here is to show, in an identical setting and without finite size effects, that both anticurriculum and curriculum can indeed outperform the baseline. authors response to reviewer s1ip 1 overall, the analytical solution is between 2 and 6 orders of magnitude faster. we will add this approximate speedup factor and discussion to the supplement or if space allows, the revision. 2 the large input limit means that the input size n and the dataset size m go to infinity with finite ratio alphamn. this is an important point that must have gotten lost during the iterations, thank you for catching this. we will reintroduce it in the revised version. 3 figure 1 is based on notations and definitions introduced in section 3 and is not easily understood at the point it is presented. reply we will replace the image with explicit notation. 4 line 143 starting from a large initialisation, i assume it means the random initialization scale of the student or teacher network. it is not entirely clear. please include more details about this initialization. does it use a normal random distribution? reply yes, we use a normal distribution with a fixed variance. when we refer to largesmall initialization we mean largesmall initial variance. we will clarify this in the revised version. 5 figure 1 the curriculum boundary lies at \u03b1  12. what is the curriculum boundary? it is never defined. the abstract talks about curriculum boundary consolidation, but it is not further elaborated. reply we mean the switching point between the two levels of difficulty. we will clarify this. 6 halfway through the paper section numbers stop being used. what i assume are sections 5 and 6 do not have numbers. and i am not sure if the subsections of section 4 belong there. reply we will add 5 and 6 to the last two sections. 7 figure 3c accuracy hard samples. is not an accurate title. it is accuracy of all samples, easy and hard. its just that anticurriculum performs best, which is influenced by hard samples. reply we will replace it with just accuracy. 8 the figure captions are inconsistent in the uses of a, b, left, center, right, and top, bottom. for example figure 3 says a, b and then the right panel. reply we will use only the a,b,c notation. 9 as stated in the paper, the answer does depend on the setup and in principle we would expect different behaviours for nonconvex models. in particular, the presence of different basins of attraction towards different minima would suggest that initializing close to a good one would produce a performance improvement. however, note that empirical results in the ml field are not showing clear signals in this direction. a possible explanation is that relying on memory effects in the learning dynamics would require one to hit a sweet spot in the learning rate value and in the number of training epochs, and this seems hard to be achieved consistently. for this reason, we speculate that explicitly enforcing this memory by altering the loss with the curriculum information could be useful even in these settings. we will further emphasize that this statement about memorylessness applies only to our setting. 10 we thank the reviewer for pointing out several elements that were missing in this version we will make sure to include them in the revised version of this paper.", "accepted": null}
{"paper_id": "nips_2022_rP9xfRSF4F", "review_text": "the reviewers all appreciate the direction of this work, and while the merits and significance of the work have limitations  as reflected in the weak scores  all reviewers were positive and found no reason to reject. i agree with this and recommend acceptance on the basis that the merits of the contribution and having this as part of the program outweigh the concerns regarding significance. that said, i strongly encourage the authors to use the detailed feedback from the reviewers to improve their paper.", "accepted": null}
{"paper_id": "nips_2022_fRbvozXEGTb", "review_text": "this work studies minimax optimization for convexconcave objective. it studies the population loss version of this question and shows linear time differentially private algorithms for this problem that achieve the optimal privacy utility tradeoff. the algorithm is based on the phased erm approach. the reviewers were in agreement that this problem is of interest and the paper makes a significant improvement on previous work to be interesting. i would recommend acceptance.", "accepted": 1}
{"paper_id": "nips_2022_2TdPjch_ogV", "review_text": "the paper explores the advantages of both gcn and gat by proposing a learnable network that can interpolate between gcn, gat and cat for each layer automatically. the proposed research idea is novel and discovers an interesting perspective by combining and interpolating between convolution and attention networks. the paper is theoretically sound, and extensive experiments are conducted over 15 datasets with a comprehensive analysis. however, all the reviewers consistently raise concerns regarding incremental improvement compared with baselines, and another common concern is that the authors do not extend the proposed method with more advanced convolutional and attention networks. the authors argue that their intuition is to design a more robust replacement to gcngat, not a sota. however, the author should be aware that lcat is able to extend to other networks does not guarantee it will work, and it is possible that the convolution and attention network may conflict during the training. since the proposed method is a novel and general paradigm, solid experiments are needed to thoroughly evaluate its performance. the motivation is promising, but more experiments should be conducted to sufficiently prove the superiority of the proposed method.", "accepted": 0}
{"paper_id": "nips_2022_DylWBluOgqN", "review_text": "the paper considers a collaborative decisionmaking setting in which one agent can suggest actions for a listening agent to execute. the listening agent is not bound to take these suggested actions. the paper models the listening agents decisionmaking process as a pomdp, treating the suggestions provided by other agents as observations i.e., dependent only on the state that are used to update the belief state. the paper describes two representations of the suggested actions and presents empirical results on simulated domains that demonstrate that the listening agents performance improves when following the suggestions of a perfect oracle, while it degrades given imperfect suggestions in proportion to the level of noise. the paper was reviewed by three researchers who read the author response and discussed the paper together with the ac. the reviewers largely agree that the collaborative decisionmaking problem as formulated is interesting. the reviewers find that the idea of formulating suggestionfollowing as a pomdp with suggestions modeled as observations is clever, and that the description of the proposed formulation is clear and easy to follow. the two formulations of the observation function are reasonable, albeit very simple initial models, though the paper does not provide much insight into when one should be used over the other. a primary concern with the paper is that it only provides empirical results. the work would have significantly benefited from theoretical results, which would help to understand how the method would generalize to other domains.", "accepted": null}
{"paper_id": "nips_2022_gsdHDI-p6NI", "review_text": "the authors propose a novel verification technique for polynomial neural networks. they compare their approach against competitive baselines and demonstrates improvements in the quality of bounds obtained and their ability to verify inputoutput properties. the reviewers agreed that the paper contains novel and interesting ideas and all concerns brought up by the reviewers were thoroughly addressed in the rebuttal phase. hence, i recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_ePhEbo039l", "review_text": "all the reviewers acknowledge that the paper is wellwritten, novel, and shows strong performance gain. besides, all the reviewers are satisfied with the authors response to the raised concerns. ac doublechecks the paper, reviews, and response, and finds that the paper is wellshaped and generally flawless. ac recommends acceptance.", "accepted": null}
{"paper_id": "nips_2022_xUK4E1jpV7z", "review_text": "the reviewers agree that this is a solid contribution. please do revise the paper according to the reviewers comments and the discussion. in particular, the reviewers point out that the presentation can and should be improved this is important because neurips is a broad conference and the theoretical paper is init should be accessible to the general learning theory community.", "accepted": null}
{"paper_id": "nips_2022_u5oLvX8x4wH", "review_text": "the paper studies msgd and sgd methods and provide convergence under weaker requirement on stepsizes. the reviewers in general agree that the result is novel, has interesting techniques, and is above bar for neurips publication. however, the paper should tone down its claim about advantage of large step size as it is mostly based on comparing upper bounds.", "accepted": null}
{"paper_id": "nips_2022_LceHl9wKmoQ", "review_text": "the paper presents a hierarchical chunking model, which builds meaningful and interpretable representations of larger units from sequential data. effectively parsing the data. the key to this model is that it more accurately predicts human response times than baselines e.g. rnn in the original paper and parser in response. the approach is also generalized to show performance on visualspatial and fmri domains parthole and sequential. independence testing and memory decay are the two key tunable parameters. primary concerns by reviewers were around clarifications addressed and the introduction of a stronger baseline parser  addressed.", "accepted": 1}
{"paper_id": "nips_2022_a7YeDeacHpL", "review_text": "this paper proposes a calibration loss function and uncertainty quantification mechanism to improve object detection calibration for both indomain and outofdomain detections. the reviewers have found this paper novel and clearly written with strong empirical results. given this, i am happy to recommend this paper for acceptance at neurips.", "accepted": 1}
{"paper_id": "nips_2022_r6_zHM2POTd", "review_text": "in spite of a somewhat weak empirical evaluation, reviewers appreciate the novel combination of multiple theories on recurrent networks.", "accepted": 0}
{"paper_id": "nips_2022_tQRoZ9nRgM", "review_text": "we thank the authors for their submission. the paper studies a matching problem between jobs and workers, where jobs arrive sequentially and workers have unknown skills that need to be learned. the number of workers is much larger than the number of jobs choice overload, meaning not all workers can be explored  a setting seldom considered in bandit literature. the work further presents novel lower and algorithmic upper bounds on the regret. the paper is clearly written.", "accepted": null}
{"paper_id": "nips_2022_YYyAVk8TrOQ", "review_text": "i thank the authors for their submission and active participation in the discussions. the paper investigates natural language constrained reward maximization. while this application paper is borderline, all reviewers unanimously agree that this papers strengths outweigh its weaknesses. in particular, reviewers noted that the paper presents an interesting 8yrg,wnxo and impactful wzez approach, evaluated using appropriate baselines 8yrg,wnxo, with insightful qualitative analyses ot9z, opensource code ot9z and an overall clearly written paper 8yrg,9kih,wzez,wnxo. thus, i am recommending acceptance of the paper and encourage the authors to further improve their paper based on the reviewer feedback.", "accepted": null}
{"paper_id": "nips_2022_4FSfANJp8Qx", "review_text": "this paper offers an analysis for sgd, and then a variance reduced method pager, under the general kl assumption. this is a very large family of functions, that includes many interesting nonconvex objectives, and thus is interesting for the community. and quoting one of the reviewers the rates provided by the paper in corr. 1 and thm. 3 recover the best prior rates when specialised to the setting of alpha  2, and extend these settings for general alpha and for sgd, under assumption 4 specifically. this means we gain more generality at no additional cost. the reviewers also eventually agreed that the technical novelties introduced to establish the proof are also interesting and new.", "accepted": 0}
{"paper_id": "nips_2022_UmDaZksRyk", "review_text": "the paper proposes a new algorithm for training kernel support vector machines svm, that exploits a leave one out lemma to develop a significantly faster algorithm. the key advances are a data reduction approach, a consolidated cross validation method, and a warm start. empirical comparisons to magicsvm, kernlab, and libsvm demonstrate an order of magnitude speedup. the paper proposes a major advance to kernel svms, without resorting to approximations or linearization. the combination of theoretical insight, exploitation of the structure of the method, as well as careful reuse of computation are all excellent contributions. four reviewers considered the submission, and three reviewers were very pleased by the novel approach for a classic algorithm. one reviewer found several issues with the submission, and did not further engage during the rebuttal period. a different reviewer actually was positively influenced by the author response to the negative review. it gives me great pleasure to recommend this paper for acceptance to neurips 2022. congratulations!", "accepted": 1}
{"paper_id": "nips_2022_2EUJ4e6H4OX", "review_text": "reviewers like that  paper is well written and interesting  experiments are solid this paper should be accepted.", "accepted": null}
{"paper_id": "nips_2022_1X5zpwWoHwu", "review_text": "given samples from an unknown discrete distribution, the goal of the paper is to test if it is a histogram over k bins or epsilon far away from all such distributions in the total variation distance. authors provide a computationally efficient algorithm and further show that the sample complexity is near optimal. the reviewers agree that the results are interesting and novel and i recommend acceptance. as reviewers remark, the paper can benefit by a discussion on the motivation of this problem formulation and the practicality of the proposed approach. i strongly encourage authors to add a discussion addressing these comments in the final version of the paper.", "accepted": 1}
{"paper_id": "nips_2022_xNeAhc2CNAl", "review_text": "reviewers agree that this paper presents a systematic study on the impact of hyperparameters and training strategies of previous works. based on those empirical observations, they propose a simplified model with layer reduction and singlestage distillation, which do not rely on a complicated and adhoc training strategy. extensive experiments are conducted with thorough comparison with existing works. authors also clearly pointout their current limitations. the major concern is that this paper is more focused on discussion of the effectiveness of training strategies in previous methods, while the theoretical contribution is somehow limited. it would be much better if authors could explain their observations more training epochs is needed while additional distillation stages can be discarded from a theoretical perspective, although this may be far out of the scope of this work. nonetheless, this paper presents valuable empirical study over existing transformer compression methods and may inspire following research; therefore, ac recommends acceptance.", "accepted": null}
{"paper_id": "nips_2022_8LeCgKb6UX", "review_text": "this paper studies how to order inmemory sequences for graph embedding. there was a positive consensus that the studied problem is interesting and results are sufficiently discussed. there were some concerns on missing results, which were addressed during rebuttals.", "accepted": null}
{"paper_id": "nips_2022_xqyDqMojMfC", "review_text": "the paper considers constrained smooth nonconvex problems in a stochastic setting where the data comes from a markov chain. for this setting, the paper proposes an algorithm that converges to an epsilonstationary point with stochastic oracle complexity 1epsilon2.5. the paper further shows that in the settings where the projection oracle is expensive to compute e.g., under nuclear norm constraints, the algorithm can be implemented with mathcalofrac1epsilon5.5 calls to a linear minimization oracle. on the technical side, the principal contributions are in designing a novel movingaverage gradient estimator suitable for markovian data and in designing an auxiliary markov chain based on a noise decomposition idea similar to amp05, whose iterates are close to the iterates of the original algorithm. the analysis is sufficiently general to handle both constrained and unconstrained settings. the motivation for studying the considered problems comes from strategic classification an example used in the numerical experiments provided in the paper and reinforcement learning. these problems are of high interest to the ml community and the contributions of the paper seem sufficient. the results will plausibly lead to more developments in this area.", "accepted": 0}
{"paper_id": "nips_2022_WxWO6KPg5g2", "review_text": "the authors describe a method of scaling the benefits of quality diversity qd optimization for automatic generation of rl environments, by replacing costly agent evaluation with a learnt surrogate model. they then demonstrate that this method improves agent performance in two settings a maze environment using rl and a mario environment using a planning. the reviewers agree that the proposed dsage algorithm is both novel and technically sound. where they disagree is on whether qd optimization as a field of research has inherent value for rl research or the broader neurips readership. to summarize each of their respective stances r1 ... i still agree this method is solid and improves previous qd optimization methods. but i didnt directly see its contribution to a broader community. r2 ... i have some skepticisms about qd methods in general and prefer unsupervised methods that do not require human specifications of what constitutes an interesting environment. however, it seems unfair to hold this paper responsible for general disagreements with qd style methods and instead seems like the criteria should be whether this is an interesting or useful contribution to researchers who either work on environment design or on researchers who work on qd  rl. r3 ... this is a novel problem setting for the neurips community, combining level generation from the games community and rl. it is highly relevant in combination with new work on generalization in rl in particular, and evaluating the robustness of our agents. i have no great insight on whether qd methods will prove to be valuable for improving agent generalization or robustness in future. but i tend to agree with r3; in my opinion neurips is the right community to be exploring these questions, and this work has made meaningful and rigorous contributions to this particular line of research. i am recommending this paper for acceptance but note that this is borderline and may require reevaluation in the context of other submissions.", "accepted": 0}
{"paper_id": "nips_2022_MNQMy2MpbcO", "review_text": "the paper proposes a new batch active learning algorithm for multifidelity tasks. the proposal is a sequential greedy algorithmwith approximation guarantees and authors provide extensive experimental comparison. the reviewers generally agree that the paper should be accepted. in the cameraready version, the authors are strongly encouraged to include  additional discussions that they promised in their rebuttal  additional experimental results with more than 3 fidelities  a pointer to their opensourced code and datasets", "accepted": null}
{"paper_id": "nips_2022_qSs7C7c4G8D", "review_text": "the paper was appreciated by all three reviewers, and all gave strong endorsement. some of the positive comments include  the paper is wellwritten in general, all assumptions and definitions are clearly stated.  all formulations of theorems are clear.  theoretical analysis seems to be correct. i checked the appendix and i did not find mistakes, but i might miss something.  obtained results are interesting and they lead to new prospectives and insights.  the experimental comparison is meaningful and illustrative.  the paper is largely wellwritten and provides adequate intuition and explanation for the math introduced.  the paper makes a relevant and significant contribution, addressing the problem of arbitrary client participation.  in general, the paper is well written and provides meaningful insight into federated learning with a unified convergence analysis for arbitrary client participation.  the paper theoretically and empirically addresses an important problem of federated learning which is arbitrary client participation. while many work in federated learning considers a fixed client participation pattern, in real implementations of crossdevice federated learning, clients can leave or join arbitrarily depending on their circumstances. there are not much work that has a unified analysis of different patterns including stochastic of client participation, and this paper contributes to the federated learning community largely by presenting such analysis.  the paper compares its analysis with relevant other work including 11, 27 referenced in the paper as well as the other work which shows linear speedup in convergence with partial client participation in federated learning 31. it is interesting to see in what scenarios for arbitrary client participation we can achieve a similar linear speedup which the paper thoroughly provides insights on.  although the contribution of this paper is more on the theoretical side, it proposes an interesting amplification method line 1114 in algorithm 1 which does not really need additional computationcommunication at the client side, and only requires additional memory saving at the server side which in general is a plus for federated learning. this method also achieves 0 convergence error in some client participation patterns which is interesting and validated in the experiments. i have read the reviews, rebuttal, and also skimmed through the paper. virtually all criticism was successfully addressed; and i would ask the authors to make sure all changes that were promised would be implemented. i agree with the reviewers that this paper clearly passes the acceptance bar. congratulations on such a nice paper! ac", "accepted": null}
{"paper_id": "nips_2022_lYZQRpqLesi", "review_text": "the authors presented an alternating sparse training method for building dynamic inference networks. the method comes with an extensive empirical study over multiple benchmark datasets showcasing its advantages over prior dynamic inference methods. the paper is generally well organized and clearly presented. the technical novelty, however, seems to be somewhat limited given that the inspiration of method and theoretical interpretations are drawn largely from the findings in repitle for gradient based metalearning. the reviewers also raised several other concerns regarding the connection to metalearning, additional baselines and experiment details. the authors managed to address some of these in their responses. after discussions, the reviewers reached a majority consensus that this is an interesting and technically sound paper where strengths outweigh weaknesses. therefore, i recommend that paper can be accepted if room available and the promised revision is made to address the issues raised.", "accepted": 0}
{"paper_id": "nips_2022_Lz2N6UqRYqB", "review_text": "the reviewers carefully analyzed this work and agreed that the topics investigated in this paper are important and relevant to the field. this paper studies the offpolicy evaluation problem ope on pomdps. the authors established nonparametric identification results linking the value of a policy to a sequence of vbridge functions. one reviewer argued that this is an important problem and that the theoretical results in this paper are solid. as limitations, this reviewer argued that the paper makes many assumptions, and that it would be beneficial for readers if the authors could include further discussions on why such assumptions are reasonable. another reviewer also acknowledged the papers contributions to studying the ope problem within the setting of partially observable mdps. they believe the work is novel and that the authors have provided relevant theoretical results considering error bounds. as limitations, this reviewer points out that the method appears to require a set of hyperparameters to be carefully tuned, which may not always be possible in ope setting; and that it is unclear how the misspecification of such hyperparameters may impact errors. they also point out that the paper has no empirical analyses, but mentioned that this is acceptable since this is primarily a theory paper. finally, a third reviewer argued that this paper makes a nice contribution to the field by approaching the ope problem for pomdps from an unconventional perspective. as a limitation, however, this reviewer points out that it is hard to read the paper, given the complex notation used, little to no intuitions given, and the lack of experiments. overall, all reviewers were positively impressed with the quality of this work and look forward to an updated version of the paper that addresses the suggestions mentioned in their reviews.", "accepted": null}
{"paper_id": "nips_2022_LtJMqnbslJe", "review_text": "the paper was wellreceived. the main idea is fairly simple, but the problem is important and the writing and empirical evaluation are solid. based on the reviewers advice, i am recommending acceptance. please make sure to incorporate the reviewer feedback into the final version.", "accepted": null}
{"paper_id": "nips_2022_JpZ5du_Kdh", "review_text": "summary of the paper this paper makes two contributions as far as i can tell 1. the paper attempts to characterize training instability in large language models. 2. the paper presents sequence length warmup slw, a technique that the authors claim to reduce training instability and that empirically makes training llms much more efficient.  metareview the paper isnt perfect, but its an obvious accept. even in the most pessimistic assessment of the paper, the slw technique is a clear win for efficient training regardless of how harshly you judge the analysis of training instability. the biggest weaknesses of the paper are 1 all of the methodological and scientific questions around trying to grapple with the phenomena around training stability and 2 claims of a connection between slw and training instability. on the topic of instability in large model training 1, there has been plenty of griping on twitter but very little scientific analysis of the phenomenon. this papers analysis will hardly be the last word on that topic, but its a reasonable first attempt at something that other researchers will doubtlessly build on and hone over the coming years. i share reviewer c8jbs concerns that correlation with gradient norms is insufficient to make claims about training stability or slw and that more rigorous definitions of training stability are necessary. however, even if, in the very worstcase scenario, this paper is one of the first attempts at characterizing a vexing phenomenon and inspires future researchers to tear apart the modes of analysis and findings in this paper to improve upon them, this paper will have been a worthwhile contribution to the scholarly literature. on the topic of claims of a connection between slw and training instability with the concerns most poignantly expressed by reviewer s5qt, this is really a byproduct of the scientific analysis of training instability. this connection or lack thereof will become clearer as time goes on and the science improves, and i simply ask that the authors acknowledge the uncertainty here. if the above scenarios are the very worst possible outcome for this paper, its still a major contribution to both science and practice. i therefore advocate for accepting this paper, and the reviewers seem to agree with that assessment both the good and the bad. i urge the authors to prepare the cameraready version of the paper by carefully incorporating the feedback of the reviewers and, in particular, reviewer c8jb, who had some very thoughtful comments about ways to clarify the scientific aspects of the paper. to satisfy the reviewers and future readers, i highly recommend openly and frequently acknowledging the vast uncertainty we have about the scientific aspects of this paper as we strive to make sense of this strange training instability phenomenon. the reviewers were enthusiastic and engaged, and the discussion was lively, which suggests to me that this is an exciting paper that deserves to be featured to the community via publication at neurips. there were some other common comments that i urge the authors to address in order to produce the best and most influential possible version of this paper  the reason why instabilities lead to worse performance when they do not cause divergence was not discussed much., the reason why using longer sequences creates instability was not discussed much. the authors discussed this a bit during the discussion period, but its worth emphasizing these as open questions so that other researchers know its important to follow up on.  its worth being crystal clear about truncation vs. just focusing on shorter sequences. its an important experimental detail that may seem surprising or counterintuitive if its not clearly laid out.", "accepted": 1}
{"paper_id": "nips_2022_k4KHXS6_zOV", "review_text": "the paper characterizes how reparametrizedoverparametrized models can be understood as a version of mirror descent on a different objective through a notion they call commuting parameterization. the reviews are all positive and agree that the paper improves our understanding of this phenomenon.", "accepted": 1}
{"paper_id": "nips_2022_WljzqTo9xzw", "review_text": "this work proposes a sdp approach to computing wasserstein gradient direction for 2layers nns, without the need of training the underlying nn. to compute the gradient direction, the authors construct a leastsquare regression problem, and add a polynomial regularization term. then, they show that the relaxed dual is an sdp problem. _pros_  the idea of casting the wasserstein gradient direction as an sdp is novel, and interesting. it also paves the way to more general formulations  the obtained optima is global _cons_  the exposition is lacking some motivation at some points. i think the authors could have move some technical discussion for instance after thm 1 to give a better insight on the motivations. some previous works is also sometimes not put in context for instance regarding the dimensionality reduction.  for a mostly theoretical paper, the statements are sometimes not precise enough, e.g., what is an equivalent problem having the same minimum? the same argmin? in prop 1, what properties are required on the function space? dont you need hypothesis on psi? etc.  from a pratical point of view  but i dont think that practicality is the core aspect of the paper  the computational cost is totally prohibitive as discussed by all referees.  it does not seems that there is a strong practical improvements with respect to training directly the nn after the parameterization of the wasserstein gradient. i believe the idea of casting the wasserstein gradient direction as an sdp problem is interesting, but with respect to the ratio of proscons above, and the lack of a strong positive opinion on this work, i recommend to reject this submission in its current state. i encourage the authors to revise the manuscript in the light of the comments by all reviewers and my own comments for a future submission. in particular, the revision should include the discussion with reviewer rwms which better highlights your work.", "accepted": null}
{"paper_id": "nips_2022_b6to5kfFhQh", "review_text": "the reviewers found this empirical papers observations and explanations regarding search guided by a dnnbased heuristic insightful and definitely worth publishing, despite the fact that the study was done on only one benchmark, sokoban.", "accepted": 1}
{"paper_id": "nips_2022_DdxNka9tMRd", "review_text": "this is mainly a theoretical piece of work studying the problem of clustered federated learning under a mixed regression setting. the authors establish convergence guarantees related to the statistical error incurred by the method; the results include eigengapfree bounds on subspace estimation and vc dimension analyses of certain classes of polynomials. the reviewers highlighted many positive attributes of the work, including  the core contribution of the paper as it is claimed in the paper is novel and exciting rigorous guarantees on clustered federated learning are few and far between, and i believe that trying to attain this is a significant strength of the paper.  i think the authors do a good job of motivating this type of approach and giving a rigorous taxonomy of the related literature.  at a high level, i believe that the papers theoretical results are significant and enough to warrant acceptance. it is clear that there is a lot of interesting facets to the analysis, and the authors mainly do a good job of blending together a variety of disparate analytical viewpoints. in particular, the strategy outlined to prove theorems 1  2 seems promising and mainly correct. i have not verified every detail though, but the portions i have looked at all seem correct. in particular, i especially appreciated the application of the milnorthom theorem though there is a slight, fixable issue with that, see the suggestions below.  i think this is a theoretically strong paper that studies an important topic in federated learning.  the method and theory are both novel to me. typically, in phase 1, the authors utilize clients with low data volume to help reduce the sample complexity of estimating the parameters of anchor clients, which is clever. besides, the theoretical results are highly nontrivial.  the methodology and theoretical analysis are of good quality. especially, the development of the theoretical results is highly nontrivial.  the paper is in general clear. i can follow the main ideas of the paper.  both methodologies and theories are significant. besides these words of praise, some criticism was mentioned. the remaining criticism after rebuttal and discussion was not very major, however. i therefore, and with pleasure, propose to accept this paper. i would wish to stress that it is important to properly address all issues in the cameraready version of the paper. best regards, ac", "accepted": null}
{"paper_id": "nips_2022__h29VprPHD", "review_text": "the paper presents a novel method for offline goalconditional rl that is based on reformulating offline goalcontitional rl as a stateoccupancy matching problem. from this observation, the author are able to leverage and adapt previous work in particular smodice to their setting. reviewers were in agreement this was a strong paper, with excellent writing, good mathematical rigor and thorough experimental work. there were some concerns regarding similarity to smodice which the authors addressed in their rebuttal.", "accepted": 1}
{"paper_id": "nips_2022_gt-l9Hu2ndd", "review_text": "the submission proposes an interpolative decomposition scheme for neural network compression to reduce flops and number of parameters at a reduced cost in accuracyfaithfulness to the original model. the authors provide theoretical evidence, albeit in the two layer case, and empirical evidence on a set of architectures and datasets of the soundness of their claims. while the most negative review 12se contained several inaccuracies that misrepresented the submission, discussions with reviewers have nonetheless helped clarified several points in their paper to the point that the majority of reviewers were satisfied with the submission. therefore, i recommend this paper for acceptance.", "accepted": 1}
{"paper_id": "nips_2022_fARM4P0gAJV", "review_text": "the paper proposes to use shape functions as basis functions to characterize spatial dependencies. they incorporate the shape functions into the spatial regression model and demonstrate strong forecasting performances compared to graph convolutionbased methods. while the techniques are interesting, it is not directly relevant to the iclr deep learning community. the argument for explainability is also subjective and less convincing.", "accepted": 0}
{"paper_id": "nips_2022_5hgYi4r5MDp", "review_text": "this paper studies the disparate effect of model pruning across classes and proposes a new method to reduce the recall distortion across classes. this is a critically important problem, and one which has just begun to be carefully studied in the literature, so this work is timely and relevant. all reviewers recognized the relevance of the problem and the novelty of the authors approach, both with respect to the new approach presented here, as well as the detailed analysis of the various factors which impact recall distortion. there were some concerns regarding the complexity of the pruning algorithms studied, but the authors provided a number of additional experiments on other pruning approaches in their response, finding qualitatively similar effects as might be expected given the reliance of many of these approaches on some form of magnitude pruning. i think this paper will be a valuable addition to a poorly understood and important research area, and should be accepted.", "accepted": null}
{"paper_id": "nips_2022_jqzoJw7xamd", "review_text": "guided by theoretical analysis, this paper proposes a new objective for behavioral cloning based on penalizing deviations in higherorder taylor series terms of policies and demonstrates the benefits in mujoco experiments. the method is limited by its need for the jacobians of the policy to be imitated demonstrated human trajectories in standard imitation learning do not provide such information, instead algorithmicallyproduced policies are needed. however, the authors provide some examples that seem sufficiently motivating. another emphasized reviewer concern is the feasibility of identifying the necessary conditions for the theoretical analysis in actual tasksdemonstration policies, but i tend to agree with the authors that this is an orthogonal question to operationalizing the assumption to develop the proposed methods. overall, i think the authors have addressed many of the lesser concerns of the reviewers and agree with the majority positive opinion of the reviewers and recommend the paper be accepted.", "accepted": 0}
{"paper_id": "nips_2022_LdKdbHw3A_6", "review_text": "the paper proposes a new algorithm called gentd for the estimation of multiple general value functions predictive and retrospective from offpolicy data. the paper shows convergence guarantees for this algorithm to the ground truth for a certain class of general value functions with causal filtering. the initial reviews were mixed. on the positive side, the reviewers found the writing to be clear overall, found the studied problem important and appreciated the theoretical results. on the negative side, several reviewers voiced concerns regarding the experimental evaluation. other concerns are the limitation of the linear setting and possible extensions to the nonlinear setting as well as the significance, specifically, whether this work is merely a combination gtd and density ratio estimation. the authors response could alleviate these concerns, further clarifying the contributions of the paper as well as adding additional experimental results. after the discussion with the authors, all reviewers view the paper positively and the ac agrees. all in all, this paper is recommended to be accepted.", "accepted": null}
{"paper_id": "nips_2022_rg_yN3HpCp", "review_text": "the paper finally received three unanimous scores 667 and all the reviewers think the authors have addressed their initial concerns. the ac also think positive to this work and suggst to accept this paper.", "accepted": null}
{"paper_id": "nips_2022_8gQEmEgWAkc", "review_text": "the paper proposes an adversarial attack strategy, for audiotoaudio modeling, that preserves users privacy against speaker recognition models. all reviewers agree this is a relevant topic for neurips, with a strong contribution on voice privacy. in addition, everyone agrees the experimental section is solid concerns have been addressed during the rebuttal phase.", "accepted": 1}
{"paper_id": "nips_2022_OrcLKV9sKWp", "review_text": "all the reviewers, including the ac, agree that the paper makes a significant contribution to the field that deserves publication at neurips. the ac refers the readers to the reviews for the discussions on the pros and cons of the paper.", "accepted": null}
{"paper_id": "nips_2022_sFapsu4hYo", "review_text": "this paper studies dynamic sparse training dst with finegrained structured pruning. a topic of practical relevance, the authors proposed hardwarefriendly shuffled blocking and simultaneously pruneandgrow. while this paper seems to be a mild extension from chen et.al. 2022, the contributions are clear. one main concern raised is whether evaluating the endtoend speedup with layerwise cuda execution time is a correct practice. i agree with the authors, that is common in the ml literature and more real evaluation requires systemlevel engineering often beyond the scope of an algorithm paper. three out of four reviewers acknowledged they were convinced by the authors rebuttal. one reviewer mainly questioned one missing reference while acknowledging distinct  several algorithm step clarifications, to which the authors delivered pointbypoint responses. therefore, given the overall sentiment among reviewers, im recommending accept.", "accepted": 0}
{"paper_id": "nips_2022_CwQCeJnteii", "review_text": "the paper proposed a new decisionbased black box attack approach for vits. the reviewers appreciate the novelty, extensive experiments and clear writing and unanimously vote for acceptance. authors responses helped clarify reviewer concerns and new results, as well as analysis, were presented in the rebuttal. acs suggest accepting the paper and would request authors to include suggested changes in the final version.", "accepted": null}
{"paper_id": "nips_2022_h8Bd7Gm3muB", "review_text": "the paper provides a novel and practical algorithm for dataset distillation. the paper is clearly written and the reviewers felt that this is a nice contribution to the field. the results demonstrate a clear improvement over state of the art and the experiments are sound. the reviewers raised concerns about limited novelty of the paper but, after a fruitful discussions, agreed that the paper should be accepted.", "accepted": null}
{"paper_id": "nips_2022_8qugS9JqAxD", "review_text": "summary this paper studies symmetries of the space of neural network parameters, i.e. invertible transformations of the parameters which leave the forward function invariant. the authors compute this set of symmetries, called the intertwiner group, for networks with different activation functions. these symmetries have been defined and studied before as notede by the authors. the primary focus of this work is on the impact of these symmetries on learned hidden representations in the network. the author investigate to what extent networks trained from different random initializations effectively learn the same hidden features, and what amount of variation is due to parameter space symmetry. through splicing experiments section 4, they show that representations from one network can spliced into another using the intertwiner group, which results in only a small drop in performance. in section 5, the authors compute use two novel metrics to compute how close learned hidden representations are to being related by a single intertwiner for the whole dataset. section 6 shows, using network dissection, that the significance neuron activations versus linear combinations of activations to interpretability depends on the choice of activation function. strengths reviewers eay8 and kcuw commented that using the intertwiner group to explain weightspace symmetries of neural networks is an interesting and promising approach. moreover, the paper is well written and wellorganized if difficult to understand. the intertwiner group and its properties are wellpresented. reviewers kcuw and cbdu further note that the paper presents a good combination of empirical and theoretical results. discussion of conditions when theorem 3.3 do not hold are reported. reviewer kcuw comments that focusing on relu is helpful, since this is a commonly used activation function. reviewer cbdu comments that while authors are not the first to point out the existence of these symmetries, their treatment is rigorous, more general than in related work, and easy to follow. the reviewers indicates that a contribution relative to related work is that the authors connect weightspace symmetries to the questio of variability of learned representations. the authors achieve good results in splicing experiments with g_relu, which in principle require combinatorial optimzation over a permutation group, by using a convex relaxation using doublystochastic matrices. moreover, the appendix and its proofs appears quite thorough. weaknesses reviewer eay8 found the paper difficult to read. an example is that early figures cannot be understood until reading the section about experiments. the reviewer also found that it was not sufficiently clear how novel or significant the use of the intertwiner group is relative to previous studies based on permutation and scaling groups. other reviewers also had comments on the experiments. reviewer kcuw notes that more reruns for the experiments might be helpful. reviewer cbdu comments that splicing experiments could be more exhaustive. the reviewer also finds tha the interpretability argument in section 6 is not that strong, and perhaps beside the point of the paper. finally, results in section 5 are hard to interpret without baseline numbers. author reviewer discussion in response to reviewer kcuw, the authors performed additional runs now 32 in total. in response to reviewer cbdu they added experiments on pairwise comparison for stitching experiments, as well as answering question. the reviewer notes that they appreciate the responses. to address concerns by reviewer eay8 the authors improved captions in early figures. they also clarified that in their opinion, the main significance of this paper is not the use of intertwiner group per se though this does provide a uniform perspective, but that considerations related to intertwiners motivate the experiments in sec 46, which lead to interesting connections with existing representation learning and interpretability research. reviewer eay8 raised their score 56 after discussion. reviewer ac discussion reviewer cbdu reiterates that they are happy with the paper and the author responses and that they feel this paper can appear. other reviewers did not engage, which the ac interprets as a signal that they do not object to the paper appearing. overall recommendation the ac is satisfied with the level of engagement from both reviewers and authors during the review process. while there is no strong champion for this paper, there is also consensus that this paper is above the threshold for acceptance. on this ground the ac considers this a relatively clear accept.", "accepted": null}
{"paper_id": "nips_2022_JXY11Tc9mwY", "review_text": "overall, this paper achieves strong and interesting results regarding the query complexity of submodular maximization. one reviewer was concerned that the lower bound result was maybe a folklore result that is easy to prove. however, sufficient evidence to justify that claim was not provided and other reviewers did not share the same concern.", "accepted": 1}
{"paper_id": "nips_2022_L2Niz4Olng", "review_text": "the paper develops a multigrid variant of the transformer architecture and applies it to video segmentation tasks. after the author response and discussion phase, one reviewer recommends accept, but three of the four reviewers lean towards rejecting the paper. in discussion, these three reviewers all acknowledged having read the author rebuttal and chose not to improve their scores. the common concerns voiced across these reviews center on questionable novelty, clarity of explanation, and incremental experimental impact. the area chair has also taken a detailed look at the paper, reviews, and author responses, and agrees with the concerns raised by these three reviewers. reviewer hsqg notes that the idea of reasoning across multiple feature levels of a cnnsimilarity tensors is not novel as pointed out by the authors too and section 2.2 seems to be inflating the contribution here. section 2.2 and the author response highlights bidirectional information exchange across scales, inspired by multigrid methods as a key contribution. however, reference 14 ke et al., multigrid neural architectures, cvpr17 explores exactly this idea bidirectional information flow across scales, within a cnn architecture and even utilizes the same multigrid terminology. from the point of neural network architecture design, the current papers novelty appears limited to adapting previously established ideas to transformers. so as not to appear to overclaim, the paper needs a broader discussion of the relationship of the proposed design to 14 as well as other prior work spanning multiscale, multiresolution, and feature pyramid architectures. reviewer concerns over experiments include marginal gains over ablated variants of the system table 1, and mixed results in comparison to danet provided in the author response. overall, the response left unresolved questions over of contribution novelty, presentation clarity, and practical impact.", "accepted": 0}
{"paper_id": "nips_2022_sjaQ2bHpELV", "review_text": "the reviewers appreciated the direction of the work, but they all had rather low confidence despite each one being a foremost expert in rl, which may indicate a potential mismatch of interest with neurips as a venue. they also found the potential applications limited, especially given the great amount of formalization. nonetheless, they judged the developments on this new problem insightful and having the potential to inspire further work in the area and also found no reason to reject. in view of this, i recommend acceptance as i think the potential benefits outweigh the concerns of fit or practical relevance, and given there are no concerns regarding validity.", "accepted": null}
{"paper_id": "nips_2022_VVsNTPK1FBp", "review_text": "this paper studies a sequential recommendation problem where user preferences change over time based on the items selected by the user. the authors show that no sublinear regret algorithm exists for this setting. under the constraint on preference diversity, they derive an algorithm with ot3  4 regret. the original ratings of the paper were 7, 7, 5, and 5; and they did not change after the rebuttal. the reviewers generally like the paper because it studies both adaptive users and diverse recommendations, while most other works try to avoid these topics. this is the strongest point of the paper and my recommendation for acceptance is based on that. on the other hand, the paper is overly theoretical and lacks experiments. therefore, it is unlikely to appeal to a general recommender systems audience.", "accepted": null}
{"paper_id": "nips_2022_bDyLgfvZ0qJ", "review_text": "this paper proposes a new method to adjust the proposal used by sequential monte carlo. existing methods struggle for this problem since the resampling step poses difficulties for reparameterizationbased methods. this paper proposes a twisting method to learn a density ratio by training a classifier to distinguish between two distributions. reviewers agreed the method was novel, relevant, and sufficiently supported by experiments.", "accepted": 1}
{"paper_id": "nips_2022_Ri3T9dwZ_rG", "review_text": "the main contribution of this paper is an algorithm to compute the spectral density of the free multiplicative convolution which appears in the expression of the jacobian of neural networks. some concerns have been raised regarding the impact on a general ml audience reviewers pun2 and 1dwd, and about the presentation of the material especially abstract and introduction, see review gquv. i share part of these concerns. at the same time, the proposed newton lilypads algorithm not only provides a speedup over prior work by pennington et al., but it appears to be an original and interesting contribution in free probability beyond its ml application. therefore, i ultimately agree with the reviewers, who have reached a consensus of accepting this paper. the novelty aspect of this paper will make it an interesting addition to the neurips 2022 technical program. as a final note, i would like to strongly encourage the authors to include in the camera ready the additional experiments on fashionmnist and cifar10, as well as the discussions related to the feedback from the reviewers including, if possible, the suggested titleabstract change.", "accepted": null}
{"paper_id": "nips_2022_dUSI4vFyMK", "review_text": "the reviewers and i agree that the contributions of the paper are of interest and useful addition to the literature. therefore, i recommend accepting the paper. please consider the reviewers comments when preparing the cameraready version.", "accepted": 1}
{"paper_id": "nips_2022_AqexjBWRQFx", "review_text": "this work examines the relationship between fmri recordings of people who read short programs and different properties and representations of the programming code. the aim of the work is to understand what properties of code are encoded by different brain systems, and to understand how similar the representations of code in the brain are to those encoded by selfsupervised language models that are pretrained to encode programming code. more specifically, the authors separate the brain data into language ls and multipledemand md networks and show that 1 many code properties are represented in both networks, but 2 more information about several properties is represented in the md network. the paper presents a fairly original idea, namely that of generalizaing brain encoding of language to code. the work presents the thorough experimentation and improved discussion and motivation thorough discussion of the literature, good motivation of the analyses, sound interpretation of the results, with clear and concise writing. what is less clear overall is the results, that may not bring as much insights as one would hoped. there are some small differences between the findings here and those from earlier work on this dataset ivanova et al., 2020, particularly in assigning a more general role during code comprehension to the language network. yet these differences are hard to interpret in view of current knowledge. furthermore, the md system is overall better at decoding every representation than ls, which does not reveal specific functional characteristics. on the other hand, novel experiments show that simple tokenlevel models are a better match to ls, while the tfidf model, which is also tokenlevel, is better correlated with the md decoding than ls. these results hardly provide a consistent picture. besides, one may wonder whether the strategy of focussing on such networks that are not as homogeneous as claimed by the authors is really a good one. for this reason, there is a large variance between reviewers and a very long discussion, which remained open. in particular, the authors are providing new results in appendix, which complement the ones described in the original submission, but are also hard to gather with the original results. overall, my feeling is tha the paper opens an interesting direction, and could be accepted at neurips to further feed the discussion.", "accepted": 0}
{"paper_id": "nips_2022_Ncyc0JS7Q16", "review_text": "this paper applies existing certificationbased adversarial robustness techniques to spiking neural networks. they achieve this through upper and lower relaxations of the spiking equations. review scores were high variance, ranging from 4 through 8. reviews were generally of high quality. the largest concern was that the use of rate coding for the networks output limited the applicability of the technique. i found the authors response to this concern satisfying. i appreciate that this paper is the first to apply certificationbased techniques to spiking neural networks. i believe it has the potential to produce significant impact for that reason. based upon the reviews, and my judgement of the potential impact, i recommend the paper be accepted.", "accepted": 1}
{"paper_id": "nips_2022_rwyISFoSmXd", "review_text": "this work extends the grouplevel fairness definitions that were primarily established for supervised learning tasks to the problem of conformalized quantile regression. a conceptual contribution is to redefine the grouplevel fairness using the average prediction to quantiles. based on this adaptation, the authors further developed a postprocessing technique to revise a trained quantile regressor to satisfy the modified fairness definition for quantiles. all reviewers acknowledged that the paper is reasonably written and the main idea delivers smoothly. a mixture of theoretical and experimental results are provided there were some questions raised prior to the rebuttal and were successfully addressed, including clarifying the running time of computing quantiles, comparing them to other fair approaches, and partially misinterpreting dp. the authors are strongly encouraged to incorporate these comments into the final version. some reviewers had remaining questions about the novelty of the paper in light of prior results but the meta reviewer feels the introduced concept of fairness on quantile can be a good addition to the literature and might inspire followup works.", "accepted": 0}
{"paper_id": "nips_2022_24fiAU_9vT", "review_text": "this paper proposes a learning framework for deep generative modeling called nonparametric learning by compression with latent variables npclv with a strong theoretical support. the results on image classification benchmarks look compelling and promising. its impressive that this unsupervised approach achieves strong results even compared with supervised and semisupervised approaches. the reviewers unanimously think that this work contains novel and interesting ideas to connect the deep generative modeling, nonparametric learning and compressed sensing in an elegant manner. the reviewers also pointed out that more thorough discussion and investigation on generalization ability of the proposed method to other scenarios beyond vae and image classification would further strengthen the paper. a good discussion on limitations is presented in the paper. after the rebuttal, most concerns that the reviewers raised have been well addressed. the ac recommends acceptance.", "accepted": 1}
{"paper_id": "nips_2022_QeaYt6w5Xa1", "review_text": "the authors study a mixture of a single known distribution h0 and an mixture model whose parameters are unknown. they propose new notions of distinguishability and partial distinguishability that they use for characterizing convergence rates. the reviewers had a mixed opinion about the paper and had several comments about the technical novelty of the work. the authors addressed these suitably in their rebuttal. in particular, their revised introduction is much clearer about the motivation and contributions. i am happy to recommend acceptance of the paper.", "accepted": null}
{"paper_id": "nips_2022_DoQElY73YR", "review_text": "this paper proposes a poisoning defense that unlike existing methods breaks various types of poisoning attacks with a small drop in the generalization. the key claim is that attacks exploit sharp loss regions to craft adversarial perturbations which can substantially alter examples gradient or representations under small perturbations. the authors then propose to generate noise patterns which maximally perturb examples while minimizing performance degradation. i think the framing of this paper is very important and the authors have done a good job at it. they are claiming to have a defense that is non attackspecific as long as it is restricted to the class of attacks involving visually imperceptible inputs. i believe this claim, if substantiated, to be of sufficient significance to the neurips community. unfortunately, i noticed that the reviewers largely did not respond to the author rebuttal, other than pzz3. pzz3s main concerns were with lack of novelty with respect to the antibackdoor learning paper, different settings untargeted, backdoor triggers, and substantiation for the sharp loss hypothesis. having read carefully the authors rebuttal to these, i believe the authors have a done a good job of alleviating the concerns andor misunderstandings. for example ive read the abl paper and agree it is dealing with a different setting. it was nice to see that the authors actually did experiments to show that abl is not effective in this setting and vice versa. reviewer sp5u was concerned with the attackspecificity of the defense to which the authors rebutted appropriately that is is data specific but not attackspecific as long as it is restricted to the class of attacks involving visually imperceptible inputs. as far as i can tell, there were no other strong concerns. based on my own assessment, i believe that the central claim of the paper has sufficient evaluations to support it. the attacks considered are highly varied in their techniques and are also recent and sota. i therefore recommend accept.", "accepted": 0}
{"paper_id": "nips_2022__LceCyuVcH", "review_text": "while the reviewers are divided, i agree with those with accept. the paper introduces an interesting alternative without training on videos and numbers useful for the community. the use of pretrained models in a clever way without finetuning is not a weakness but a contribution. a reviewer raised a concern about lacking analysis on how the proposed pipeline contributes to the fewzeroshot capability but it is already widely accepted that largescale pretrained language models can do well in fewzeroshot settings with proper prompts. also, the use of clip is just a way of extracting textualcategorical representation from the input video using a pretrained network and i believe the authors chose to use clip mainly because it is trained on a largescale dataset with an open vocabulary. i think engineering this component is not the main focus of the paper and the lack of ablations on this should not discount the papers novelty.", "accepted": 0}
{"paper_id": "nips_2022_hqRwcqzegr7", "review_text": "three reviewers recommend accept. the reviewers praised the significant extensions of previous works, the novelty of the ideas, and found the theoretical analysis sound and insightful. the author responses to initial feedback were found to be insightful and reassured the reviewers about their recommendations. hence i am recommending accept. i encourage the authors to take the reviewers feedback carefully into consideration when preparing the final manuscript and to work on the items promised in the discussion period.", "accepted": null}
{"paper_id": "nips_2022_6scShPCpdDu", "review_text": "all reviewers agree that the paper proposes an interesting approach that aims at efficiently solving the inverse problem of stochastic simulators. although some reviewers have some technical concerns at their first reviews, basically those have been resolved by the authors responses. thus, although there are some points that should be modified from the current form, i think we can expect the authors modify the paper in the cameraready by reflecting the discussion. based on these, i recommend acceptance for this paper.", "accepted": 1}
{"paper_id": "nips_2022_edgCBcwZxgd", "review_text": "this paper studies the relationship between connectivity of a deep network and its convergence, both theoretically and empirically. the paper also studies simpler metrics such as effective depth and width to guide the architecture search. overall this is an impressive theoretical paper supported by empirical evidences. all the three reviewers find the paper a valuable contribution to an important theoretical problem in deep learning. after reading the rebuttals, reviewer rabp recommended to accept this paper in its current form. reviewer d7qw felt that all the concerns had been well addressed, and increased the score by one. reviewer 6d9f agreed with the authors response.", "accepted": null}
{"paper_id": "nips_2022_r5rzV51GZx", "review_text": "this paper focuses on improving the robustness of the model for collaborative inference. a preprocessingbased defense method is proposed against inference phase attacks. both theoretical analyses and empirical evaluations are provided to demonstrate the effectiveness of the proposed method. in the review process, mixed reviews are achieved. four reviewers are positive about this paper, while one reviewer is negative. reviews appreciate the strengths of the paper 1 the idea is interesting; 2 the preprocessing strategy does not require much computing cost; 3 the theoretical analysis is detailed and solid; 4 empirical evaluations are overall convincing. one reviewer who is negative about this paper points out major concerns 1 theoretical results on the sparsity; 2 empirical evaluations on the sparsity; 3 the problems in the presentations of the paper. major concerns have been addressed during the rebuttal. we suggest the author carefully merge the rebuttals in the final version.", "accepted": 0}
{"paper_id": "nips_2022_ajH17-Pb43A", "review_text": "this paper proposes automata, an approach that uses gradmatch to select subsets of data in order to accelerate hyperparameter tuning. the reviewers all found the approach to be practical and empirically effective. there were concerns about the robustness to different subset sizes, particularly across different datasets, but the authors demonstrated that automata works well across a number of settings during the rebuttal period. the remaining criticism largely revolves around the novelty of the approach, but the majority of the reviewers believe that this is a useful application of gradientbased subset selection.", "accepted": 0}
{"paper_id": "nips_2022_ZLsZmNe1RDb", "review_text": "i thank the authors for their submission and active participation in the discussions. the paper investigates language instructions and descriptions as a way to teach a student rl agent. all reviewers unanimously agree that this is a solid paper worthy of acceptance. in particular, reviewers found the paper to be well motivated vzcm and well written mi6j, tacklign an important problem fele. the experiments are convincing vzcm, and insightful mi6j, and the method interesting aj8p and novel 8vxv. thus, i am recommending acceptance of the paper and encourage the authors to further improve their paper based on the reviewer feedback.", "accepted": null}
{"paper_id": "nips_2022_C2Mikd2WpOc", "review_text": "the burermonteiro method is widely used for solving large scale semidefinite programs. it works based on replacing an n times n positive semidefinite matrix x with y yt where y is n times p. this has the benefit that it is more space efficient to store y than x, but it transforms a convex optimization problem into a nonconvex one. above the barvinokpataki bound an analogue of the notion of a basic feasible solution for semidefinite rather than linear programming we are at least guaranteed that there is a lowrank optimal solution. but does the nonconvex problem have spurious critical points? recent works have studied the critical points under a smoothed analysis model, and shown that the burermonteiro method works almost down to the barvinokpataki bound. the main contribution of this paper is to complete the analysis of the landscape, by showing that without smoothing, even for the maxcut sdp, there are spurious critical points even for p  n2. one reviewer had doubts about the relationship to the work of bhojanapalliboumaljainnetrapalli, but i found the author reply to be convincing that the setting and techniques are fundamentally different. the other reviewers were uniformly positive. this is a nice contribution to the literature on the burermonteiro method. as a comment to the authors, i would suggest elaborating on the connection to the work of mei et al. i agree that showing the global and local optima are close in objective value can be somewhat orthogonal to showing that the sdp recovers e.g. some underlying clustering in community detection. this provides a further justification why it is important to understand the loss landscape, and not just bound the suboptimality of any locally optimal solution. indeed, from what i remember of the mei et al. paper, the locally optimal solutions do not get nontrivial performance for the associated community detection problem, so i think investigating this further and explaining it would be helpful, since these are subtle distinctions.", "accepted": 1}
{"paper_id": "nips_2022_vDeh2yxTvuh", "review_text": "this paper is an empirical investigation comparing two popular optimization techniques, namely sam and swa. authors compare the performance of these two methods over a wide range of tasks and architectures. they also inspect and compare the properties of minima found by these methods. given the existing interest in the ml community to understand these methods, reviewers are in agreement that insights from these paper are valuable and impactful enough to accept this paper.", "accepted": null}
{"paper_id": "nips_2022_WrIrYMCZgbb", "review_text": "post rebuttal, three out of four reviewers were in favor of acceptance. the holdout reviewer pjyt was primarily concerned with the fact that the method isnt specifically tailored to the task in the sense that other domains have similar features and therefore the method ought to be tested in more settings. the ac understands pjyts perspective in the sense that deep problemspecific understanding of a domain often drives methodological contributions. however, the ac does not find this to be a convincing argument for rejecting the paper and thinks the work provides enough of a strong contribution. accordingly, the ac recommends accepting the paper.", "accepted": 0}
{"paper_id": "nips_2022_22hMrSbQXzt", "review_text": "this paper studies safe reinforcement learning and proposes a new policy optimization method with a rigorous safety guarantee. during the authorreviewer discussion period, the reviewers concerns were mostly resolved. the reviewers have reached the consensus that the contribution of the proposed method is sufficient to be borderline accepted. i recommend it for acceptance and suggest the authors incorporate the reviewers comments into the final version.", "accepted": null}
{"paper_id": "nips_2022_pNEisJqGuei", "review_text": "the reviewers carefully analyzed this work and agreed that the topics investigated in this paper are important and relevant to the field. however, reviewers expressed different opinions on the merits and contributions of this work. one reviewer pointed out that similar ideas have been explored before and that the paper does not necessarily give new insights into value decomposition. the authors counterargued by saying that the proposed method is the first concrete valuedecomposition framework with these particular properties to be proposed in the literature. another reviewer had a more pessimistic view of the merits of this work. they argued that the paper lacks novelty and simply applies the idea of value decomposition to a different settingin the context of actorcritic ac methods. they also argued that they expected a more formal discussion about the impact of value decomposition on ac methods and on whether reweighting reward components could cause instability. the authors responded that offpolicy drl algorithms typically do not have convergence guarantees and that their goal was orthogonal to provide more insight into the design of agents. the authors also argued that standard convergence guarantees apply if the reward function stops changing. ultimately, this reviewers two main points of contention were 1. from their point of view, methods that help implement agents via iterative design could, in principle, be interesting for discussion in other venues, such as a workshop paper, but that this level of contribution does not meet the bar for a full conference paper; and 2. having dynamic weights could cause the method to become unstable and the paper did not study whether this could be a problem in practice. two other reviewers, by contrast, expressed very positive views on this work. one reviewer argued that even though the general idea has been explored before, using it to guide the reward function design is an original contribution. they also argued that the experiments were welldesigned. after reading the authors rebuttal, this particular reviewer said that the authors addressed all their technical questions. another reviewer expressed similarly strong positive views on this paper, arguing that 1. it explores an important direction that ultimately leads to the design and improvement of new algorithms. 2. even if the underlying idea is not new, this is the first extension of that idea to actorcritic algorithms. 3. the paper proposes a novel perspective for diagnosing agent failures and iteratively improving agents, and that this new viewpoint directly engages with the practical difficulties of deploying rl agents. 4. this reviewer further supported their positive view of this paper by stating that the experiments are broad, interesting, and reveal the considerable potential behind the proposed method. for these reasons, they believe this paper will likely be of general interest to the community. ultimately, the main point of contention between the opinion of these two reviewers seems to result from their opposite views on what, precisely, is the contribution of this work  one reviewer sees this paper as one that merely applies the idea of value decomposition of actorcritic methods, without providing thorough theoretical and formal analyses. this reviewer argued that this contribution does not meet the bar for publication as a full conference paper.  the other reviewer views this paper as one that introduces novel diagnostic tools for understanding learning agents. they argued that they found the cleanliness around the proposed diagnostic tools to be appealing and that the resulting proposal for iterative design, based on these diagnostics, is broadly useful for rl. this reviewer highlighted three kinds of diagnostic unlocked by the perspective introduced in the paper 1 identifying insufficient state features; 2 value prediction errors; and 3 reward and exploration. they believe these avenues are new and useful when viewed from the perspective of iteratively identifying and correcting failure modes of agents. ultimately, they disagreed with the reviewer above by arguing that they did not find the attachment to actorcritic algorithms all that critical. overall, thus, it seems like there is disagreement regarding the merits of this paper primarily due to two reviewers not agreeing on whether the proposed iterative design method is in principle a sufficient contribution to a conference paper. regarding this matter, a fourth reviewer expressed a positive view of this work by stating that the questions studied here address an important aspect missing in the literature how to diagnose agents. they argued thatgiven this papers contributions towards that goalthe lack of formal analyses should not be considered a critical reason for rejection. all reviewers encourage the authors to update their work based on their constructive criticisms and, in particular, in a way that tackles the points of contention mentioned in the original reviews and their postrebuttal comments.", "accepted": 0}
{"paper_id": "nips_2022_7fdVZR_cl7", "review_text": "the authors show how to perfectly sample a discrete distribution, given sample access to the bradleyterryluce model on subsets of size 2. while the learning problem has been previously studied extensively, this work initiates the study of sampling. technically, the authors introduce reweighting and rejection sampling ideas that speed up coupling from the past by utilizing an approximate learning algorithm; these techniques could be useful in other applications, as the authors hint in the rebuttal. the reviewers agreed that the paper is technically quite strong and that its quite well written. the authors responded to all remaining questions by the reviewers, clearing the path to the paper being accepted.", "accepted": 1}
{"paper_id": "nips_2022_m16lH6XJsbb", "review_text": "the authors have introduced a method of data augmentation for imagebased reinforcement learning that performs masking in the frequency domain, combined with techniques for stabilizing qlearning, to achieve improved performance on a number of dmcontrol generalization benchmark tasks. there was agreement among the reviewers that this work is novel and technically sound, and their concerns were mainly related to the breadth of tasks explored in the initial submission. during the review process the authors have gone to considerable effort to introduce new tasks e.g. drawerworld, robosuite and carla and the improved performance of their method appears to generalize well. i believe that this work will be of broad interest to the rl community and recommend it for acceptance.", "accepted": null}
{"paper_id": "nips_2022_z2cG3k8xa3C", "review_text": "all reviewers are in agreement that the main factors in particular, the results and their presentation are above the bar for neurips. no significant concerns remain following the author response and the discussion period. i encourage the authors to carefully take into account all of the minor comments when preparing the cameraready version.", "accepted": null}
{"paper_id": "nips_2022_9BL0-oS7W7_", "review_text": "the paper studies the problem of enhancing neural network robustness from a dynamic system perspective. to this end, the authors proposed a nonautonomous neural ordinary differential equation asode that makes clean instances be their asymptotically stable equilibrium points. in this way, the asymptotic stability will reduce the adversarial noise to bring the nearby adversarial examples close to the clean instance. the empirical studies show that the proposed method can defend against existing attacks and outperform sota defense methods. most reviewers rated this work positively and agreed that the proposed method is interesting, technically sound, and theoretically grounded. in the original reviews, they also raised several concerns including missing ablation study and sensitivity analysis, missing references, and some presentation issues. the authors properly addressed these concerns in the rebuttal and after the discussion, some reviewers increased their scores and the majority leaned towards acceptance. overall, given the general support from the reviewers and the revised version of the paper, i recommend accepting the paper.", "accepted": 1}
{"paper_id": "nips_2022_o3HXEEBKnD", "review_text": "this paper studies the singlepositive multilabel learning problem. to address this problem, the authors adopt the pseudolabels to recover the potential positive labels and adopt the global consistency regularization for labelwise embeddings to learn more distinctive feature representations. experimental results demonstrate the superiority of the proposal. all reviewers agree to accept this paper, so i recommend acceptance. moreover, i still have some more suggestions 1 the font size in figure 3 could be larger to make the plot clear. 2 the reference format is not unified. i suggest the authors revise the reference format carefully.", "accepted": 1}
{"paper_id": "nips_2022_DbEVhhuNjr", "review_text": "this paper proposes a novel and interesting perspective on leveraging large masked language models as ways to initialize posterior distributions across probabilistic programming language ppl tasks. the idea is that this distribution can later be fine tuned over different probabilistic programs. all reviewers acknowledged that the idea is a novel application of masked language models and, despite being a natural analogy to the way these models are already used nowadays in nlp, can be potentially impactful for amortized inference in ppls. the paper is accepted upon the introduction of the discussions emerged during the rebuttal concerning related works and presentation. i also advise the authors to think about substituting the term foundation with a more precise technical term masked language model posteriors?, transformer posteriors?,.... imprecise umbrella terms, nowadays, bring more noise than help and inflate the hype around simple concepts.", "accepted": 1}
{"paper_id": "nips_2022_qwjrO7Rewqy", "review_text": "a reasonably interesting paper on deep models to approximate a popular class of objects, extended persistence diagrams, in topological data analysis. this is both an important problem, since these can be used throughout various parts of graph machine learning, and is a challenging one that requires technical innovations. the authors deliver on this, introducing quite good results. the reviewers and i are in agreement about the paper, and the additional results the authors provided in response to the reviews are convincing. for example, on large sparse graphs, the proposed algorithms maintain a big speedup advantage. perhaps the only question that came up is whether this work is more of a tdaspecific topic, but in general the paper clearly fits well within the machine learning community.", "accepted": null}
{"paper_id": "nips_2022_5OLcPQaYTVg", "review_text": "the paper introduces a general design approach for algorithms that learn predictors. this is achieved by identifying a functional dependence of the performance measure on the prediction quality, and applying techniques from online learning to learn predictors against adversarial instances, tune robustnessconsistency tradeoffs, and obtain new statistical guarantees. the problem is wellmotivated and the proposed solution is general and interesting. majority of the reviewers concerns are addressed by the rebuttal. the paper will be significantly stronger if the authors benefit from the additional page to incorporate the feedback into the main paper.", "accepted": null}
{"paper_id": "nips_2022_Hb37zNk14e5", "review_text": "this work presents an approach to learning to prove loopinvariant theorems, organized around jointly training teacher and solver models. reviewers praised its originality and creativity, as well as the quality of the software artifacts produced. both the ideas and the code could be valuable for the community. at the same time, there is a consensus which i agree with that the work oversells itself by claiming to be a general framework for learning to prove theorems. might be true that you could in principle apply the framework to other kinds of theorems, but that would have to be shown empirically. ditto for the claim that this can be applied to program synthesis, which the paper makes in the very first sentence of the abstract. given these overreaches, the camera ready version of this paper _needs_ to soften its claims about its broad applicability for theorem proving and program synthesis. the authors also need to change the title so that it has loop invariant in it or something similar, which they are receptive to in the rebuttal. the paper can talk about these loftier ambitions in the conclusion, but should clearly demarcate the actual extent of the empirical results.", "accepted": 0}
{"paper_id": "nips_2022_ePgJfxYxl7m", "review_text": "this paper is very strong in some regards  it gives a clear technical setup, a sophisticated but accessible analysis impressively so, and solid coverage of related work. at the same time, i believe that it is missing key pieces  ones that would naturally concern a reader in the neurips community. the paper mainly claims a theoretical advancement, and while the path to establishing it is interesting, the end result is not especially impactful in theory, partly because it is not especially connected to other work whether theoretical or practical. on the side of strengths, the paper is very clear and well written, including the technical walkthough. i appreciate the intuitive support that the authors work hard to ensure e.g. figure 3, and the grounding in constructed examples e.g. figure 4. beyond merely being useful for following, it was altogether enjoyable to read. the paper is also very thorough and complete in its presentation. there are details on every matter in the appendix. whats missing roughly comes down to a groundingmotivation and b effect of the end result. naturally a could strengthen b, but to comment on these individually  motivation the neural net architecture studied in this paper is also first introduced in this paper. why this architecture? it bears some relation to another architecture, namely rbfns, but the authors stress novely of the definition. for instance, they highlight that this specific type of rbfn has not previously appeared in the literature. in turn, my understanding is that the results do not directly bear on rbfns of prior interest. there is separately a clear opportunity to motivate radial neural nets by experiment, as highlighted by two reviewers g15c and nux4. however the authors contend that comprehensive empirical studies are ... beyond the scope of this work. short of a comprehensive study, the paper does not investigate this emprically at all the only experiments learn the scalar function expx2, so this key question remains without even partial evidence.  impact of result say we assume the architecture is motivated, and consider the theoretical contributions. the end results are a a universality theorem and b a proof of equivalence between gd in a compressed model and pgd in an uncompressed one. regarding a, i appreciate that proving universality of a strict subclass of networks is as the author response puts it generally harder than proving it for the entire class. however difficulty is not an indication of impact in this case, and many types of networks are universal in some way. the boundedwidth aspect makes a more technically substantial, and i understand that reviewer rxp7 finds the result and techniques valuable in the context of approximation theory. however, i would not recommend this for neurips on this grounds alone. regarding b, compressibility is an appealing property, but again the impact reduces back to the question of whether the full model is useful theoretically or practically. this is again where even the simplest experiment could go a long way. better yet, a positive observation here seems like it could have really meaningful consequences! the reviewers did not come to a consensus on ratings, but there was also no clear argument among them for acceptance. many of the points from reviews and discussion  both approvals and concerns  are reflected above. others concerns initially raised in reviews were completely and thoroughly addressed by the authors in their response. for example, reviewer g15c initially questioned the relation to rbfns, and the authors reply was comprehensive. as a side note, the papers font deviates from the conference format. this did not factor in my evaluation at all, but as a general tip i would avoid these sorts of modifications.", "accepted": 0}
{"paper_id": "nips_2022_4rm6tzBjChe", "review_text": "in this paper, the authors propose visl, a structure learning method that simultaneously infers structures between groups of variables under missing data and performs missing value imputations. the authors conduct extensive experiments on synthetic, semisynthetic, and realworld education data sets and they show improved performances on both imputation and structure learning. the reviewers overall agree that this is a strong and acceptable contribution. 7oh7 has some remaining concerns about the novelty of the proposed approach as the individual components used in the approach are sort of wellknown technics and not hard to address and implement. i nevertheless think that the proposed approach is a worthwhile and powerful combination for missing value imputation. two reviewers bhza and 9ntp are fairly confident to accept the manuscript. overall, this work is an important step towards better missing value imputation in more challenging settings and i support the acceptance of the manuscript.", "accepted": null}
{"paper_id": "nips_2022_ofRmFwBvvXh", "review_text": "this paper provides an approximation error for sobolev type functions by using deep cnns. the approximation error achieves the optimal rate and has adaptivity to the low dimensionality of the support of the input data distribution. they also derive a classification error bound using the approximation error result. this paper gives a novel and important theoretical result. due to the cnn structure, it requires a different techniques from that for fnns and thus the analysis is not trivial. this gives an important contribution to the literature. thus, i recommend acceptance for this paper.", "accepted": 1}
{"paper_id": "nips_2022_G3fswMh9P8y", "review_text": "this work provides an analysis explaining why fedavg can produce more generalizable representations than distributed sgd. theoretical guarantees are presented for a multitask linear regression setting and further empirical results demonstrate the effectiveness of learning representations with image classification tasks. the theoretical analysis presented can be an important building block for the study of more complex settings in federated optimization. all reviewers recommend acceptance. please take the few suggestions by the reviewer into account, and also incorporate the explanations and clarifications provided during the rebuttal in the camera ready version.", "accepted": null}
{"paper_id": "nips_2022_vy7B8z0-4D", "review_text": "this paper uses optimal transport for aligning cortical surfaces based on the similarity of their functional signatures under different stimulations. the paper is well written, and the experimental setup is sound. the authors added experiments and clarifications to address reviewers comments and concerns. the reviewers provided a consensus accept rating for this paper.", "accepted": null}
{"paper_id": "nips_2022_458a8dN8L6", "review_text": "this paper received generally positive reviews, with all reviewers backing acceptance albeit typically quite weakly after discussion with the authors. the paper was praised on a number of fronts such as novelty, potential significance, clarity, and experimental evaluation. though there were a few highlevel concerns raised about the work by the reviewers at least not those which were not successfully explained away by the authors, there were some notable lowerlevel technical concerns, such as the drop in performance for nonadversarial inputs and increased inference time. my own view of the paper is that the underlying ideas and core contributions of the work are very strong and clearly worthy of acceptance, but there are also quite a large number of technical and lowerlevel issues that need to be sorted out for publication as laid out in my comment to the authors. i do not feel that any of these is individually especially serious, but together they do represent quite a large body of things that would need addressing for the camera ready and one could argue that they would collectively represent too large of an update to accept the work. on balance, my recommendation is to give the authors the benefit of the doubt and accept the work, as the underlying contribution is clearly solid and there are no fundamental flaws. most importantly, i think all of the issues raised, though numerous, are addressable for the camera ready and the authors seem to be already making good progress towards this. i also feel that the large number of lowlevel issues raised are partly due to the relatively high level of scrutiny the paper has undergone, rather than being purely a reflection of the number of issues actually present. i do hope though that the authors take the concerns raised seriously and make appropriate updates for the camera ready. additional comments to those given in the previous comment to authors  the introduction should make it much clearer that you are utilising the decoder to help protect the encoder. for me, the key to what is going on here is that the encoder is what is used for downstream tasks and is also the thing that is vulnerable to attack because it is applied before the randomness of the embedding. using the, more robust, decoder to help protect the encoder is a really neat idea, but i do not think this is currently properly emphasised at present. without making this clear i think the approach is quite confusing the first impression i got when you say that you use mcmc to fix the latent code is that you are utilising forbidden information about the original input, rather than utilising the fact we essentially have access to two models.  the first page of the paper has a lot of grammatical errors and some poorly worded sentences, it needs a few lowlevel edit passes. this is not such an issue later in the paper, but there are still some errors later to correct.", "accepted": null}
{"paper_id": "nips_2022_SPoiDLr3WE7", "review_text": "this paper proposes a simple and effective idea to only use evenorder neighbors to improve the robustness and generalization ability of spectral gnns. it is based on the intuition that a friends friend is friend, and an enemys enemy is also friend, thus only using evenorder neighbors improves the generalization across different homophilyheterophily levels. considering that spectral gnns expressive power have been shown to easily saturate recently, analyzing their generalization power is of great importance and is a natural next step. all the reviewers agree on the value of the paper. however, they also point out several issues, such as concerns on the theoretical analysis. i encourage the authors to further polish the paper and improve the theoretical analysis in the camera ready version.", "accepted": null}
{"paper_id": "nips_2022_m_JSC3r9td7", "review_text": "the decision is to accept the paper. the paper provides a theoretical treatment of using dnns for nuisance function estimation in multiplyrobust mediation analysis. the theory developed here is dnnspecific, and novel synthetic experiments that better test the ability of nuisance functions to adapt to complex groundtruth functions are proposed. a proofofconcept demonstration on fairnessoriented mediation analysis is also provided. while the subject matter is dense, the paper builds on a wellestablished line of work, and makes solid theoretical and evaluationdesign contributions. i agree with the authors that in mediation analysis, synthetic evaluation is in many ways the best we can do, especially for evaluating the theoretical claims in this paper. one suggestion i have from looking over the paper in the experiments section, i hope the authors can make clearer what the empirical basis is for judging a method to be semiparametric efficient in their synthetic experiments e.g., the authors note that deepmed is not semiparamertric efficient in some of the synthetic settingswhich is finebut dont explain in the main text how such a judgment was made. i suspect this comes from comparing performance to the estimate that used oracle nuisance functions under appropriately large sample size, but this should be stated explicitly, especially for the benefit of others in the community who might want to extend this work using the evaluation framework developed in this paper.", "accepted": null}
{"paper_id": "nips_2022_XQu7UFSbzd2", "review_text": "this paper proposes a method for predicting the next event given sequential data. for the prediction under distribution shift, the proposed method uses a backdoor adjustment, variational inference of latent context, and hierarchical branching structure. the proposed method that combines techniques from different fields is interesting. in particular, the use of causal methods for the distribution shift problem in temporal event prediction is novel. the experimental results demonstrate the effectiveness of the proposed method well quantitatively and qualitatively. the paper should be improved according to the reviewers comments, e.g., clarifying the motivation in realworld applications.", "accepted": 0}
{"paper_id": "nips_2022_ebCk2FNI1za", "review_text": "the paper studies the approximation properties of group convolutional neural networks. it establishes the ccuniversality of group cnns, i.e. that such networks can approximate any continuous function over any compact set, using a new constructive approach which is based on a generalization of the ridgelet transform. the proof is constructive, in the sense that approximating networks are given in closed form by discretizing the transform. this approach may have applications beyond the scope of the paper  most immediately, to identifying classes of functions for which neural network approximations are accurate in a quantitative sense; this can be tied to the decay properties of the ridgelet transform. reviewers found the paper to be clearly written and of high technical quality, albeit somewhat mathematically dense in its presentation. universal approximation theorems provide an important piece of theoretical background, as well as a sanity check for new network architectures; having a unified, constructive approach to derive them could stimulate further work.", "accepted": null}
{"paper_id": "nips_2022_7YXXt9lRls", "review_text": "unanimous accept from 4 reviewers this paper was initially divisive among reviewers scoring 7744, now 7756 post rebuttal while the consensus of this representation learning work had initial strengths of clear analysis of previous metrics, clear evaluations, writing, and good experimentation; the prior weaknesses were mainly confusion about derivations and equation interpretations, no analysis of learned model, no visualizations of the latents space, lack of explanation why the method only does well in some environments and not others. these confusions have since been cleared up, in particular the authors have since added additional analysis in the appendix is useful e.g. table 4 and fig 15 to satisfy reviewers nvyxs concerns over clarifications about equations", "accepted": null}
{"paper_id": "nips_2022_PW1VAoxeOU", "review_text": "the reviewers reached a consensus that the paper can be accepted by neurips. the ac notices a few weaknesses pointed out by the reviewers and personally thinks the papers results are somewhat expected. nevertheless, the ac would like to recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_o8H6h13Avjy", "review_text": "this paper studies the problem of securing a model once it is published as a service. in most prior studies the focus was on either protecting the model from extraction attacks me or from identifying the data used for training the model mi. the authors propose that a simultaneous attack on both surfaces is even more powerful since the mi attack provides more information to the me attack. the reviewers found that this work is interesting, and results are relevant to the committee as they can highlight the need for protecting the multiple surfaces of attack against ml models. however, there is a concern about the results presented in response to the comments of reviewer ljza in the response, the authors reported on the result of an experiment in which a defense against mi was simulated. from the gist of the paper, one would expect that this will result in some level of protection against me attacks too. however, the results provide only weak support to this assertion. therefore, we think the paper should address this inconsistency.", "accepted": null}
{"paper_id": "nips_2022_vbPsD-BhOZ", "review_text": "all reviewers agree that this paper deserves to be published in neurips 2022, with some minor concerns that are mostly adressed in the rebuttal. please incorporate the remaining reviewers feedback for the cameraready version.", "accepted": 1}
{"paper_id": "nips_2022_VT0Y4PlV2m0", "review_text": "the submission analyses the simplified transformer architecture from the unfolding optimisation perspective, which was recently used to analyse simpler mlp and cnn models. four reviewers are positive on the submission results and agree that they potentially can bring new insights and more powerful architectures. ac recommends acceptance.", "accepted": null}
{"paper_id": "nips_2022_T2DBbSh6_uY", "review_text": "the reviewers are enthusiastic about the work, and all recommended for the acceptance of the paper. the reviewers think the work is solid and novel, and potentially impactful. for example, reviewer mddg noted this paper transforms the geometric placement problem into multiple visual representations using three masks, which opens the possibility of using mature convolution networks to extract the global layout information. thanks to the authors for the detail rebuttal and the thorough discussion with the reviewers. incorporating these points raised in the communication will further improve the paper.", "accepted": null}
{"paper_id": "nips_2022_sYDX_OxNNjh", "review_text": "the paper identifies a problem in prior works on skilllearning some states that are visited in training by diverse skills may not be visited after skills are learned dubbed as exploration degradation problem. the problem is wellmotivated and is an important one in unsupervised skill learning. authors then propose a method to overcome the identified issue. comparisons are made to stateoftheart methods such as dads and lexa. the reviewers are split in their opinion uvqx recommends an accept, nwfc recommends borderline accept, while kkqh recommends rejecting the paper. authors addressed the primary concern of nwfc on comparison with lexa. kkqhs main concerns are about the solution not being elegant. while i agree that more elegant solutions can be found, i think that identifying a bottleneck in learning diverse skills is of good value to the community. furthermore, the proposed solution works across a range of environments. therefore, i am lean towards a positive recommendation for this paper. i encourage the authors to clearly address the reviewers suggestions and comments in the camera ready version.", "accepted": 0}
{"paper_id": "nips_2022_jxPJ4QA0KAb", "review_text": "the authors consider the traditional problem of approximating graph convolutional networks using chebyshev polynomial, which is known as chebnet. then, the authors propose a new gnn model called chebnetii enhancing for reducing the runge phenomenon; this is an important contribution to gnn. overall, the reviewers are positive about the paper. thus, i also vote for acceptance.", "accepted": 1}
{"paper_id": "nips_2022_LYXTPNWJLr", "review_text": "this paper proposes a multitask rl architecture that composes taskindependent parameters and taskdependent parameters to construct a taskspecific policy. the results on the metaworld multitask learning benchmark shows that the proposed method outperforms relevant baselines. in general, the reviewers found the idea interesting and novel, and the results are solid. several concerns about the lack of singletask baseline and the lack of scores given intermediate steps were addressed with updated results during the rebuttal period, which made all the reviewers lean towards acceptance. thus, i recommend accepting this paper. in the meantime, there are still remaining comments about the result beyond 2m steps suggested by reviewer ccrp and additional baselines suggested by reviewer dw33. i highly suggest the authors to include these results for the cameraready version.", "accepted": null}
{"paper_id": "nips_2022_eCUeRHHupF", "review_text": "this paper presents a fourstage process for training completely unsupervised machine translation models. the results are fairly strong. after some discussion all reviewers are convinced that the evaluation is sound. the reviewers are somewhat split about the novelty, some say  the proposed method is very interesting, intuitive, innovative, and wellmotivated. others say  the idea itself is not novel in that there exists prior work on isolating target language specific representation by feedforward network in the decoder side. some reviewers find the approach with its four steps cumbersome, while others dont have an issue with this. on balance, just above the decision boundary.", "accepted": null}
{"paper_id": "nips_2022_xDaoT2zlJ0r", "review_text": "although all reviewers find an interesting point and the significance of this paper, some reviewers have several critical concerns in the paper such as the readability very hard to follow, and lacking some important information and the nonconvincing empirical evaluations. although it seems that the authors could answer parts of these concerns in their responses and , those seem to require much modification from the original draft. as a total, although this paper could be a good paper after reflecting the reviewers comments, my recommendation for the current form of this paper is rejection from the relativistic perspective, compared with papers accepted to neurips.", "accepted": null}
{"paper_id": "nips_2022_f-fVCElZ-G1", "review_text": "this paper explores posttraining quantization on large transformerbased models, using techniques such as groupwise weight quantization, tokenwise activation quantization and layerbylayer distillation. the paper also provides a system backend support that demonstrate speedup on commercial gpu devices. overall, this is a solid paper. the quantization methods used in the paper are not exactly novel. not only the groupwise and tokenwise quantization as pointed out by the reviewers, but also the layerbylayer distillation using high precision models have been reported recently e.g. brecq iclr2021 and adaquanticml2021. however, it is appreciated that the authors could take these techniques further to 1 implement and optimize gpu kernels to demonstrate real speedup and 2 evaluate the techniques on large scale models. with the opensource code as promised, i think the research community and industry can all benefit from this work. in addition, the paper is well written and easy to follow. the methods are evaluated thoroughly. during the rebuttal period, the authors provide very detailed response to address the questions and concerns raised by the reviewers. therefore, this paper is recommended for acceptance.", "accepted": 0}
{"paper_id": "nips_2022_krV1UM7Uw1", "review_text": "the paper studies the problem of labeloutlier robust regression with prior on the optimal parameter. the reviewers agree that the results are novel and significant. there is certainly a concern about the novelty of the method and about additional insights provided by the result. however, as the paper studies this relatively new problem and provides solid results for it, we recommend accepting it for publication.", "accepted": null}
{"paper_id": "nips_2022_WHFgQLRdKf9", "review_text": "the reviewers found this to be a wellexecuted technical contribution, and all reviewers agree it meets the bar for acceptance. while this paper does not seem to provide a breakthrough novel insight, it does contribute useful information for the field, and i believe sharing with the community is beneficial. i recommend accepting this paper.", "accepted": null}
{"paper_id": "nips_2022_J-IZQLQZdYu", "review_text": "the reviewers unanimously agreed that the paper is wellmotivated and the theoretical results surrounding the proposed brownian mechanism are interesting. initial concerns regarding presentation and clarity were assuaged after the authors responses to the reviews. overall, the paper is a nontrivial and valuable extension of ligget et al., 2017 and should be presented at the conference.", "accepted": null}
{"paper_id": "nips_2022_ZL-XYsDqfQz", "review_text": "this paper has proposed a method named zeroshot learning for attributes to deal with a research problem about novel attribute classification and attribute labeling. the reviewers have many questions in the intial round. after the rebuttal, the authours clarify most unclear points, and some reviewers raise the score. in general, all the reviewers agree with the acceptance of this paper.", "accepted": null}
{"paper_id": "nips_2022_AQd4ugzALQ1", "review_text": "the paper proposes a model for a multiple teacher, single student setting for medical image classification. the reviewers where split, with two reviewers leaning towards accept and one leaning towards reject. the main criticism of the negative reviewers is that the proposed model only slightly outperforms the state of the art. given the extensive experimental evaluation and the fact that the proposed method consistently outperforms the state of the art, the improvement should be statistically significant. the negative reviewer has acknowledged the improvement in the discussion phase. a second criticism was the lack of significance of the proposed learning setting. as the reviewers find this setting novel and due to the relevance in the medical domain, i vote to accept the paper.", "accepted": 0}
{"paper_id": "nips_2022_PzI4ow094E", "review_text": "this paper extends the marginal sensitivity model to continuous treatments. given the developments in the discrete treatment setting, none of the parts of the paper are surprising. further, there are several simultaneous related works that carry out a generalization to continuous treatments. that being said, the work is sound and a polished contribution.", "accepted": 1}
{"paper_id": "nips_2022_QedyATtQ1H", "review_text": "this paper analyzes the convergence of policy gradient algorithms in generic stochastic games. the authors provide local convergence guarantees for projected gradient descent with the reinforce gradient estimator. reviewers were generally positive on this paper though i think it needs to be much better contextualized in the literature on gradientbased learning in games of which this is a special case. indeed while interesting in the context of marl  the results are not very surprising given that they seem very similar with other local analyses of stochastic gradientplay in games see e.g., 1. furthermore the equivalence of equilibria follows from well known manipulations of the singleagent rl loss function like those performed in 2, genericity arguments for nash equilibria 3, as well as work on variational inequality approaches to learning in games 4. the final version of the paper should really comment on these previous results. nevertheless, due to the positive reviews and the relevance to marl, i recommend this paper for acceptance. 1 chasnov, ratliff, mazumdar, burden; convergence analysis of gradientbased learning in continuous games 2 zhang, ren, li; gradient play in stochastic games stationary points, convergence, and sample complexity 3 ratliff, burden, sastry; characterization and computation of local nash equilibria in continuous games 4 mertikopolous and zhou; learning in games with continuous action sets and unknown payoff functions", "accepted": 1}
{"paper_id": "nips_2022_48Js-sP8wnv", "review_text": "the paper proposes simulated evaluations simevals to guide explainable ai xai researchers about what explanations to include in a user study. all the reviewers agreed that this is a novel contribution to a significant and timely problem. there were common questions around the empirical evaluations that the authors clarified during the feedback phase. the reviewers have acknowledged the authors responses and have confirmed that their questions were adequately addressed. by adding the new table contrasting prior work that the authors included in their feedback, as well as the clarifications from the reviewer discussion, the paper will be substantially stronger.", "accepted": 1}
{"paper_id": "nips_2022_Kf8sfv0RckB", "review_text": "the basic ideas and contribution of this paper have been positively evaluated by the reviewers. there were a few questions, but many of them have been resolved by the authors careful replies. there is an opinion that the comparison with reinforcement learning is inadequate, but since it is an application, this is not a major problem.", "accepted": null}
{"paper_id": "nips_2022_iqCO3jbPjYF", "review_text": "paper shows that outcomeconditioned behavior cloning ocbc is not guaranteed to maximize longterm reward in multitask rl. a simple, but effective variation of ocbc is proposed that does guarantee policy improvement. while the scope of the paper is rather specific a certain form of behavior cloning in multitask rl, this family of methods has gained some momentum recently, while there are still many theoretical questions around it. this paper addresses some of these in a clear way and proposes a specific improvement. a clear accept in my view.", "accepted": null}
{"paper_id": "nips_2022_wJwHTgIoE0P", "review_text": "this work presents a method for training neural nets on synthetic data. this data is collected from a collection of thousands of opengl programs that rendered images, which are then used for representation learning. the big advantage of this approach is that it avoids of a lot of the biases that are present in natural image datasets. the proposed method is competitive for supervised and unsupervised scenarios. i find the results, especially those with finetuning as done during the rebuttal period relatively compelling. while i agree with reviewer vdrw that, on the whole, the major contribution an image collection is not very strong, i still think this kind of approach will be widely interesting to the neurips community. precisely because the work shows carefully albeit empirically only that procedurally generated datasets could be useful for representation learning, especially if you want to avoid the various pitfalls of natural image sets.", "accepted": 0}
{"paper_id": "nips_2022_67NpH8-_h94", "review_text": "this paper studies how to improve learning efficiency in settings where the agent has access to an abstract, simplified model of the world. all the reviewers voted to accept and each pointed out useful clarifications and improvements to the paper. the main contributions of this work are the novel problem setting, algorithms proposed and theoretical results. there are some proof of concept experiments included as well. the paper is not without flaw though  the intro dismisses two related works because those algorithms were very different. this is either a communication problem or a bad excuse  the proof of concept experiments dont really match the motivation of the paper well. the toy environments are not representative of the targeted complexity this is not to say they must be messy, highdim real sensor data. most importantly the paper has basic correctness issues with the experiments provided  reporting results from five runs seeds in toy domains and reporting standard deviation which is likely not well estimated from 5 runs instead use a more conservative measure of confidence such as studentt ci or bootstrap intervals  no clear discussion of how the baselines where tuned they all use the same stepsize in each experiment. with adam the stepsize is less sensitive, but its very unlikely that the values of alpha used and other hypers were best. we need a clear description of the empirical methodology used here; researcher descent is a very biased process and not good enough  this is particularly problematic because the abstract claims the new method outperforms the baselines these empirical issues are common practice in the field but poor none the less. it is very likely all could be addressed and the main messages of the paper continue to hold. the empirical writeup of this work significantly weakens an otherwise strong paper.", "accepted": null}
{"paper_id": "nips_2022_LYcuTyW6Vu", "review_text": "building on the novel observation that there is a strong correlation between model quality and number of parameters in the decoder of autoregressive transformers, this work proposes a trainingfree nas algorithm for this class of models. the main concerns raised by reviewers are related to the key observation the paper is built on this correlation, that may not necessarily hold in all situations. however, authors were able to clarify important points, provide additional motivations and empirical results, eventually leading to all reviewers leaning towards acceptance. considering the popularity of this class of models, i believe this work should be of significant interest to researchers and practitioners in the field, and i am thus recommending acceptance.", "accepted": null}
{"paper_id": "nips_2022_sL7XH6-V21e", "review_text": "this paper establishes a set of theoretical results for characterizing the behavior of deep forest, an important model in deep learning. the analysis approach is novel and the results are of significant importance. the majority of the reviewers appreciate the authors contribution and all reviewers recommend acceptance. thus i would strongly recommend accepting the paper.", "accepted": 1}
{"paper_id": "nips_2022_ijzm0EhAY_w", "review_text": "this paper proposes expectationmaximization contrastive learning emcl to learn compact videoandlanguage representations for the general goal of projecting the video and text features into a common latent space. reviewers agreed that this is an important problem of modality gap, a useful way to represent the modalities into a shared latent space of learned basis vectors, and good improvements. some reviewers also expressed concerns that the approach is a bit complicated and simpler baselines should be compared to done in rebuttal; several missing related works discussion w.r.t. distillation, dimensionality reduction, transfer learningdomain adaptation; some motivation confusions about closing representation gap; more tasks should be added such as qa and captioning.", "accepted": 0}
{"paper_id": "nips_2022_pFqgUJxXXz", "review_text": "this paper proposes a task augmentation method for metalearning that generates new tasks which match the true task distribution and are also challenging for the current metalearner. this is done by training a task upsampling network with an adversarial loss as well as an emd loss between the adversarially generated and groundtruth tasks. the authors provide a theoretical analysis that the tasks generated by their adaptive task upsampling framework are indeed taskaware, or comply with the true task distribution, and validate the method on both regression and classification tasks. the results show that the proposed task upsampling method outperforms existing regularization methods or task augmentation methods. the paper initially received split reviews. reviewers were generally positive about the introduction of the desirable properties for augmented tasks in metalearning as a meaningful contribution. they also found the proposed method with adversarial taskaware upsampling as novel and interesting, and the experimental validation as adequately showing the effectiveness of the proposed method as well as each of its components. another advantage that is not mentioned by the reviewers, is that it is applicable to both regression and classification tasks. however, reviewers were also concerned with unclear, and somehow disconnected theoretical analysis from the actual framework, marginal improvements over stateoftheart baselines such as mlti, unclear effectiveness of the adversarial loss, and missing results on larger fewshot classification datasets. most of these points were addressed in the author response, which resulted in some of the reviewers raising their scores, and all reviewers leaned toward acceptance after the interactive discussion period. in sum, this is a wellwritten paper that introduces meaningful insights about task augmentation in metalearning, as well as a novel method, and may be of interest to researchers working on the topic. however, methodwise, its relatively weak improvement over existing, simpler task augmentation methods may diminish the potential practical impact of the work. yet, the advantages outweigh its drawbacks and the work is worth sharing at neurips 2022.", "accepted": null}
{"paper_id": "nips_2022_DhHqObn2UW", "review_text": "this paper studies the problem of online optimization with longterm constraints and proposes an interesting blackbox type of algorithm that works well for both the stochastic setting and the adversarial setting. one issue that reviewers brought up for the adversarial case is whether competing with only rho1rho fraction of the optimal utility is indeed meaningful or necessary. in the rebuttal the authors mentioned that this is likely optimal in light of its connection to the literature on the budgetconstrained case, but only leave the formal reduction between the two as future directions. we believe that the paper would be much more complete if this reduction is properly spelled out, and thus encourage the authors to do so in the final version.", "accepted": null}
{"paper_id": "nips_2022_Hbvlb4D1aFC", "review_text": "this work investigates a theoretical way to analyze selfsupervised learning. the identifiability of latent stochastic generative parameters in the context of masking selfsupervised prediction tasks is investigated, where hmm and gaussian hmm models are assumed for latent space sequences. the 3 main claims are proved by theorems. the paper does not provide experimental results in the main paper on how well the findings hold in practice, but experimental simulations are provided in the appendix. i suggest accepting the paper, as it deals with an important topic in understanding ssl and it is of significant and timely interest to the community by presenting novel results on the identifiability of masked prediction tasks in discrete and gaussian hmms. although the setup is a little narrow and strongly simplified, the paper is original enough. further, the paper is well written and provides thorough comparisons to related literature. in future work, addressing practical impact would be a nice addition.", "accepted": null}
{"paper_id": "nips_2022_Eccx2-_vZS4", "review_text": "this paper makes an interesting observation that imputation error metrics may work poorly for multimodal data, and proposes a creative solution. however, empirical evaluation of the idea on real datasets, and comparison to other proposals for imputation with multimodal data, are lacking. i would encourage the authors to take the reviewers suggestions seriously and resubmit to a future conference.", "accepted": 0}
{"paper_id": "nips_2022_QnajmHkhegH", "review_text": "this papers dualcoop extends the previous coop prompt learning framework to multilabel and multilabel zeroshot recognition. reviewers were broadly positive, appreciating the writing, good results, and overall idea of exploiting pretrained clip models for mlr. questions focused on comparison to vanilla coop, evaluation in the fully supervised regime, inference cost and impact of finetuning. these were all generally resolved during author feedback phase. since all reviewers are positive and questions are resolved, i recommend accept.", "accepted": null}
{"paper_id": "nips_2022_ZV9WAe-Q0J", "review_text": "the paper studies how to properly conduct adversarial training on vits to obtain adversarially robust models. the paper mainly tried to conduct a comprehensive study on vit adversarial training and identified several important training heuristics that can improve the robustness of vits. among four reviewers, three consider this as a borderline paper and one strongly supports this work. after several discussions, we think this paper will build a solid foundation for future adversarial robustness studies on vit models so decide to accept the paper. however, we do hope the authors carefully revise their paper based on the reviewers suggestions. more specifically, the main concern from reviewers is the correctness issue pointed out by reviewer 63wy. we hope the authors can carefully check and explain those issues in their final version.", "accepted": 1}
{"paper_id": "nips_2022_7k_J2kkIy3U", "review_text": "this paper addresses the problem of identifying a network of interacting entities with applications to genetic regulatory networks. the approach makes use of a poisson lognormal graphical model structure for count data. the paper shows that the approach has desirable statistical properties such as consistency of the precision matrix estimate. the paper compares the performance to some other methods on various graph structures and presents results on a singlecell rna sequencing data set. the reviewers generally found merit in the approach. the clarity of the paper was judged by the reviewers to be good. the suggestion from one reviewer that prior information about the gene regulatory network may be incorporated was welcomed by the authors, though this addition may have an impact on the consistency results. the rate of convergence was found to be rootn and while the practical applicability of that rate for relatively expensive samples such as scrnaseq is not clear, the method is applicable to a wide range of problems where a sample size that approaches the numbers simulation examples is more feasible. i would strongly encourage the authors to discuss potential limitations including 1 the interaction between the expense of scrnaseq data collection and the asymptotics of the consistency results, and 2 the degree to which model misspecification affect scientific conclusions based on parametric inference.", "accepted": null}
{"paper_id": "nips_2022_apC354ZsGwK", "review_text": "this paper studies the popularity bias of collaborative filteringbased recommendation systems. specifically, this paper proposes a biasaware margin into the contrastive loss, resulting in a modified bc loss to remedy the problem. geometric interpretation and experimental results are provided to validate the effectiveness of the proposed method. this paper received mixed review comments. a review raises concerns about the technical novelty and contribution of the proposed method mainly due to the simplicity of the proposed solution. considering that the simple solution is well supported by theoretical justification and empirical results, i lean to accept this paper.", "accepted": 1}
{"paper_id": "nips_2022_W-_4hgRkwb", "review_text": "the paper proposes a clusteringbased method for sequential testtime training, where the test data under distribution shifts arrives in a stream manner. the authors also explore a newer setting by emphasizing certain dimensions of the testtime adaptation problem they distinguish experiment setups, from the perspective of whether it uses onpass adaptation or multipass adaptation and whether it needs to alter the training phase. the distinction is important as prior works often compared under different setups, leading to unfair comparisons. the authors did a good job to address the reviewers concerns in the authorreviewer discussion phase, and at the end, all reviewers unanimously support the acceptance. ac also did not find a particular weakness for rejection. as the testtime adaptation has emerged as a realistic solution to make ml models more robust to work, ac thinks that the contribution of this paper is of interest to a broad range of neurips audience and would guide future research on the topic. overall, ac is happy to recommend the acceptance.", "accepted": null}
{"paper_id": "nips_2022_2tfv0K8Vbtf", "review_text": "the paper considers the experts problem when the online player is allowed to choose b arms instead of 1 and is rewarded according the best arm within the set it chose. the comparator considered in the first result of the paper is standard best single arm. the paper shows that the regret scales in this setting as  t1b1  recovering and extending the standard result. the paper uses this result to then further get another result when the set wise function is max. here existing result in the comparator compared with fixed size b, i.e. max size allowed, where as they present a result that allows comparison with any budget b. they also play a lnt factor more arms in each round which allows them to compare with opt as opposed to 1  1e opt. the paper according to reviews is well written, but i found section 3 hard to read, especially the notations of b and tildeb. i also think the regret statement in that theorem is not well defined, you should state clearly it is with respect to b sized sets. there are other shortcomings of the paper like the bandit results dont match the optimal in the case of b1 as discussed with a reviewer, which points to suboptimality of presented bound. the lower bounds presented are also unfortunately weaker. these are not shortcomings per se but desiderata to make the paper very strong. i also find the fact that they lnt factor more arms in osfm is not stressed appropriately. overall the results in the paper are good as presented, but the shortcomings listed above the paper put it right on the borderline. overall there is general appreciation of the results by the reviewers and as such according to that it puts it slightly above the borderline according to me. i am recommend a marginal accpet for the paper. i would like to urge the authors to look at the paper critically irrespective of the outcome and definitely look to improve presentation at the minimum. other lingering questions if answered will be a great bonus.", "accepted": 0}
{"paper_id": "nips_2022_um2BxfgkT2_", "review_text": "the paper applies transformers directly to a graph by treating all nodes and edges in the graph as tokens. the author prove this approach is theoretically at least as expressive as an invariant graph network, which is already more expressive than all messagepassing graph neural networks. the approach is simple and interesting. reviewers had concerns on the empirical studies as only one dataset was used. in the response, the authors have partially addressed the concerns by offering more results. more discussionanalysis of the empirical results is expected. considering that the paper is mainly on the theoretical side and the paper does provides interesting new insights, id recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_XYDXL9_2P4", "review_text": "the paper proposes a method addrop to drop attention weights in a network to alleviate overfitting. it randomly sample a set of token positions with respect to attribution score calculated in first pass. the authors provide a variety of experiments on multiple tasks snli, ner, mt, etc. showing effectiveness comparing to other methods. the method is slower since it needs a separate pass to calculate attention attribution.", "accepted": 0}
{"paper_id": "nips_2022_c4o5oHg32CY", "review_text": "the paper is about speeding up the saliency computation used in gradient based mixup algorithmspuzzlemix, comixup, etc. the authors propose employing the attention layer output of the transformer to replace the expensive saliency computation. since focus of the the paper is on improving the speed and accuracy aspects of gradient based mixup algorithms, the tables 1 and 2 should remove i strongly suggest the authors to remove tables 1 and 2 as it only shows irrelevant accuracy results across different network architectures. since the main focus of the paper is on improving over the previous mixup algorithms, i suggest the authors to run the following experiment and insert it as table 1 in the main paper. current table 5 does not show fair comparison across different mixup methods. 1 fix the network architecture. denote it as a i.e. cct73x1 2 report the following accuracy results. a; a  input mixup; a  manifold mixup; a  cutmix; a  puzzlemix; a  comixup; a  horizontal tm; a  vertical tm; a  horizontal tm  vertical tm here, you would need to make sure the backpropagate all the way through the individual pixels as opposed to tokens to properly run the gradient based mixup algorithms. also, i suggest the authors to add the endtoend forwardbackward computation time per image in addition to the avg. latency in table 3. overall, i like the proposed method of speeding up the saliency computation via attention maps. however, the experiment protocol needs a lot of improvement.", "accepted": null}
{"paper_id": "nips_2022_IlYS1pLa9y", "review_text": "all three reviewers lean towards the acceptance of the paper. the reviewers believe the rebuttal has addressed their concerns. the ac recommends acceptance of the paper, and suggest the authors to include the materials and the discussion they promised in the rebuttal in the final version of the paper.", "accepted": null}
{"paper_id": "nips_2022_UwzrP-B38jK", "review_text": "i thank the authors for their submission and active participation in the discussions. the paper presents a method for rule representation learning that can be transferred accross tasks. all reviewers unanimously agree that this papers strengths outweigh its weaknesses. in particular, reviewers found the method to be well motivated fedf, general fedf, novel ky2s, tackling an interesting task bvvt, achieving strong empirical results 4dou and the paper to be well written bvvt,ky2s,xdma. thus, i am recommending acceptance of the paper and encourage the authors to further improve their paper based on the reviewer feedback.", "accepted": 1}
{"paper_id": "nips_2022_8N1NDRGQSQ", "review_text": "this submission aims to ensure adversarial robustness in federated learning when label skewness exists among different local agents. the main idea of the proposed solution is to calibrate the logits to balance the predicted marginal label probabilities. most of the reivewers found the topic studied in this work relevant, important and timely. the authors have also successfully addressed the concerns from the reviewers during the rebuttal. hence i recommend acceptance.", "accepted": 1}
{"paper_id": "nips_2022_8ow4YReXH9j", "review_text": "this work makes a significant contribution to establishing the theoretical foundations for feature importance. the authors suggest a set of axioms that a feature importance score should have and introduce an algorithm that computes a feature importance score that has these required properties. in addition to the theoretical work, a compelling empirical evaluation is conducted showing significant improvement over previous results. after a good discussion between the reviewers and the authors and improvements to the paper introduced due to this discussion, the result is a good paper that is of great interest to the neurips community. however, the added content also raised some concern about the accuracy of some statements, especially with respect to the blood relation. the main concern is that it is not clear that the algorithm provided has the blood relation property. moreover, it is not clear that it is possible to fulfil this relation. here are two scenarios that may be problematic 1. in the fully observed setting if x is a confounder of y and z while z is identical to x then x blocks the backdoor from y to z and therefore, according to the blood relation axiom the importance of z should be zero while the importance of x should be positive since it has direct causal relation with y. however, the roles of x and z are indistinguishable and therefore it might as well be that z is the confounder and therefore should have a nonzero importance. 2. in the partially observed setting, if s is an unobserved uniformly distributed integer in the range 1..8, y is the sign of s, and x is an indicator of s being greater then 4 the according to the blood relation axiom, since there is a backdoor between y and x when s in unobserved then the importance of x should be nonzero. however, this setting is indistinguishable from the setting in which x and y are uncorrelated bernoulli variables in which case the importance of x should be zero. hence, is looks as if the blood relation requirement might be too strong. when reviewer suggested that this problem might be eliminated by saying that the importance of a feature is 0 if there exists a graphical model in which the feature is not in the same connected component as the target note that this is a graphical model and not a causal model. however, this corollary should be theoretically analyzed. some additional comments that emerge in the revised paper 1. the axioms do not define a unique solution. indeed, if a function imp has all three required properties axioms then multiplying it by any positive constant would generate another valid feature importance score. it would be nice to add another requirement that will force a unique solution like shapley value or mci. 2. the proof in the appendix shows that umfi has the three required properties only when certain assumptions hold on the distribution. however, in the body of the paper, these limitations are not mentioned. 3. in line 211 it is stated that the proofs are presented in section 3, however, the proofs are presented only in the appendix", "accepted": 1}
{"paper_id": "nips_2022_O4Q39aQFz0Y", "review_text": "the paper presents a new slicing methods for the wasserstein distance between probability measures over images based on convolution operators. this way memory requirements can be reduced and locality can be better preserved. experiments are conducted on generative modeling problems. reviewers noted that the idea of convolution operators on probability measures over images is natural and simple, yet novel and acknowledged theoretical and practical results. the rebuttals were indepth and provided additional clarifications. on the other hand reviewers note that csw only defines a pseudometric. overall this paper is an interesting contribution to the neurips community and should be accepted.", "accepted": 0}
{"paper_id": "nips_2022_E3LgJdPEkP", "review_text": "i agree with the reviewers that this is a wellwritten paper on an interesting application of meanfield games. the paper is a nice blend of theoretical developments and experimental evaluations. i believe that it will be wellreceived by the neurips community and recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_YZ-N-sejjwO", "review_text": "all reviewers noted the relevance of the proposed study for the neurips community. they all agreed that the paper is wellmotivated, sound, and that the proposed fourier interpolation is novel. while some reviewers had initial concerns regarding the experimental evaluation, the authors did a great job at improving their experiments and reply to the reviewerss concerns. there, we recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_o4uFFg9_TpV", "review_text": "the paper discusses a way to use pretrained models for downstream tasks. reviewers generally appreciated the paper but had questions regarding baselines, details, dataset, etc. the rebuttal addressed most of these concerns prompting the reviewers to raise their recommendation. however some questions remained e.g., httpsopenreview.netforum?ido4uffg9_tpvnoteidwpcrvv96he. ac concurs with the unanimous reviewer recommendation.", "accepted": null}
{"paper_id": "nips_2022_RP1CtZhEmR", "review_text": "the authors propose groupgan which uses a separate generator and discriminator for generating each data channel and a central discriminator for accurately capturing the correlation structure between different channels. this is a borderline paper and there were extensive discussions among the reviewers about this paper. the reviewers all agreed that having two sets of discriminators is a good idea. however, the evaluation for this paper were only done with lowdimensional time series and the reviewers voiced concerns about the generalization of this method to higher dimensions. some parts still need further clarification for example, as reviewer zeed pointed out, dimensionality reduction in this paper remains an extremely odd preprocessing step. overall, accepting this paper may start new discussions in the community. i hope the authors address the concerns raised by the reviewers in the cameraready version of the paper.", "accepted": 0}
{"paper_id": "nips_2022_177GzUAds8U", "review_text": "the manuscript presents a story about compositionality that ties together neuroscience and models; with a focus on how compositionality enables generalization in a task performed by humans undergoing fmri. reviewers were largely happy with the manuscript and authors thoroughly addressed the questions that reviewers had. the manuscript is suggestive. the experiment is in many ways limited, and its not clear what conclusions one can draw at present about the design of models or of the brain. but, it is likely a significant audience at neurips will be interested in this topic and it may spark followup work. this looks like the beginnings of an interesting line of work expanding the paradigm of mapping between brains and artificial neural networks.", "accepted": null}
{"paper_id": "nips_2022_yts7fLpWY9G", "review_text": "the paper proposes the use of an adaptive readout function in gnns together with extensive empirical work to support it. the reviewers all found the paper interesting and are generally in favor of accepting it with one marking strong accept with high confidence. therefore, i recommend the paper be accepted, and encourage the authors to take into account the reviewer comments as they have also indicated in their responses when preparing the camera ready version. in particular, i would like to encourage them to use the extra page given there to reconsider the split of materials between main paper and appendix.", "accepted": 1}
{"paper_id": "nips_2022_ADfBF9PoTvw", "review_text": "all reviewers recommend accepting the paper. if the authors want to increase the impact of their work, a demonstration on a largescale problem would help a lot.", "accepted": null}
{"paper_id": "nips_2022_Y0Bm5tL92lg", "review_text": "the reviewers agree that the paper makes an interesting contribution, connecting inference in probabilistic models with network models from computation neuroscience.", "accepted": null}
{"paper_id": "nips_2022_U-RsnLYHcKa", "review_text": "the focus of the submission is distributionally robust logistic regression when the discrepancy used in the ambiguity set is the wasserstein distance and the features are mixed i.e., they can contain both numerical and categorical variables. after showing that the resulting optimization problem 1 with the logloss function can be reformulated as a finitedimensional exponential conic program theorem 1, they i prove that 1 can be solved in polynomial time theorem 2, ii show that it does not admit a regularized logistic regression form theorem 3 as it is the case for purely numerical features, iii propose a columnandconstraint solver theorem 45. the practical efficiency of the proposed method is illustrated on 14 uci benchmarks. logistic regression lr is among the most popular tools in machine learning and statistics. handling mixed features for lr in the distributionally robust case is a relevant problem. the submission represents a solid work combining both important theoretical and empirical insights as it was evaluated by the reviewers.", "accepted": 0}
{"paper_id": "nips_2022_UPZCt9perOn", "review_text": "the paper deals with accelerated methods on riemannian manifolds. a particular challenge that the paper tries to address, which the ac believes is important, is related to the bounding of the iterates. the paper starts with an explicit bounding constraint on the manifold and relaxes to the ball constraint for certain manifolds and shows that the proposed algorithm can respect that while achieving acceleration. the reviewers including this ac see the merits of the paper. however, the paper in its current form is far from complete. a particular concern is on the empirical performance of the algorithm, resolving which should strengthen the paper. i would encourage the authors to build on the discussions and polish the paper accordingly. even though the paper has positive scores, the paper in its current form is a borderline paper with significant scope for improvement. to this end, the ac cannot accept the paper.", "accepted": null}
{"paper_id": "nips_2022_bi1BTcXa8Q", "review_text": "the paper proposes a point cloud completion method that can take an auxiliary image as guidance. all the reviewers rate the paper slightly above the bar. they like the reported strong performance over the prior baseline and also the capability of using the auxiliary input. although several reviewers raise concerns about missing experiments on real datasets such as scannet or kitti, they still think the paper has sufficient merit. the ac finds no strong reason to disagree with the reviewers.", "accepted": null}
{"paper_id": "nips_2022_SZDqCOv6vTB", "review_text": "4 knowledgable reviewers reviewed the paper, 3 of them recommending weak acceptance, 1 borderline rejection. the reviewers engaged with the authors and a discussion among the reviewers took place. the reviewers appreciate the considered problem, the novelty of the proposed approach and the reported performance improvements. at the same time, there are concerns regading the theoretical justifcation of the method, relation to existing work, and comparison with other existing methods lacking baselines and pushing the baselines to the limit. there was a discussion regading the need for a theoretical justification and i side more with the reviewers which argue that such a justification is not absolutely necessary  nevertheless, more motivation and intutition about the proposed approach should still be provided. in summary, the paper is viewerd borderline, which i agree with, but i think there are some relevant contributions which could be interesting to the community. hence i am recommending acceptance of the paper but strongly encourage the authors to carefully consider all comments and suggestions which came up in the reviews and discussions with the reviewers when preparing the final version of their paper.", "accepted": 0}
{"paper_id": "nips_2022_YgK1wNnoCWy", "review_text": "after rebuttal and discussion all reviewers recommend acceptance. the ac sees no reason to overturn this recommendation.", "accepted": null}
{"paper_id": "nips_2022_MHjxpvMzf2x", "review_text": "this paper proposes a novel, symmetry teleportation approach to optimize the parameters of ml models. the proposed approach allows iterates to move along the loss level set and improves the convergence speed. the teleportations also exploit the symmetries that are present in the optimization problem. the paper also includes very encouraging numerical experiments. i believe that the paper brings more insides and techniques that have been mostly overlooked in the community when training ml problems.", "accepted": null}
{"paper_id": "nips_2022_9a1oV7UunyP", "review_text": "this paper studies the relation between model shift and the performance of modelbased reinforcement learning. the paper proposes a new algorithm that leads to empirical improvement over certain data sets. all the reviewers agree that the paper provides useful theoretical insights into modelbased reinforcement learning, and the experiments are also consistent with the theory.", "accepted": null}
{"paper_id": "nips_2022_e3qH65r_eZS", "review_text": "this paper proposes a teacherstudent scheme for semisupervised semantic segmentation. a consistency regularization is setup between a prototypical classifier and a linear classifier and different augmentation degrees weak vs. strong are applied to the teacher and student networks. on the positive side, the reviewers have found the ideas in this paper simple and strong in practice and they have indicated that the proposed setting is interesting. while the novelty of this paper may seem incremental since consistency regularization, in general, is heavily explored in semisupervised training, the proposed setting is new for the semantic segmentation problem. one of the main criticisms of this submission is that it consists of many moving parts that are not well motivated and how they are orchestrated during training is missing from the original submission. after careful discussion, i believe that the merits of this submission outweigh the issues, and i am happy to recommend this paper for acceptance. last but not least, i strongly recommend the authors bring the algorithms to the main if possible, provide additional implementation details, and make their code publicly available.", "accepted": null}
{"paper_id": "nips_2022_bBgNsEKUxmJ", "review_text": "reviewers found the papers connections between marl and gnns interesting and wellwritten, and the experiments convincing. given the unanimous support, i recommend acceptance. that said, i encourage the authors to integrate reviewer feedback, including trying to move some of the details and plots requested to the main text.", "accepted": null}
{"paper_id": "nips_2022_Z4kZxAjg8Y", "review_text": "this paper proposes a method seal for document retrieval where a language model lm conditioned on a question generates ngrams as document identifiers. this is done by training bart on question and ngram pairs, where the ngrams are sampled from the gold passages, and at test time constraining generation to output valid ngrams that correspond to document identifiers. experiments on natural questions nq open dataset and the kilt tasks obtain strong results. overall, all reviewers agree that this is a strong paper that proposes a simple but effective approach. i agree with their assessments and recommend acceptance. however, a weakness that has been pointed out is that the paper does not perform evaluation on other common qa benchmarks msmarco, triviaqa, squad, webquestions, and entity questions where the performance of baseline models are well established. i strongly encourage the authors to train seal on at least some of those datasets and compare with stronger baselines.", "accepted": 1}
{"paper_id": "nips_2022_7-bMGPCQCm7", "review_text": "this paper proposes to use earth mover distance to measure the loss function between a predicted heatmap and ground truth heatmap. it initially received mixed reviews. after rebuttal and discussion, all reviewers converged to acceptance of the paper. reviewers believe this paper is novel and achieved significant practical performance across several models. ac follows the consensus and recommends acceptance of the paper.", "accepted": null}
{"paper_id": "nips_2022_mSiPuHIP7t8", "review_text": "the authors propose a mixture modeling approach to train gnns so that outofdistribution data can be properly downweighted during training and detected during testing. the reviews were mixed, with some reviewers criticizing the technical novelty and experimental comparison. indeed, the authors could have explained their contribution more transparently, and emphasized a bit more on the new challenges in the gnn setting, which the response has largely addressed. perhaps it is also worthwhile to discuss classic works on mixture of experts, as well as variational bayesian approaches e.g. httpsieeexplore.ieee.orgdocument5563102. as to the experimental comparison, i think the authors made some good explanations in the response and it is perhaps too ambitious for anyone to compare to every possible alternative. in the end, we think the application of the mixture modeling approach to gnns is sufficiently interesting, and the initial experimental results appear to be encouraging. we urge the authors to further revise their work by incorporating all changes during the response and better positioning the contributions in historical context.", "accepted": 0}
{"paper_id": "nips_2022_9Qjn_3gWLDc", "review_text": "this paper received three positive reviews and one borderline reject. in the rebuttal, the negative reviewer did not propose a response, but the authors have given detailed responses to the problems. and the other reviewers did not propose further concerns. thus, taking the comments of the reviewers into account, the ac decides to accept this paper.", "accepted": 0}
{"paper_id": "nips_2022_cYeYzaP-5AF", "review_text": "this is exciting work that demonstrates the ability of selfmodifying networks to solve metareinforcement learning problems. the reviewers all agree that this is strong work, and the authors have convincingly addressed most of the concerns the reviewers brought up during the reviewing phase. there are a few lingering questions about the applicability of the baselines, but these are quite minor. the authors have further promised to add analytical comparisons and additional details motivation on the hebbian update. given this, i view this paper quite positively and encourage the authors to integrate the additional experiments and details they mentioned in the feedback stage.", "accepted": 1}
{"paper_id": "nips_2022_7CONgGdxsV", "review_text": "this paper proposes sourceaware influence function if to study the influence of individual data, source, and class tuples on the performance of different label functions in the programmatic weak supervision paradigm. the proposed method has the capability to work with diverse data domains tabular, image, textual. an ample number of datasets are used in the experiments. the reviewers agree that the proposed method is interesting and sound, the experiments are thorough, and the results provide valuable insights for future work. reviewers raised concerns and questions are properly addressed by the authors response.", "accepted": null}
{"paper_id": "nips_2022_HEcYYV5MPxa", "review_text": "the reviewers generally liked the proposed approach in this paper, agreed that it is novel, and that the experiments showed good improvements over reasonable baselines. there was broad concern about the ablation study in the original paper one shared by the ac, but the authors revised that section during the discussion period to the satisfaction of three of the reviewers. while three reviewers recommend that the paper be accepted, one reviewer recommends a borderline reject. the reviewer stuck to this recommendation after the discussion period, primarily citing concerns about whether or not the method is broadly applicable versus being limited primarily to being useful for logographic languages. while i am recommending that this paper be accepted, i urge the authors to expand their discussion of the limitations of the method in appendix g. i think the discussion with reviewer kspw of the jsut results and the fact that japanese writing comprises both more alphabetic and more logographic elements would be a valuable addition to that appendix and would help to clarify the contributions and limitations of the proposed method.", "accepted": 0}
{"paper_id": "nips_2022_xwBdjfKt7_W", "review_text": "this paper proposes an adversarial training method for spike neural networks. one challenge is that spike networks are nondifferentiable and the paper develops various gradient approximation methods and builds on previous attack methods like fgsm and pgd with approximate gradients. an additional innovation is the development of a regularization method that estimates lipschitz constants. estimating lipschitz constants of spike neural networks is another technical challenge and the paper develops a rigorous bound using a concept they call spike distance. which is an upper bound to the normal lipschitz contanst. several concerns were raised by the reviewers including incomplete discussion of prior work, clarifications on the performed ablation study and comparison to prior sota. overall the authors did a good job in their rebuttal and discussion to convince the revewers and this metareviewer that the paper merits publication. this is a somewhat niche problem setting but the paper has several theoretical and practical innovations that are interesting and suitable for publication.", "accepted": null}
{"paper_id": "nips_2022_TjVU5Lipt8F", "review_text": "this paper studies the problem of multiarmed bandits under differential privacy. the reviewers are all positive about the results and presentation of the paper.", "accepted": 1}
{"paper_id": "nips_2022_uOQNvEfjpaC", "review_text": "the paper presents a new approach, using two pretrained models clip and blip as supervision to enable three tasks, including the newly proposed task wwbl, which is a joint open vocabulary description and groundinglocalization task trained only with weak supervision. i recommend acceptance based on the revised paper, the reviewers comments, and the author response. i think the paper sufficiently contributes  overall idea and architecture  the wwbl task, even if similar to previous task  extensive experimental evaluation and comparison to prior work  solid ablation study the paper received mixed review scores with 2 borderline rejects, 1 borderline accept, and 1 strong accept. the authors have in my opinion largely addressed the concerns and revised the paper, one of the remaining concerns of the weak reject reviewers is novelty, which i think is sufficient. my recommendation for acceptance is under expectation that the authors revise the paper to address any outstanding points made by reviewers, e.g.  additional alternative models reviewer mvej if possible additionally, i think it would be great if the authors discuss the relation ship of wwbl to to the task of dense captioning task more clearly in the paper.", "accepted": 1}
{"paper_id": "nips_2022_lkrnoLxX1Do", "review_text": "all three reviewers voted to accept the paper, and the detailed rebuttals from the authors helped to clarify reviewers original concerns. one remaining concern from one of the reviewers is whether this method should be referred to as selfsupervised. however, authors clarified that it is reasonable to consider this method selfsupervised for realworld data. i am fine with therefore leaving selfsupervised in the title.", "accepted": null}
{"paper_id": "nips_2022_rUc8peDIM45", "review_text": "the paper investigates an important topic of why sgd converges to flat minima. overall the reviewers felt that this is a nicely written paper with a nice contribution to the state of the art.", "accepted": null}
{"paper_id": "nips_2022_QotmVXC-8T", "review_text": "the paper eventually received a perfectly consistent evaluation from all the reviewers 4 times accept, so i can only recommend the acceptance.", "accepted": null}
{"paper_id": "nips_2022_2EufPS5ABlJ", "review_text": "this paper has generated a long discussion and although it has strong theoretical merits, we all concord that the paper lacks of empirical motivations as well as a strong empirical evaluations with respect to distance distributions not exploiting manifold sructure and thosed define on a manifold. hence, we believe that at this point it would be preferable to have such empirical evidence ideally with quantitative results on realworld problems before accepting the paper. given that, we are sure that the paper will be much stronger and of broader interest to the ml community.", "accepted": 0}
{"paper_id": "nips_2022_6hzH8pohyPY", "review_text": "thank the authors for their submission. the paper studies combinatorial multiarmed bandit with probabilistically triggered arms. that is an mab setting in which, at each round, the learner chooses a subset of the arms and obtains a reward that is some function of expected rewards of the chosen arms. in addition, the learner only observes feedback on a random subset of her chosen arms triggered arms. the paper relaxes a smoothness assumption laid by a previous work, and further improves the dependence on k in the regret bound, where k is the batch size maximum number of triggered arms the authors provide computationallyefficient algorithms that are based on bernstein concentration inequality, facilitating the improved bounds. the paper is wellwritten and organized, and the theoretical results are sound.", "accepted": null}
{"paper_id": "nips_2022_zGvRdBW06F5", "review_text": "in this work the authors propose a framework for training cv models on tiny iot devices with very limited memory. the reviewers agreed that the paper is well written and represents a valuable contribution to the area of efficient  ondevice ml. questions raised by reviewers were sufficiently addressed in the response.", "accepted": 1}
{"paper_id": "nips_2022_UZJHudsQ7d", "review_text": "reviewers find the paper original, useful, thorough in its numerics in the revision, and clearly written.", "accepted": null}
{"paper_id": "nips_2022_dMK7EwoTYp", "review_text": "there was a range of reactions to this paper from borderline reject to strong accept. although several of the reviewers highlighted that the contribution could be viewed as incremental, it is clearly described, and robust across different types of scenes, and i concur with the three reviewers that give positive ratings. therefore i am accepting this paper.", "accepted": 1}
{"paper_id": "nips_2022_TrsAkAbC96", "review_text": "consistent reviews, both in content and in score. the crossidentity motion transfer is a good test of the papers capability  it would improve the paper to provide more such examples, which are clearly more challenging than the sameidentity case. the concerns about the limited diversity of example subjects mentioned by r1 are indeed relevant. the video examples are all male, with quite light skin tone. please include examples with female subjects, darker fitzpatrick 6 skin tone, and other ethnicities. to be clear the rebuttals current response we will emphasize that training on a diverse dataset is a must does not go far enough. it is very important that qualitiative examples are shown, even more important than that the test datasets are diverse. if the results are less good, efforts should be made before neurips to improve them e.g. by retraining, and if improvement is not possible, this should be very clearly stated in the limitations of the final copy and neurips presentationposter.", "accepted": null}
{"paper_id": "nips_2022_O5arhQvBdH", "review_text": "from the ratings alone this paper appears borderline leaning towards acceptance, however, i want to highlight to the authors that in discussion with reviewers and my own reading of the paper there are aspects that shifted this even closer to the decision boundary. in the end, my own conflicted views of the work and the lack of further discussion from the more negative reviewer led to a recommendation of accept. ill briefly review some of the strengths and weakness in the latest revision as i see them.  the work truly studies the effect of controlling multiple objectives of communication in the emergent communication setting and how these affect tradeoffs in complexity, informativeness, and utility. this scientific approach to the experimental work is in my view a clear strength of the work.  there is a clear motivation for the choice of objectives based upon existing work in related fields. although i have reservations about how well the realization, building on the zaslavsky et. al.s work and within emergent communication is something i think will benefit the community.  the writing itself is clear and easy to read. figures in the main text and appendix were informative and quite interesting.  important details, especially around the math and experimental setup, are lacking. some of thing examples that bothered me were ambiguity on definition of terms in the objective all three terms could be stated more precisely but for ux, y we are not told what y is and for ix, c it is not clear if this is coming from 3.2.2 or 3.2.4; lacking clarity around gradient flow passing gradients back to the sender in this type of setup should be stated very clearly and upfront.  connection between ix, c and complexity. as i was reading i interpreted this to be as in 3.2.2 the kl divergence of \u03bcx and \u03c3x from a unit gaussian, and found this is to be a very strange choice not as an objective but as a measure of the complexity of the language. this is, i believe, a different concern than what was raised by one of the reviewers. the issue i saw there was that the codebook need not be uniformly distributed in the continuous space, and therefore the implications on message complexity of a particular variance in the continuous latent space may be inconsistent. for some regions of latent space a unit variance could imply a single message with high probability, while other areas of the latent space may be more closely clustered causing the same unit variance to be nearly uniform over several different messages. moving on to 3.2.4, we see perhaps evidence that the authors encountered the repercussions of this choice simply penalizing this term in training was insufficient to train vqvib agents to use fewer unique discrete embedding. the motivation is great, but i strongly suspect that there are better ways of realizing it and that this particular choice is not capturing what the authors intended.  limited novelty of the method. the combination of vib and vqvae seemed like something that would have already been published, since regularizing the prior of the latent distribution is such a well used maybe even well understood method, but i looked and must grant that this does seem to be a novel combination. that said, it is not so novel that it would be able to stand on its own without the specific setting itself and connection with models of complexity in human languages to support it.  minor the authors made several additional changes to address reviewer concerns around prior work and putting their design choices into context, but there remain some issues in this space. i found the discussion of related work to be fairly shallow and at times even dismissive. it reads as though the work was undertaken entirely devoid of consideration for related work except for that which directly motivated the approach, and then was added defensively but without making real connections between this work and others. i mark this as minor because it is unfortunately somewhat common and because it is a more subjective evaluation. i hope this and the other reviews will help authors to understand how the work may be experienced by readers and potentially make further refinements. overall, despite limitations, i do believe this work will be of interest to researchers in emergent communication and potentially more broadly due to common underlying questions around trading off complexity and other primary learning objectives.", "accepted": 0}
{"paper_id": "nips_2022_d4JmP1T45WE", "review_text": "the authors propose a novel training algorithm to train spiking neural networks snns in an eventdriven manner with backpropagation. they perform experiments on standard benchmarks such as cifar10 and cifar100 to verify the effectiveness of the method. the algorithm achieves sota performance on these data sets. eventdriven methods are interesting from a hardware perspective as gradient have to propagated only at spike times. the manuscript received mixed ratings, a clear agreement could not be found. pros  the authors provide an analysis of eventdriven backprop in snns which helps to adjust the usual learning procedure.  the authors performed experiments on several data sets and achieved soa performance w.r.t. other eventdriven methods.  they also tackled cifar100 for which no results were previously shown with eventdriven algorithms  the paper is wellwritten, although language could be improved at places. cons  improvements over competing eventdriven techniques are rather small  performance is still clearly below nonevent based surrogate gradient methods but this is not surprising  not clear how the method scales up beyond cifar100 since the ratings were mixed, i read the paper and believe it is publishable in neurips although it is somewhat borderline.", "accepted": 1}
{"paper_id": "nips_2022_gRK9SLQHTDV", "review_text": "this work studies a narrow, but important problem of how much cardinal information is needed to achieve near optimal matchings. the authors show that with just two queries one is required for any nontrivial results they can achieve nontrivial results in a very general setting. moreover, they show that their results are tight.", "accepted": 0}
{"paper_id": "nips_2022_YxUdazpgweG", "review_text": "the reviewers tend to agree on the value of this 3d dataset, but point to some questions about labelling and accuracy. the rebuttal very convincingly addresses these points, clarifying the novelty and value of this new dataset. i agree with the authors that datasets are clearly in scope for the main neurips program and that the datasets track explicitly includes as a faq my work is in scope for this track but possibly also for the main conference. where should i submit it? with the answer this is ultimately your choice.", "accepted": 0}
{"paper_id": "nips_2022_qq84D17BPu", "review_text": "reviewers were unanimous in recommending that the paper be accepted, and i accordingly recommend the same. i encourage the authors to take into account suggestions made by reviewers so as to further improve the text in the cameraready version.", "accepted": 1}
{"paper_id": "nips_2022_mMT8bhVBoUa", "review_text": "technically solid paper that introduces and benchmarks a novel inference framework, with application to inference in gps. all reviewers recommend to accept, after a decent amount of discussion in which reviewers raised their scores in response to a fairly significant round of updates to the manuscript itself. recommend to accept, despite some questions regarding overall impact.", "accepted": null}
{"paper_id": "nips_2022_3I8VTXMhuPx", "review_text": "this paper studies a novel variation of image steganography. the proposed approach is different from prior work mostly building on autoencoders and uses a gan and hide a secret image in one particular location of the learned distribution. the central idea of the paper seems novel and interesting. the reviewers raised several concerns about limited evaluation and complexities of comparing to other methods that generate directly images. overall, this paper seems to have novelty and interesting ideas and the benefits seem to overcome the limitations, based on the rebuttal and discussions.", "accepted": 0}
{"paper_id": "nips_2022_TTM7iEFOTzJ", "review_text": "the reviewers found the method simple and effective and considered it a contribution of interest to the community. claims are well supported by experiments and design choices have been validated. the paper is well written. furthermore, the authors provided highly detailed responses to all questions by reviewers, which creates confidence that reviewers remarks will be addressed in the final paper.", "accepted": null}
{"paper_id": "nips_2022_DhmYYrH_M3m", "review_text": "the paper got split reviews 1x reject, 1x borderline reject, 1x weak accept, 1x accept. all reviewers found the impressive performance on the challenging carla leaderboard to be a major strength of the paper. reviewer concerns stem from two factors a not enough technical contribution to warrant publication at neurips but results are still publication worthy at more domainspecific conferences eg icra, iros, and b bulk of the impressive performance 19 points coming from the ensembling heuristic and only 6 points coming from proposed architectural modifications shared backbone, multistep control, temporal module and trajectory guided attention. the metareviewer read through the paper, the reviews, the author response, and reviewer discussion. for the metareviewer, the impressiveness of the empirical results on a wellstudied and important benchmark dominates the above reviewer concerns. as long as there is clear attribution and some understanding as to where this impressive performance improvement is coming from, the community will benefit from being aware of the results even though the proposed method may not be as technically deep as typical neurips papers. the authors are encouraged to include the additional experiments conducted during the rebuttal phase into the final version of the paper, in particular the ones that help distill out the contribution of the different parts of the proposed system.", "accepted": 0}
{"paper_id": "nips_2022_jAL8Rt7HqB", "review_text": "four reviewers provided detailed feedback on this paper. the authors responded to the reviews and i appreciate the authors comments and clarifications, specifically that each questioncomment is addressed in detail. additional experiments were also performed. the authors also uploaded a revised version of the paper. after the two discussion periods, one of the four reviewers suggest to reject the paper while three reviewers rate the paper as weak accept, so no reviewer strongly advocates for acceptance. i considered the reviewers and authors comments and also tried to assess the paper directly. i believe that the paper should not be accepted to neurips in its current form. weaknesses include  readability while at least one reviewer describes part of the paper as clear and easy to follow one other reviewer mentions clarity as the main weakness and one other reviewer also comments in this direction. i personally found the paper hard to read as well even after the improvements made in the revision, and i found some of the claims to be fairly generic and partially not well supported. e.g. resolve the issues of overfitting and lengthy training time of vit, or the proposed scheme preserves the original architecture of vit, which results in its general employment regardless of the architecture of vit.  experimental results several questions have been raised regarding the experimental results e.g. influence of the attention link, choice of hyperparameters. these have been addressed in the discussion, but it seems to me that they were at best partially resolved.  relation to distillation the results in the lowdata regime rely on learning from a teacher model. this relation to distillation is recognized but somewhat underexplored. this could be a confounding factor in the analysis of the approach. for example, in one response, the authors argue that however, we believe our source of performance gain is due to transferring cnns inductive bias with attention. it remained somewhat unclear, whether this transfer would also hold when the cnn is not a more powerful teacher model. strengths include  the idea of regularizing the global tokens attention maps with the cnn activation maps is novel and interesting.  the reported experimental improvements in the lowdata regime are interesting. despite recommending the paper for rejection in its current form, i would like to encourage the authors to continue this line of work and present it again to the community with more focused discussions, insights and possibly experiments. this is an interesting paper and it was evaluated to be close to but below the acceptance threshold.", "accepted": 0}
{"paper_id": "nips_2022_7a2IgJ7V4W", "review_text": "this paper explores semivit, a semisupervised learning approach for vision transformers. semivit buildon three stages pipeline such as simclrv2. the authors introduce a probabilistic mixup for the semisupervised finetuning stage which gives consistent experimental improvements. semivit shows strong empirical results as it achieves 80 top1 accuracy on imagenet using only 1 labels, which is comparable with inceptionv4 using 100 imagenet labels, demonstrating that vitsemisupervised training enables to reach 80 top1 accuracy on 1 imagenet is novel and of potential interest to the ssl community. i therefore recommend acceptance. however, i would encourage the authors to clarify that the threestages pipeline is not a contribution of the paper and focus the novelty on the probabilistic mixup and the experimental study.", "accepted": 1}
{"paper_id": "nips_2022_gtCPWaY5bNh", "review_text": "this paper proposes an interesting new way to think about how to use a model zoo of pretrained models extract modular building blocks that are swappable from the networks and then stitch them together. to do the former, a cover set optimization methods is proposed, and the blocks can then be combined in a way that respects various resource and performance constraints. the idea is both interesting and ambitious, and has the potential to open up various avenues of research if done well. the paper lives up to the task it is wellwritten k5p1, 3qgq, q2yk, conducts experiments to validate if the such stitched networks can do well, and proposes an intuitive principled method to extract the blocks 3qgq, k5p1. the reviewers did express some concerns about scalabilitygeneralizability to other tasks k5p1, 3qgq, larger zoos k5p1 ,3qgq, other architectures all reviewers, and computation q2yk as well as several other potential issues such as limited performance improvements. the authors provided strong rebuttals to these, including some new experiments. at the end of the process, the reviewers were all satisfied with most of the concerns, and the overall consensus on the paper seems to be with high scores. given the potentially highimpact, novel perspective as well as the solid execution, i highly recommend this paper for acceptance.", "accepted": 1}
{"paper_id": "nips_2022_xL8sFkkAkw", "review_text": "the paper introduces a new procedure to initialize the optimisation in training process of dnn models, including the recent vit architecture. all the reviewers recommend acceptance and appreciate the promising empirical results backed by the strong theoretical foundations. ac recommends acceptance as well.", "accepted": null}
{"paper_id": "nips_2022_Siv3nHYHheI", "review_text": "the authors propose an online training algorithm ottt for spiking neural networks snns using eligibility traces and instantaneous loss values. they show empirically that this method performs better than previous ones in feedforward spiking neural networks. all reviewers agree that the empirical results are impressive and that the method is interesting for neuromorphic hardware. the authors also provide a mathematical analysis of the learning method. weaknesses  networks are mostly applied to static tasks, while more temporal tasks are potentially more interesting for snns  comparison to previously proposed methods is missing. in general, a very interesting and strong paper. i propose acceptance.", "accepted": null}
{"paper_id": "nips_2022_PO6cKxILdi", "review_text": "motivated by the often overly conservative characteristics of distributionally robust mdps, this paper employs nested bayesian posterior distributions to model the uncertainty over mdp parameters. the programming solution is similar to belief state approximation methods for pomdps. the experiments after revision seem to demonstrate the advantages of this approach. the reviewers believe the paper could be improved with better theoretical analyses andor more compelling experiments higher dimensional tasks in particular. the paper lacks a strong advocate among the reviewers, but their aggregate sentiment is that it is worth accepting to the conference unless there are other more deserving works that it would displace.", "accepted": 0}
{"paper_id": "nips_2022_Q8GnGqT-GTJ", "review_text": "the paper proposes retroprompt, which builds a knowledgestore with training examples and improves fewshot and zeroshot performance. all reviewers appreciate the improvements over competitive baselines and the quality of presentation. the main weaknesses are the lack of ablations to clarify where the gains come from and unclear positioning with respect to previous works knnlm, retro, realm, rag. the reviewers were unanimous in accepting and the authors addressed some of the issues raised.", "accepted": null}
{"paper_id": "nips_2022_pCrB8orUkSq", "review_text": "prerebuttal, this paper had mixed reviews. postrebuttal, the paper had two strong supporters, a6gt and vdbh, who argued that the paper provides valuable insights into an important field, as well as a supporter dlu6, who commented in the discussion below that they are in favor of the paper although did not update their review. the only remaining criticism comes from 2bcv. the ac does not find 2bcvs review persuasive a6gts comments summarize the acs perspective well and 2bcv did not participate in discussion. the ac is inclined to accept the paper and encourages the authors to use their extra page to integrate their responses to the reviewers.", "accepted": 0}
{"paper_id": "nips_2022_VrJWseIN98", "review_text": "the paper proposes a novel method that takes the best of both worlds synchronous and asynchronous onpolicy rl methods. the rebuttal nicely addressed the concerns of most reviewers. why the method makes sense and has benefits is rather straightforward and intuitive which is a good thing!. the paper is clearly an experimental paper  systems paper with very extensive evaluations that show significant practical benefits. the method potentially has large practical impact, which is an important contribution to the community and a valid  though less common  neurips paper format. hence i disagree with reviewer 45yb about requiring a theoretical noveltycontribution.  not visible to authors as posted after discussion phase  agree to raise the score neurips 2022 conference paper1458 reviewer ep2x 18 aug 2022 excellent work, impressive results! i will raise my score.", "accepted": 0}
{"paper_id": "nips_2022_tvwkeAIcRP8", "review_text": "this paper had reviews ranging from borderline reject to strong accept. the most negative reviewer had concerns about the assumptions in the framework point light sources, and the loss of accuracy as the number of light sources decreases, but the remaining reviewers were compelled by the ability to hand scenes with lights not at infinity and the integration of the shadow constraints to give constraints on the structures of scene parts not directly viewed. overall i agree with the three positive reviewers that this paper considers an interesting variation of the photometric stereo problem with coherent experimental evaluation that shows the contributions of each of the different pieces of their overall system therefore i accept this paper.", "accepted": 1}
{"paper_id": "nips_2022_JSha3zfdmSo", "review_text": "this paper present an algorithm with strong theoretical guarantees for a fundamental problem of broad interest. it is wellwritten.", "accepted": null}
{"paper_id": "nips_2022_8FuITQn6rG3", "review_text": "reviewers generally agreed that this paper is innovative the decomposition of highlevel concepts into subconcepts in particular sets this paper apart from existing conceptbased methods, and appreciated its potential practical utility for the explainability community for example by providing localization in input space in addition to concepts, which can be used to debug model errors. all reviewers however also agreed on the main weaknesses, i.e. the limits of the validation of the method which is restricted to only 1 dataset and lacking rigorous quantitative metrics, and a related lack of clarity in terms of technical motivation and use cases enabled for endusers of the method. despite some useful clarifications and improvements in the presentation of results provided in the rebuttals and the ensuing exchange, reviewers were left unsatisfied by the responses addressing the mentioned main weaknesses, which raised renewed concerns about their seriousness. as a result, the discussion after the rebuttal period was marked by the opinion expressed by two reviewers that the papers weaknesses outweigh the albeit clear merits and that in its current version the paper is not ready for being accepted.", "accepted": 0}
{"paper_id": "nips_2022_jtq4KwZ9_n9", "review_text": "all reviewers were in favor of acceptance. the ac examined the paper, reviews, and author response, and is inclined to agree. the ac would encourage the authors to incorporate their responses to the reviewers into the final version of the paper.", "accepted": null}
{"paper_id": "nips_2022_luGXvawYWJ", "review_text": "the reviewers originally had concerns but these have been well addressed by the authors in a thorough rebuttal and there is a consensus for acceptance. we encourage the authors to incorporate all the comments from the reviewers in the final version.", "accepted": null}
{"paper_id": "nips_2022_q6bZruC3dWJ", "review_text": "this paper makes an interesting observation on knowledge distillation such that excluding certain undistillable classes improves performance. this observation is quite interesting and potentially impactful for a better understanding of knowledge distillation. the authors use this observation to consistently improve the existing knowledge distillation methods in the experiments. one weakness is that the explanation of why it is beneficial to exclude certain classes is not very satisfactory. nevertheless, the strength of this paper outweighs the weakness. all the reviewers are positive about this paper and i also recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_kZnGYt-3f_X", "review_text": "this submission was reviewed by four reviewers. all reviewers provided detailed and informative reviews. during rebuttal, the authors actively submitted detailed rebuttals, which lead to improved evaluations by the reviewers with improved scores. overall, this is an interesting paper and an accept is recommended.", "accepted": 1}
{"paper_id": "nips_2022_9aLbntHz1Uq", "review_text": "this paper has divergent views in the sense two reviewers have given positive assessments 6 and 7 while the other reviewer has given a negative assessment score of 3. this paper also had very heavy discussions between the reviewer with negative opinion and the authors. first of all i would like to thank the reviewer involved in patiently discussing with the authors dedicating valuable personal time. let me start with the aspects all reviewers more or less agree on  a the main technical piece is an efficient algorithm and provable guarantees for identifying definite nondescendants and definite descendants from an mpdag  maximum partially directed acyclic graph  the equivalence class of causal dags one obtains after incorporating any arbitrary side information. previous such results were known for cpdags and they dont carry over to mpdags. therefore it is a non trivial result specifically lemma 4.4 . so all reviewers agree that finding definite non descendants in equivalence classes that also include side information is a very solid contribution. b the aspect in which reviewers had divergent opinion is this the papers claim to be able to train counterfactual fair classifiers leveraging the result from kusner et. al 2017 that any function of nondescendants is counterfactually fair. one of the reviewers strong contention is that in most fairness datasets, most variables that are highly predictive of outcomes will also be downstream of sensitive attributes like race etc.. and therefore relying only on nondescendants is not exactly a realistic application. authors cited their empirical structure learning results that show very few descendants and comments from kusner et. al 2017 paper to bolster their case. reviewer responded by citing alternate statements from the same paper etc.. my opinion is that in a specific context when fairness with respect to a specific sensitive attribute is desired, there are also often other features that has no causal relationship with the sensitive attribute but has a correlation examples include age and race, race and gender etc.. . to cite a recent reference please see example 15 in httpsarxiv.orgpdf2207.11385.pdf this reference is recent and i am not expecting authors or anyone else to have known this  it is just to demonstrate the point. the example shows testable correlations between sensitive attributes and nondescendants in compas and adult datasets. this shows that a neither causal sufficiency nor b the nonexistence of nondescendants are realistic . in fact, spuriously related nondescendants give rise to spurious bias which may not be an object of correction for fairness broadly speaking. this shows that causal sufficiency is a strong assumption as authors have assumed and also nondescendants do exist. c another point to be noted is that kusner et. al. 2017 do consider confounded models unlike the authors. once you view exogenous, endogenous observed variables and sensitive attribute as one full deterministic system, their point is all exogenous  non descendant endogenous variables are nondescendants topologically and therefore could be used. they did not imply nondescendants endogenous only as the authors contend in their discussions. in fact, the algorithm section in kusner et. al. 2017  advocates for sampling exogenous from some side information level 2 and 3 information and forming a predictor as a function of exogenous and non descendant endogenous variables. therefore, reviewer has a valid point on the discussed aspect as well. authors may want to pay attention to this. in summary authors contention that kusner et al 2017 paper advocates for nondescendant endogenous as their main sufficient criterion appears to be not exactly correct. however, nondescendants and their confounding with sensitive attribute is a more realistic model. however, authors core technical structure learning contribution is also noteworthy. if this line of work is to be pursued where one could find non descendants even under limited confounding between sensitive attributes and non descendants  a mild violation of causal sufficiency  it would be a step towards obtaining counterfactually fair classifiers although even such a classifier would have to sacrifice a lot on accuracy depending on how many descendants one observes. however, even positive reviewers have opined that main strength of the paper is a solid structure learning result that identifies nondescendants in a fully observational setting. recommendation in the spirit of not blocking valid ideas that are fundamental and also the fact that one cannot always make the weakest set of assumptions to make progress, i tend to favor acceptance. a very strong suggestion to authors  i would place structure learning as the centerpiece and motivate it by a need to learn nondescendants in the general sense motivated by kusner et al 2017. authors also need to highlight fair relax  a relaxation that they have proposed that uses possible descendants and definite non descendants to predict  it seems to be closer than other approaches to counterfactually fair one and therefore removing the singular focus on as the discussions would have one believe only observed definite nondescendants.", "accepted": 0}
{"paper_id": "nips_2022_7rcuQ_V2GFg", "review_text": "the paper studies the use of random weights together with learnable masks. authors demonstrate that such training approach for neural network can reduce the model storage requirements and has applications to network compression. reviewer appreciated the novelty of the idea and the extensive experiments on various architectures. adding experiments that would go beyond smallscale datasets would further strengthen the quality of the paper and its potential impact.", "accepted": 1}
{"paper_id": "nips_2022_kEPAmGivMD", "review_text": "this paper proses an inference method that combines gradient ascent and normalizing flows. the idea is that one could, in principle, simulate the deterministic fokkerplanck equation, but this would require access to the density of the evolving approximating density, which is intractable. thus, the paper proposes to maintain a set of particles and update a normalizing flow to approximate this density. the resulting procedure is deterministic, with an accuracy that depends on the number of particles and the power of the normalizing flow. reviewers agreed this was an interesting approach and the experimental results are promising albeit fairly lowdimensional. however, there were a few apparent weaknesses firstly, there was a lack of clarity about the theoretical guarantees. the authors state that this is all clear in the manuscript, but readers would undoubtedly benefit from a much more centralizedexplicit description of what approximations are involved, and under what guarantees the method is claimed to work, which could be put in a single place. in addition, in trying to understand exactly what was done in the experiments, it is difficult to understand several of the details. algorithm 1 is very helpful in this regard, but it would be beneficial to have a selfcontained elaboration of all of the points perhaps even in an appendix. finally, the experimental results are all relatively lowdimensional. often particle methods do not scale gracefully to higher dimensions. i do not see this as a huge flaw because the method could be useful even if it does not scale, but reader would benefit from evidence on this point either way. in the end, however, most of the above issues are issues of clarity and i am willing to trust the authors to fix these before final submission. the paper appears to present a novel idea and the community would benefit from seeing it and discussing it.", "accepted": 0}
{"paper_id": "nips_2022_fT9W53lLxNS", "review_text": "three out of four reviewers provided positive reviews and scores for this submission. they agreed that savi makes meaningful improvements over a previously proposed savi model. importantly, while most past approaches evaluate on synthetic data, this submission evaluates the proposed model on a real world dataset. the proposed model clearly improves over the baseline and a clear ablation analysis shows where the improvements come from. one reviewer had concerns about the evaluation using just one real world dataset. this was also brought up by other reviewers, who mentioned that the waymo dataset has less diversity and fewer videos than others. while a more thorough evaluation would make this a stronger submission, the leap from synthetic evaluations to real world evaluations in this line of research is notable and sets the bar for future work. i also note, based on the discussion, that the employed dataset is not trivial and has several challenges for the model. another concern by the reviewer was about missing baselines. the authors did provide additional baselines in their response. while these baselines do not exactly match the ones requested by the reviewer, i think they provide good evidence that the proposed method is able to employ the depth signal effectively. overall, this paper makes solid progress on the problem, provides value to the readers and provides strong results on a real world dataset. given these reasons, i recommend acceptance.", "accepted": 0}
{"paper_id": "nips_2022_mmzkqUKNVm", "review_text": "this submission got a mixed rating 1 borderline reject, 2 week accept and 1 accept. most of the concerns lie in the explanations on the details and experimental comparison with certain baselinesvariants. the authors addressed them well by providing additional experiment results in their response. the remained concern from the reviewers giving borderline reject lies in theoretical justification of the proposed operation. the authors managed to provide a theoretical interpretation from the viewpoint of diffusion process, which partially addresses the reviewers question. overall, all the reviewers agree that this submission introduces a simple and effective method for the segmentation field. the effectiveness of the proposed method has been validated via extensive experiments. the performance improvement is significant. the manuscript is written clearly. the contribution is sufficient. based on the above considerations, ac recommends accept for this submission.", "accepted": 0}
{"paper_id": "nips_2022_pk1C2qQ3nEQ", "review_text": "the majority of reviewers found this paper to be confusing in its presentation, lacking novelty e.g. section 3, and not well motivated e.g. balentacq, with 3 out of 4 recommending rejection. i find that the paper particularly falters in its explanation of the point process entropy and derivation of the ultimate acquisition function. as the authorreview discussion makes clear, the deviation from shannon  differential entropy to point process entropy is at the core of the paper, but this is lost in the current draft. due to this and moderatetominor issues such as the validity of the beta approximation, the use of only mc dropout, and relationship to elr schemes, i recommend rejection at this time.", "accepted": 0}
{"paper_id": "nips_2022_59pMU2xFxG", "review_text": "this paper introduces a human evaluation framework for benchmarking current explainers. there was an engaged discussion between authors and reviewers. many concerns were clarified and the average score was raised from 4.75 to 5.5. some concerns remain regarding the intrinsic limit of human evaluation, but overall, there are no major flaws with the current study and the framework is likely to be valuable since human evaluation of explainers is central to the progress of xai. hence, i recommend acceptance of the paper.", "accepted": null}
{"paper_id": "nips_2022_JoZyVgp1hm", "review_text": "this submission was reviewed by three reviewers. all three reviewers provided detailed comments during the review period. the authors provided detailed responses to the initial set of reviews. the rebuttals lead to improved scores of some reviewers while other reviewers confirmed that their concerns have been addressed. given the above evaluations and interactions, an accept is recommended.", "accepted": 1}
{"paper_id": "nips_2022_Blbzv2ZjT7", "review_text": "the reviewers appreciate both main contributions, namely the ptie concept and the feature and reward engineering. while there are concerns that neither may generalize beyond the specific game of doudizhu, and that ptie may be somewhat incremental given ctde or even not novel at all; several ctde works use the entire state for the centralized training, successfully demonstrating ptie on even this single domain and methodically evaluating its effect should be of interest to the community. the paper is mostly well written, and the authors are requested to incorporate the specific reviewer feedback.", "accepted": null}
{"paper_id": "nips_2022_-GgDBzwZ-e7", "review_text": "in this paper, the authors provide new theoretical guarantees for augmenting algorithms with learned predictions. based on discrete convex analysis dca, they generalize previous results of dinitz et al, and obtain better time complexity bounds for a number of online problems. the application of dca to online algorithms with predictions is interesting, and the improvements in the bounds are significant.", "accepted": null}
{"paper_id": "nips_2022_6avZnPpk7m9", "review_text": "after a lively and interactive author discussion period all reviewers ended up recommending to accept this paper. the work examines the ways in which different data augmentation schemes can increase knowledge distillation performance, providing some theoretical analysis with actionable insights and experiments to back it up. the work focuses on the generalization gap of the student under different sampling schemes, and asserts that their study leads to the conclusion that a good data augmentation scheme should reduce the variance of the empirical distilled risk between the teacher and student. reviewers were generally positive about the clarity of the manuscript after some changes during the author discussion. the ac recommends acceptance.", "accepted": null}
{"paper_id": "nips_2022_lgj33-O1Ely", "review_text": "this paper was reviewed by three experts in the field. based on the reviewers feedback, the decision is to recommend the paper for acceptance to neurips 2022. the reviewers did raise some valuable concerns that should be addressed in the final cameraready version of the paper. for example, more discussion can be added on the key limitation of totalselfscan when applied to animation. the authors are encouraged to make the necessary changes to the best of their ability. we congratulate the authors on the acceptance of their paper!", "accepted": null}
{"paper_id": "nips_2022_0cn6LSqwjUv", "review_text": "this paper describes spdnet, a dataset for spatial precipitation downscaling. experiments are provided using a fairly wide set of alternative methods  14 models including kriging which is a widely used standard method in the meteorological community  as well as a novel architecture proposed by the authors. the authors also extended srgan, edsr, esrgan from single image super resolution sisr methods to video super resolution vsr methods. while the level of innovation on the neural architecture side of the work is not extreme, clear value is provided in terms of contributions to neural architecture development. reviewers felt that the dataset itself, the wide variety of models examined and the large set of evaluation metrics offers value to the community and that this dataset could help bring more interest to the problem domain. during the discussion period it was made clear that all relevant codes and datasets are opensource for research purposes and that the dataset and the code are not proprietary. we will build a dedicated github repository and website for users to easily use our datasets and codes. it is important that this is indeed is fully executed by the authors. three of four reviewers recommended acceptance. for all these reasons the ac recommends acceptance.", "accepted": 0}
{"paper_id": "nips_2022_evWx_rWWJuG", "review_text": "after the rebuttal and discussion all reviewers are positive, and recommend acceptance. the ac agrees with this recommendation.", "accepted": null}
{"paper_id": "nips_2022_fU-m9kQe0ke", "review_text": "this paper proposes a novel method for vision transformers quantization. the irm and dgd scheme is developed to solve the bottleneck of lowbit quantized vision transformers. all the reviewers agree that the proposed method is novel and effective. the concerns and questions are well addressed during the rebuttal period. the overall quality is clearly above the bar, and thus the paper should be accepted for publication.", "accepted": 1}
{"paper_id": "nips_2022_eQfuHqEsUj", "review_text": "this paper focuses on expanding the problem of unsupervised object discovery detection to a new setup, where a 3d point cloud is available as well as an rgb sequence. the paper received three detailed reviews from expert reviewers, all of which had their major concerns about the paper resolved through the author rebuttal and authorreviewer discussion period. with the extra analyses and experiments presented in the discussion period, the paper has reached the level of impact and contribution expected by neurips papers. the authors are recommended to add these extra items to the final version of the paper.", "accepted": null}
{"paper_id": "nips_2022_Qb-AoSw4Jnm", "review_text": "the three reviewers had significantly diverging final opinions strong accept, borderline accept and weak reject. the authors addressed many of the concerns in their rebuttal. i read the paper carefully, and i agree with the concerns from one reviewer about why the improvements in stage1 do not lead to significant improvements in stage2. i think this concern needs to be properly addressed, because otherwise it is unclear what the benefit of this approach would be for real applications. while previous work has shown that improved stage1 performance leads to improved stage2 performance, why was it not replicated in this situation? i also found the analysis of why the spatially conditioned normalization improves reconstruction to be lacking. if the jagged structures are addressed by this work, then understanding why with simple examples would have shed more insight into the technical contribution. however, in summary, i think this paper is slightly above the acceptance bar, and addressing the above concerns is recommended for the final version.", "accepted": 0}
{"paper_id": "nips_2022_wS23xAeKwSN", "review_text": "the proposed augmentation method for lidar scans is to crop, cut, and mix two 3d scans at both scenelevel and instancelevel. the approach is not novel and a simple extension of the idea of the mix of 3d scenes and rotating bounding boxes. another limitation is that the method cannot be applied to general 3d scenes. the reviews include a7, wa6, ba5, two br 4. after carefully checking out the rebuttals and discussions, i recommend the paper to be presented for the neurips community.", "accepted": 0}
{"paper_id": "nips_2022_h10xdBrOxNI", "review_text": "the recommendation is based on the reviewers comments, the area chairs personal evaluation, and the postrebuttal discussion. this paper proposed a new training method to defend against backdoor attacks. while all reviewers see merits in this paper, some discussions about 1 the practicality of the defense using clean data samples and 2 fair comparisons to existing defenses have been raised and discussed. during the authorreviewer discussion phase, the reviewer had detailed interactions with the authors to clarify different use cases and practical scenarios of the proposed defense and the fairness of the evaluation. so both major concerns are adequately addressed. another reviewer also champions acceptance in the internal discussion. all in all, i am recommending acceptance. my confidence is lower compared to other submissions simply because this paper has the lowest average rating score of all papers i recommend acceptance.", "accepted": 0}
{"paper_id": "nips_2022_lXUp6skJ7r", "review_text": "simple and practical way to do better at domain generalization when it comes to semantic segmentation. advstyle can generate images that are hard during training, and prevent the model from overfitting on the source domain. given that it works well, is relatively simple to implement and conceptually sound, i think it will appeal to a large portion of the neurips audience that works on domain generalization.", "accepted": 0}
{"paper_id": "nips_2022_8rZYMpFUgK", "review_text": "overall, reviews for this paper are quite positive. the paper presents an interesting and effective new approach to incorporating a dag constraint into an optimization problem by using a characterization of dags in terms of the logdet function. during discussion, the reviewers raised several important questionspoints for clarification, which the authors largely addressed in their responses. i encourage the authors to use these responses to guide editing of the paper for the final version. there was some disagreement during authorreviewer discussion in regards to a comparison to golem made by one of the reviewers. after the discussion period, the reviewer has provided some useful information on possible reasons for inconsistencies. i hope that the authors will investigate these points carefully and update empirical resultsdiscussions as needed in the final version. from the reviewer i carefully compared my version of golem with the version of golem used by the author released with the golem paper, and there are several differences. 1 my version is implemented in pytorch, theirs is implemented in tensorflow, 2 in my version, a learning rate scheduler is used to apply smaller and smaller learning rates to solve the problem as it approaches a local optimum, and in theirs, a fixed learning rate is used. considering this, i think it is reasonable that the authors did not observe the same performance as i did. here the learning rate scheduler might play an important role since i also observed that with some large learning rate golem may not converge so that a fixed learning rate may finally converge to a bad solution, or fail to converge.  i think the paper can be further enhanced if the authors can replace the dag constraint in golem with theirs to obtain a new algorithm. from my experience, it is highly possible that with a proper optimization algorithm it can achieve far better performance than the current version.", "accepted": 1}
{"paper_id": "nips_2022_6RoAxmwj0L2", "review_text": "this paper proposes a new segmentation method with geometric insight to deal with the distortions. as pointed out by our reviewers, this paper is featured with important practical value, clear problem definition, and interesting mathematical insight. during the rebuttal phase, most of the reviewers confirmed their support for an weak acceptance, and i believe this paper should be accepted as a poster paper.", "accepted": null}
{"paper_id": "nips_2022_QXLue5WoSBE", "review_text": "after rebuttal the new version of the paper reads much better and all reviewers were positive, despite some remaining criticisms. hence the paper should be accepted.", "accepted": null}
{"paper_id": "nips_2022_2nWUNTnFijm", "review_text": "all reviewers agreed that this paper should be accepted because of the strong author response during the rebuttal phase. specifically the reviewers appreciated the motivation of the paper, its clarity, and added explanation and experiments included during the rebuttal. authors please carefully revise the manuscript based on the suggestions by the reviewers they made many careful suggestions to improve the work and stressed that the paper should only be accepted once these changes are implemented. to these suggestions i urge the authors to add another i strongly suggest removing section 3.2. this data generating process is not validated and is not at all necessary for your approach. the scm is never referred to again outside of this section. all that is necessary is that one can view molecules as coming from different environments or contexts and predicting this context is useful to improve generalization. finally, with the added space i suggest expanding figure 1 to add more examples of environments and make this clearer in the figure right now you only mention briefly in the caption that different scaffolds can be thought of as different environments. if you could include more  better examples that align with your experiments this will make the motivation clearer. once these changes are made the paper will be a nice addition to the conference!", "accepted": null}
{"paper_id": "nips_2022_k7xZKpYebXL", "review_text": "the paper proposed an interesting lower bound in the learning to hash scenarios and builds on that to show a good algorithm that outperforms several learning to hash methods. there were concerns about the size and scale of experiments which was sufficiently addressed in the rebuttal. the reviewers were not in consensus primarily because of the writing. we think that the writing concerns are fixable and the authors will improve the draft using the reviewers comment for the final version.", "accepted": 0}
{"paper_id": "iclr_2018_HktK4BeCZ", "review_text": "the reviewers are unanimous in finding the work in this paper highly novel and significant. they have provided detailed discussions to back up this assessment. the reviewer comments surprisingly included a critique that the scientific content of the work has critical conceptual flaws ! however, the author rebuttal persuaded the reviewers that the concerns were largely addressed.", "accepted": 1}
{"paper_id": "iclr_2018_B1gJ1L2aW", "review_text": "the paper characterizes the latent space of adversarial examples and introduces the concept of local intrinsic dimenstionality lid. lid can be used to detect adversaries as well build better attacks as it characterizes the space in which dnns might be vulnerable. the experiments strongly support their claim.", "accepted": 1}
{"paper_id": "iclr_2018_SyYe6k-CW", "review_text": "this paper is not aimed at introducing new methodologies and does not claim to do so, but instead it aims at presenting a wellexecuted empirical study. the presentation and outcomes of this study are quite instructive, and with the evergrowing list of academic papers, this kind of studies are a useful regularizer.", "accepted": null}
{"paper_id": "iclr_2018_SyMvJrdaW", "review_text": "this paper proposes a warp operator based on taylor expansion that can replace a block of layers in a residual network, allowing for parallelization. taking advantage of multigpu parallelization the paper shows increased speedup with similar performance on cifar10 and cifar100. r1 asked for clarification on rotational symmetry. the authors instead removed the discussion that was causing confusion replacing with additional experimental results that had been requested. r2 had the most detailed review and thought that the idea and analysis were interesting. they also had difficulty following the discussion of symmetry noted above. they also pointed out several other issues around clarity and had several suggestions for improving the experiments which seem to have been taken to heart by the authors, who detailed their changes in response to this review. there was also an anonymous public comment that pointed out a fatal mathematical flaw and weak experiments. there was a lengthy exchange between this reviewer and the authors, and the paper was actually corrected and clarified in the process. this anonymous poster was rather demanding of the authors, asking for latexformatted equations, pseudocode, and giving direction on how to respond to hisher rebuttal. i dont agree with the point that the paper is flawed by only presenting a speedup over resnet, and furthermore the comment of not everyone has access to parallelization isnt a fair criticism of the paper.", "accepted": null}
{"paper_id": "iclr_2018_B1n8LexRZ", "review_text": "this paper presents a learned inference architecture which generalizes hmc. it defines a parameterized family of mcmc transition operators which share the volume preserving structure of hmc updates, which allows the acceptance ratio to be computed efficiently. experiments show that the learned operators are able to mix significantly faster on some simple toy examples, and evidence is presented that it can improve posterior inference for a deep latent variable model. this paper has not quite demonstrated usefulness of the method, but it is still a good proof of concept for adaptive extensions of hmc.", "accepted": 1}
{"paper_id": "iclr_2018_ry80wMW0W", "review_text": "overall this paper seems to make an interesting contribution to the problem of subtask discovery, but unfortunately this only works in a tabular setting, which is quite limiting.", "accepted": null}
{"paper_id": "iclr_2018_ByS1VpgRZ", "review_text": "the paper proposes a simple modification to conditional gans, where the discriminator involves an inner product term between the condition vector y and the feature vector of x. this formulation is reasonable and well motivated from popular models e.g., loglinear, gaussians. experimentally, the proposed method is evaluated on conditional image generation and superresolution tasks, demonstrating improved qualitative and qualitative performance over the existing stateoftheart acgan.", "accepted": null}
{"paper_id": "iclr_2018_H1vEXaxA-", "review_text": "the paper considers learning an nmt systems while pivoting through images. the task is formulated as a referential game. from the modeling and setup perspective it is similar to previous work in the area of emergent communication  referential games, e.g., lazaridou et al iclr 17 and especially to havrylov  titov nips 17, as similar techniques are used to handle the variablelength channel rnn encoders  decoders  the st gumbelsoftmax estimator. however, its multilingual version is interesting and the results are sufficiently convincing e.g., comparison to nakayama and nishida, 17. the paper would more attractive for those interested in emergent communication than the nmt community, as the setup using pivoting through images may be perceived somewhat exotic by the nmt community. also, the model is not attentionbased unlike soa in seq2seq  nmt, and it is not straightforward to incorporate attention see r2 and author response.  an interesting framing of the weaklysupervised mt problem  well written  sufficiently convincing results  the setup and framework e.g., nonattention based is questionable from practical perspective", "accepted": 1}
{"paper_id": "iclr_2018_rypT3fb0b", "review_text": "the paper proposes to regularize via a family of structured sparsity norms on the weights of a deep network. a proximal algorithm is employed for optimization, and results are shown on synthetic data, mnist, and cifar10. pros the regularization scheme is reasonably general, the optimization is principled, the presentation is reasonable, and all three reviewers recommend acceptance. cons the regularization is conceptually not terribly different from other kinds of regularization proposed in the literature. the experiments are limited to quite simple data sets.", "accepted": 1}
{"paper_id": "iclr_2018_S1XolQbRW", "review_text": "the submission proposes a method for quantization. the approach is reasonably straightforward, and is summarized in algorithm 1. it is the analysis which is more interesting, showing the relationship between quantization and adding gaussian noise appendix b  motivating quantization as regularization. the submission has a reasonable mix of empirical and theoretical results, motivating a simpletoimplement algorithm. all three reviewers recommended acceptance.", "accepted": 1}
{"paper_id": "iclr_2018_rkYTTf-AZ", "review_text": "this work presents some of the first results on unsupervised neural machine translation. the group of reviewers is highly knowledgeable in machine translation, and they were generally very impressed by the results and the think it warrants a whole new area of research noting the fact that this is possible at all is remarkable.. there were some concerns with the clarity of the details presented and how it might be reproduced, but it seems like much of this was cleared up in the discussion. the reviewers generally praise the thoroughness of the method, the experimental clarity, and use of ablations. one reviewer was less impressed, and felt more comparison should be done.", "accepted": 1}
{"paper_id": "iclr_2018_rJQDjk-0b", "review_text": "the reviewers agree that the proposed method is theoretically interesting, but disagree on whether it has been properly experimentally validated. my view is that the the theoretical contribution is interesting enough to warrant inclusion in the conference, and so i will err on the side of accepting.", "accepted": 1}
{"paper_id": "iclr_2018_ByKWUeWA-", "review_text": "the reviewers agree that the method is original and mostly well communicated, but have some doubts about the significance of the work.", "accepted": null}
{"paper_id": "iclr_2018_S18Su--CW", "review_text": "this paper is borderline. the reviewers agree that the method is novel and interesting, but have concerns about scalability and weakness to attacks with larger epsilon. i will recommend accepting; but i think the paper would be well served by imagenet experiments, and hope the authors are able to include these for the final version", "accepted": null}
{"paper_id": "iclr_2018_HktJec1RZ", "review_text": "this submission introduces soft local reordering to the recently proposed swan layer wang et al., 2017 to make it suitable for machine translation. although only in smallscale experiments, the results are convincing.", "accepted": 1}
{"paper_id": "iclr_2018_rkfOvGbCW", "review_text": "the proposed approach nicely incorporates various ideas from recent work into a single metalearning or domain adaptation or incremental learning or ... framework. although better empirical comparison to existing however recent they are approaches would have made it stronger, the reviewers all found this submission to be worth publication, with which i agree.", "accepted": 1}
{"paper_id": "iclr_2018_HyUNwulC-", "review_text": "paper presents a way in which linear rnns can be computed fprop, bprop using parallel scan. they show big improvements in speedups and show application on really long sequences. reviews were generally favorable.", "accepted": null}
{"paper_id": "iclr_2018_BJGWO9k0Z", "review_text": "the paper got generally positive scores of 6,7,7. the reviewers found the paper to be novel but hard to understand. the ac feels the paper should be accepted but the authors should revise their paper to take into account the comments from the reviewers to improve clarity.", "accepted": null}
{"paper_id": "iclr_2018_HJ94fqApW", "review_text": "the paper received scores either side of the borderline 6 r1, 5 r2, 7 r3. r1 and r3 felt the idea to be interesting, simple and effective. r2 raised a number of concerns which the rebuttal addressed satisfactorily. therefore the ac feels the paper can be accepted.", "accepted": null}
{"paper_id": "iclr_2018_rknt2Be0-", "review_text": "this paper investigates emergence of language from raw pixels in a twoagent setting. the paper received divergent reviews, 3,6,9. two acs discussed this paper, due to a strong opinion from both positive and negative reviewers. the acs agree that the score 9 is too high the notion of compositionality is used in many places in the paper and even in the title, but never explicitly defined. furthermore, the zeroshot evaluation is somewhat disappointing. if the grammar extracted by the authors in sec. 3.2 did indeed indicate the compositional nature of the emergent communication, the authors should have shown that they could in fact build a message themselves, give it to the listener with an image and ask it to answer. on the other hand, 3 is also too low of a score. in this renaissance of emergent communication protocol with multiagent deep learning systems, one missing piece has been an effort toward seriously analyzing the actual properties of the emergent communication protocol. this is one of the few papers that have tackled this aspect more carefully. the acs decided to accept the paper. however, the authors should take the reviews and comments seriously when revising the paper for the camera ready.", "accepted": 1}
{"paper_id": "iclr_2018_HJCXZQbAZ", "review_text": "this paper marries the idea of gaussian word embeddings and order embeddings, by imposing order among probabilistic word embeddings. two reviewers vote for acceptance, and one finds the novelty of the paper incremental. the reviewer stuck to this view even after rebuttal, however, acknowledges the improvement in results. the ac read the paper, and agrees that the novelty is somewhat limited, however, the idea is still quite interesting, and the results are promising. the ac was missing more experiments on other tasks originally presented by vendrov et al. overall, this paper is slightly over the bar.", "accepted": 1}
{"paper_id": "iclr_2018_HJhIM0xAW", "review_text": "this work shows interesting potential applications of known machine learning techniques to the practical problem of how to devise a retina prosthesis that is the most perceptually useful. the paper suffers from a few methodological problems pointed out by the reviewers e.g., not using the more powerful neural network encoding in the subsequent experiments of the paper, but is still interesting and inspiring in its current state.", "accepted": null}
{"paper_id": "iclr_2018_S1uxsye0Z", "review_text": "the reviewers agreed that the work addresses an important problem. there was disagreement as to the correctness of the arguments in the paper one of these reviewers was eventually convinced. the other pointed out another two issue in their final post, but it seems that 1. the first is easily adopted and does not affect the correctness of the experiments and 2. the second was fixed in the second revision. ideally these would be rechecked by the third reviewer, but ultimately the correctness of the work is the authors responsibility. some related work by mcallister was pointed out late in the process. i encourage the authors to take this related work seriously in any revisions. it deserves more than two sentences.", "accepted": null}
{"paper_id": "iclr_2018_Sk9yuql0Z", "review_text": "paper proposes adding randomization steps during inference time to cnns in order to defend against adversarial attacks. pros  results demonstrate good performance, and the team achieve a high rank 2nd place on a public benchmark.  the benefit of the proposed approach is that it does not require any additional training or retraining. cons  the approach is very simple, common sense would tend to suggest that adding noise to images would make adversarial attempts more difficult. though perhaps simplicity is a good thing.  update paper does not cite related and relevant work, which takes a similar approach of requiring no retraining, but rather changing the inference stage httpsarxiv.orgpdf1709.05583.pdf grammatical suggestions this paper would benefit from polishing. for example  abstract sentence 1 replace their powerful ability to high accuracy  abstract sentence 3 replace i.e., clean images with for example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail  abstract sentence 4 replace utilize randomization to implement randomization at inference time or something similar to make more clear that this procedure is not done during training.  abstract sentence 7 replace also enjoys with provides main text capitalize references to figures i.e. figure 1 to figure 1. introduction paragraph 4 again, please replace randomization with randomization at inference time or something similar to better address reviewer concerns.", "accepted": null}
{"paper_id": "iclr_2018_BkXmYfbAZ", "review_text": "pros 1. clear, interesting idea. 2. largely convincing evaluation 3. good writing cons 1. the model used in the evaluation is a resnet50 and could have been more convincing with a more sota model. 2. there is some concern about the whether the comparison of results fig 6c is really apples to apples.", "accepted": null}
{"paper_id": "iclr_2018_rkrC3GbRW", "review_text": "viewing the problem of determining the validity of highdimensional discrete sequences as a sequential decision problem, the authors propose learning a q function that indicates whether the current sequence prefix can lead to a valid sequence. the paper is fairly well written and contains several interesting ideas. the experimental results appear promising but would be considerably more informative if more baselines were included. in particular, it would be good to compare the proposed approach both conceptually and empirically to learning a generative model of sequences. also, given that your method is based on learning a q function, you need to explain its exact relationship to classic qlearning, which would also make for a good baseline.", "accepted": null}
{"paper_id": "iclr_2018_SywXXwJAb", "review_text": "this paper seemingly joins a cohort of iclr submissions which attempt to port mature concepts from physics to machine learning, make a complex and nontrivial theoretical contribution, and fall short on the empirical front. the one aspect that sets this apart from its peers is that the reviewers agree that the theoretical contribution of this work is clear, interesting, and highly nontrivial. while the experiment sections mnist! is indubitably weak, when treating this as a primarily theoretical contribution, the reviewers in particular 6 and 3 are happy to suggest that the paper is worth reading. taking this into account, and discounting somewhat the short and, by their own admission, uncertain assessment of reviewer 5, i am leaning towards pushing for the acceptance of this paper. at very least, it would be a shame not to accept it to the workshop track, as this is by far the strongest paper of this type submitted to this conference.", "accepted": 1}
{"paper_id": "iclr_2018_S1WRibb0Z", "review_text": "this paper offers a theoretical and empirical analysis of the expressivity of rnns, in particular in comparison to tt decomposition. the reviewers argued the results was interesting and important, although there were issues with clarity of some of the explanations. more critical reviewers argued the comparison basis with cp networks was not fair in that their shallowness restricted their expressivity w.r.t. tt. the experiments could be strengthened by making the explanations surrounding the set up clearer. this paper is borderline acceptable, and would have benefited from a more active discussion between the reviewers and the author. from reading the reviews and the author responses, i am leaning towards recommending acceptance to the main conference rather than the workshop track, as it is important to have theoretical work of this nature discussed at iclr.", "accepted": null}
{"paper_id": "iclr_2018_HJvvRoe0W", "review_text": "this paper addresses an important application in genomics, i.e. the prediction of chromatin structure from nucleotide sequences. the authors develop a novel method for converting the nucleotide sequences to a 2d structure that allows a cnn to detect interactions between distant parts of the sequence. the reviewers found the paper innovative, interesting and convincing. two reviewers gave a 7 and there was one 6. the 6, however, indicated during rather lengthy discussion that they were willing to raise their scores if their comments were addressed. hopefully the authors will address these comments in the camera ready version. overall a solid application paper with novel insights and technical innovation.", "accepted": null}
{"paper_id": "iclr_2018_r1Ddp1-Rb", "review_text": "the paper presents a simple but surprisingly effective data augmentation technique which is thoroughly evaluated on a variety of classification tasks, leading to improvement over stateoftheart baselines. the paper is somewhat lacking a theoretical justification beyond intuitions, but extensive evaluation makes up for that.", "accepted": null}
{"paper_id": "iclr_2018_HyiAuyb0b", "review_text": "this is an interesting piece of work that provides solid evidence on the topic of bootstrapping in deep reinforcement learning.", "accepted": null}
{"paper_id": "iclr_2018_HyWrIgW0W", "review_text": "dear authors, based on the comments and your rebuttal, i am glad to accept your paper at iclr.", "accepted": 1}
{"paper_id": "iclr_2018_HyRnez-RW", "review_text": "the authors did a good job addressing reviewer concerns and analyzing and testing their model on interesting datasets with convincing results.", "accepted": null}
{"paper_id": "iclr_2018_SJA7xfb0b", "review_text": "the paper provides a useful analysis of the role of gradient penalties and the performance of the proposed approach in semisupervised cases.", "accepted": 1}
{"paper_id": "iclr_2018_SkHl6MWC-", "review_text": "r1 thought the proposed method was novel and the idea interesting. however, heshe raised concerns with consistency in the experimental validation, the tradeoff between accuracy and running time, and the positioningmotivation, specifically the claim about interpretability. the authors responded to these concerns, and r1 upgraded their score. r2 didnt raise major concerns or strengths. r3 questioned the novelty of the work and the experimental validations. all reviewers raised concerns with the writing. though i think the work is interesting, issues raised about experiments and writing make me hesitant to go against the overall recommendation of the reviewers, which is just below the bar. i think this is a paper that could make a good workshop contribution.", "accepted": null}
{"paper_id": "iclr_2018_SyUkxxZ0b", "review_text": "this paper studies the interplay between adversarial examples and generalization in the uniform setting not specific assumptions on the architecture in a toy highdimensional setting. in particular, the authors show a fundamental tradeoff between generalization error and the average distance of adversarial examples. reviewers were skeptical about the possible significance of this work, but the paper underwent a major revision that greatly improved the quality of presentation. that said, the results are still preliminary since they only consider a toy dataset concentric spheres. the ac recommends resubmitting this work to the workshop track.", "accepted": 0}
{"paper_id": "iclr_2018_rkxY-sl0W", "review_text": "the problem is interesting, and the approach is also interesting. however, the reviewers have found that this manuscript would benefit from more experiments, potentially involving some real data even at least for evaluation in addition to largely synthetic data sets used in the submission. i also agree with them and encourage authors to consider this option.", "accepted": 0}
{"paper_id": "iclr_2018_HkcTe-bR-", "review_text": "the paper creates a dataset for exploration of rl for molecular design and i think this makes it a strong contribution to the community at the intersection of the two. for a methods focussed conference such as iclr however, it may not be the best fit. hence i would recommend submitting to a workshop track or targeting a more focussed venue such as a bioinformatics conference.", "accepted": 0}
{"paper_id": "iclr_2018_Syr8Qc1CW", "review_text": "the method proposed in the paper for latent disentanglement and attributeconditional image generation is novel to the best of my understanding but reviewers anon1 and anon3 have expressed concerns on the quality of results celeba images as well as on the technical presentation and claims in the paper. given the novelty of the proposed method, i would not like to recommend a reject for this paper but the concerns raised by the reviewers on the quality of results and lack of quantitative results seem valid. authors rule out possibility of any quantitative results in their response but i am not fully convinced  in particular, effectiveness of attributeconditional image generation can be captured by first training an attribute classifier on the generated images and then measuring how often the predicted attributes are flipped when conditioning signal is changed. there are also other metrics in the literature for evaluating generative models. i would recommend inviting it to the workshop track, given that the work is novel and interesting but has scope for improvements.", "accepted": 0}
{"paper_id": "iclr_2018_ryDNZZZAW", "review_text": "pros  lays out bounds for multidomain adaptation based on earlier work on a single sourcetarget domain pair.  shows gains over choosing the best source domain for a target domain, or naively combining domains. cons  the reviewers agree that the extensions are relatively straightforward extensions to single sourcetarget pair.  hardmax doesnt consider the partial contribution of multiple source domains, and considers the worstcase scenario.  softmax addresses some of these issues; the authors provide reasonable justification for the algorithm but its not clear that the specific choice of alphas leads to the tightest bound. the reviewers noted that the authors significantly improved the paper during the revision process. the ac feels that the presented techniques would be of interest to the community and would help lead discussions towards theoretically optimal ways to do domain adaptation given multiple domains. the authors are therefore encouraged to submit to the workshop track.", "accepted": null}
{"paper_id": "iclr_2018_Hk91SGWR-", "review_text": "this paper turned out to be quite difficult to call. my take on the proscons is 1. the research topic, how and why humans can massively outperform dqn, is unanimously viewed as highly interesting by all participants. 2. the authors present an original human subject study, aiming to reveal whether human outperformance is due to human knowledge priors. the study is well conceived and well executed. i consider the study to be a contribution by itself. 3. the study provides prima facie evidence that human priors play a role in human performance, by changing the visual display so that the priors cannot be used. 4. however, the study is not definitive, as astutely argued by anonreviewer2. experiments using rl agents with presumably no human priors yield behavior that is similar to human behavior. so it is possible that some factor other than human prior may account for the behavior seen in the human experiments. 5. it would indeed be better, as argued by anonreviewer2, to use some informationtheoretic measure to distinguish the normal game from the modified games. 6. the paper has been substantially improved and cleaned up from the original version. 7. anonreviewer1 provided some thoughtful detailed discussion of how the authors may be overstating the conclusions that one can draw from the paper. bottom line given the procs and cons of the paper, the committee recommends this for workshop.", "accepted": 0}
{"paper_id": "iclr_2018_ByaQIGg0-", "review_text": "differentiable neural networks used as a measure of design optimality in order to improve efficiency of automated design. pros  genetic algorithms, which are the dominant optimization routine for automated design systems, can be computationally expensive. this approach alleviates this bottleneck under certain circumstances and applications. cons  primarily application paper, machine learning advancement is marginal.  multiple reviewers generalization capability not clear. for example, some utility systems may be stochastic i.e. turbulence and require multiple trials to measure fitness, which this method would not be able to model. overall, the committee feels this paper is interesting enough to appears as a workshop paper.", "accepted": 0}
{"paper_id": "iclr_2018_HyDMX0l0Z", "review_text": "the paper presents a really interesting take on the mode collapse problem and argue that the issue arises because of the current gan models try to model distributions with disconnected support using continuous noise and generators. the authors try to fix this issue by training multiple generators with shared parameters except for the last layer. the paper is well written and authors did a good job in addressing some of the reviewer concerns and improving the paper. even though arguments presented are novel and interesting, reviewers agree that the paper lacks sufficient theoretical or experimental analysis to substantiate the claimsarguments made in the paper. limited quantitative and subjective results are not always in favor of the proposed algorithm. more controlled toy experiments and results on larger datasets are needed. the central argument about tunneling is interesting and needs deeper investigation. overall the committee recommends this paper for workshop.", "accepted": 0}
{"paper_id": "iclr_2018_BkM3ibZRW", "review_text": "in general, the reviewers and myself find this work of some interest, though potentially somewhat incremental in terms of technical novelty compared to the work for makhzani et al. another bothersome aspect is the question of evaluation and understanding how well the model actually does; i am not convinced that the interpolation experiments are actually giving us a lot of insights. one interesting ablation experiment suggested privately by one of the reviewers would be to try aae with wasserstein and without a learned generator  this would disambiguate which aspects of the proposed method bring most of the benefit. as it stands, the submission is just shy of the acceptance bar, but due to its interesting results in the natural language domain, i do recommend it being presented at the workshop track.", "accepted": 1}
{"paper_id": "iclr_2018_SySaJ0xCZ", "review_text": "the paper proposes a method for architecture search using network morphisms, which allows for faster search without retraining candidate models. the results on cifar are worse than the state of the art, but reasonably competitive, and achieved using limited computation resources. it would have been interesting to see how the method would perform on large datasets imagenet andor other tasks and search spaces. i would encourage the authors to extend the paper with further experimental evaluation.", "accepted": 0}
{"paper_id": "iclr_2018_rkaT3zWCZ", "review_text": "the authors present an environment for semantic navigation that is based on an existing dataset, suncg. datasetsenvironments are important for deep rl research, and the contribution of this paper is welcome. however, this paper does not offer enough novelty in terms of approachmethod and its claims are somewhat misleading, so it would probably be a better fit to publish it at a workshop.", "accepted": 1}
{"paper_id": "iclr_2018_H15RufWAW", "review_text": "this paper proposes an implicit model of graphs, trained adversarially using the gumbelsoftmax trick. the main idea of feeding random walks to the discriminator is interesting and novel. however, 1 the task of generating sibling graphs, for some sort of bootstrap analysis, isnt wellmotivated. 2 the method is complicated and presumably hard to tune, with two separate earlystopping thresholds that need to be tuned 3 there is not even a mention of a large existing literature on generative models of graphs using variational autoencoders.", "accepted": 0}
{"paper_id": "iclr_2018_ryj38zWRb", "review_text": "this paper attempts to decouple two factors underlying the success of gans the inductive bias of deep cnns and adversarial training. it shows that, surprisingly, the second factor is not essential. r1 thought that comparisons to generative moment matching networks and variational autoencoders should be provided note this was added to a revised version of the paper. they also pointed out that the paper lacked comparisons to newer flavors of gans. while r1 pointed out that the use of 128x128 and 64x64 images was weak, i tend to disagree as this is still common for many gan papers. r2 was neutral to positive about the paper and thought that most importantly, the training procedure was novel. r3 also gave a neutral to positive review, claiming the paper was easy to follow and interesting. like r1, r3 thought that a stronger claim could be made by using different datasets. in the rebuttal, the authors argued that the main point was not in proposing a stateoftheart generative model of images but to provide more an introspection on the success of gans. overall, i found the work interesting but felt that the paper could go through one more reviewrevision cycle. in particular, it was very long. without a champion, this paper did not make the cut.", "accepted": 0}
{"paper_id": "iclr_2018_SkfNU2e0Z", "review_text": "this paper presents a toolbox for the exploration of layerwiseparallel deep neural networks. the reviewers were consistent in their analysis of this paper it provided an interesting class of models which warranted further investigation, and that the toolbox would be useful to those who are interested in exploring further. however, there was a lack of convincing examples, and also some concern that theano no longer maintained was the only supported backend. the authors responded to say that they had subsequently incorporated tensorflow support, they were not able to provide any more examples due to several reasons time, pending ip concerns, open technical details, sufficient presentation quality, page restriction. i agree with the consensus reached by the reviewers.", "accepted": 0}
{"paper_id": "iclr_2018_HJ1HFlZAb", "review_text": "given that the paper proposes a new evaluation scheme for generative models, i agree with the reviewers that it is essential that the paper compare with existing metrics even if they are imperfect. the choice of datasets was very limited as well, given the nature of the paper. i acknowledge that the authors took care to respond in detail to each of the reviews.", "accepted": 0}
{"paper_id": "iclr_2018_SyvCD-b0W", "review_text": "the reviewers have pointed out that there is a substantial amount of related work that this paper should be acknowledging and building on.", "accepted": 0}
{"paper_id": "iclr_2018_SJ71VXZAZ", "review_text": "the paper reports experiments where a lstm language model is pretrained on a large corpus of reviews, and then the produced representation is used within a classifier on a number of sentiment classification datasets. the relative success of the method is not surprising. the novelty is very questionable, the writing quality is mixed e.g., typos, the model is not even properly described. there are many gaps in evaluation e.g., from the intro it seems that the main focus is showing that byte level modeling is preferable to more standard setups  characters  bpe  words. however, there are almost no experiments supporting this claim. the same is true for the sentiment neuron its effectiveness is also not properly demonstrated. in general, the results are somewhat mixed. pros  good results on some datasets cons  limited novelty  some claims are not tested  issues with evaluation  writing quality is not sufficient  clarity issues overall, the reviewers are in agreement that the paper does not meet iclr standards.", "accepted": 0}
{"paper_id": "iclr_2018_ryH_bShhW", "review_text": "the reviewers all outlined concerns regarding novelty and the maturity of this work. it would be helpful to clarify the relation to doubly stochastic kernel machines as opposed to random kitchen sinks, and to provide more insight into how this stochasticity helps. finally, the approach should be tried on more difficult image datasets.", "accepted": 0}
{"paper_id": "iclr_2018_SySisz-CW", "review_text": "thank you for submitting you paper to iclr. the paper presents an interesting analysis, but the utility of this analysis is questionable e.g. it is not clear how this might lead to improved vaesgans. the authors did add an additional experimental result in their revised paper, but questions still remain. in light of this the significance of the paper is on the low side and it is therefore not ready for publication in iclr without more work.", "accepted": null}
{"paper_id": "iclr_2018_S1EwLkW0W", "review_text": "this paper presents a theoretical justification for the adam optimizer in terms of decoupling the signs and magnitudes of the gradients. the overall analysis seems reasonable, though theres been much backandforth with the reviewers about particular claims and assumptions. overall, the contributions dont feel quite substantial enough for an iclr publication. the interpretation in terms of signs is interesting, but its very similar to the motivation for rmsprop, of which adam is an extension. the performance result on diagonally dominant noisy quadratics is interesting, but it feels unsurprising that a diagonal curvature approximation would work well in this setting. i dont recommend acceptance at this point, though these ideas could potentially be developed further into a strong submission.", "accepted": 0}
{"paper_id": "iclr_2018_Bkl1uWb0Z", "review_text": "in this work reviewers use structured attention as a way to induce grammatical structure in nmt models. reviewers liked th motivation of the work and found experiments mostly well done. however reviewers found the paper a bit difficult to follow, with several commenting that distinctions made between the different sub types of attention were not clear. mainly the reviewers were not overwhelmed by the results of the work, saying that these gains, while clearly isolated to the use of structure were not significantly large. additionally there were some concerns about the claimed novelty of the work, particularly compared to liu and lapata and other use of syntax in translation, and also which aspects were new or necessary.", "accepted": 0}
{"paper_id": "iclr_2018_SJCPLLpaW", "review_text": "while this paper has some very interesting ideas the majority view of the reviewers and their aggregate numerical ratings are just too low to warrant acceptance.", "accepted": 0}
{"paper_id": "iclr_2018_H1srNebAZ", "review_text": "while one reviewer did upgrade their rating from 6 to 7, the most negative reviewer maintains overall, i find this work interesting and current results surprising. however, i find it to be a preliminary work and not yet ready for publication. the paper still lacks a conclusion  a leading hypothesis  an explanation for the shown results. i find this conclusion indispensable even for a small scientific study to be published. after the rebuttal. with scores of 754 it is just not possible for the ac to recommend acceptance.", "accepted": 0}
{"paper_id": "iclr_2018_SJLy_SxC-", "review_text": "the paper presents an empirical study into sparse connectivity patterns for densenets. whilst sparse connectivity is potentially interesting, the paper does not make a strong argument for such sparse connectivity patterns in particular, the results on imagenet suggest that sparse connectivity performs substantially worse than full connectivity at the same flopslevel, logdensenet obtains 2.5 lower accuracy than baseline densenet models, and the best logdensenet is 4 worse than the best densenet. on camvid, both network architectures appear to perform on par. the paper motivates the model architecture by the high memory consumption of densenets but, frankly, that is a very weak motivation densenets are actually very memoryefficient if implemented correctly httpsarxiv.orgpdf1707.06990.pdf. the fact that such implementations are not wellsupported by tensorflowpytorch is a shortcoming of those deeplearning frameworks, not in densenets. in fact, the memory management features that deeplearning frameworks have implemented to make residual networks memoryefficient for instance, caching gpu memory allocation in pytorch are far more complex than the thousand lines of c currently needed to implement a densenet correctly. such issues will likely be resolved relatively soon by better implementations, and are hardly a good motivation for a different network architecture.", "accepted": null}
{"paper_id": "iclr_2018_SkrHeXbCW", "review_text": "the paper received three good quality reviews which were in agreement that the paper was below the acceptance threshold. the authors are encouraged to follow the suggestions from the reviews to revise the paper and resubmit to another venue.", "accepted": 0}
{"paper_id": "iclr_2018_SJVHY9lCb", "review_text": "three reviewers recommended rejection and there was no rebuttal.", "accepted": 0}
{"paper_id": "iclr_2018_Bym0cU1CZ", "review_text": "this work takes dialogue acts into account to generate responses in a humanmachine conversation. however, incorporating dialogue acts into opendomain dialogue was already the focus of zhao et als acl 2017 paper, learning discourselevel diversity for neural dialog models using conditional variational autoencoders, and using dialogue acts in a policy for humanmachine conversation was also an idea that already appeared in serban et al 2017, a deep reinforcement learning chatbot. despite the authors response that tries to adjust their claims and incorporate a more thorough overview, i encourage the authors to rework their research with a much more careful and reliable examination of previous work and how their effort should be understood in that more comprehensive context.", "accepted": 0}
{"paper_id": "iclr_2018_S1GUgxgCW", "review_text": "this paper combines existing models to detect topics and generate responses, and the resulting model is shown to be slightly preferred by human evaluators over baselines. this is quite incremental and the results are not impressive enough to stand on their own merit.", "accepted": 0}
{"paper_id": "iclr_2018_SkBHr1WRW", "review_text": "this paper deals with the important topic of learning better graph representations and shows promise in helping to detect critical substructures of graph that would help with the interpretability of representations. unfortunately, this work fails to accurately portray how it relates to previous work in particular, niepert et al, kipf et al, duvenaud et al and falls short of providing clear and convincing explanations of what it can do that these models cant, without including all of them in experimental comparisons.", "accepted": 0}
{"paper_id": "iclr_2018_B1i7ezW0-", "review_text": "the paper proposes a novel approach for dnn inversion mainly targeted towards semisupervised learning. however the semisupervised learning results are not competitive enough. although the authors mention in the authorresponse that semisupervised learning is not the main goal of the paper, the experiments and claims of the paper are mainly targeted towards semisupervised learning. as the approach for inversion is novel, the paper could be motivated from a different angle with appropriate supporting experiments. in its current form its not suitable for publication.", "accepted": 0}
{"paper_id": "iclr_2018_rJTGkKxAZ", "review_text": "reviewers recognize the proposed method of hierarchical extension to ali to be potentially novel and interesting but have expressed strong concerns on the experiments section. the paper also needs to have comparisons with relevant hierarchical generative model baselines. not suitable for publication in its current form.", "accepted": 0}
{"paper_id": "iclr_2018_H1-oTz-Cb", "review_text": "the experiments are not sufficient to support the claim. the authors plan to improve it for future publication.", "accepted": 0}
{"paper_id": "iclr_2018_SyL9u-WA-", "review_text": "pros  clearly written paper.  good theoretical analysis of the expressivity of the proposed model.  efficient model update is appealing.  reviewers appreciated the addition of results on the copy and adding tasks in appendix c. cons  evaluation was on lessstandard rnn tasks. a language modeling task should have been included in the empirical evaluation because language modeling is such an important application of rnns. this paper is close to the decision boundary, but the reviewers strongly felt that demonstration of the method on a language modeling task was necessary for acceptance.", "accepted": null}
{"paper_id": "iclr_2018_HJ39YKiTb", "review_text": "none of the reviewers are enthusiastic about the paper, primarily due to lack of proper evaluation. the response of the authors towards this criticism is also not sufficient. the final results are mixed which does not show very clearly that the presented associative model performs better than the sole seq2seq baseline that the authors use for comparison. we think that addressing these immediate concerns would improve the quality of this paper.", "accepted": 0}
{"paper_id": "iclr_2018_r1TA9ZbA-", "review_text": "all reviewers agree that the contribution of this paper, a new way of training neural nets to execute montecarlo tree search, is an appealing idea. for the most part, the reviewers found the exposition to be fairly clear, and the proposed architecture of good technical quality. two of the reviewers point out flaws in implementing in a single domain, 10x10 sokoban with four boxes and four targets. since their training methodology uses supervised training on approximate groundtruth trajectories derived from extensive plain mcts trials, it seems unlikely that the trained dnn will be able to generalize to other geometries beyond 10x10x4 that were not seen during training. sokoban also has a low branching ratio, so that these experiments do not provide any insight into how the methodology will scale at much higher branching ratios. pros good technical quality, interesting novel idea, exposition is mostly clear. good empirical results in one very limited domain. cons single 10x10x4 sokoban domain is too limited to derive any general conclusions. point for improvement the paper compares performance of mctsnet trials vs. plain mcts trials based on the number of trials performed. this is not an appropriate comparison, because the nn trials will be much more heavyweight in terms of cpu time, and there is usually a time limit to cut off mcts trials and execute an action. it will be much better to plot performance of mctsnet and plain mcts vs. cpu time used.", "accepted": 0}
{"paper_id": "iclr_2018_H18uzzWAZ", "review_text": "this is a nice but very narrow study of domain invariance in a microscopic imaging application. since the problem is very general, the paper should include much more substantial context, e.g. discussion of various alternative methods e.g. the ones cited in sun et al. 2017. in order to contribute to the broader iclr community, ideally the paper would also include application to more than just the one task.", "accepted": 0}
{"paper_id": "iclr_2018_S191YzbRZ", "review_text": "this paper proposes an approach for predicting transcription factor tf binding sites and tftf interaction. the approach is interesting and may ultimately be valuable for the intended application. but in its current state, the paper has insufficient technical novelty e.g. relative to matching networks of vinyals 2016, insufficient comparisons with prior work, and unclear benefit of the approach. the reviewers also had some concerns about clarity.", "accepted": null}
{"paper_id": "iclr_2018_SyGT_6yCZ", "review_text": "the paper addresses the training time of cnns, in the common setting where a cnn is trained on one domain and then used to extract features for another domain. the paper proposes to speed up the cnn training step via a particular proposed training schedule with a reduced number of epochs. training time of the pretrained cnn is not a huge concern, since this is only done once, but optimizing training schedules is a valid and interesting topic of study. however, the approach here does not seem novel; it is typical to adjust training schedules according to the desired tradeoff between training time and performance. the experimental validation is also thin, and the writing needs improvement.", "accepted": 0}
{"paper_id": "iclr_2018_HJqUtdOaZ", "review_text": "the presented method essentially builds a model that remaps features into a new space that optimizes nearestneighbor classification. the model is a neural network, and the optimization is carried out through a genetic algorithm. pros  one major issue with neural network classification is that of a lack of explainability. many networks are currently black box approaches. by moving to the optimization problem to that of building a feature space for nearest neighbor classification, one can, to a degree, alleviate the black box issue by providing the discovered nearest neighbor instances as evidence of the decision.  authors use established datasets. cons  authors do not properly cite previous work, as brought up by reviewers. there is much literature on optimization of feature spaces such as the entire field of metric learning, as well as prior approaches using genetic optimization. the originality and significance here is therefore not clear.", "accepted": 0}
{"paper_id": "iclr_2018_SJahqJZAW", "review_text": "the paper proposes to use multiple discriminators to stabilize the gan training process. additionally, the discriminators only see randomly projected real and generated samples. some valid concerns raised by the reviewers which makes the paper weak  multiple discriminators have been tried before and the authors do not clearly show experimentally  theoretically if the random projection is adding any value.  authors compare only with dcgan and the results are mostly subjective. how much improvement the proposed approach provides when compared to other gan models that are developed with stability as the main goal is hence not clear.", "accepted": 1}
{"paper_id": "iclr_2018_S1EfylZ0Z", "review_text": "the authors propose to detect anomaly based on its representation quality in the latent space of the gan trained on valid samples. reviewers agree that  the proposed solution lacks novelty and similar approaches have been tried before.  the baselines presented in the paper are primitive and hence do not demonstrate the clear benefits over traditional approaches.", "accepted": 0}
{"paper_id": "iclr_2018_rJ8rHkWRb", "review_text": "the paper presents yet another approach for modeling words based on their characters. unfortunately the authors do not compare properly to previous approaches and the idea is very incremental.", "accepted": 0}
{"paper_id": "iclr_2018_ryCM8zWRb", "review_text": "while the use of rnns for building sessionbased recommender systems is certainly an important class of applications, the main strength of the paper is to propose and benchmark practical modifications to prior rnnbased systems that lead to performance improvements. the reviewers have pointed out that the writing in the paper needs improvement, modifications are somewhat straightforward and some expected baselines such as comparisons against state of the art matrixfactorization based methods is missing. as such the paper could benefit from a revision and resubmission elsewhere.", "accepted": 1}
{"paper_id": "iclr_2018_ryZERzWCZ", "review_text": "the paper provides a constrained mutual information objective function whose lagrangian dual covers several existing generative models. however reviewers are not convinced of the significance or usefulness of the proposed unifying framework at least from the way results are presented currently in the paper. authors have not taken any steps towards revising the paper to address these concerns. improving the presentation to bring out the significanceutility of the proposed unifying framework is needed.", "accepted": 0}
{"paper_id": "iclr_2018_B1ydPgTpW", "review_text": "reviewers concur that the paper and the application area are interesting but that the approaches are not sufficiently novel to justify presentation at iclr.", "accepted": 0}
{"paper_id": "iclr_2018_BJRxfZbAW", "review_text": "the paper proposes augmenting neural statistician with a metacontext variable that specifies the partitioning of the latent context into the perdataset and perdatapoint dimensions. this idea makes a lot of sense but the reviewers found the experimental section clearly insufficient to demonstrate its effectiveness convincingly. also introducing only the unsupervised version of the model, which looks challenging to train, but performing all the experiments with the less interesting semisupervised version makes the paper both less compelling and harder to follow.", "accepted": 0}
{"paper_id": "iclr_2018_SyhRVm-Rb", "review_text": "in principle, the idea behind the submission is sound use a generative model gans in this case to learn to generate desirable goals subsets of the state space and use that instead of uniform sampling for goals. overall i tend to agree with reviewer 3 in that the current set of results is not convincing in terms of it being able to generate goals in a highdimensional state space, which seems to be be whole raison detre of gans in this proposed method. the coverage experiment in figure 5 seems like a good illustration of the method, but for this work to be convincing, i think we would need a more diverse set of experiments a la figure 2 showing how this method performs on complicated tasks. i encourage the authors to sharpen the definitions, as suggested by reviewers, and, if possible, provide experiments where the assumptions being made in section 3.3 are violated somehow to actually test how the method fails in those cases.", "accepted": 1}
{"paper_id": "iclr_2018_H1BO9M-0Z", "review_text": "while the problem of learning word embeddings for a new domain is important, the proposed method was found to be unclearly presented and missing a number of important baselines. the reviewers found the technical contribution to be of only limited value.", "accepted": 0}
{"paper_id": "iclr_2018_Bk6qQGWRb", "review_text": "this work develops a methodology for exploration in deep qlearning through thompson sampling to learn to play atari games. the major innovation is to perform a bayesian linear regression on the last layer of the deep neural network mapping from frames to qvalues. this bayesian linear regression allows for efficiently drawing approximate samples from the network. a careful methodology is presented that achieves impressive results on a subset of atari games. the initial reviews all indicated that the results were impressive but questioned the rigor of the empirical analysis and the implementation of the baselines. the authors have since improved the baselines and demonstrated impressive results across more games but questions over the empirical analysis remain by anonreviewer3 for instance and the results still span only a small subset of the atari suite. the reviewers took issue with the treatment of related work, placing the contributions of this paper in relation to previous literature. in general, this paper shows tremendous promise, but is just below borderline. it is very close to a strong and impressive paper, but requires more careful empirical work and a better treatment of related work. hopefully the reviews and the discussion process will help make the paper much stronger for a future submission. pros  very impressive results on a subset of atari games  a simple and elegant solution to achieving approximate samples from the qnetwork  the paper is well written and the methodology is clearly explained cons  questions remain about the rigor of the empirical analysis comparison to baselines  requires more thoughtful comparison in the manuscript to related literature  the theoretical justification for the proposed methods is not strong", "accepted": null}
{"paper_id": "iclr_2018_By-IifZRW", "review_text": "the authors propose the use of gaussian processes as the prior over activation functions in deep neural networks. this is a purely mathematical paper in which the authors derive an efficient and scalable approach to their problem. the idea of having flexible distributions over activation functions is interesting and possibly impactful. one reviewer recommended acceptance with low confidence. the other two found the idea interesting and compelling but confidently recommended rejection. these reviewers are concerned that the paper is unnecessarily complex in terms of the mathematical exposition and that it repeats existing derivations without citation. it is very important that the authors acknowledge existing literature for mathematical derivations. furthermore, the reviewers question the correctness of some of the statements e.g. is the variational bound preserved?. these reviewers agreed that the paper is incomplete without any empirical validation. pros  a compelling and promising idea  the approach seems to be scalable and highly plausible cons  no experiments  significant issues with citing of related work  significant questions about the novelty of the mathematical work", "accepted": 0}
{"paper_id": "iclr_2018_SyF7Erp6W", "review_text": "this paper does not seem completely appropriate for iclr.", "accepted": 0}
{"paper_id": "iclr_2018_Sy-tszZRZ", "review_text": "dear authors, the reviewers appreciated your work and recognized the importance of theoretical work to understand the behaviour of deep nets. that said, the improvement over existing work especially montufar, 2017 is minor. this, combined with the limited attraction of such work, means that the paper will not be accepted. i acknowledge the major modifications done but it is up to the reviewers to decide whether or not they agree to rereview a significantly updated version.", "accepted": 0}
{"paper_id": "iclr_2018_r1ISxGZRb", "review_text": "the reviewers were uniformly unimpressed with the contributions of this paper. the method is somewhat derivative and the paper is quite long and lacks clarity. moreover, the tactic of storing autoencoder variables rather than full samples is clearly an improvement, but it still does not allow the method to scale to a truly lifelong learning setting.", "accepted": null}
{"paper_id": "iclr_2018_BJgVaG-Ab", "review_text": "the authors make an argument for constructing an mdp from the formal structures of temporal logic and associated finite state automata and then applying rl to learn a policy for the mdp. this does not provide a solution for lowlevel skill composition, because there are discontinuities between states, but does provide a means for high level skill composition. the reviewers agreed that the paper suffered from sloppy writing and unclear methods. they had concerns about correctness, and were not impressed by the novelty combining tl and rl has been done previously. these concerns tip this paper to rejection.", "accepted": 0}
{"paper_id": "iclr_2018_ByW5yxgA-", "review_text": "the paper addresses and interesting problem, but the reviewers found that the paper is not as strong as it could be improving the range of evaluated data significantly improve the convincingness of the experiments, and clearly adressing any alternatives, their limitations and as baselines.", "accepted": null}
{"paper_id": "iclr_2018_Skx5txzb0W", "review_text": "the subject of model evaluation will always be a contentious one, and the reviewers were not yet fullyconvinced by the discussion. the points you bring up at the end of your rresponse already point to directions for improvement as well as a greater degree of precision and control.", "accepted": 0}
{"paper_id": "iclr_2019_H1xSNiRcF7", "review_text": "the manuscript presents a promising new algorithm for learning geometricallyinspired embeddings for learning hierarchies, partial orders, and lattice structures. the manuscript builds on the build on the box lattice model, extending prior work by relaxing the box embeddings via gaussian convolutions. this is shown to be particularly effective for nonoverlapping boxes, where the previous method fail. the primary weakness identified by reviewers was the writing, which was thought to be lacking some context, and may be difficult to approach for the nondomain expert. this can be improved by including an additional general introduction. otherwise, the manuscript was well written. overall, reviewers and ac agree that the general problem statement is timely and interesting, and well executed. in our opinion, this paper is a clear accept.", "accepted": 1}
{"paper_id": "iclr_2019_HygBZnRctX", "review_text": "this paper proposes an approach for learning to transfer knowledge across multiple tasks. it develops a principled approach for an important problem in metalearning short horizon bias. nearly all of the reviewers concerns were addressed throughout the discussion phase. the main weakness is that the experimental settings are somewhat nonstandard i.e. the omniglot protocol in the paper is not at all standard. i would encourage the authors to mention the discrepancies from more standard protocols in the paper, to inform the reader. the results are strong nonetheless, evaluating in settings where typical metalearning algorithms would struggle. the reviewers and i all agree that the paper should be accepted, and i think it should be considered for an oral presentation.", "accepted": 1}
{"paper_id": "iclr_2019_HylzTiC5Km", "review_text": "all reviewers recommend acceptance, with two reviewers in agreement that the results represent a significant advance for autoregressive generative models. the ac concurs.", "accepted": 1}
{"paper_id": "iclr_2019_B1GAUs0cKQ", "review_text": "the authors describe a very counterintuitive type of layer one with mean zero gaussian weights. they show that various bayesian deep learning algorithms tend to converge to layers of this variety. this work represents a step forward in our understanding of bayesian deep learning methods and potentially may shine light on how to improve those methods.", "accepted": null}
{"paper_id": "iclr_2019_B1e0X3C9tQ", "review_text": "the reviewers acknowledge the value of the careful analysis of gaussian encoderdecoder vae presented in the paper. the proposed algorithm shows impressive fid scores that are comparable to those obtained by state of the art gans. the paper will be a valuable addition to the iclr program.", "accepted": 1}
{"paper_id": "iclr_2019_B1exrnCcF7", "review_text": "all reviewers agree that the proposed method interesting and well presented. the authors rebuttal addressed all outstanding raised issues. two reviewers recommend clear accept and the third recommends borderline accept. i agree with this recommendation and believe that the paper will be of interest to the audience attending iclr. i recommend accepting this work for a poster presentation at iclr.", "accepted": null}
{"paper_id": "iclr_2019_B1g30j0qF7", "review_text": "there has been a recent focus on proving the convergence of bayesian fully connected networks to gps. this work takes these ideas one step further, by proving the equivalence in the convolutional case. all reviewers and the ac are in agreement that this is interesting and impactful work. the nature of the topic is such that experimental evaluations and theoretical proofs are difficult to carry out in a convincing manner, however the authors have done a good job at it, especially after carefully taking into account the reviewers comments.", "accepted": null}
{"paper_id": "iclr_2019_B1lKS2AqtX", "review_text": "strengths strong results on future frame video prediction using a 3d convolutional network. use of future video prediction to jointly learn auxiliary tasks shown to to increase performance. good ablation study. weaknesses comparisons with older action recognition methods. some concerns about novelty, the main contribution is the e3dlstm architecture, which r1 characterized as an lstm with an extra gate and attention mechanism. contention authors point to novelty in 3d convolutions inside the rnn. consensus all reviewers give a final score of 7 well done experiments helped address concerns around novelty. easy to recommend acceptance given the agreement.", "accepted": null}
{"paper_id": "iclr_2019_BJeWUs05KQ", "review_text": "this paper proposes an approach for imitation learning from unsegmented demonstrations. the paper addresses an important problem and is wellmotivated. many of the concerns about the experiments have been addressed with followup comments. we strongly encourage the authors to integrate the new results and additional literature to the final version. with these changes, the reviewers agree that the paper exceeds the bar for acceptance. thus, i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2019_BJg_roAcK7", "review_text": "this manuscript proposes a new algorithm for instancewise feature selection. to this end, the selection is achieved by combining three neural networks trained via an actorcritic methodology. the manuscript highlight that beyond prior work, this strategy enables the selection of a different number of features for each example. encouraging results are provided on simulated data in comparison to related work, and on real data. the reviewers and ac note issues with the evaluation of the proposed method. in particular, the evaluation of computer vision and natural language processing datasets may have further highlighted the performance of the proposed method. further, while technically innovative, the approach is closely related to prior work l2x  limiting the novelty. the paper presents a promising new algorithm for training generative adversarial networks. the mathematical foundation for the method is novel and thoroughly motivated, the theoretical results are nontrivial and correct, and the experimental evaluation shows a substantial improvement over the state of the art.", "accepted": null}
{"paper_id": "iclr_2019_BJl6AjC5F7", "review_text": "this paper investigates learning to represent edit operations for two domains text and source code. the primary contributions of the paper are in the specific task formulation and the new dataset for source code edits. the technical novelty is relatively weak. pros the paper introduces a new dataset for source code edits. cons reviewers raised various concerns about human evaluation and many other experimental details, most of which the rebuttal have successfully addressed. as a result, r3 updated their score from 4 to 6. verdict possible weak accept. none of the remaining issues after the rebuttal is a serious deal breaker e.g., task simplification by assuming the knowledge of when and where the edit must be applied, simplifying the realworld application of the automatic edits. however, the overall impact and novelty of the paper is relatively weak.", "accepted": null}
{"paper_id": "iclr_2019_BJlgNh0qKQ", "review_text": "this paper proposes a method for unsupervised learning that uses a latent variable generative model for semisupervised dependency parsing. the key learning method consists of making perturbations to the logits going into a parsing algorithm, to make it possible to sample within the variational autoencoder framework. significant gains are found through semisupervised learning. the largest reviewer concern was that the baselines were potentially not strong enough, as significantly better numbers have been reported in previous work, which may have a result of overstating the perceived utility. overall though it seems that the reviewers appreciated the novel solution to an important problem, and in general would like to see the paper accepted.", "accepted": 1}
{"paper_id": "iclr_2019_BkfbpsAcF7", "review_text": "this paper studies the roots of the existence of adversarial perspective from a new perspective. this perspective is quite interesting and thoughtprovoking. however, some of the contributions rely on fairly restrictive assumptions andor are not properly evaluated. still, overall, this paper should be a valuable addition to the program.", "accepted": null}
{"paper_id": "iclr_2019_BklCusRct7", "review_text": "this is a wellwritten paper that shows how to use optimal transport to perform smooth interpolation, between two random vectors sampled from the prior distribution of the latent space of a deep generative model. by encouraging the marginal of the interpolated vector to match the prior distribution, these interpolated distributionpreserving random vectors in the latent space are shown to result in better image interpolation quality for gans. the problem is of interest to the community and the resulted solutions are simple to implement. as pointed out by reviewer 1, the paper could be made clearly more convincing by showing that these distribution preservation operations also help perform interpolation in the latent space of vaes, and the ac strongly encourages the authors to add these results if possible. the ac appreciates that the authors have added experiments to satisfactorily address hisher concern suppose z_1,z_2 are independent, and drawn from nmu,sigma, then t z_1  1tz_2  nmu, t21t2sigma. if one lets y  z_1, z_2  nt z_1  1tz_2, 1t21t2sigma as the latent space interpolation, then marginally we have y  nmu, sigma. this is an extremely simple and fast procedure to make sure that the latent space interpolation y is highly related to the linear interpolation t z_1  1tz_2 but also satisfies y  nmu, sigma. the ac strongly encourages the authors to add these new results into their revision, and highlight smooth interpolation as an important characteristic in addition to distribution preserving. a potential suggestion is changing distribution preserving operations in the title to distribution preserving smooth operations.", "accepted": null}
{"paper_id": "iclr_2019_BylBr3C9K7", "review_text": "all of the reviewers agree that this is a wellwritten paper with the novel perspective of minimizing energy consumption in neural networks, as opposed to maximizing sparsity, which does not always correlate with energy cost. there are a number of promised clarifications and additional results that have emerged from the discussion that should be put into the final draft. namely, describing the overhead of converting from sparse to dense representations, adding the imagenet sparsity results, and adding the time taken to run the projection step.", "accepted": null}
{"paper_id": "iclr_2019_BylIciRcYQ", "review_text": "the proposed notion of star convexity is interesting and the empirical work done to provide evidence that it is indeed present in realworld neural network training is appreciated. the reviewers raise a number of concerns. the authors were able to convince some of the reviewers with new experiments under mse loss and experiments showing how robust the method was to the reference point. the most serious concerns relate to novelty and the assumptions that individual functions share a global minima with respect to which the path of iterates generated by sgd satisfies the star convexity property. im inclined to accept the authors rebuttal, although it would have been nicer had the reviewer reengaged. overall, the paper is on the borderline.", "accepted": 1}
{"paper_id": "iclr_2019_ByloJ20qtm", "review_text": "this paper provides an approach to jointly localize and repair varmisuse bugs, where a wrong variable from the context has been used. the proposed work provides an endtoend training pipeline for jointly localizing and repairing, as opposed to independent predictions in existing work. the reviewers felt that the manuscript was very wellwritten and clear, with fairly strong results on a number of datasets. the reviewers and ac note the following potential weaknesses 1 reviewer 4 brings up related approaches from automated program repair apr, that are much more general than the varmisuse bugs, and the paper lacks citation and comparison to them, 2 the baselines that were compared against are fairly weak, and some recent approaches like deepbugs and sk_p are ignored, 3 the approach is trained and evaluated only on synthetic bugs, which look very different from the realistic ones, and 4 the contributions were found to be restricted in novelty, just uses a pointerbased lstm for locating and fixing bugs. the authors provided detailed comments and a revision to address and clarify these concerns. they added an evaluation on realistic bugs, along with differences from deepbugs and sk_p, and differences between neural and automated program repair. they also added more detail comparisons, including separating the localization vs repair aspects by comparing against enumeration. during the discussion, the reviewers disagree on the weakness of the baseline, as reviewers 1 and 4 feel it is a reasonable baseline as it builds upon the allamanis paper. they found, to different degrees, that the results on realistic bugs are much more convincing than the synthetic bug evaluation. finally, all reviewers agree that the novelty of this work is limited. although the reviewers disagree on the strength of the baselines a recent paper and the evaluation benchmarks, they agreed that the results are quite strong. the paper, however, addressed many of the concerns in the responserevision, and thus, the reviewers agree that it meets the bar for acceptance.", "accepted": null}
{"paper_id": "iclr_2019_Byx83s09Km", "review_text": "the paper introduces a method for using information directed sampling, by taking advantage of recent advances in computing parametric uncertainty and variance estimates for returns. these estimates are used to estimate the information gain, based on a formula from kirschner  krause, 2018 for the bandit setting. this paper takes these ideas and puts them together in a reasonably easytouse and understandable way for the reinforcement learning setting, which is both nontrivial and useful. the work then demonstrates some successes in atari. though it is of course laudable that the paper runs on 57 atari games, it would make the paper even stronger if a simpler setting some toy domain was investigated to more systematically understand this approach and some choices in the approach.", "accepted": null}
{"paper_id": "iclr_2019_H1MW72AcK7", "review_text": "the paper makes progress on a problem that is still largely unexplored, presents promising results, and builds bridges with prior work on optimal control. it designs input convex recurrent neural networks to capture temporal behavior of dynamical systems; this then allows optimal controllers to be computed by solving a convex model predictive control problem. there were initial critiques regarding some of the claims. these have now been clarified. also, there is in the end a compromise between the necessary approximations of the inputconvex model and the true dynamics, and being able to compute an optimal result. overall, all reviewers and the ac are in agreement to see this paper accepted. there was extensive and productive interaction between the reviewers and authors. it makes contributions that will be of interest to many, and builds interesting bridges with known control methods.", "accepted": 1}
{"paper_id": "iclr_2019_H1edIiA9KQ", "review_text": "the submission proposes a model to generate images where one can control the finegrained locations of objects. this is achieved by adding an object pathway to the gan architecture. experiments on a number of baselines are performed, including a number of reviewersuggested metrics that were added postrebuttal. the method needs bounding boxes of the objects to be placed and labels. the proposed method is simple and likely novel and i like the evaluating done with yolov3 to get a sense of the object detection performance on the generated images. i find the results qual  quant and writeup compelling and i think that the method will be of practical relevance, especially in creative applications. because of this, i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2019_H1g2NhC5KQ", "review_text": "the paper shows how techniques introduced in the context of unsupervised machine translation can be used to build a style transfer methods. pros  the approach is simple and questions assumptions made by previous style transfer methods specifically, they show that we do not need to specifically enforce disentanglement.  the evaluation is thorough and shows benefits of the proposed method  multiattribute style transfer is introduced and benchmarks are created  given the success of unsupervised nmt, it makes a lot of sense to see if it can be applied to the style transfer problem cons  technical novelty is limited  some findings may be somewhat trivial e.g., we already know that offline classifiers are stronger than the adversarials, e.g., see elazar and goldberg, emnlp 2018.", "accepted": null}
{"paper_id": "iclr_2019_H1gsz30cKX", "review_text": "the paper explores the effect of normalization and initialization in residual networks, motivated by the need to avoid exploding and vanishing activations and gradients. based on some theoretical analysis of stepsizes in sgd, the authors propose a sensible but effective way of initializing a network that greatly increases training stability. in a nutshell, the method comes down to initializing the residual layers such that a single step of sgd results in a change in activations that is invariant to the depth of the network. the experiments in the paper provide supporting evidence for the benefits; the authors were able to train networks of up to 10,000 layers deep. the experiments have sufficient depth to support the claims. overall, the method seems to be a simple but effective technique for learning very deep residual networks. while some aspects of the network have been used in earlier work, such as initializing residual branches to output zeros, these earlier methods lacked the rescaling aspect, which seems crucial to the performance of this network. the reviewers agree that the papers provides interesting ideas and significant theoretical and empirical contributions. the main concerns by the reviewers were addressed by the author responses. the ac finds that the remaining concerns raised by the reviewers are minor and insufficient for rejection of the paper.", "accepted": null}
{"paper_id": "iclr_2019_H1lqZhRcFm", "review_text": "the paper proposes a new unsupervised learning scheme via utilizing local maxima as an indicator function. the reviewers and ac note the novelty of this paper and good empirical justifications. hence, ac decided to recommend acceptance. however, ac thinks the readability of the paper can be improved.", "accepted": 1}
{"paper_id": "iclr_2019_H1ziPjC5Fm", "review_text": "this was a difficult decision to converge to. r2 strongly champions this work, r1 is strongly critical, and r3 did not participate in the discussions or take a stand. on the one hand, the ac can sympathize with r1s concerns  insights developed on synthetic datasets may fail to generalize and fundamentally, the burden is not on a reviewer to be able to provide to authors a realistic dataset for the paper to experiment on. having said that, a carefully constructed synthetic dataset is often exactly what the community needs as the first step to studying a difficult problem. moreover, it is better for a proceeding to include works that generate vigorous discussions than the routine bland incremental works that typically dominate. welcome to iclr19.", "accepted": 1}
{"paper_id": "iclr_2019_HJMHpjC9Ym", "review_text": "this paper propose a novel cnn architecture for learning multiscale feature representations with good tradeoffs between speed and accuracy. reviewers generally arrived at a consensus on accept.", "accepted": null}
{"paper_id": "iclr_2019_HJf9ZhC9FX", "review_text": "the authors give a characterization of stochastic mirror descent smd as a conservation law 17 in terms of the bregman divergence of the loss. the identity allows the authors to show that smd converges to the optimal solution of a particular minimax filtering problem. in the special overparametrized linear case, when smd is simply sgd, the result recovers a recent theorem due to gunasekar et al. 2018. the consequences for the overparametrized nonlinear case are more speculative. the main criticisms are around impact, however, im inclined to think that any new insight on this problem, especially one that imports results from other areas like control, are useful to incorporate into the literature. i will comment that the discussion of previous work is wholly inadequate. the authors essentially do not engage with previous work, and mostly make throwaway citations. this is a real pity. i would be nice to see better scholarship.", "accepted": null}
{"paper_id": "iclr_2019_HJlLKjR9FQ", "review_text": "the ideas presented in the paper are quite intriguing and draw on a variety of different connections  the presentation has a lot of room for improvement. in particular, the statement of theorem 1, in its current form, requires rephrasing and making it more rigorous. still, the general consensus is that, once these presentation shortcomings are address, this will be an interesting paper.", "accepted": null}
{"paper_id": "iclr_2019_Hk4dFjR5K7", "review_text": "the submission proposes a method to construct adversarial attacks based on deforming an input image rather than adding small peturbations. although deformations can also be characterized by the difference of the original and deformed image, it is qualitatively and quantitatively different as a small deformation can result in a large difference. on the positive side, this paper proposes an interesting form of adversarial attack, whose success can give additional insights on the forms of existing adversarial attacks. the experiments on mnist and imagenet are reasonably comprehensive and allow interesting interpretation of how the image deforms to allow the attack. the paper is also praised for its clarity, and cleaner formulation compared to xiao et al. see below. additional experiments during rebuttal phase partially answered reviewer concerns, and provided more information e.g. about the effect of the smoothness of the deformation. there were some concerns that the paper primarly presents one idea, and perhaps missed an opportunity for deeper analysis r1. r2 would have appreciated more analysis on how to defend against the attack. a controversial point is the relation  novelty with respect to xiao et al., iclr 2018. as e.g. pointed out by r1 the paper originates from a document provably written in late 2017, which is before the deposit on arxiv of another article by different authors, early 2018 which was later accepted to iclr 2018 xiao and al.. this remark is important in that it changes my rating of the paper being more indulgent with papers proposing new ideas, as otherwise the novelty is rather low compared to xiao and al.. on the balance, all three reviewers recommended acceptance of the paper. regarding novelty over xiao et al., even ignoring the arguable precedence of the current submission, the formulation is cleaner and will likely advance the analysis of adversarial attacks.", "accepted": null}
{"paper_id": "iclr_2019_HklY120cYm", "review_text": "the authors discuss an improved distillation scheme for parallel wavenet using a gaussian inverse autoregressive flow, which can be computed in closedform, thus simplifying training. the work received favorable comments from the reviewers, along with a number of suggestions for improvement which have improved the draft considerably. the ac agrees with the reviewers that the work is a valuable contribution, particularly in the context of endtoend neural texttospeech systems.", "accepted": 1}
{"paper_id": "iclr_2019_HkxKH2AcFm", "review_text": "the paper argues for a gan evaluation metric that needs sufficiently large number of generated samples to evaluate. authors propose a metric based on existing set of divergences computed with neural net representations. r2 and r3 appreciate the motivation behind the proposed method and the discussion in the paper to that end. the proposed nnd based metric has some limitations as pointed out by r2r3 and also acknowledged by the authors  being biased towards gans learned with the same nnd metric; challenge in choosing the capacity of the metric neural network; being computationally expensive, etc. however, these points are discussed well in the paper, and r2 and r3 are in favor of accepting the paper with r3 bumping their score up after the author response. r1s main concern is the lack of rigorous theoretical analysis of the proposed metric, which the ac agrees with, but is willing to overlook, given that it is nontrivial and most existing evaluation metrics in the literature also lack this. overall, this is a borderline paper but falling on the accept side according to the ac.", "accepted": 0}
{"paper_id": "iclr_2019_HyeFAsRctQ", "review_text": "this paper proposes verification algorithms for a class of convexrelaxable specifications to evaluate the robustness of neural networks under adversarial examples. the reviewers were unanimous in their vote to accept the paper. note the remaining score of 5 belongs to a reviewer who agreed to acceptance in the discussion.", "accepted": null}
{"paper_id": "iclr_2019_Hyx6Bi0qYm", "review_text": "bmis need perpatient and persession calibration, and this paper seeks to amend that. using vaes and rnns, it relates seeg to semg, in principle a tenyear old approach, but do so using a novel adversarial approach that seems to work. the reviewers agree the approach is nice, the statements in the paper are too strong, but publication is recommended. clinical evaluation is an important next step.", "accepted": 1}
{"paper_id": "iclr_2019_S1EERs09YQ", "review_text": "important problem making nn more transparent; reasonable approach for identifying which linguistic concepts different neurons are sensitive to; rigorous experiments. paper was reviewed by three experts. initially there were some concerns but after the author response and reviewer discussion, all three unanimously recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2019_S1xLN3C9YX", "review_text": "the authors propose a method to learn a neural network architecture which achieves the same accuracy as a reference network, with fewer parameters through bayesian optimization. the search is carried out on embeddings of the neural network architecture using a train bidirectional lstm. the reviewers generally found the work to be clearly written, and well motivated, with thorough experimentation, particularly in the revised version. given the generally positive reviews from the authors, the ac recommends that the paper be accepted.", "accepted": null}
{"paper_id": "iclr_2019_SJgw_sRqFQ", "review_text": "this work analyses the use of parameter averaging in gans. it can mainly be seen as an empirical study while also a convergence analysis of ema for a concrete example provides some minor theoretical result but experimental results are very convincing and could promote using parameter averaging in the gan community. therefore, even if the technical novelty is limited, the insights brought by the paper are intesting.", "accepted": null}
{"paper_id": "iclr_2019_SJxu5iR9KQ", "review_text": "the authors present a learnt scheduling mechanism for managing communications in bandwidthconstrained, contentious multiagent rl domains. this is wellpositioned in the rapidly advancing field of marl and the contribution of the paper is both novel, interesting, and effective. the agents learn how to schedule themselves, how to encode messages, and how to select actions. the approach is evaluated against several other methods and achieves a good performance increase. the reviewers had concerns regarding the difficulty of evaluating the overall performance and also about how it would fare in more realworld scenarios, but all agree that this paper should be accepted.", "accepted": 1}
{"paper_id": "iclr_2019_Ske5r3AqK7", "review_text": "word vectors are well studied but this paper adds yet another interesting dimension to the field.", "accepted": null}
{"paper_id": "iclr_2019_SkeRTsAcYm", "review_text": "the authors propose an algorithm for enhancing noisy speech by also accounting for the phase information. this is done by adapting unets to handle features defined in the complex space, and by adapting the loss function to improve an appropriate evaluation metric. strengths  modifies existing techniques well to better suit the domain for which the algorithm is being proposed. modifications like extending unet to complex unet to deal with phase, redefining the mask and loss are all interesting improvements.  extensive results and analysis. weaknesses  the work is centered around speech enhancement, and hence has limited focus. even though the paper is limited to speech enhancement, the reviewers agreed that the contributions made by the paper are significant and can help improve related applications like asr. the paper is well written with interesting results and analysis. therefore, it is recommended that the paper be accepted.", "accepted": null}
{"paper_id": "iclr_2019_SkloDjAqYm", "review_text": "this paper is about representation learning for calcium imaging and thus a bit different in scope that most iclr submissions. but the paper is wellexecuted with good choices for the various parts of the model making it relevant for other similar domains.", "accepted": 1}
{"paper_id": "iclr_2019_SyNPk2R9K7", "review_text": "this paper presents a dataset and method for training a model to infer, from a visual scene, the program that would generatedescribe it. in doing so, it produces abstract disentangled representations of the scene which could be used by agents, models, and other ml methods to reason about the scene. this is yet another paper where the reviewers disappointingly did not interact. the first round of reviews were mediocretoacceptable. the authors, i think, did a good job of responding to the concerns raised by the reviewers and edited their paper accordingly. unfortunately, not one of the reviewers took the time to consider author responses. in light of my reading of the responses and the revisions in the paper, i am leaning towards treating this as a paper where the review process has failed the authors, and recommending acceptance. the paper presents a novel method and dataset, and the experiments are reasonably convincing. the paper has flaws and the authors are advised to carefully take into account the concerns flagged by reviewersmany of which they have responded toin producing their final manuscript.", "accepted": 0}
{"paper_id": "iclr_2019_r1gNni0qtm", "review_text": "ar1 finds that extension of the previously presented iclr18 paper are interesting and sufficient due to the provided analysis of universality and depth efficiency. ar2 is concerned with the lack of any concrete toy example between the proposed architecture and rnns. kindly make an effort to add such a basic stepbystep illustration for a simple chosen architecture e.g. in the supplementary material. ar3 is the most critical the analysis ttrnn based on the product nonlinearity done before, particular case of rectifier nonlinearity is used, etc. despite the authors cannot guarantee the existence of corresponding weight tensor w in less trivial cases, the overall analysis is very interesting and it is the starting point for further modeling. thus, ac advocates acceptance of this paper. the review scores do not indicate this can be an oral paper, e.g. it currently is unlikely to be in top few percent of accepted papers. nonetheless, this is a valuable and solid work. moreover, for the cameraready paper, kindly refresh your list of citations as a mere 1 page of citations feels rather too conservative. this makes the background of the paper and related work obscure to average reader unfamiliar with this topic, tensors, tensor outer products etc. there are numerous works on tensor decompositions that can be acknowledged  multilinear analysis of image ensembles tensorfaces by vasilescou et al.  multilinear projection for face recognition via canonical decomposition by vasilescou et al.  tensor decompositions for learning latent variable models by anandkumar et al.  fast and guaranteed tensor decomposition via sketching by anandkumar et al. one good example of the use of the outer product sums over rank one outer products of higherorder is paper from 2013. they perform higherorder pooling on encoded feature vectors although this seems to be the shallow setting similar to eq. 2 and 3 this submission  higherorder occurrence pooling on midand lowlevel features visual concept detection by koniusz et al. e.g. equations equations 49 and 50 or 1, 16 and 17 realize eq. 3 and 13 in this submission  higherorder occurrence pooling for bagsofwords visual concept detection similar followup work other related papers include  longterm forecasting using tensortrain rnns by anandkumar et al.  tensor regression networks with various lowrank tensor approximations by cao et al. of course, the authors are encouraged to cite even more related works.", "accepted": null}
{"paper_id": "iclr_2019_rJedV3R5tm", "review_text": "pros  wellwritten and clear  good evaluation with convincing ablations  moderately novel cons  reviewers 1 and 3 feel the paper is somewhat incremental over previous work, combining previously proposed ideas. reviewer 2 originally had concerns about the testing methodology but feels that the paper has improved in revision reviewer 3 suggests an additional comparison to related work which was addressed in revision i appreciate the authors revisions and engagement during the discussion period. overall the paper is good and im recommending acceptance.", "accepted": 1}
{"paper_id": "iclr_2019_rJg6ssC5Y7", "review_text": "the field of deep learning optimization suffers from a lack of standard benchmarks, and every paper reports results on a different set of models and architectures, likely with different protocols for tuning the baselines. this paper takes the useful step of providing a single benchmark suite for neural net optimizers. the set of benchmarks seems welldesigned, and covers the range of baselines with a variety of representative architectures. it seems like a useful contribution that will improve the rigor of neural net optimizer evaluation. one reviewer had a long backandforth with the authors about whether to provide a standard protocol for hyperparameter tuning. i side with the authors on this one it seems like a bad idea to force a onesizefitsall protocol here. as a lesser point, im a little concerned about the strength of some of the baselines. as reviewers point out, some of the baseline results are weaker than typical implementations of those methods. one explanation might be the lack of learning rate schedules, something thats critical to get reasonable performance on some of these tasks. i get that using a fixed learning rate simplifies the grid search protocol, but im worried it will hurt the baselines enough that effective learning rate schedules and normalization issues come to dominate the comparisons. still, the benchmark suite seems well constructed on the whole, and will probably be useful for evaluation of neural net optimizers. i recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2019_rJgTTjA9tX", "review_text": "this paper makes a substantial contribution to the understanding of the approximation ability of deep networks in comparison to classical approximation classes, such as polynomials. strong results are given that show fundamental advantages for neural network function approximators in the presence of a natural form of latent structure. the analysis techniques required to achieve these results are novel and worth reporting to the community. the reviewers are uniformly supportive.", "accepted": null}
{"paper_id": "iclr_2019_rJlnB3C5Ym", "review_text": "the paper presents a lot of empirical evidence that fine tuning pruned networks is inferior to training them from scratch. these results seem unsurprising in retrospect, but hindsight is 2020. the reviewers raised a wide range of issues, some of which were addressed and some which were not. i recommend to the authors that they make sure that any claims they draw from their experiments are sufficiently prescribed. e.g., the lottery ticket experiments done by anonymous in response to this paper show that the random initialization does poorer than restarting with the initial weights other than in resnet, though this seems possibly due to the learning rate. there is something different in their setting, and so your claims should be properly circumscribed. i dont think the standard versus nonstandard terminology is appropriate until the actual boundary between these two behaviors is identified. i would recommend the authors make guarded claims here.", "accepted": null}
{"paper_id": "iclr_2019_rke4HiAcY7", "review_text": "this paper considers the information bottleneck lagrangian as a tool for studying deep networks in the common case of supervised learning predicting label y from features x with a deterministic model, and identifies a number of troublesome issues. 1 the information bottleneck curve cannot be recovered by optimizing the lagrangian for different values of \u03b2 because in the deterministic case, the ib curve is piecewise linear, not strictly concave. 2 uninteresting representations can lie on the ib curve, so information bottleneck optimality does not imply that a representation is useful. 3 in a multilayer model with a low probability of error, the only tradeoff that successive layers can make between compression and prediction is that deeper layers may compress more. experiments on mnist illustrate these issues, and supplementary material shows that these issues also apply to the deterministic information bottleneck and to stochastic models that are nearly deterministic. there was a substantial degree of disagreement between the reviewers of this paper. one reviewer r3 suggested that all the conclusions of the paper are the consequence of px,y being degenerate. the authors responded to this criticism in their response and revision quite effectively, in the opinion of the ac. because r3 failed to participate in the discussion, this review has been discounted in the final decision. the other two reviewers were considerably more positive about the paper, with one r1 having basically no criticisms and the other r2 expression some doubts about the novelty of the observations being made in the paper and their importance for practical machine learning scenarios. following the revision and discussion, r2 expressed general satisfaction with the paper, so the ac is recommending acceptance. the ac thinks that the final paper would be clearer if the authors were to carefully distinguish between groundtruth labels used in training and the labels estimated by the model for a given input. at the moment, the symbol y appears to be overloaded, standing for both. perhaps the authors should place a hat over y when it is standing for estimated labels?", "accepted": 1}
{"paper_id": "iclr_2019_rkemqsC9Fm", "review_text": "strengths this paper gives a detailed treatment of the connections between rate distortion theory and variational lower bounds, culminating in a practical diagnostic tool. the paper is wellwritten. weaknesses many of the theoretical results existed in older work. points of contention most of the discussion was about the novelty of the lower bound. consensus r3 and r2 both appear to recommend acceptance r2 in a comment, and have both clearly given the paper detailed thought.", "accepted": null}
{"paper_id": "iclr_2019_rkxQ-nA9FX", "review_text": "this paper conducted theoretical analysis of the effect of batch normalisation to auto ratetuning. it provides an explanation for the empirical success of bn. the assumptions for the analysis is also closer to the common practice of batch normalization compared to a related work of wu et al. 2018. one of the concerns raised by the reviewer is that the analysis does not immediately apply to practical uses of bn, but the authors already discussed how to fill the gap with a slight change of the activation function. another concern is about the lack of empirical evaluation of the theory, and the authors provide additional experiments in the revision. r1 also points out a few weaknesses in the theoretical analysis, which i think would help improve the paper further if the authors could clarify and provide discussion in their revision. overall, it is a good paper that will help improve our theoretical understanding about the power tool of batch normalization.", "accepted": null}
{"paper_id": "iclr_2019_ryeYHi0ctQ", "review_text": "a deep neural network pipeline for multiview stereo is presented. after rebuttal and discussion, all reviewers learn toward accepting the paper. reviewer3 points to good results, but is concerned that the technical aspects are somewhat straightforward, and thus the contribution in this area is limited. the ac concurs with the reviewers.", "accepted": null}
{"paper_id": "iclr_2019_ryetZ20ctX", "review_text": "the reviewers agree the paper brings a novel perspective by controlling the conditioning of the model when performing quantization. the experiments are convincing experiments. we encourage the authors to incorporate additional references suggested in the reviews. we recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2019_ryfMLoCqtQ", "review_text": "the authors provide a new analysis of generalization in deep linear networks, provide new insight through the role of task structure. empirical findings are used to cast light on the general case. this work seems interesting and worthy of publication.", "accepted": 1}
{"paper_id": "iclr_2019_rygqqsA9KX", "review_text": "this paper offers a novel perspective for learning latent multimodal representations. the idea of segmenting the information into multimodal discriminative and modalityspecific generating factors is found to be intriguing by all reviewers and the ac. the technical derivations allow for an efficient implementation of this idea. there have been some concerns regarding the experimental section, but they have all been addressed adequately during the rebuttal period. therefore the ac suggests this paper for acceptance. it is an overall nice and wellthought work.", "accepted": null}
{"paper_id": "iclr_2019_ryl5khRcKm", "review_text": "the reviewers all agreed that the problem application is interesting, and that there is little new methodology, but disagreed as to how that should translate into a score. the highest rating seemed to heavily weight the importance of the method to biological application, whereas the lowest rating heavily weighted the lack of technical novelty. however, because the iclr call for papers clearly calls out applications in biology, and all reviewers agreed on its strength in that regard, and it was wellwritten and executed, i would recommend it for acceptance.", "accepted": 1}
{"paper_id": "iclr_2019_rylV-2C9KQ", "review_text": "in this work, the authors propose a simple, under parameterized network architecture which can fit natural images well, when fed with a fixed random input signal. this allows the model to be used for a number of tasks without requiring that the model be trained on a dataset. further, unlike a recently proposed related method dip; ulyanov et al., 18, the method does not require regularization such as earlystopping as with dip. the reviewers noted the simplicity and experimental validation, and were unanimous in recommending acceptance.", "accepted": 1}
{"paper_id": "iclr_2019_ryxSrhC9KX", "review_text": "the reviewers viewed the work favorably, with only one reviewer providing a score slightly below acceptance. the authors thoroughly addressed the reviewers original concerns, and they adjusted their score upwards afterwards. the lowrating reviewer remains skeptical of the significance of the work, but the other two reviewers make firm cases for the appeal of the work to the iclr audience. in followup discussion after the authors responses were submitted and discussed, the lowrating reviewer did not make a clear case for rejecting the paper, and further, the higherrating reviewers arguments for the impact of the paper were convincing. therefore, i recommend accepting this paper.", "accepted": null}
{"paper_id": "iclr_2019_ryxxCiRqYX", "review_text": "this paper relates deep learning to convex optimization by showing that the forward pass though a dropout layer, linear layer either convolutional or fully connected, and a nonlinear activation function is equivalent to taking one \u03c4nice proximal gradient descent step on a a convex optimization objective. the paper shows 1 how different activation functions correspond to different proximal operators, 2 that replacing bernoulli dropout with additive dropout corresponds to replacing the \u03c4nice proximal gradient descent method with a variancereduced proximal method, and 3 how to compute the lipschitz constant required to set the optimal step size in the proximal step. the practical value of this perspective is illustrated in experiments that replace various layers in convnet architectures with proximal solvers, leading to performance improvements on cifar10 and cifar100. the reviewers felt that most of their concerns were adequately addressed in the discussion and revision, and that the paper should be accepted.", "accepted": 1}
{"paper_id": "iclr_2019_B1MIBs05F7", "review_text": "all reviewers agreed that this paper addresses an important question in deep learning why doesnt svrg help for deep learning? but the paper still has some issues that need to be addressed before publication, thus the ac recommends revise and resubmit.", "accepted": null}
{"paper_id": "iclr_2019_B1MX5j0cFX", "review_text": "the topic of universal adversarial perturbation is quite intriguing and fairly poorly studied and the paper provides a mix of new insights, both theoretical and empirical in nature. however, the significant presentation issues make it hard to properly understand and evaluate them. in particular, the theoretical part feels rushed and not sufficiently rigorous, and it is unclear why focusing on the case of equivariant network is crucial. also, it would be useful if the authors put more effort in explaining how their contributions fit into the context of prior work in the area. overall, this paper has a potential of becoming a solid contribution, once the above shortcomings are addressed.", "accepted": 0}
{"paper_id": "iclr_2019_B1l9qsA5KQ", "review_text": "the manuscript describes a novel technique predicting metal fatigue based on eeg measurements. the work is motivated by an application to driving safety. reviewers and the ac agreed that the main motivation for the proposed work, and perhaps the results, are likely to be of interest to the applied bci community. the reviewers and acs noted weakness in the original submission related to the clarity of the presentation and breadth of empirical evaluation. in particular, only a few baselines were considered. as a result, for the nonexpert, it is also unclear if the proposed methods are compared against the state of the art. there was also a particular concern that this work may not be a good fit for an iclr audience.", "accepted": 0}
{"paper_id": "iclr_2019_B1x-LjAcKX", "review_text": "this paper proposes a new training approach for deep neural interfaces. the idea is to bootstrap from critics of other layers instead of using the final loss as target. the method is evaluated of cifar10 and cifar100 and found to improve performance slightly upon sobolev training while being simpler. the reviewers found the idea interesting but were concerned about the strength of the experimental results. the datasets are similar and the significance of the results is not clear. the revision submitted by the authors was only able to address some of these issues such as the evaluation protocol.", "accepted": null}
{"paper_id": "iclr_2019_BJWfW2C9Y7", "review_text": "dear authors, all reviewers pointed to severe issues with the analysis, making the paper unsuitable for publication to iclr. please take their comments into account should you decide to resubmit this work.", "accepted": 0}
{"paper_id": "iclr_2019_BJeWOi09FQ", "review_text": "the paper addresses the problem semantic segmentation using a sequential patchbased model. i agree with the reviewers that the contributions of the paper are not enough for a machine learning venue 1 there has been prior work on using sequence models for segmentation and 2 the complexity of the proposed approach is not fully justified. the authors did not submit a rebuttal. i encourage the authors to take the feedback into account and improve the paper.", "accepted": 0}
{"paper_id": "iclr_2019_BkgFqiAqFX", "review_text": "the reviewers reached a consense on that the paper is not quite ready for publication at icrl. the main potential drawback include a the exposition of the paper can be improved; b its not entirely clear that some of the assumptions such as the threshold for the first layer, the polynomial approximation of higher layers are meaningful , and it seems that the proof technique exploits heavily some of these assumptions and some of the key intermediate steps wont hold in practice. see reviewer 3s comment for more details. the authors clarify the writing and intuitions in the response, but overall the ac decided that the paper is not quite ready for publications at the moment.", "accepted": 0}
{"paper_id": "iclr_2019_BklAEsR5t7", "review_text": "the paper addresses the problem of large scale finegrained classification by estimating pairwise potentials in a crf model. the reviewers believe that the paper has some weaknesses including 1 the motivation for approximate learning is not clear 2 the approximate objective is not well studied and 3 the experiments are not convincing. the authors did not submit a rebuttal. i encourage the authors to take the feedback into account to improve the paper.", "accepted": 0}
{"paper_id": "iclr_2019_BklMYjC9FQ", "review_text": "the paper proposes an approach to remedying mode collapse problem in gans. this approach relies on using multiple discriminators and assigning a different portion of each minibatch to each discriminator.  preventing mode collapse in gan training is an important problem  the exact motivation for the proposed techniques is not fully fleshed out  the evaluation and baselines used are lacking", "accepted": 0}
{"paper_id": "iclr_2019_Bye5OiR5F7", "review_text": "both r3 and r1 argue for rejection, while r2 argues for a weak accept. given that we have to reject borderline paper, the ac concludes with revise and resubmit.", "accepted": 0}
{"paper_id": "iclr_2019_ByePUo05K7", "review_text": "this paper claims to demonstrate that cnns, unlike human vision, do not have a bias towards reliance on shape for object recognition. both anonreviewer1 and anonreviewer2 point to fundamental flaws in the papers argument, which the rebuttal fails to resolve. anonreviewer1s criticisms are unfortunately conflated with anonreviewer1s reluctance to view neuroscience or biological vision as an appropriate topic for iclr; nonetheless anonreviewer1s technical criticism stands. these observations are anonreviewer2 authors have carefully designed a set of experiments which shows cnns will overfit to nonshape features that they added to training images. however, this outcome is not surprising. anonreviewer1 the experiments dont seem to effectively demonstrate the main claim of the paper that categorization cnns do not have inductive shape bias the best way to demonstrate this would have been to subject a trained imagecategorization cnn to test data with object shapes in a way that the appearance information couldnt be used to predict the object label. the paper doesnt do this. none of the experiments logically imply that with an unaltered training regime, a trained network would not be predictive of the category label if shapes corresponding to that category are presented. the ac agrees with both of these observations. cnn behavior is partially a product of the training regime. to examine the scientific question of whether cnns have similar biases as human vision, the training regimes should be similar. conversely, if human vision evolved in an environment in which shortcut recognition cues were available via indicator pixels, perhaps it would not have a shape bias. this paper appears fundamentally flawed in its approach. the results are not informative about differences between human vision and cnns, nor are they surprising to machine learning practitioners.", "accepted": 0}
{"paper_id": "iclr_2019_H1GLm2R9Km", "review_text": "the reviewers mostly raised two concerns regarding the paper a why this algorithm is more interpretability than bp which is just gradient descent; b the exposition of the paper is somewhat confusing at various places; c the lack of largescale experiment results to show this is practically relevant. in the acs opinion, a principled kernelbased approach can be counted as interpretable, and there the ac would support the paper if a is the only concern. however, c seems to be a serious concern since the paper doesnt seem to have experiments beyond fashion mnist e.g., cifar is pretty easy to train these days and doesnt have experiments with convolutional models. based on c, the ac decided that the paper is not quite ready for acceptance.", "accepted": null}
{"paper_id": "iclr_2019_H1e0-30qKm", "review_text": "the paper received mixed reviews. it proposes a variant of siamese network objective function, which is interesting. however, its unclear if the performance of the unguided method is much better than other baselines e.g., infogan. the guided version of the method seems to require much domainspecific knowledge and design of the feature function, which makes the paper difficult to apply to broader cases.", "accepted": null}
{"paper_id": "iclr_2019_H1eadi0cFQ", "review_text": "the paper proposes a method to escape saddle points by adding and removing units during training. the method does so by preserving the function when the unit is added while increasing the gradient norm to move away from the critical point. the experimental evaluation shows that the proposed method does escape when positioned at a saddle point  as found by the newton method. the reviewers find the theoretical ideas interesting and novel, but they raised concerns about the methods applicability for typical initializations, the experimental setup, as well as the terminology used in the paper. the title and terminology were improved with the revision, but the other issues were not sufficiently addressed.", "accepted": 0}
{"paper_id": "iclr_2019_H1gNHs05FX", "review_text": "there was discussion of this paper, and the accept reviewer was not willing to argue for acceptance of this paper, while the reject reviewers, specifically pointing to the clarity of the work, argued for rejection. there appear to be many good ideas related to wavelets, and hopefully the authors can work on polishing the paper and resubmitting.", "accepted": 0}
{"paper_id": "iclr_2019_H1gZV30qKQ", "review_text": "the paper studies whether the best strategy for transfer learning in rl is to transfer value estimates or policy probabilities. the paper also presents a modelbased valuecentric mvc framework for continuous rl. the reviewers raised concerns regarding 1 the coherence of the story, 2 the novelty and importance of the mvc framework and 3 the significance of the experiments. i encourage the authors to either focus on the algorithmic aspect or the transfer learning aspect and expand on the experimental results to make them more convincing. i appreciate the changes made to improve the paper, but in its current form the paper is still below the acceptance threshold at iclr. ps in my view one can think of value as shifted and scaled log of policy. hence, it is a bit ambiguous to ask whether to transfer value or policy.", "accepted": 0}
{"paper_id": "iclr_2019_H1glKiCqtm", "review_text": "all three reviewers agree that the research questionshould pretrained embeddings be used in code understanding tasksis a reasonable one. however, there were some early issues with the way in which the paper reported results involving both metrics and baselines. after some discussion with the reviewers, it seems that the paper now presents a clear picture of the results, but that these results are not sufficiently strong to warrant acceptance. im wary to turn down a paper over what are basically negative results, but for results like this to be useful to the community, theyd have to come from a very thorough experiment, and theyd have to be accompanied by a frank and detailed discussion. neither of the two more confident authors are convinced that this paper meets that bar.", "accepted": 0}
{"paper_id": "iclr_2019_HJe3TsR5K7", "review_text": "this paper proposes a new image to image translation technique, presenting a theoretical extension of wasserstein gans to the bidirectional mapping case. although the work presents promise, the extent of miscommunication and errors of the original presentation was too great to confidently conclude about the contribution of this work. the authors have already included extensive edits and comments in response to the reviews to improve the clarity of method, experiments and statement of contribution. we encourage the authors to further incorporate the suggestions and seek to clarify points of confusion from other reviewers and submit a revised version to a future conference.", "accepted": 0}
{"paper_id": "iclr_2019_HJeB0sC9Fm", "review_text": "this paper proposes a new measure to detect memorization based on how well the activations of the network are approximated by a lowrank decomposition. they compare decompositions and find that nonnegative matrix factorization provides the best results. they evaluate of several datasets and show that the measure is well correlated with generalization and can be used for early stopping. all reviewers found the work novel, but there were concerns about the usefulness of the method, the experimental setup and the assumptions made. some of these concerns were addressed by the revisions but concerns about usefulness and insights remained. these issues need to be properly addressed before acceptance.", "accepted": 1}
{"paper_id": "iclr_2019_HJeNIjA5Y7", "review_text": "reviewers are in full agreement for rejection.", "accepted": 0}
{"paper_id": "iclr_2019_HJeQbnA5tm", "review_text": "the paper proposes a regularization method that introduces an information bottleneck between parameters and predictions. the reviewers agree that the paper proposes some interesting ideas, but those idea need to be clarified. the paper lacks in clarity. the reviewers also doubt whether the paper is expected to have significant impact in the field.", "accepted": 0}
{"paper_id": "iclr_2019_HJepJh0qKX", "review_text": "there is no author response for this paper. the paper formulates a definition of easy and hard examples for training a neural network nn in terms of their frequency of being classified correctly over several repeats. one repeat corresponds to training the nn from scratch. top 10 and bottom 10 of the samples with the highest and the lowest frequency define easy and hard instances for training. the authors also compare easy and hard examples across different architectures of nns. on the positive side, all the reviewers acknowledge the potential usefulness of quantifying easy and hard examples in training nns, and r1 was ready to improve hisher initial rating if the authors revisited the paper. on the other hand, all the reviewers and ac agreed that the paper requires 1 major improvement in presentation clarity  see detailed comments of r1 on how to improve as well as commentsquestions from r3 and r2; try to avoid confusing terminology such as contradicted patterns. r1 raised important concerns that the proposed notion of easiness is drawn from the experiment in fig. 1 of arpit et al 2017 which is not properly attributed. r3 and r2 agreed that in its current state the experimental results are not conclusive and often non informative. to strengthen the paper the reviewers suggested to include more experiments in terms of different datasets, to propose a better metric for defining easy and hard samples see r3s suggestions. we hope the reviews are useful for improving the paper.", "accepted": 0}
{"paper_id": "iclr_2019_HJgODj05KX", "review_text": "dear authors, the reviewers pointed out a number of concerns about this work. it is thus not ready for publication. should you decide to resubmit it to another venue, please address these concerns.", "accepted": 0}
{"paper_id": "iclr_2019_HJx7l309Fm", "review_text": "the authors propose an approach for a learnt attention mechanism to be used for selecting agents in a multi agent rl setting. the attention mechanism is learnt by a central critic, and it scales linearly with the number of agents rather than quadratically. there is some novelty in the proposed method, and the authors clearly explain and motivate the approach. however the empirical evaluation feels quite limited and does not show conclusively that the method is superior to the others. moreover, the simple empirical results dont give any evidence how the attention mechanism is working or whether it is truly the attention that is affecting the results. the reviewers were split on their recommendation and did not come to a consensus. the ac feels that the paper is not quite strong enough and encourages the authors to broaden the work with additional experiments and analysis.", "accepted": 0}
{"paper_id": "iclr_2019_HkElFj0qYQ", "review_text": "this paper presents a new defense against adversarial examples using random permutations and a fourier transform. the technique is clearly novel, and the paper is clearly written. however, as the reviewers and commenters pointed out, there is a significant degradation in natural accuracy, which does not seem to be easily recoverable. this degradation is due to the random permutation of the images, which effectively disallows the use of convolutions. furthermore, reviewer 1 points out that the baselines are insufficient, as the authors do not explore a learning the transformation, or b using expectation over transformation to attack the model. this concern is further validated by the fact that blackbox attacks are often the bestperforming, which is a sign of gradient masking. the authors try to address this by performing an attack against an ensemble of models, and against a substitute model attack. however, attacking an ensemble is not equivalent to optimizing the expectation, which would require sampling a new permutation at each step. the paper thus requires significantly stronger baselines and attacks.", "accepted": 0}
{"paper_id": "iclr_2019_Hkesr205t7", "review_text": "the paper addresses generalized zero shot learning test data contains examples from both seen as well as unseen classes and proposes to learn a shared representation of images and attributes via multimodal variational autoencoders. the reviewers and ac note the following potential weaknesses 1 low technical contribution, i.e. the proposed multimodal vae model is very similar to vedantam et al 2017 as noted by r2, and to jmvae model by suzuki et al, 2016, as noted by r1. the authors clarified in their response that indeed vae in vedantam et al 2017 is similar, but it has been used for image synthesis and not classificationgzsl. 2 empirical evaluations and setup are not convincing r2 and not clear  r3 has provided a very detailed review and a follow up discussion raising several important concerns such as i absence of a validation set to test generalization, ii the hyperparameters set up; iii not clear advantages of learning a joint model as opposed to unidirectional mappings r1 also supports this claim. the authors partially addressed some of these concerns in their response, however more indepth analysis and major revision is required to assess the benefits and feasibility of the proposed approach.", "accepted": 0}
{"paper_id": "iclr_2019_HkezfhA5Y7", "review_text": "both authors and reviewers agree that the ideas in the paper were not presented clearly enough.", "accepted": 0}
{"paper_id": "iclr_2019_Hkg1YiAcK7", "review_text": "the paper proposes a learning by teaching lbt framework to train an implicit generative model via an explicit one. it is shown experimentally, that the framework can help to avoid mode collapse. the reviewers commonly raised the question why this is the case, which was answered in the rebuttal by pointing to the differences between the kl and the jsdivergence and by showing a toy problem for which the jsdivergence has local minima while the kldivergence has not. however, it still remains unclear why this should be generally and for explicit models with insufficient capacity the case, and if the model will be scalable to larger settings, therefore the paper can not be accepted in the current form.", "accepted": null}
{"paper_id": "iclr_2019_HkgSk2A9Y7", "review_text": "the reviewers liked the paper in general but the empirical evaluation lacks studies on a wider range of different data sets.", "accepted": null}
{"paper_id": "iclr_2019_HkgnpiR9Y7", "review_text": "the paper presents a method to learn inference mapping for gans by reusing the learned discriminators features and fitting a model over these features to reconstruct the original latent code z. r1 pointed out the connection to infogan which the authors have addressed. r2 is concerned about limited novelty of the proposed method, which the ac agrees with, and lack of comparison to a related igan work by zhu et al. 2016. the authors have provided the comparison in the revised version but the proposed method seems to be worse than igan in terms of the metrics used psnr and ssim, though more efficient. the benefits of using the proposed metrics for evaluating gan quality are also not established well, particularly in the context of other recent metrics such as fid and gilbo.", "accepted": 0}
{"paper_id": "iclr_2019_HkgxasA5Ym", "review_text": "the paper studies the problem of uncertainty estimation of neural networks and proposes to use bayesian approach with noice contrastive prior. the reviewers and ac note the potential weaknesses of experimental results 1 lack of sufficient datasets with moderatetohigh dimensional inputs, 2 arguable choices of hyperparameters and 3 lack of direct evaluations, e.g., measuring network calibration is better than active learning. the paper is well written and potentially interesting. however, ac decided that the paper might not be ready to publish in the current form due to the weakness.", "accepted": 0}
{"paper_id": "iclr_2019_Hkx-ii05FQ", "review_text": "the paper investigates a variant of the crossentropy method cme for heuristic combinatorial optimization, based on stochastically improving a search distribution via policy optimization in a surrogate objective. unfortunately, the reviewers unanimously recommended rejection, noting that the significance of the contribution over cme remains far from clear and insufficiently supported by the given evidence. the experimental evaluation was unconvincing to all of the reviewers, particularly since only one artificial problem clique finding was considered in the paper with an additional problem, kmedoid clustering, briefly and incompletely considered in the appendix. several additional concerns were raised about the experimental evaluation, which triggered lengthy author responses but really need to be properly handled in the paper itself  the sensitivity of performance to the optimization algorithm is a concern and requires more detailed understanding so that reasonable choices can be made in practice.  the independence assumption between search components is an extreme simplification that limits the appeal and applicability of the proposed approach. even after author response, it remains unconvincing that an independent search distribution over subcomponents can be effective in challenging combinatorial spaces. concrete evidence on challenging problems would be a more effective evidence than discussion.  the comparisons omitted any tailored algorithms for the specific problems. even if the authors insist on only comparing to more general purpose methods, there is a large space of evolutionary and bayesian optimization strategies that have been neglected from the comparison. a justification is needed for such an omission if indeed it is even justifiable.", "accepted": 0}
{"paper_id": "iclr_2019_HkxAisC9FQ", "review_text": "this paper suggests augmenting adversarial training with a lipschitz regularization of the loss, and suggests that this improves the adversarial robustness of deep neural networks. the idea of using such regularization seems novel. however, several reviewers were seriously concerned with the quality of the writing. in particular, the paper contains claims that not only are not needed but also are incorrect. also, the reviewer 2 in particular was also concerned with the presentation of prior work on lipschitz regularization. such poor quality of the presentation makes it impossible to properly evaluate the actual paper contribution.", "accepted": 0}
{"paper_id": "iclr_2019_HkxWrsC5FQ", "review_text": "this manuscript proposes a generative model for images, then proposes a training procedure for fitting a convolutional neural network based on this model. one novelty if this result is that the generative procedure seems to be more complex than generative assumptions required for previous work. it is clear that the problem addressed  training methods that may improve on sgd, with convergence guarantees  is of significant interest to the community. the reviewers and ac note several issue i the initial version of the manuscript includes several assumptions that are not clearly stated. this seems to have been fixed in the updated manuscript ii reviewers suspect that the accumulation of stated assumptions may result in an easily separable generative model  limiting the generality of the results iii experiemental results are underwhelming, and only comparable to much older published results.", "accepted": 0}
{"paper_id": "iclr_2019_HkzOWnActX", "review_text": "this paper proposes a metalearning algorithm that extends maml, particularly focusing on multimodal task distributions. the paper is generally wellwritten, especially with the latest revisions, and the qualitative experiments show some interesting structure recovered. the primary weakness of the paper is that the experiments are largely on relatively simple benchmarks, such as omniglot and lowdimensional regression problems. metalearning papers with convincing results have shown results on miniimagenet, cifar, celeba, andor other natural image datasets. hence, the paper would be more compelling with more difficult experimental settings. in the papers current form, the reviewers and the ac agree that it does not meet the bar for iclr.", "accepted": 0}
{"paper_id": "iclr_2019_HyGDdsCcFQ", "review_text": "the paper aims to clean data samples with label noise in the training procedure. the reviewers and ac note the following potential weaknesses 1 the assumption of uniform noise, which is not the case in practice, 2 marginal gains under realworld datasets and 3 highly empirical and adhoc approach. ac thinks the proposed method has potential and is interesting, but decided that the authors need more significant works to publish the work.", "accepted": null}
{"paper_id": "iclr_2019_HyMxAi05Km", "review_text": "the reviewers vary in their scores but overall there is agreement that this paper is not ready for acceptance.", "accepted": null}
{"paper_id": "iclr_2019_HyNbtiR9YX", "review_text": "this paper proposes a document classification algorithm based on partitioned word vector averaging. i agree with even the most positive reviewer. more experiments would be good. this is a very developed old area.", "accepted": 0}
{"paper_id": "iclr_2019_HyNmRiCqtm", "review_text": "paper studies an important problem  producing contrastive explanations why did the network predict class b not a?. two major concerns raised by reviewers  the use of one learned blackbox method to explain another and lack of humanstudies to quantify results  make it very difficult to accept this manuscript in its current state. we encourage the authors to incorporate reviewer feedback to make this manuscript stronger for a future submission; this is an important research topic.", "accepted": null}
{"paper_id": "iclr_2019_HygYqs0qKX", "review_text": "the paper presents an interesting idea, but there are significant concerns about the presentation issues and experimental results e.g., comparisons with baselines. overall, it is not ready for publication.", "accepted": 0}
{"paper_id": "iclr_2019_S1MB-3RcF7", "review_text": "the reviewers found that paper is well written, clear and that the authors did a good job placing the work in the relevant literature. the proposed method for using multiple discriminators in a multiobjective setting to train gans seems interesting and compelling. however, all the reviewers found the paper to be on the borderline. the main concern was the significance of the work in the context of existing literature. specifically, the reviewers did not find the experimental results significant enough to be convinced that this work presents a major advance in gan training.", "accepted": null}
{"paper_id": "iclr_2019_S1MeM2RcFm", "review_text": "the reviews agree the paper is not ready for publication at iclr.", "accepted": 0}
{"paper_id": "iclr_2019_S1eVe2AqKX", "review_text": "all reviewers rate the paper as below threshold. while the authors responded to an earlier request for clarification, there is no rebuttal to the actual reviews. thus, there is no basis by which the paper can be accepted.", "accepted": 0}
{"paper_id": "iclr_2019_S1e_ssC5F7", "review_text": "all three reviewers found that the motivation for the proposed method was lacking and recommend rejection. the ac thus recommends the authors to take these comments in consideration when revising their manuscript.", "accepted": 0}
{"paper_id": "iclr_2019_S1ej8o05tm", "review_text": "1. describe the strengths of the paper. as pointed out by the reviewers and based on your expert opinion. the paper tackles an interesting and relevant problem for iclr optical character recognition in document images. 2. describe the weaknesses of the paper. as pointed out by the reviewers and based on your expert opinion. be sure to indicate which weaknesses are seen as salient for the decision i.e., potential critical flaws, as opposed to weaknesses that the authors can likely fix in a revision.  the authors propose to use small networks to localize text in document images, claiming that for document images smaller networks work better than standard sota networks for scene text. as pointed out in the reviews, the authors didnt make any comparisons to sota object detection networks trained either on scene text or on document images so their central claim has not been experimentally verified.  the reviewers were unanimous that the work lacks novelty as object detection pipelines have already been used for ocr so a contribution of considering smaller detection networks is minor.  there were serious issues with formatting and clarity. these three issues all informed the final decision. 3. discuss any major points of contention. as raised by the authors or reviewers in the discussion, and how these might have influenced the decision. if the authors provide a rebuttal to a potential reviewer concern, its a good idea to acknowledge this and note whether it influenced the final decision or not. this makes sure that author responses are addressed adequately. there were no major points of contention and no author feedback. 4. if consensus was reached, say so. otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another. the reviewers reached a consensus that the paper should be rejected.", "accepted": null}
{"paper_id": "iclr_2019_S1lVniC5Y7", "review_text": "in this work, the authors explore using genetic programming to search over network architectures. the reviewers noted that the proposed approach is simple and fast. however, the reviewers expressed concerns about the experimental validation e.g., experiments were conducted on small tasks; issues with comparisons cf. feedback from reviewer2, and the fact that the method were not compared against various baseline methods related to architecture search.", "accepted": 0}
{"paper_id": "iclr_2019_SJMnG2C9YX", "review_text": "the paper studies learning from complementary labels  the setting when example comes with the label information about one of the classes that the example does not belong to. the paper core contribution is an unbiased risk estimator for arbitrary losses and models under this learning scenario, which is an improvement over the previous work, as rightly acknowledged by r1 and r2. the reviewers and ac note the following potential weaknesses 1 r3 raised an important concern that the core technical contribution is a special case of previously published more general framework which is not cited in the paper. the authors agree with r3 on this matter; 2 the proposed unbiased estimator is not practical, e.g. it leads to overfitting when the crossentropy loss is used, it is unbounded from below as pointed out by r1; 3 the two proposed modifications of the unbiased estimator are biased estimators, which defeats the motivation of the work and limits its main technical contributions; 4 r2 rightly pointed out that the assumption that complementary label is selected uniformly at random is unrealistic  see r2s suggestions on how to address this issue. while all the reviewers acknowledged that the proposed biased estimators show advantageous performance on practice, the ac decides that in its current state the paper does not present significant contributions to the prior work, given 13, and needs major revision before submitting for another round of reviews.", "accepted": null}
{"paper_id": "iclr_2019_SJe8DsR9tm", "review_text": "this paper proposes a new method for speeding up convolutional neural networks. it uses the idea of early terminating the computation of convolutional layers. it saves flops, but the reviewers raised a critical concern that it doesnt save wallclock time. the time overhead is about 4 or 5 times of the original model. there is not any reduced execution time but much longer. the authors agreed that the overhead on the inference time is certainly an issue of our method. the work is not mature and practical. recommend for rejection.", "accepted": null}
{"paper_id": "iclr_2019_SJxfxnA9K7", "review_text": "all three reviewers argue for rejection on the basis that this paper does not make a sufficiently novel and substantial contribution to warrant publication. the ac follows their recommendation.", "accepted": 0}
{"paper_id": "iclr_2019_SJxzPsAqFQ", "review_text": "this paper proposes an adversarial learning framework for dialogue generation. the generator is based on previously proposed hierarchical recurrent encoderdecoder network hred by serban et al., and the discriminator is a bidirectional rnn. noise is introduced in generator for response generation. the approach is evaluated on two commonly used corpora, movie data and ubuntu corpus. in the original version of the paper, human evaluation was missing, an issue raised by all reviewers, however, this has been added in the revisions. these supplement the previous automated measures in demonstrating the benefits and significant gains from the proposed approach. all reviewers raise the issue of the work being incremental and not novel enough given the previous work in hredvhred and use of hierarchical approaches to model dialogue context. furthermore, noise generation seems new, but is not well motivated, justified and analyzed.", "accepted": 0}
{"paper_id": "iclr_2019_SkeL6sCqK7", "review_text": "the authors admit the paper was not written carefully enough and requires major rewriting. this seems to be a frustratingly common phenomenon with work on the information bottleneck.", "accepted": 0}
{"paper_id": "iclr_2019_SkghBoR5FX", "review_text": "with scores of 5, 4 and 3 the paper is just too far away from the threshold for acceptance.", "accepted": 0}
{"paper_id": "iclr_2019_SkgiX2Aqtm", "review_text": "the presented approach demonstrates an invertible architecture for autoencoding, which demonstrates improvements in performance relative to vae and waes on mnist. pros  r3 the idea of pseudoinversion is interesting.  r3 manuscript is clear. cons  r1,2,3 additional experiments needed on cifar, imagenet, others.  r1 presentation unclear. authors have not made any apparent attempt to improve the clarity of the manuscript, though they make their point that the method allows dimensionality reduction in their response.  r1, r2 main advantages not clear.  r3 text could be compressed further to allow room for additional experiments. reviewers lean reject, and authors have not updated experiments. authors are encouraged to continue to improve the work.", "accepted": 0}
{"paper_id": "iclr_2019_Skl3M20qYQ", "review_text": "the paper introduces a form of variational auto encoder for learning disentangled representations. the idea is to penalise synergistic mutual information. the introduction of concepts from synergy to the community is appreciated. although the approach appears interesting and forward looking in understanding complex models, at this point the paper does not convince on the theoretical nor on the experimental side. the main concepts used in the paper are developed elsewhere, the potential value of synergy is not properly examined. the reviewers agree on a not so positive view on this paper, with ratings either ok, but not good enough, or clear rejection. there is a consensus that the paper needs more work.", "accepted": 0}
{"paper_id": "iclr_2019_SklgHoRqt7", "review_text": "while there was some support for the ideas presented, the majority of reviewers did not think this paper was ready for publication at iclr. in particular the experiments need more work, including the protocol for validation, and attention to overfitting.", "accepted": 0}
{"paper_id": "iclr_2019_SkxANsC9tQ", "review_text": "all reviewers agree to reject. while there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.", "accepted": 0}
{"paper_id": "iclr_2019_SyGjQ30qFX", "review_text": "this paper proposes topicgan, a generative adversarial approach to topic modeling and text generation. topicgan operates in two steps it first generates latent topics and produces bagofwords corresponding to those latent topics. in the second step, the model generates text conditioning on those topic words. pros it combines the strength of topic models interpretable topics that are learned unsupervised with gan for text generation. cons there are three major concerns raised by reviewers 1 clarity, 2 relatively thin experimental results, and 3 novelty. of these, the first two were the main concerns. in particular, r1 and r2 raised concerns about insufficient componentwise evaluation e.g., text classification from topic models and insufficient ganbased baselines. also, the topic model part of topicgan seems somewhat underdeveloped in that the model assumes a single topic per document, which is a relatively strong simplifying assumption compared to most other topic models r1, r3. the technical novelty is not extremely strong in that the proposed model combines existing components together. but this alone would have not been a deal breaker if the empirical results were rigorous and strong. verdict reject. many technical details require clarification and experiments lack sufficient comparisons against prior art.", "accepted": 0}
{"paper_id": "iclr_2019_SyMras0cFQ", "review_text": "this paper shows how to obtain more homogeneous activation of atoms in a dictionary. as reviewers point out, the paper is well written and indeed shows that the propose scheme results in a more uniform activation. however, the value of this contribution rests on making a case that uniformity is indeed a desirable outcome per se. as two reviewers explain, this crucial point is left unaddressed, which makes the paper too weak for iclr.", "accepted": 1}
{"paper_id": "iclr_2019_Syfz6sC9tQ", "review_text": "the paper proposes a method of training implicit generative models based on moment matching in the feature spaces of pretrained feature extractors, derived from autoencoders or classifiers. the authors also propose a trick for tracking the moving averages by appealing to the adam optimizer and deriving updates based on the implied loss function of a moving average update. it was generally agreed that the paper was well written and easy to follow, that empirical results were good, but that the novelty is relatively low. generative models have been built out of pretrained classifiers before e.g. generative plug  play networks, feature matching losses for generator networks have been proposed before e.g. salimans et al, 2016. the contribution here is mainly the extensive empirical analysis plus the ama trick. after receiving exclusively confidence score 3 reviews, i sought the opinion of a 4th reviewer, an expert on gans and ganlike generative models. their remaining sticking points, after a rapid rebuttal, are with possible degeneracies in the loss function and classlevel information leakage from pretrained classifiers, making these results are not properly unconditional. the authors rebutted this by suggesting that unlike salimans et al 2016, there is no signal backpropagated from the label layer, but i find this particularly unconvincing the objective in that work maximizes a noneoftheabove class and thus minimizes all classes. the gradient backpropagated to the generator is uninformative about which particular class a sample should imitate, but the features learned by the discriminator needing to discriminate between classes shape those gradients in a particular way all the same, and the result is samples that look like distinct cifar classes. in the same way, the gradients used to train gfmn are shaped by particular classdiscriminative features when trained against a classifier feature extractor. from my own perspective, while there is no theory presented to support why this method is a good idea why matching arbitrary features unconnected with the generative objective should lead to good results, the idea of optimizing a moment matching objective in classifier feature space is rather obvious, and it is unsurprising that with enough elbow grease it can be made to work. the adam moving average trick is interesting but a deeper analysis and ablation of why this works would have helped convince the reader that it is principled. this paper was very much on the borderline. aside from quibbles over the fairness of comparisons above, i was forced to ask myself whether i could imagine that this would be a widely read, influential, and frequently cited piece of work. i believe that the carefully done empirical investigation has its merits, but that the core ideas are rather obvious and the added novelty of a poorly understood stabilized moving average is not enough to warrant acceptance.", "accepted": null}
{"paper_id": "iclr_2019_SygxYoC5FX", "review_text": "ar2 is concerned about the marginal novelty, weak experiments and very high complexity of the algorithm. ar3 is concerned about lack of theoretical analysis and parameter setting. ar4 is concerned that the proposed method is useful in very restricted settings and the paper is incremental. unfortunately, with strong critique from reviewers regarding the novelty, complexity, poor presentation and restricted setting, this draft cannot be accepted by iclr.", "accepted": 0}
{"paper_id": "iclr_2019_SyxknjC9KQ", "review_text": "this work presents an interesting take on how to combine basic functions to lead to better activation functions. while the experiments in the paper show that the approach works well compared to the baselines that are used as reference, reviewers note that a more adequate assessment of the contribution would require comparing to stronger baselines or switching to tasks where the chosen baselines are indeed performing well. authors are encouraged to follow the many suggestions of reviewers to strengthen their work.", "accepted": null}
{"paper_id": "iclr_2019_SyxnvsAqFm", "review_text": "the authors propose a technique for quantizing neural networks, which consist of repeated quantizationdequantization operations during training, and the second step learns scale factors. the method is simple, clearly presented, and requires no change in the training procedure. however, the authors noted that the work is somewhat incremental, and is similar to previously proposed approaches. as noted by the reviewers, the ac agrees that the work would be significantly strengthened by additional analysis of complexity in terms of computational time and memory relative to the other techniques.", "accepted": 0}
{"paper_id": "iclr_2019_r1GgDj0cKX", "review_text": "this paper propose to obtain high pruning ratio by adding constraints to obtain small weights. reviewers have a consensus on rejection due to not convincing experiments and lack of novelty.", "accepted": 0}
{"paper_id": "iclr_2019_r1GkMhAqYm", "review_text": "the reviewers raise a number of concerns including no methodological novelty, limited experimental evaluation, and relatively uninteresting application with very limited realworld application. this set of facts has been assessed differently by the three reviewers, and the scores range from probable rejection to probable acceptance. i believe that the work as is would not result in a wide interest by the iclr attendees, mainly because of no methodological novelty and relatively simplistic application. the authors rebuttal failed to address these issues and i cannot recommend this work for presentation at iclr.", "accepted": 0}
{"paper_id": "iclr_2019_r1MSBjA9Ym", "review_text": "the paper studies difficulties in training deep and narrow networks. it shows that there is high probability that deep and narrow relu networks will converge to an erroneous state, depending on the type of training that is employed. the results add to our current understanding of the limitations of these architectures. the main criticism is that the analysis might be very limited, being restricted to very narrow networks of width about 10 or less which are not very common in practice, and that the observed collapse phenomenon can be easily addressed by non symmetric initialization. there were some issues with the proofs that were covered in the discussed between authors and reviewers. the revision is relatively extensive. this is a borderline case. the paper receives one good rating, one negative rating, and a borderline accept rating. although the paper contributes interesting insights to a relevant problem that clearly needs contributions in this direction, the analysis presented in the paper and its applicability in practice seems to be very restrictive at this point.", "accepted": 0}
{"paper_id": "iclr_2019_r1MxciCcKm", "review_text": "i enjoyed reading the paper myself and i appreciate the unifying framework connecting raml and spg. while i do not put a lot of weight on the experiments, i agree with the reviewers that the experimental results are not very strong, and i am not convinced that the theoretical contribution meets the bar at iclr. in the interpolation algorithm, there seems to be an additional annealing parameter and two tuning parameters. it is important to describe how the parameters are tuned. given the additional hyperparameters, one may consider giving all of the algorithms the same budget of hyperparameter tuning. i also agree with reviewers that the policy gradient baseline seems to underperform typical results. one possible way to strengthen the experiments is to try to replicate the results of spg or raml and discuss the behavior of each algorithm as a function of hyperparameters.", "accepted": null}
{"paper_id": "iclr_2019_r1eO_oCqtQ", "review_text": "perhaps the biggest issue with the proposed approach is that the proposed approach, which supposedly addresses the issue of capturing longterm dependency with a faster convergence, was only tested on problems with largely fixed length. with the proposed k_n gate being defined as a gaussian with a single mean per unit? and variance, it is important and interesting to know how this network would cope with examples of vastly varying lengths. in addition, r3 made good points about comparison against conventional lstm and how it should be done with careful hyperparameter tuning and based on conventional known setups. this submission will be greatly strengthened with more experiments using a better set of benchmarks and by more carefully placing its contribution w.r.t. other recent advances.", "accepted": null}
{"paper_id": "iclr_2019_r1exVhActQ", "review_text": "this paper studies the properties of l1 regularization for deep neural network. it contains some interesting results, e.g. the stationary point of an l1 regularized layer has bounded number of nonzero elements. on the other hand, the majority of reviewers has concerns on that experimental supports are weak and suggests rejection. therefore, a final rejection is proposed.", "accepted": 0}
{"paper_id": "iclr_2019_r1ez_sRcFQ", "review_text": "based on the majority of reviewers with reject ratings 4,6,3, the current version of paper is proposed as reject.", "accepted": 0}
{"paper_id": "iclr_2019_r1gR2sC9FX", "review_text": "this paper considers an interesting hypothesis that relu networks are biased towards learning learn low frequency fourier components, showing a spectral bias towards low frequency functions. the paper backs the hypothesis with theoretical results computing and bounding the fourier coefficients of relu networks and experiments on synthetic datasets. all reviewers find the topic to be interesting and important. however they find the results in the paper to be preliminary and not yet ready for publication. on theoretical front, the paper characterizes the fourier coefficients for a given piecewise linear region of a relu network. however the bounds on fourier coefficients of the entire network in theorem 1 seem weak as they depend on number of pieces n_f and max lipschitz constant over all pieces l_f, quantities that can easily be exponentially big. authors in their response have said that their bound on fourier coefficients is tight. if so then the paper needs to discussprove why quantities n_f and l_f are expected to be small. such a discussion will help reviewers in appreciating the theoretical contributions more. on experimental front, the paper does not show spectral bias of networks trained over any real datasets. reviewers are sympathetic to the challenge of evaluating fourier coefficients of the network trained on real data sets, but the paper does not outline any potential approach to attack this problem. i strongly suggest authors to address these reviewer concerns before next submission.", "accepted": 0}
{"paper_id": "iclr_2019_r1glehC5tQ", "review_text": "the paper investigates an interesting question and points at a promising research direction in relation to whether adversarial examples are distinguishable from natural examples. a concern raised in the reviews is that the technical contribution of the paper is weak. a main concern with the paper is that the experiments have been conducted only on one simple data set. the authors proposed to add more experiments and improve other points, but a revision didnt follow. the reviewers consistently rate the paper as ok, but not good enough. i would encourage the authors to conduct the improvements proposed by the reviewers and the authors themselves.", "accepted": 0}
{"paper_id": "iclr_2019_r1xkIjA9tX", "review_text": "this paper proposes a new type of activations function based on qcalculus. the reviewers found that the papers is significantly lacking in its presentation, in clarity, and in its experimental evaluation. the motivation of the method raises several significant questions to the reviewers, and the proposed method is not sufficiently compared to existing approaches for noisy activation functions. after reviews, the authors have failed to present any updates to their paper.", "accepted": null}
{"paper_id": "iclr_2019_rJ4vlh0qtm", "review_text": "the reviewers raised a number of major concerns including the incremental novelty of the proposed and a poor readability of the presented materials lack of sufficient explanations and discussions. the authors decided to withdraw the paper.", "accepted": 0}
{"paper_id": "iclr_2019_rJEyrjRqYX", "review_text": "the submission suggests reducing the parameters in a convlstm by replacing the 3 gates in the standard lstm with one gate. the idea is to get a more efficient convolutional lstm and use it for video prediction. two of the reviewers found the manuscript and description of the work difficult to follow and the justification for the proposed method lacking. additionally, the contribution of this submission feels rather thin, and the experimental results are not very convincing the absolute training time is too coarse of a measurement and convergence may depend on many factors, and the improvements over prednet seem somewhat marginal. finally, i agree with the reviewer that mentioned that a proper comparison with baselines should be done in such a way that the number of parameters is comparable if params is a main claim of the paper!. it is entirely plausible that if you reduce the number of parameters in prednet by 40 in some other way, its performance would also benefit. with all this in mind, i do not recommend this paper be accepted at this time.", "accepted": 0}
{"paper_id": "iclr_2019_rJgz8sA5F7", "review_text": "this work is effectively an extension of progressive nets, where the task id is not given at test time. there were several concerns about novelty of this work and the evaluation being insufficient. there was a reasonable back and forth between the reviewers and authors, and the reviewers are all aligned with the idea that this work would need a substantial rewrite in order to be accepted at iclr.", "accepted": 0}
{"paper_id": "iclr_2019_rk4Wf30qKQ", "review_text": "the reviewers generally had concerns that the goal of recovering only the model architecture was unmotivated given that knowing the architecture is not a large threat on its own, and there are existing attacks that work without knowledge of the model architecture. moreover, given the strength of the assumed attack model, recovering model architecture is a fairly unambitious goal again, more serious attacks have already been demonstrated under weaker attack models. finally, though less seriously, the analysis is fairly preliminary, e.g. it is unclear if the attack can generalize to nearby architectures that were outside the training set.", "accepted": 0}
{"paper_id": "iclr_2019_rke8ZhCcFQ", "review_text": "while the main idea of the paper is nice, the reviewers are not satisfied with the clarity of the material and the execution.", "accepted": 0}
{"paper_id": "iclr_2019_rkgZ3oR9FX", "review_text": "paper develops a dataset and model for learning to refer to 3d objects. reviewers raised concerns about lack of novelty. fundamentally, it seems unclear what if any the takeaway for an mlaudience would be after reading this paper. we encourage the authors to incorporate reviewer feedback and submit a stronger manuscript at a future perhaps a more applied venue.", "accepted": 0}
{"paper_id": "iclr_2019_rkgd0iA9FQ", "review_text": "the reviewers and acs acknowledge that the paper has a solid theoretical contribution because it give a convergence to critical points of the adam and rmsprop algorithms, and also shows that nag can be tuned to match or outperform sgd in test errors. however, reviewers and the ac also note that potential improvements for the paper a the expositionnotations can be improved; b better comparison to the prior work could be made; c the theoretical and empirical parts of the paper are somewhat disconnected; d the proof has an error that is fixed by the authors with additional assumptions. therefore, the paper is not quite ready for publications right now but the ac encourages the authors to submit revisions to other top ml venues.", "accepted": 0}
{"paper_id": "iclr_2019_rkx8l3Cctm", "review_text": "the paper studies safer policy improvement based on nonexpert demonstrations. the paper contains some interesting ideas, and is supported by reasonable empirical evidence. overall, the work has a good potential. the author response was also helpful. that said, after considering the paper and rebuttal, the reviewers were not convinced the paper is ready for publication, as the significance of this work is limited by a rather strong assumption see reviews for details. furthermore, the presentation of the paper also requires some work to improve see reviews for detailed comments.", "accepted": null}
{"paper_id": "iclr_2019_rkxJus0cFX", "review_text": "i have implemented algorithm 2 and 3. in algorithm 2, i think line 6s condition should be nnz  k. because increasing threshold will only make nnz even bigger. also in algorithm 3, line 10 and 12, i think ratio is the value that should be assigned to r and l respectively, or else this doesnt make any sense for a binary search. please confirm if my findings are correct or not. thanks. paper focuses on residual gradient compression rgc as a promising approach to reducing the synchronization cost of gradients in a distributed settings. prior approaches focus on the theoretical value of good compression rates without looking into the overall cost of the changes. this paper introduces redsync that builds on the existing approaches by picking the most appropriate ones that reduce the overall cost for gradient reduction without unduly focusing on the compression rate. the paper does this by providing an analysis of the cost of rgc and also the limitations in scaling as the bandwidth required grows with the number of nodes. it also highlights the value of applying different algorithms in this process for compression and the benefits and issues with each. pros  useful analysis that will help direct research in this area  shows that this approach works for models that have a high communication to computation ratio  provides a useful approach that works for a number of models cons  positive experimental results are on models that are typically not used in practice e.g. alexnet and vgg16  speedups shown on lstms dont see worthwhile to scale, and in practice a modelparallelism approach may scale better corrections  typo in notes for table 1 last sentence rcg  rgc  typo in first sentence in section 3.2 redsycn  redsync  section 3.3, 2 last sentence maybe overdrafts  overshadows ? this paper introduces a set of implementation optimizations for minimizing communication overhead and thereby reducing the training time in distributed settings. the method relies on existing gradient compression and pruning techniques and is tested on synchronousdataparallel settings. the contribution and impact of the paper is unclear. the authors claim implementation innovations that show true performance gains of gradient compression techniques. but again it is unclear what those innovations are and how they can be reused for accelerating training for a new model. the authors did perform an extensive set of experiments and while the method works well for some models and batch sizes, it doesnt work well for some other models. what would make the paper much more compelling would be if it came up with ways to systematically explore the relationship between training batch size, model parameter size, communicationcomputationdecompression ratos, and based on these properties, it can come up with best strategies to accelerate distributed data parallel training for any new model. the paper needs to be polished as it has multiple typos. quality and clarity the paper proposes an approach to reduce the communication bandwidth and overhead in distributed deep learning. the approach leverages on previous work mainly the residual gradient compression rgc algorithm, and proposes several implementation optimizations. from what i can read, it is the basic rgc algorithm that is used, but with some clever optimization to improve the performance of it. the quality of the paper is good, it is wellwritten and easy to read. the evaluation of the proposed approach is well done, using several diverse datasets and models, and executed on two different parallel systems. however, the reasons why rgc and qrgc sometimes have better accuracy than sgd needs to be analyzed and explained. originality and significance the originality of the paper is relatively low optimization of an existing algorithm and the contributions are incremental. however, the paper addresses an important practical problem in distributed learning, and thus can have a significant practical impact on how distributed deep learning systems are implemented. pros  addresses an important issue.  good performance.  good evaluation on two different systems. cons  limited contribution. although i like implementation papers very important, i think the contribution is to low for iclr. minor  in general, the figures are hard to read the main problem is to small text  compression in the title is slightly misleading, since its mainly selection that is done top0.1 gradients. although the values are packed in a data structure for transmission, its not compression in a information theory perspective. hi there, in this paper, you claim that you design a costefficient method for communication, but the core of the algorithms is already shown in lin et al 2018 iclr. so from the technical perspective, i didnt see anything new here. then you combine the encoding technique which is well established in the past years, and nothing new in this paper. if you say the technical contribution is only for the implementation part, and then i think it is too weak for the contribution, or you can call it programming skills in real practice. thanks for reading your paper. this paper proposed residual gradient compression as a promising approach to reduce the synchronization cost of gradients in a distributed settings. it provides a useful approach that works for a number of models. the reviewers have a consensus that the quality is below acceptance standard due to practicality of experiments and lack of contribution.", "accepted": null}
{"paper_id": "iclr_2019_rkxtl3C5YX", "review_text": "this work examines the alphago zero algorithm, a selfplay reinforcement learning algorithm that has been shown to learn policies with superhuman performance on 2 player perfect information games. the main result of the paper is that the policy learned by agz corresponds to a nash equilibrium, that and that the crossentropy minimization in the supervised learninginspired part of the algorithm converges to this nash equillibrium, proves a bound on the expected returns of two policies under the and introduces a robust mdp view of a 2 player zerosum game played between the agent and nature. r3 found the paper wellstructured and the results presented therein interesting. r2 complained of overly heavy notation and questioned the applicability of the results, as well as the utility of the robust mdp perspective though did raise their score following revisions. the most detailed critique came from r1, who suggested that the bound on the convergence of returns of two policies as the kl divergence between their induced distributions decreases is unsurprising, that using it to argue for agzs convergence to the optimal policy ignores the effects introduced by the suboptimality of the mcts policy while really interesting part being understanding how agz deals with, and whether or not it closes, this gap, and that the robust mdp view is less novel than the authors claim based on the known relationships between 2 player zerosum games and minimax robust control. i find r1s complaints, in particular with respect to robust mdps a criticism which went completely unaddressed by the authors in their rebuttal, convincing enough that i would narrowly recommend rejection at this time, while also agreeing with r3 that this is an interesting subject and that the results within could serve as the bedrock for a stronger future paper.", "accepted": null}
{"paper_id": "iclr_2019_ryfaViR9YX", "review_text": "the authors propose a generative model based on variational autoencoders that provides means to manipulate the highlevel attributes of a given input. the attributes can be either predefined ground truth attributes or unknown attributes automatically discovered from the data. while the reviewers acknowledged the potential usefulness of the proposed approach, they raised important concerns that were viewed by ac as a critical issue 1 very limited experimental evaluation e.g. no baseline or ablation results, no quantitative results; comparisons on other more complex datasets and more indepth analysis would substantially strengthen the evaluation and would allow to assess the scope of the contribution of this work  see, for example, r3s suggestion to use other dataset like dsprites or celeba, where the ground truth attributes are known; 2 lack of presentation clarity  see r2s latest comment how to improve. a general consensus among reviewers and ac suggests, in its current state the manuscript is not ready for a publication. it needs clarification, more empirical studies and polish to achieve the desired goal.", "accepted": 0}
{"paper_id": "iclr_2019_rygnfn0qF7", "review_text": "this paper proposes to pretrain hierarchical document representations for use in downstream tasks. all reviewers agreed that the results were reasonable. however, the methodological novelty is limited. while i believe there is a place for solid empirical results, even if not incredibly novel, there is also little qualitative or quantitative analysis to shed additional insights. given the high quality bar for iclr, i cant recommend the paper for acceptance at this time.", "accepted": null}
{"paper_id": "iclr_2019_rylIy3R9K7", "review_text": "the paper studies the convergence of a primaldual algorithm on a special minmax problem in wgan where the maximization is with respect to linear variables linear discriminator and minimization is over nonconvex generators. experiments with both simulated and real world data are conducted to show that the algorithm works for wgans and multitask learning. the major concern of reviewers lies in that the linear discriminator assumption in wgan is too restrictive to general nonconvex minimax saddle point problem in gans. linear discriminator implies that the maximization part in minmax problem is concave, and it is thus not surprise that under this assumption the paper converts the original problem to a nonconvex optimization instance and proves its first order convergence with descent lemma. this technique however cant be applied to general nonconvex saddle point problem in gans. also the experimental studies are also not strong enough. therefore, current version of the paper is proposed as borderline lean reject.", "accepted": 0}
{"paper_id": "iclr_2020_rkgpv2VFvr", "review_text": "this paper considers the benefits of deep multitask rl with shared representations, by deriving bounds for multitask approximate value and policy iteration bounds. this shows both theoretically and empirically that shared representations across multiple tasks can outperform single task performance. there were a number of minor concerns from the reviewers regarding relation to prior work and details of the analysis, but these were clarified in the discussion. this paper adds important theoretical analysis to the literature, and so i recommend it is accepted.", "accepted": null}
{"paper_id": "iclr_2020_H1loF2NFwr", "review_text": "this is one of several recent parallel papers that pointed out issues with neural architecture search nas. it shows that several nas algorithms do not perform better than random search and finds that their weight sharing mechanism leads to low correlations of the search performance and final evaluation performance. code is available to ensure reproducibility of the work. after the discussion period, all reviewers are mildly in favour of accepting the paper. my recommendation is therefore to accept the paper. the papers results may in part appear to be old news by now, but they were not when the paper first appeared on arxiv in parallel to li  talwalkar, so similarities to that work should not be held against this paper.", "accepted": null}
{"paper_id": "iclr_2020_HkgTTh4FDH", "review_text": "this paper provides theoretical guarantees for adversarial training. while the reviews raise a variety of criticisms e.g., the results are under a variety of assumptions, overall the paper constitutes valuable progress on an emerging problem.", "accepted": 1}
{"paper_id": "iclr_2020_H1xPR3NtPB", "review_text": "this paper presents results of looking at the inside of pretrained language models to capture and extract syntactic constituency. reviewers initially had neutral to positive comments, and after the author rebuttal which addressed some of the major questions and concerns, their scores were raised to reflect their satisfaction with the response and the revised paper. reviewer discussions followed in which they again expressed that they became more positive that the paper makes novel and interesting contributions. i thank the authors for submitting this paper to iclr and look forward to seeing it at the conference..", "accepted": 1}
{"paper_id": "iclr_2020_rylJkpEtwS", "review_text": "this paper develops the notion of the arrow of time in mdps and explores how this might be useful in rl. all the reviewers found the paper thought provoking, wellwritten, and they believe the work could have significant impact. the paper does not fit the typical mold it presents some ideas and uses illustrative experiments to suggest the potential utility of the arrow without nailing down a final algorithm or make a precise performance claim. overall it is a solid paper, and the reviewers all agreed on acceptance. there are certainly weaknesses in the work, and there is a bit of work to do to get this paper ready. r2 had a nice suggestion of a baseline based on simply learning a transition model its described in the updated reviewplease include it. the description of the experimental methodology is a bit of a mess. most of the experiments in the paper do not clearly indicate how many runs were conducted or how errorbars where computed or what they represent. it is likely that only a handful of runs were used, which is surprising given the size of some of the domains used. in many cases the figure caption does not even indicate which domain the data came from. all of this is dangerously close to criteria for rejection; please do better. readability is also known as empowerment and it would be good to discuss this connection. in general the paper was a bit light on connections outlining how information theory has been used in rl. i suggest you start here httpwww2.hawaii.edusstillstillprecup2011.pdf to improve this aspect. finally, the paper has a very large appendix 14 oages with many many more experiments and theory. i am still not convinced that the balance is quite right. this is probably a journal or long arxiv paper. maybe this paper should be thought of as a nectar version of a longer standalone arxiv paper. finally, relying on effectiveness of random exploration is no small thing and there is a long history in rl of ideas that would work well, given it is easy to gather data that accurately summarizes the dynamics of the world e.g. protovalue, funcs. many ideas are effective given this assumption. the paper should clearly and honestly discuss this assumption, and provide some arguments why there is hope.", "accepted": 1}
{"paper_id": "iclr_2020_SylOlp4FvH", "review_text": "this paper proposes an extension of mpo for onpolicy reinforcement learning. the proposed method achieved promising results in a relatively hyperparameter insensitive manner. one concern of the reviewers is the lack of comparison with previous works, such as original mpo, which has been partially addressed by the authors in rebuttal. in addition, blind review 3 has some concerns with the fairness of the experimental comparison, though other reviews accept the comparison using standardized benchmark. overall, the paper proposes a promising extension of mpo; thus, i recommend it for acceptance.", "accepted": null}
{"paper_id": "iclr_2020_Bylx-TNKvH", "review_text": "this work proves that the weights of feedforward relu networks are determined, up to a specified set of symmetries, by the functions they define. reviewers found the paper easy to read and the proof technically sound. there was some debate over the motivation for the paper, reviewer 1 argues that there is no practical significance for the result, a point that the authors do not deny. i appreciate the concerns raised by reviewer 1, theorists in machine learning should think carefully about the motivation for their work. however, while there is no clear practical significance of this work, i believe there is value to accepting it. because the considered question concerns a sufficiently fundamental property of neural networks, and the proof is both easy to read and provides insights into a well studied class of models, i believe many researchers will find value in reading this paper.", "accepted": 1}
{"paper_id": "iclr_2020_BkepbpNFwr", "review_text": "this paper introduces an rnn based approach to incremental domain adaptation in natural language processing, where the rnn is progressively augmented with the parameterized memory bank which is shown to be better than expanding the rnn states. reviewers and ac acknowledge that this paper is well written with interesting ideas and practical value. domain adaptation in the incremental setting, where domains come in a streaming way with only the current one accessible, can find some realistic application scenarios. the proposed extensible attention mechanism is solid and works well on several nlp tasks. several concerns were raised by the reviewers regarding the comparative and ablation studies, which were well resolved in the rebuttal. the authors are encouraged to generalize their approach to other application domains other than nlp to show the generality of their approach. i recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2020_rJeINp4KwH", "review_text": "the paper proposes a new approach to multiactor rl, which ensure diversity and performance of the population of actors, by distilling the policy of the best performing agent in a soft way and maintaining some distance between the agents. the authors show improved performance over several stateoftheart monoactor algorithms and over several other multiactor rl algorithms. initially, reviewers were concerned with magnitude of the contributionnovelty, as well as some technical issues e.g. the beta update, and relative lack of baseline comparisons. however, after discussion the reviewers largely agree that their main concerns have been addressed. therefore, i recommend this paper for acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_ryg48p4tPH", "review_text": "the authors address the challenge of sampleefficient learning in multiagent systems. they propose a model that distinguishes actions in terms of their semantics, specifically in terms of whether they influence the acting agent and environment or whether they influence other agents. this additional structure is shown to substantially benefit learning speed when composed with a range of state of the art multiagent rl algorithms. during the rebuttal, technical questions were well addressed and the overall quality of the paper improved. the paper provides interesting novel insights on how the proposed structure improves learning.", "accepted": null}
{"paper_id": "iclr_2020_SkxBUpEKwH", "review_text": "this paper proposes to extract a character from a video, manually control the character, and render into the background in real time. the rendered video can have arbitrary background and capture both the dynamics and appearance of the person. all three reviewers praises the visual quality of the synthesized video and the paper is well written with extensive details. some concerns are raised. for example, despite an excellent engineering effort, there is few things the reader would scientifically learn from this paper. additional ablation study on each component would also help the better understanding of the approach. given the level of efforts, the quality of the results and the reviewers comments, the acs recommend acceptance as a poster.", "accepted": null}
{"paper_id": "iclr_2020_BygSP6Vtvr", "review_text": "the paper investigates how to distill an ensemble effectively using a prior network in order to reap the benefits of uncertainty estimation provided by ensembling in addition to the accuracy gains provided by ensembling. overall, the paper is nicely written, and makes a valuable contribution. the authors also addressed most of the initial concerns raised by the reviewers. i recommend the paper for acceptance, and encourage the authors to take into account the reviewer feedback when preparing the final version.", "accepted": 1}
{"paper_id": "iclr_2020_BJlguT4YPr", "review_text": "this paper proposes an approach to representing a symbolic knowledge base as a sparse matrix, which enables the use of differentiable neural modules for inference. this approach scales to large knowledge bases and is demonstrated on several tasks. postdiscussion and rebuttal, all three reviewers are in agreement that this is an interesting and useful paper. there was intiially some concern about clarity and polish, but these have been resolved upon rebuttal and discussion. therefore i recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2020_S1g7tpEYDS", "review_text": "this paper proposes an extension to deterministic autoencoders, namely instead of noise injection in the encoders of vaes to use deterministic autoencoders with an explicit regularization term on the latent representations. while the reviewers agree that the paper studies an important question for the generative modeling community, the paper has been limited in terms of theoretical analysis and experimental validation. the authors, however, provided further experimental results to support the claims empirically during the discussion period and the reviewers agree that the paper is now acceptable for publication in iclr2020.", "accepted": 1}
{"paper_id": "iclr_2020_SJx-j64FDr", "review_text": "this paper studies how the architecture and training procedure of binarized neural networks can be changed in order to make it easier for sat solvers to verify certain properties of them. all of the reviewers were positive about the paper, and their questions were addressed to their satisfaction, so all reviewers are in favor of accepting the paper. i therefore recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_H1lmhaVtvr", "review_text": "the authors present a method to learn the expected number of time steps to reach any given state from any other state in a reinforcement learning setting. they show that these socalled dynamical distances can be used to increase learning efficiency by helping to shape reward. after some initial discussion, the reviewers had concerns about the applicability of this method to continuing problems without a clear goal state, learning issues due to the dependence of distance estimates on policy and vice versa, experimental thoroughness, and a variety of smaller technical issues. while some of these were resolved, the largest outstanding issue is whether the proper comparisons were made to existing work other than diayn. the authors appear to agree that additional baselines would benefit the paper, but are uncertain whether this can occur in time. nonetheless, after discussion the reviewers all appeared to agree on the merit of the core idea, though i strongly encourage the authors to address as many technical and baseline issues as possible before the camera ready deadline. in summary, i recommend this paper for acceptance.", "accepted": null}
{"paper_id": "iclr_2020_HkgH0TEYwH", "review_text": "issues raised by the reviewers have been addressed by the authors, and thus i suggest the acceptance of this paper.", "accepted": null}
{"paper_id": "iclr_2020_Hyg-JC4FDr", "review_text": "this work addresses new insights in the imitation learning setting, and shows how a popular type of approach can be extended in a principled way to the offpolicy learning setting. several requests for clarification were addressed in the rebuttal phase, in particular regarding the empirical evaluation in offpolicy settings. the authors improved the empirical validation and overall clarity of the paper. the resulting manuscript provides valuable new insights, in particular in its principled connections, and extension to previous work.", "accepted": null}
{"paper_id": "iclr_2020_HklkeR4KPB", "review_text": "this works improves the mixmatch semisupervised algorithm along the two directions of distribution alignment and augmentation anchoring, which together make the approach more dataefficient than prior work. all reviewers agree that the impressive empirical results in the paper are its main strength, but express concern that the method is overly complicated and hacking together many known pieces, as well as doubt as to the extent of the contribution of the augmentation method itself, with requests for better augmentation controls. while some of these concerns have not been addressed by authors in their response, the strength of empirical results seems enough to justify an acceptance recommendation.", "accepted": null}
{"paper_id": "iclr_2020_SJeYe0NtvH", "review_text": "this paper introduces a new objective for text generation with neural nets. the main insight is that the standard likelihood objective assigns excessive probability to sequences containing repeated and frequent words. the paper proposes an objective that penalizes these patterns. this technique yields better text generation than alternative methods according to human evaluations. the reviewers found the paper to be written clearly. they found the problem to be relevant and found the proposed solution method to be both novel and simple. the experiments were carefully designed and the results were convincing. the reviewers raised several concerns on particular details of the method. these concerns were largely addressed by the authors in their response. overall, the reviewers did not find the weaknesses of the paper to be serious flaws. this paper should be published. the paper provides a clearly presented solution for a relevant problem, along with careful experiments.", "accepted": 0}
{"paper_id": "iclr_2020_HJlSmC4FPS", "review_text": "this paper focuses on studying neural networkbased denoising methods. the paper makes the interesting observation that most existing denoising approaches have a tendency to overfit to knowledge of the noise level. the authors claim that simply removing the bias on the network parameters enables a variety of improvements in this regard and provide some theoretical justification for their results. the reviewers were mostly postive but raised some concerns about generalization beyond gaussian noise and not being very well theoretically motivated. these concerns seem to have at least partially been alleviated during the discussion period. i agree with the reviewers. i think the paper looks at an important phenomena for denoising role of variance parameter and is well suited to iclr. i recommend acceptance. i suggest that the authors continue to further improve the paper based on the reviewers comments.", "accepted": null}
{"paper_id": "iclr_2020_rygG4AVFvH", "review_text": "this paper proposes to optimize the code optimal code in dnn compilers using adaptive sampling and reinforcement learning. this method achieves significant speedup in compilation time and execution time. the authors made strong efforts in addressing the problems raised by the reviewers, and promised to make the code publicly available, which is of particular importance for works of this nature.", "accepted": 0}
{"paper_id": "iclr_2020_Byl5NREFDr", "review_text": "two knowledgable reviewers recommend accepting the paper, and the less familiar reviewer is also positive. the final decision is to accept the paper. its an interesting and timely topic with insightful results.", "accepted": 1}
{"paper_id": "iclr_2020_BJx040EFvH", "review_text": "this paper provides a surprising result that randomization and fgsm can produce robust models faster than previous methods given the right mix of cyclic learning rate, mixed precision, etc. this paper produced a fair bit of controversy among both the community and the reviewers to the point where there were suggestions of bugs, evaluation problems, and other issues leading to the results. in the end, the authors released the code and made significant updates to the paper based on all the feedback. multiple reviewers checked the code and were happy. there was an extensive author response, and all the reviewers indicated that their primary concerns were address, save concerns about the sensitivity of stepsize and the impact of early stopping. overall, the paper is well written and clear. the proposed approach is simple and well explained. the result is certainly interesting, and this paper will continue to generate fruitful debate. there are still things to address to improve the paper, listed above. i strongly encourage the authors to continue to improve the work and make a more concerted effort to carefully discuss the impacts of early stopping.", "accepted": 1}
{"paper_id": "iclr_2020_rJeXS04FPH", "review_text": "the authors design a deep model architecture for learning word embeddings with better performance andor more efficient use of parameters. results on language modeling and machine translation are promising. pros interesting idea and nice results. new model may have some independent value beyond nlp. cons empirical comparisons could be more thorough. for example, it is not clear to me at least what would be the benefits of this approach applied to whole words versus a competitor using subword units.", "accepted": 0}
{"paper_id": "iclr_2020_SkxJ8REYPH", "review_text": "this paper presents a new approach, slowmo, to improve communicationefficient distribution training with sgd. the main method is based on the bmuf approach and relies on workers to periodically synchronize and perform a momentum update. this works well in practice as shown in the empirical results. reviewers had a couple of concerns regarding the significance of the contributions. after the rebuttal period some of their doubts were clarified. even though they find that the solutions of the paper are an incremental extension of existing work, they believe this is a useful extension. for this reason, i recommend to accept this paper.", "accepted": null}
{"paper_id": "iclr_2020_Hkl1iRNFwS", "review_text": "this paper studies numerous ways in which the statistics of network weights evolve during network training. reviewers are not entirely sure what conclusions to make from these studies, and training dynamics can be strongly impacted by arbitrary choices made in the training process. despite these issues, the reviewers think the observed results are interesting enough to clear the bar for publication.", "accepted": 1}
{"paper_id": "iclr_2020_rJeQoCNYDS", "review_text": "this is an interesting paper that is concerned with single episode transfer to reinforcement learning problems with different dynamics models, assuming they are parameterised by a latent variable. given some initial training tasks to learn about this parameter, and a new test task, they present an algorithm to probe and estimate the latent variable on the test task, whereafter the inferred latent variable is used as input to a control policy. there were several issues raised by the reviewers. firstly, there were questions with the number of runs and the baseline implementations, which were all addressed in the rebuttals. then, there were questions around the novelty and the main contribution being wallclock time. these issues were also adequately addressed. in light of this, i recommend acceptance of this paper.", "accepted": 1}
{"paper_id": "iclr_2020_HJeVnCEKwH", "review_text": "this is an interesting contribution that sheds some light on a wellstudied but still poorly understood problem. i think it might be of interest to the community.", "accepted": null}
{"paper_id": "iclr_2020_r1gRTCVFvB", "review_text": "this paper presents an approach for the longtailed image classification, where the class frequencies during supervised training of an image classifier are heavily skewed, so that the classifier underfits on underrepresented classes. the authors responses to the reviews clarified most of their concerns, although some reviewers pointed out that some of the details regarding experiments such as the construction of the validation set and the selection of balancedimbalanced sets remain unclear. overall, we believe this paper contains interesting observations to be shared.", "accepted": 1}
{"paper_id": "iclr_2020_SJgmR0NKPr", "review_text": "the paper proposes an alternative to bptt for training recurrent neural networks based on an explicit state variable, which is trained to improve both the prediction accuracy and the prediction of the next state. one of the benefits of the methods is that it can be used for online training, where bptt cannot be used in its exact form. theoretical analysis is developed to show that the algorithm converges to a fixed point. overall, the reviewers appreciate the clarity of the paper, and find the theory and the experimental evaluation to be reasonably well balanced. after a round of discussion, the authors improved the paper according to the reviews. the final assessments are overall positive, and im therefore recommending accepting this paper.", "accepted": 0}
{"paper_id": "iclr_2020_rkgt0REKwS", "review_text": "this paper studies learning with noisy labels by integrating the idea of curriculum learning. all reviewers and ac are happy with novelty, clear writeup and experimental results. i recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2020_HJx81ySKwr", "review_text": "this paper proposed to use an autoencoder based approach for anomaly localization. the method shows promising on inpainting task compared with traditional autoencoder. first two reviewers recommend this paper for acceptance. the last review has some concerns about the experimental design and whether vae is a suitable baseline. the authors provide reasonable explanation in rebuttal while the reviewer did not give further comments. overall, the paper proposes a promising approach for anomaly localization; thus, i recommend it for acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_B1gskyStwr", "review_text": "the reviewers are unanimous in their evaluation of this paper, and i concur.", "accepted": null}
{"paper_id": "iclr_2020_r1lOgyrKDS", "review_text": "the paper presents a novel reinforcement learningbased algorithm for contextual sequence generation. specifically, the paper presents experimental results on the application of the gradient arsm estimator of yin et al. 2019 to challenging structured prediction problems neural program synthesis and image captioning. the method consists in performing correlated monte carlo rollouts starting from each token in the generated sequence, and using the multiple rollouts to reduce gradient variance. numerical experiments are presented with promising performance. reviewers were in agreement that this is a nontrivial extension of previous work with broad potential application. some concerns about better framing of contributions were mostly resolved during the author rebuttal phase. therefore, the ac recommends publication.", "accepted": 1}
{"paper_id": "iclr_2020_rJlnxkSYPS", "review_text": "the authors addressed the issues raised by the reviewers, so i suggest the acceptance of this paper.", "accepted": null}
{"paper_id": "iclr_2020_ryxmb1rKDS", "review_text": "this paper proposes a novel method for learning hamiltonian dynamics from data. the data is obtained from systems subjected to an external control signal. the authors show the utility of their method for subsequent improved control in a reinforcement learning setting. the paper is well written, the method is derived from first principles, and the experimental validation is solid. the authors were also able to take into account the reviewers feedback and further improve their paper during the discussion period. overall all of the reviewers agree that this is a great contribution to the field and hence i am happy to recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_SJeY-1BKDS", "review_text": "main content blind review 3 summarizes it well this paper presents results on dictionary learning through l4 maximization. the authors base this paper heavily off of the formulation and algorithm in zhai et. al. 2019 complete dictionary learning via l4norm maximization over the orthogonal group. the paper draws connections between complete dictionary learning, pca, and ica by pointing out similarities between the objectives functions that are optimized as well as the algorithms used. the paper further presents results on dictionary learning in the presence of different types of noise awgn, sparse corruptions, outliers and show that the l4 objective is robust to different types of noise. finally the authors apply different types of noise to synthetic and real images and show that the dictionaries that they learn are robust to the noise applied.  discussion reviews agree about the interesting work, including the connections of complete dictionary learning with classic pca and ica after further clarification during the rebuttal period. additional empirical strengthening during the rebuttal period also addressed a reviewer concern.  recommendation and justification as review 3 wrote, overall this paper makes significant contributions by extending the work in zhai et. als 2019 complete dictionary learning via l4norm maximization over the orthogonal group to noisy dictionary learning settings.", "accepted": 1}
{"paper_id": "iclr_2020_HkxdQkSYDB", "review_text": "the work proposes a graph convolutional network based approach to multiagent reinforcement learning. this approach is designed to be able to adaptively capture changing interactions between agents. initial reviews highlighted several limitations but these were largely addressed by the authors. the resulting paper makes a valuable contribution by proposing a wellmotivated approach, and by conducting extensive empirical validation and analysis that result in novel insights. i encourage the authors to take on board any remaining reviewer suggestions as they prepare the camera ready version of the paper.", "accepted": null}
{"paper_id": "iclr_2020_rklTmyBKPH", "review_text": "main content paper proposes a fast network adaptation fna method, which takes a pretrained image classification network, and produces a network for the task of object detectionsemantic segmentation summary of discussion reviewer1 interesting paper with good results, specifically without the need to do pretraining on imagenet. cons are better comparisons to existing methods and run on more datasets. reviewer2 interesting idea on adapting source network network via parameter remapping that offers good results in both performance and training time. reviewer3 novel method overall, though some concerns on the concrete parameter remapping scheme. results are impressive recommendation interesting idea and good results. paper could be improved with better comparison to existing techniques. overall recommend weak accept.", "accepted": null}
{"paper_id": "iclr_2020_HylxE1HKwS", "review_text": "the authors propose a new method for neural architecture search, except its not exactly that because model training is separated from architecture, which is the main point of the paper. once this network is trained, subnetworks can be distilled from it and used for specific tasks. the paper as submitted missed certain details, but after this was pointed out by reviewers the details were satisfactorily described by the authors. the idea of the paper is original and interesting. the paper is correct and, after the revisions by authors, complete. in my view, this is sufficient for acceptance.", "accepted": null}
{"paper_id": "iclr_2020_Byx4NkrtDS", "review_text": "navigation is learned in a twostage process, where the recurrent network is first pretrained in a taskagnostic stage and then finetuned using qlearning. the analysis of the learned network confirms that what has been learned in the taskagnostic pretraining stage takes the form of attractors. the reviewers generally liked this work, but complained about lack of comparison studies  baselines. the authors then carried out such studies and did a major update of the paper. given that the extensive update of the paper seems to have addressed the reviewers complaints, i think this paper can be accepted.", "accepted": 0}
{"paper_id": "iclr_2020_HyxjNyrtPr", "review_text": "the paper has initially received mixed reviews, with two reviewers being weakly positive and one being negative. following the authors revision, however, the negative reviewer was satisfied with the changes, and one of the positive reviewers increased the score as well. in general, the reviewers agree that the paper contains a simple and wellexecuted idea for recovering geometry in unsupervised way with generative modeling from a collection of 2d images, even though the results are a bit underwhelming. the authors are encouraged to expand the related work section in the revision and to follow our suggestion of the reviewers.", "accepted": 0}
{"paper_id": "iclr_2020_Syx1DkSYwB", "review_text": "congratulations on getting your paper accepted to iclr. please make sure to incorporate the reviewers suggestions for the final version.", "accepted": 1}
{"paper_id": "iclr_2020_SkgGCkrKvH", "review_text": "the authors present an algorithm chocosgd to make use of communication compression in a decentralized setting. this is an interesting problem, and the paper is wellmotivated and wellwritten. on the theoretical side, the authors prove the convergence rate of the algorithm on nonconvex smooth functions, which shows a nearly linear speedup. the experimental results on several benchmark datasets validate the algorithm achieves better performance than baselines. these can be made more convincing by comparing with more baselines including deepsqueeze and other centralized algorithms with a compression scheme, and on larger datasets. the authors should also clarify results on consensus.", "accepted": 0}
{"paper_id": "iclr_2020_SylL0krYPS", "review_text": "this paper considers adversarial attacks in continuous action modelbased deep reinforcement learning. an optimisationbased approach is presented, and evaluated on mujoco tasks. there were two main concerns from the reviewers. the first was that the approach requires strong assumptions, but in the rebuttal some relaxations were demonstrated e.g., not attacking every step. additionally, there were issues raised with the choice of baselines, but in the discussion the reviewers did not agree on any other reasonable baselines to use. this is a novel and interesting contribution nonetheless, which could open the field to much additional discussion, and so should be accepted.", "accepted": 0}
{"paper_id": "iclr_2020_Hkem-lrtvH", "review_text": "this paper proposes a queryefficient blackbox attack that uses bayesian optimization in combination with bayesian model selection to optimize over the adversarial perturbation and the optimal degree of search space dimension reduction. the method can achieve comparable success rates with 25 times fewer queries compared to previous stateoftheart blackbox attacks. the paper should be further improved in the final version e.g., including more results on imagenet data.", "accepted": 0}
{"paper_id": "iclr_2020_ryx6WgStPB", "review_text": "this paper considers ensemble of deep learning models in order to quantify their epistemic uncertainty and use this for exploration in rl. the authors first show that limiting the ensemble to a small number of models, which is typically done for computational reasons, can severely limit the approximation of the posterior, which can translate into poor learning behaviours e.g. overexploitation. instead, they propose a general approach based on hypermodels which can achieve the benefits of a large ensemble of models without the computational issues. they perform experiments in the bandit setting supporting their claim. they also provide a theoretical contribution, proving that an arbitrary distribution over functions can be represented by a linear hypermodel. the decision boundary for this paper is unclear given the confidence of reviewers and their scores. however, the tackled problem is important, and the proposed approach is sound and backed up by experiments. most of reviewers concerns seemed to be addressed by the rebuttal, with the exception of few missing references which the authors should really consider adding. i would therefore recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_B1xMEerYvB", "review_text": "the paper discusses smooth market games and demonstrate the merit of the approach. the reviewers agree on the quality of the paper, and the comments have been addressed well by the authors.", "accepted": 1}
{"paper_id": "iclr_2020_HJloElBYvB", "review_text": "this submission presents a theoretical study of phase transitions in ib adjusting the ib parameter leads to stepwise behaviour of the prediction. quoting r3 the core result is given by theorem 1 the phase transition betas necessarily satisfy an equation, where the lhs is expressed in terms of an optimal perturbation of the encoding function xz. this paper received a borderline review and two votes for weak accept. the main comment for the borderline review was about the rigor of a proof and the use of  symbols. the authors have updated the proof using limits as requested, addressing this primary concern. on the balance, the paper makes a strong contribution to understanding an important learning setting and a contribution to theoretical understanding of the behavior of information bottleneck predictors.", "accepted": 0}
{"paper_id": "iclr_2020_rkevSgrtPr", "review_text": "this is a nice paper on the classical problem of universal approximation, but giving a direct proof with good approximation rates, and providing many refinements and ties to the literature. if possible, i urge the authors to revise the paper further for camera ready; there are various technical oversights e.g., 1lambda should appear in the approximation rates in theorem 3.1, and the proof of theorem 3.1 is an uninterrupted 2.5 page block splitting it into lemmas would make it cleaner, and also those lemmas could be useful to other authors.", "accepted": 1}
{"paper_id": "iclr_2020_S1gEIerYwH", "review_text": "this paper presents a theoretically motivated method based on homotopy continuation for transfer learning and demonstrates encouraging results on fashionmnist and cifar10. the authors draw a connection between this approach and the widely used finetuning heuristic. reviewers find principled approaches to transfer learning in deep neural networks an important direction, and find the contributions of this paper an encouraging step in that direction. alongside with the reviewers, i think homotopy continuation is a great numerical tool with a lot of untapped potentials for ml applications, and i am happy to see an instantiation of this approach for transfer learning. reviewers had some concerns about experimental evaluations reporting test performance in addition to training, and the writing of the draft. the authors addressed these in the revised version by including test performance in the appendix and rewriting the first parts of the paper. two out of three reviewers recommend accept. i also find the homotopy analysis interesting and alongside with majority of reviewers, recommend accept. however, please try to iterate at least once more over the writing; simply long sentences and make sure the writing and flow are, for the camera ready version.", "accepted": 1}
{"paper_id": "iclr_2020_rJxe3xSYDS", "review_text": "the paper proposes a fast training method for extreme classification problems where number of classes is very large. the method improves the negative sampling method which uses uniform distribution to sample the negatives by using an adversarial auxiliary model to sample negatives in a nonuniform manner. this has logarithmic computational cost and minimizes the variance in the gradients. there were some concerns about missing empirical comparisons with methods that use sampledsoftmax approach for extreme classification. while these comparisons will certainly add further value to the paper, the improvement over widely used method of negative sampling and a formal analysis of improvement from hard negatives is a valuable contribution in itself that will be of interest to the community. authors should include the experiments on small datasets to quantify the approximation gap due to negative sampling compared to full softmax, as promised.", "accepted": 1}
{"paper_id": "iclr_2020_rkgg6xBYDH", "review_text": "this paper presents a generalization bound for rnns based on matrix1 norm and fisherrao norm. as the initial bound relies on nonsignularity of input covariance, which may not always hold in practice, the authors present additional analysis by noise injection to ensure covariance is positive definite. through the resulted bound, the paper discusses how weight decay and gradient clipping in the training can help generalization. there were some concerns raised by reviewers, including rigorous report of the experiment results, claims on generalization in imdb experiment, claims of no explicit dependence on the size of networks, and the relationship of small eigenvalues in input covariance to high frequency features. the authors responded to these and also revised their draft to address most of these concerns in particular, authors added a new section in the appendix that includes additional experimental results. reviewers were mainly satisfied with the responses and the revision, and they all recommend accept.", "accepted": null}
{"paper_id": "iclr_2020_BygPO2VKPH", "review_text": "the paper extends lista by introducing gain gates and overshoot gates, which respectively address underestimation of code components and compensation of small step size of lista. the authors theoretically analyze these extensions and backup the effectiveness of their proposed algorithm with encouraging empirical results. all reviewers are highly positive on the contributions of this paper, and appreciate the rigorous theory which is further supported by convincing experiments. all three reviewers recommended accept.", "accepted": 1}
{"paper_id": "iclr_2020_r1lfF2NYvH", "review_text": "this paper proposes a graph embedding method for the whole graph under both unsupervised and semisupervised setting. it can extract a fixed length graphlevel representation with good generalization capability. all reviewers provided unanimous rating of weak accept. the reviewers praise the paper is well written and is value to different fields dealing with graph learning. there are some discussions on the novelty of the approach, which was better clarified after the response from the authors. overall this paper presents a new effort in the active topic of graph representation learning with potential large impact to multiple fields. therefore, the acs recommend it to be an oral paper.", "accepted": null}
{"paper_id": "iclr_2020_r1lZ7AEKvB", "review_text": "the paper focuses on characterizing the expressiveness of graph neural networks. the reviewers were satisfied that the authors answered their questions suffciiently and uniformly agree that this is a strong paper that should be accepted.", "accepted": 1}
{"paper_id": "iclr_2020_ryxjnREFwH", "review_text": "main content blind review 1 summarizes it well this paper presents a semantic parser that operates over passages of text instead of a structured data source. this is the first time anyone has demonstrated such a semantic parser siva reddy and several others have essentially used unstructured text as an information source for a semantic parser, similar to openie methods, but this is qualitatively different. the key insight is to let the semantic parser point to locations in the text that can be used in further symbolic operations. this is excellent work, and it should definitely be accepted. i have a ton of questions about this method, but they are good questions.  discussion the reviews all agree on a generally positive assessment, and focus on details that have been addressed, rather than major problems.  recommendation and justification this paper should be accepted. even though novelty in terms of fundamental machine learning components is minimal, but the architecture employing neural models to do symbolic work is a good contribution in a crucial direction especially in the theme of iclr.", "accepted": 1}
{"paper_id": "iclr_2020_S1lOTC4tDS", "review_text": "this paper presents an approach to modelbased reinforcement learning in highdimensional tasks. the approach involves learning a latent dynamics model, and performing rollouts thereof with an actorcritic model to learn behaviours. this is extensively evaluated on 20 visual control tasks. this paper was favourably received, but there were concerns around it being incremental relative to planet and svg. the authors highlighted the differences in the rebuttal, clarifying the novelty of this work. given the interesting ideas presented, and the convincing results, this paper should be accepted.", "accepted": 1}
{"paper_id": "iclr_2020_S1xCPJHtDB", "review_text": "this paper presents a modelbased rl approach to atari games based on video prediction. the architecture performs remarkably well with a limited amount of interactions. this is a very significant result on a question that engages many in the research community. reviewers all agree that the paper is good and should be published. there is some disagreement about the novelty of it. however, as one reviewer states, the significance of the results is more important than the novelty. many conference attendees would like to hear about it. based on this, i think the paper can be accepted for oral presentation.", "accepted": 1}
{"paper_id": "iclr_2020_ByeGzlrKwH", "review_text": "this paper has a few interesting contributions a a bound for uncompressed networks in terms of the compressed network this is in contrast to some prior work, which only gives bounds on the compressed network; b the use of local rademacher complexity to try to squeeze as much as possible out of the connection; c an application of the bound to a specific interesting favorable condition, namely lowrank structure. as a minor suggestion, id like to recommend that the authors go ahead and use their allowed 10th body page!", "accepted": 1}
{"paper_id": "iclr_2020_Syx79eBKwr", "review_text": "this paper explores several embedding models skipgram, bert, xlnet and describes a framework for comparing, and in the end, unifying them. the framework is such that it actually suggests new ways of creating embeddings, and draws connections to methodology from computer vision. one of the reviewers had several questions about the derivations in your paper and was worried about the papers clarity. but all of the reviewers appreciated the contributions of the paper, which joins multiple seemingly disparite models under into one theoretical framework. the reviewers were positive about the paper, and in particular were happy to see the active response of authors to their questions and willingness to update the paper with their suggested improvements.", "accepted": 1}
{"paper_id": "iclr_2020_rJg8TeSFDH", "review_text": "after the revision, the reviewers agree on acceptance of this paper. lets do it.", "accepted": 1}
{"paper_id": "iclr_2020_S1e2agrFvS", "review_text": "this paper is consistently supported by all three reviewers and thus an accept is recommended.", "accepted": 1}
{"paper_id": "iclr_2020_HkxQRTNYPH", "review_text": "this paper proposes a novel method for considering translations in both directions within the framework of generative neural machine translation, significantly improving accuracy. all three reviewers appreciated the paper, although they noted that the gains were somewhat small for the increased complexity of the model. nonetheless, the baselines presented are already quite competitive, so improvements on these datasets are likely to never be extremely large. overall, i found this to be a quite nice paper, and strongly recommend acceptance, perhaps as an oral presentation.", "accepted": 1}
{"paper_id": "iclr_2020_S1gSj0NKvB", "review_text": "reviewers unanimously accepted this paper.", "accepted": 1}
{"paper_id": "iclr_2020_SJeD3CEFPH", "review_text": "this papers contribution is twofold 1 it proposes a new metarl method that leverages offpolicy metalearning by importance weighting, and 2 it demonstrates that current popular metarl benchmarks dont necessarily require metalearning, as a simple nonmetalearning algorithm td3 conditioned on a context variable of the trajectory is competitive with sota metalearning approaches. the reviewers all agreed that the approach is interesting and the contributions are significant. id like to thank the reviewers for engaging in a spirited discussion about this paper, both with each other and with the authors. there was also a disagreement about the semantics of whether the approach can be classified as metalearning, but in my opinion this argument is orthogonal to the practical contributions. after the revisions and rebuttal, reviewers agreed that the paper was improved and increased their ratings as a result, with all recommending accept. theres a good chance this work will make an impactful contribution to the field of metareinforcement learning and therefore i recommend it for an oral presentation.", "accepted": 1}
{"paper_id": "iclr_2020_Ske31kBtPr", "review_text": "this paper was very well received by the reviewers with solid accept ratings across the board. the subject matter is quite interesting  mathematical reasoning in latent space, and it was suggested by a reviewer that this could be a good candidate for an oral. the ac agrees and recommends acceptance as an oral. some of the intuitions of what is being done in this paper could be better visualized and presented and i encourage the authors to think carefully about how to present this work if an oral presentation is granted by the pcs.", "accepted": 1}
{"paper_id": "iclr_2020_rygixkHKDH", "review_text": "this paper investigates the use nonconvex optimization for two dictionary learning problems, i.e., overcomplete dictionary learning and convolutional dictionary learning. the paper provides theoretical results, associated with empirical experiments, about the fact that, that when formulating the problem as an l4 optimization, gives rise to a landscape with strict saddle points and as such, they can be escaped with negative curvature. as a result, descent methods can be used for learning with provable guarantees. all reviews found the work extremely interesting, highlighting the importance of the results that constitute a solid improvement over the prior understandings on overcomplete dl and extends our understanding of provable methods for dictionary learning. this is an interesting submission on nonconvex optimization, and as such of interest to the ml community of iclr . im recommending this work for acceptance.", "accepted": 1}
{"paper_id": "iclr_2020_BJlQtJSKDB", "review_text": "the paper investigates parallelizing mcts. the authors propose a simple method based on only updating the exploration bonus in puct by taking into account the number of currently ongoing  unfinished simulations. the approach is extensively tested on a variety of environments, notably including atari games. this is a good paper. the approach is simple, well motivated and effective. the experimental results are convincing and the authors made a great effort to further improve the paper during the rebuttal period. i recommend an oral presentation of this work, as mcts has become a core method in rl and planning, and therefore i expect a lot of interest in the community for this work.", "accepted": 1}
{"paper_id": "iclr_2020_BklSwn4tDH", "review_text": "this paper focuses on avoiding overfitting in the presence of noisy labels. the authors develop a two phase method called prestopping based on a combination of early stopping and a maximal safe set. the reviewers raised some concern about illustrating maximal safe set for all data sets and suggest comparisons with more baselines. the reviewers also indicated that the paper is missing key relevant publications. in the response the authors have done a rather through job of addressing the reviewers comments. i thank them for this. however, given the limited time some of the reviewers comments regarding adding new baselines could not be addressed. as a result i can not recommend acceptance because i think this is key to making a proper assessment. that said, i think this is an interesting with good potential if it can outperform other baselines and would recommend that the authors revise and resubmit in a future venue.", "accepted": 0}
{"paper_id": "iclr_2020_SJlDDnVKwS", "review_text": "evolutionary strategies are a popular class of method for blackbox gradientfree optimization and involve iteratively fitting a distribution from which to sample promising input candidates to evaluate. cmaes involves fitting a gaussian distribution and has achieved stateoftheart performance on a variety of blackbox optimization benchmarks when the underlying function is cheap to evaluate. in this work the authors replace this distribution instead with a much more flexible deep generative model i.e. nice. they demonstrate empirically that this method is effective on a number of synthetic global optimization benchmarks e.g. rosenbrock and three direct policy search reinforcement learning problems. the reviewers all believe the paper is above borderline for acceptance. however, two of the reviewers said they were on the low end of their respective scores i.e. one wanted to give a 5 instead of a 6 and another a 7 instead of 8. a major issue among the reviewers was the experiments, which they noted were simple and not very convincing with one reviewer disagreeing. the synthetic global optimization problems do seem somewhat simple. in the rl problems, its not obvious that the proposed method is statistically significantly better, i.e. the error bars are overlapping considerably. thus the recommendation is to reject. hopefully stronger experiments and incorporating the reviewer comments in the manuscript will make this a stronger paper for a future conference.", "accepted": 1}
{"paper_id": "iclr_2020_HJx_d34YDB", "review_text": "there is no author response for this paper. the paper addresses the affective analysis of video sequences in terms of continual emotions of valence and arousal. the authors propose a multimodal approach combining modalities such as audio, pose estimation, basic emotions and scene analysis and a multiscale temporal feature extractor to capture short and long temporal context via lstms to tackle the problem. all the reviewers and ac agreed that the paper lacks 1 novelty, as the proposed approach is a combination of the existing wellstudied techniques without explanations why and when this could be advantageous beyond the considered task, 2 clarity and motivation  see r2s and r3s concerns and suggestions on how to improve. we hope the reviews are useful for improving the paper.", "accepted": 0}
{"paper_id": "iclr_2020_BJgdOh4Ywr", "review_text": "the main concern raised by reviewers is limited novelty, poor presentation, and limited experiments. all the reviewers appreciate the difficulty and importance of the problem. the rebuttal helped clarify novelty, but the other concerns remain.", "accepted": 1}
{"paper_id": "iclr_2020_r1lkKn4KDS", "review_text": "this paper presents a novel option discovery mechanism through incrementally learning reusasble options from a small number of policies that are usable across multiple tasks. the primary concern with this paper was with a number of issues around the experiments. specifically, the reviewers took issue with the definition of novel tasks in the atari context. a more robust discussion and analysis around what tasks are considered novel would be useful. comparisons to other option discovery papers on the atari domains is also required. additionally, one reviewer had concerns on the hard limit of option execution length which remain unresolved following the discussion. while this is really promising work, it is not ready to be accepted at this stage.", "accepted": 0}
{"paper_id": "iclr_2020_SJg4Y3VFPS", "review_text": "the authors propose group connected multilayer perceptron networks which allow expressive feature combinations to learn meaningful deep representations. they experiment with different datasets and show that the proposed method gives improved performance. the authors have done a commendable job of replying to the queries of the reviewers and addresses many of their concerns. however, the main concern still remains the improvements are not very significant on most datasets except the mnist dataset. i understand the authors argument that other papers have also reported small improvements on these datasets and hence it is ok to report small improvements. however, the reviewers and the ac did not find this argument very convincing. given that this is not a theoretical paper and that the novelty is not very high as pointed out by r1 strong empirical results are accepted. hence, at this point, i recommend that the paper cannot be accepted.", "accepted": 0}
{"paper_id": "iclr_2020_HJxf53EtDr", "review_text": "the paper makes an interesting attempt at connecting graph convolutional neural networks gcn with matrix factorization mf and then develops a mf solution that achieves similar prediction performance as gcn. while the work is a good attempt, the work suffers from two major issues 1 the connection between gcn and other related models have been examined recently. the paper did not provide additional insights; 2 some parts of the derivations could be problematic. the paper could be a good publication in the future if the motivation of the work can be repositioned.", "accepted": null}
{"paper_id": "iclr_2020_S1gNc3NtvB", "review_text": "the authors present a method that optimizes a differentiable neural computer with evolutionary search, and which can transfer abstract strategies to novel problems. the reviewers all agreed that the approach is interesting, though were concerned about the magnitude of the contribution  novelty compared to existing work, clarity of contributions, impact of pretraining, and simplicity of examples. while the reviewers felt that the authors resolved the many of their concerns in the rebuttal, there was remaining concern about the significance of the contribution. thus, i recommend this paper for rejection at this time.", "accepted": 0}
{"paper_id": "iclr_2020_B1xDq2EFDH", "review_text": "this paper received two weak and one strong reject from the reviewers. the major issues cited were 1 a lack of strong enough baselines or empirical results, 2 novelty with respect to certified adversarial robustness via randomized smoothing and 3 a limitation to gaussian noise perturbations. unfortunately, as a result the reviewers agreed that this work was not ready for acceptance. adding stronger empirical results and a careful treatment of related work would make this a much stronger paper for a future submission.", "accepted": 0}
{"paper_id": "iclr_2020_Hyx5qhEYvH", "review_text": "this work extends leaky integrate and fire lif by proposing a recurrent version. all reviewers agree that the work as submitted is way too preliminary. prior art is missing many results, presentation is difficult to follow and incomplete and contains errors. even if these concerns were addressed, the benefit of the proposed method is unclear. authors have not responded. we thus recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_ryg7jhEtPB", "review_text": "the authors argue that directly optimizing the is proposal distribution as in rws is preferable to optimizing the iwae multisample objective. they formalize this with an adaptive is framework, aisle, that generalizes rws, iwaestl and iwaedreg. generally reviewers found the paper to be wellwritten and the connections drawn in this paper interesting. however, all reviewers raised concerns about the lack of experiments reviewer 3 suggested several experiments that could be done to clarify remaining questions and practical takeaways. the authors responded by explaining that the main practical takeaway from our work is the following if one is interested in the biasreduction potential offered by iwaes over plain vaes then the adaptive importancesampling framework appears to be a better starting point for designing new algorithms than the specific multisample objective used by iwae. this is because the former retains all of the benefits of the latter without inheriting its drawbacks. i did not find this argument convincing as a primary advantage of variational approaches over ws is that the variational approach optimizes a unified objective. at least in principle, this is a serious drawback of the ws approaches. experiments andor a discussion of this is warranted. this paper is borderline, and unfortunately, due to the high number of quality submissions this year, i have to recommend rejection at this point.", "accepted": 1}
{"paper_id": "iclr_2020_BJeUs3VFPH", "review_text": "three reviewers have scored this paper as 113 and they have not increased their rating after the rebuttal and the paper revision. the main criticism revolves around the choice of datasets, missing comparisons with the existing methods, complexity and practical demonstration of speed. other concerns touch upon a loose bound and a weak motivation regarding the lowrank mechanism in connection to da. on balance, the authors resolved some issues in the revised manuscripts but reviewers remain unconvinced about plenty other aspects, thus this paper cannot be accepted to iclr2020.", "accepted": 0}
{"paper_id": "iclr_2020_BJevihVtwB", "review_text": "this paper introduces a closedform expression for the steins unbiased estimator for the prediction error, and a boosting approach based on this, with empirical evaluation. while this paper is interesting, all reviewers seem to agree that more work is required before this paper can be published at iclr.", "accepted": 0}
{"paper_id": "iclr_2020_BkxFi2VYvS", "review_text": "the paper presents a semisupervised learning approach to handle semantic classification pixellevel classification. the approach extends hung et al. 18, using a confidence map generated by an auxiliary network, aimed to improve the identification of small objects. the reviews state that the paper novelty is limited compared to the state of the art; the reviewers made several suggestions to improve the processing pipeline including all images, including the confidence weights. the reviews also state that the paper needs be carefully polished. the area chair hopes that the suggestions about the contents and writing of the paper will help to prepare an improved version of the paper.", "accepted": 0}
{"paper_id": "iclr_2020_HJlF3h4FvB", "review_text": "this paper tries to bridge early stopping and distillation. 1 in section 2, the authors empirically show more distillation effect when early stopping. 2 in section 3, the authors propose a new provable algorithm for training noisy labels. in the discussion phase, all reviewers discussed a lot. in particular, a reviewer highlights the importance of section 3. on the other hand, other reviewers pointed out what is the role of section 2, as the abstractintro tends to emphasize the content of section 2. i mostly agree all pros and cons pointed out by reviewers. i agree that the paper proposed an interesting idea for refining noisy labels with theoretical guarantees. however, the major reason for my reject decision is that the current writeup is a bit below the borderline to be accepted considering the high standard of iclr, e.g., many typos what is the172norm in page 4? and misleading introabstractorganization. in overall, it was also hard for me to read the paper. i do believe that the paper should be much improved if the authors make more significant editorial efforts considering a more broad range of readers. i have additional suggestions for improving the paper, which i hope are useful.  put section 3 earlier i.e., put section 2 later and revise introabstract so that the reader can clearly understand what is the main contribution.  section 2.1 is weak to claim more distillation effect when early stopping. more experimental or theoretical study are necessary, e.g., you can control temperature parameter t of knowledge distillation to provide the early stopping effect without actual early stopping the choice of t is not mentioned in the draft as it is the important hyperparameter.  more experimental supports for your algorithm should be desirable, e.g., consider more datasets, stateoftheart baselines, noisy types, and neural architectures e.g., nlp models.  softening some sentences for avoiding some potential overclaims to some readers.", "accepted": 1}
{"paper_id": "iclr_2020_Byg-An4tPr", "review_text": "the authors propose a framework for relating adversarial robustness, privacy and utility and show how one can train models to simultaneously attain these properties. the paper also makes interesting connections between the dp literature and the robustness literature thereby porting over composition theorems to this new setting. the paper makes very interesting contributions, but a few key points require some improvement 1 the initial version of the paper relied on an approximation of the objective function in order to obtain dp guarantees. while the authors clarified how the approximation impacts model performance in the rebuttal and revision, the reviewers still had concerns about the utilityprivacyrobustness tradeoff achieved by the algorithm. 2 the presentation of the paper seems tailored to audiences familiar with dp and is not easy for a broader audience to follow. despite this limitations, the paper does make significant novel contributions on an improtant problem simultaneously achieveing privacy, robustness and utility and could be of interest. overall, i consider this paper borderline and vote for rejection, but strongly encourage the authors to improve the paper wrt the above concerns and resubmit to a future venue.", "accepted": 0}
{"paper_id": "iclr_2020_S1gyl6Vtvr", "review_text": "this paper presents a method to learn a pruned convolutional network during conventional training. pruning the network has advantages in deployment of reducing the final model size and reducing the required flops for compute. the method adds a pruning mask on each layer with an additional sparsity loss on the mask variables. the method avoids the cost of a trainpruneretrain optimization process that has been used in several earlier papers. the method is evaluated on cifar10 and imagenet with three standard convolutional network architectures. the results show comparable performance to the original networks with the learned sparse networks. the reviewers made many substantial comments on the paper and most of these were addressed in the author response and subsequent discussion. for example, reviewer1 mentioned two other papers that promote sparsity implicitly during training q3, and the authors acknowledged the omission and described how those methods had less flexibility on a target metric flops that is not parameter size. many of the author responses described changes to an updated paper that would clarify the claims and results r1 q27, r2q3. however, the reviewers raised many concerns for the original paper and they did not see an updated paper that contains the proposed revisions. given the numerous concerns with the original submission, the reviewers wanted to see the revised paper to assess whether their concerns had been addressed adequately. additionally, the paper does not have a comparison experiment with stateof the art results, and the current results were not sufficiently convincing for the reviewers. reviewer1 and author response to questions 1315 suggest that the experimental results with resnet34 are inadequate to show the benefits of the approach, but results for the proposed method with the larger resnet50 which could show benefits are not yet ready. the current paper is not ready for publication.", "accepted": 0}
{"paper_id": "iclr_2020_S1xxx64YwH", "review_text": "this paper investigates how the properties of an environment affect the success of reinforcement learning, and in particular finds that random dynamics and nonepisodic learning makes learning easier, even though these factors make learning more difficult when applied individually. the paper was reviewed by three experts who gave reject, weak reject, and weak reject recommendations. the main concerns are about missing connections to related work, overstating some contributions, and experimental details. while the author response addressed many of these issues, reviewers felt another round of peer review is really needed before this paper can be accepted; r2s postrebuttal comments give some specific, constructive, concrete suggestions for preparing a revision.", "accepted": 0}
{"paper_id": "iclr_2020_rkx1b64Fvr", "review_text": "this paper proposes a cnnbased text classification model that uses words, characters, and labels as its input. it also presents an attention block to replace the pooling operation that is typically used in a cnn. the proposed method is evaluated on six benchmark classification datasets, achieving reasonably good results. while the proposed method performs reasonably well compared to baselines in the papers, all reviewers pointed out that there is no discussion or comparison with existing sota based on pretrained models e.g., bert, xlnet, which would strengthen the main claim of the paper. all three reviewers also suggested that the writing of the paper could be improved. the authors did not respond to these reviews, so there was little discussion needed to arrive at a consensus. i agree with all reviewers and recommend to reject the paper.", "accepted": null}
{"paper_id": "iclr_2020_HJgFW6EKvH", "review_text": "this paper proposes a method called iterative proportional clipping ipc for generating adversarial audio examples that are imperceptible to humans. the efficiency of the method is demonstrated by generating adversarial examples to attack the wav2letter model. overall, the reviewers found the work interesting, but somewhat incremental and analysis of the method and generated samples incomplete, and im thus recommending rejection.", "accepted": 0}
{"paper_id": "iclr_2020_SkgtbaVYvH", "review_text": "the authors propose a method for automatic tuning of learning rates. the reviewers liked the idea but felt that there are much more extensive experiments to be done especially better baselines. also, clarifying what aspect is automated is important, because no method can be truly automatic they all have some hyperparameters.", "accepted": 0}
{"paper_id": "iclr_2020_H1lWzpNKvr", "review_text": "this paper tackles the multivariate bandit problem akin to a factorial experiment where the player faces a sequence of decisions that can be viewed as a tree before obtaining a reward. the authors introduce a framework combining thompson sampling with path planning in treesgraphs. more specifically, they consider four path planning strategies, leading to four approaches. the resulting approaches are empirically evaluated on synthetic settings. unfortunately, the proposed approaches lack theoretical justification and the current experiments are not strong enough to support the claims made in the paper. given that most of reviewers concerns remained valid after rebuttal, i recommend to reject this paper.", "accepted": 0}
{"paper_id": "iclr_2020_SJgzXaNFwS", "review_text": "the reviewers were unanimous that this submission is not ready for publication at iclr in its present form. concerns raised included lack of relevant baselines, and lack of sufficient justification of the novelty and impact of the approach.", "accepted": 0}
{"paper_id": "iclr_2020_HJeIX6EKvr", "review_text": "the authors present an approach to learning from noisy labels. the reviews were mixed and several issues remain unresolved. i do not accept the following as a valid response we fully agree that noisily collected labels are common for many problems other than image classification. however, the focus of our paper is image classification, and we thus concentrate on classification problems related to the widely popular cifar10 and imagenet classification problems. iclr is a conference on theoretical and applied ml, and the fact that a technique has not been used for image classification before, does not mean you bring something to the table by doing so. the nlp literature is abundant with interesting work on label noise and should obviously be considered related work. that said, theres also missing references directly related to the connection between early stoppingregularization and label bias correction, including 0 httpsarxiv.orgpdf1904.11238.pdf 1 httpsarxiv.orgpdf1705.03419.pdf 2 httpproceedings.mlr.pressv80ma18dma18d.pdf see also this paper submitted to this conference httpsopenreview.netforum?idsjldu6etds", "accepted": 0}
{"paper_id": "iclr_2020_rygw7aNYDS", "review_text": "this paper examined a pure exploration method for efficient action value estimates in tabular reinforcement learning. the paper is on the theoretical properties of value estimates in the large sample regime. the method is shown to outperform baseline algorithms for this task in tabular reinforcement learning. the reviewers were divided on the merits of this work. the use of the central limit theorem was viewed as elegant, and the results were thought to be potentially useful. however, the reviewers several limitations. they found the assumption of a communicating mdp to be overly restrictive reviewer 1. the algorithm may be computationally inefficient reviewer 2. the nature of exploration in this work is not the conventional meaning in reinforcement learning reviewer 3. the paper is not yet ready for publication at iclr. the theoretical results do not clearly convey insights for reinforcement learning with function approximation, and the reviewers are also not in agreement that the current results are applicable to a general mdp setting.", "accepted": 0}
{"paper_id": "iclr_2020_rkl44TEtwH", "review_text": "the submission presents a semiparametric approach to motion synthesis. the reviewers expressed concerns about the presentation, the relationship to existing work, and the scope of the results. after the authors responses and revision, concerns remain. the ac also notes that the submission is 10 pages long. the ac recommends rejecting the submission.", "accepted": 0}
{"paper_id": "iclr_2020_B1liraVYwr", "review_text": "this paper tackles neural response generation with generative adversarial nets gans, and to address the training instability problem with gans, it proposes a local distribution oriented objective. the new objective is combined with the original objective, and used as a hybrid loss for the adversarial training of response generation models, named as localgan. authors responded with concerns about reviewer 3s comments, and i agree with the authors explanation, so i am disregarding review 3, and am relying on my read through of the latest version of the paper. the other reviewers think the paper has good contributions, however they are not convinced about the clarity of the presentations and made many suggestions even after the responses from the authors. i suggest a reject, as the paper should include a clear presentation of the approach and technical formulation as also suggested by the reviewers.", "accepted": 0}
{"paper_id": "iclr_2020_SyepHTNFDS", "review_text": "the authors propose a graph residual flow model for molecular generation. conceptual novelty is limited since it is simple extension and there isnt much improvement over state of art.", "accepted": 0}
{"paper_id": "iclr_2020_HkxAS6VFDB", "review_text": "the authors propose a hardwareagnostic metric called effective signal norm esn to measure the computational cost of convolutional neural networks. they then demonstrate that models with fewer parameters achieve far better accuracy after quantization. the main novelty is on the metric esn. however, esn is based on ideal hardware, and thus not suitable for existing hardware. assumptions made in the paper are hard to be proved. experimental results are not convincing, and related pruning methods are not compared. finally, the paper is not written clearly, and the structure and some arguments are confusing.", "accepted": 0}
{"paper_id": "iclr_2020_SJxeI6EYwS", "review_text": "this paper proposes to use stacked layers of gaussian latent variables with a maxent objective function as a regulariser. i agree with the reviewers that there is very little novelty and the experiments are not very convincing.", "accepted": 0}
{"paper_id": "iclr_2020_B1em8TVtPr", "review_text": "this paper proposes a new benchmark to evaluate natural language processing models on discourserelated tasks based on existing datasets that are not available in other benchmarks sentevalgluesuperglue. the authors also provide a set of baselines based on bert, elmo, and others; and estimates of human performance for some tasks. i think this has the potential to be a valuable resource to the research community, but i am not sure that it is the best fit for a conference such as iclr. r3 also raises a valid concern regarding the performance of finetuned bert that are comparable to human estimates on half of the tasks 3 out of 5, which slightly weakens the main motivation of having this new benchmark. my main suggestion to the authors is to have a very solid motivation for the new benchmark, including the reason of inclusion for each of the tasks. i believe that this is important to encourage the community to adopt it. for something like this, it would be nice although not necessary to have a clean website for submission as well. i believe that someone who proposes a new benchmark needs to do as best as they can to make it easy for other people to use it. due to the above issues and space constraint, i recommend to reject the paper.", "accepted": null}
{"paper_id": "iclr_2020_Bkl086VYvH", "review_text": "the paper received scores of wr r1 wr r2 wa r3, although r3 stated that they were borderline. the main issues were i lack of novelty and ii insufficient experiments. the ac has closely look at the reviewscommentsrebuttal and examined the paper. unfortunately, the ac feels that with noone strongly advocating for acceptance, the paper cannot be accepted at this time. the authors should use the feedback from reviewers to improve their paper.", "accepted": 0}
{"paper_id": "iclr_2020_ryg8wpEtvB", "review_text": "the paper investigates calibration for regression problems. the paper identifies a shortcoming of previous work by kuleshov et al. 2018 and proposes an alternative. all the reviewers agreed that while this is an interesting direction, the paper requires more work before it can be accepted. in particular, the reviewers raised several concerns about motivation, clarity of the presentation and lack of indepth empirical evaluation. i encourage the authors to revise the draft based on the reviewers feedback and resubmit to a different venue.", "accepted": 0}
{"paper_id": "iclr_2020_SyevDaVYwr", "review_text": "while two reviewers rated this paper as an accept, reviewer 3 strongly believes there are unresolved issues with the work as summarized in their postrebuttal review. this work seems very promising and while the ac will recommend rejection at this time, the authors are strongly encouraged to resubmit this work.", "accepted": 1}
{"paper_id": "iclr_2020_rJxwDTVFDB", "review_text": "the reviewers have uniformly had significant reservations for the paper. given that the authors did not even try to address them, this suggests the paper should be rejected.", "accepted": 0}
{"paper_id": "iclr_2020_HJxJdp4YvS", "review_text": "the authors present a deep model for probabilistic clustering and extend it to handle time series data. the proposed method beats existing deep models on two datasets and the representations learned in the process are also interpretable. unfortunately, despite detailed responses by the authors, the reviewers felt that some of their main concerns were not addressed. for example, the authors and the reviewers are still not converging on whether somvae uses a vae or an autoencoder. further, the discussion about the advantages of vae over ae is still not very convincing. currently the work is positioned as a variational clustering method but the reviewers feel that it is a clustering method which uses a vae yes, i understand that this difference is subtle but needs to be clarified. the reviewers read the responses of the author and during discussions with the ac suggested that there were still not convinced about some of their initial questions. given this, at this point i would prefer going by the consensus of the reviewers and recommend that this paper cannot be accepted.", "accepted": 0}
{"paper_id": "iclr_2020_Syl-_aVtvH", "review_text": "this manuscript personalization techniques to improve the scalability and privacy preservation of federated learning. empirical results are provided which suggests improved performance. the reviewers and ac agree that the problem studied is timely and interesting, as the tradeoffs between personalization and performance are a known concern in federated learning. however, this manuscript also received quite divergent reviews, resulting from differences in opinion about the novelty and clarity of the conceptual and empirical results. reviewers were also unconvinced by the provided empirical evaluation results.", "accepted": 1}
{"paper_id": "iclr_2020_S1xI_TEtwS", "review_text": "the paper proposes a modification for adversarial training in order to improve the robustness of the algorithm by developing an annealing mechanism for pgd adversarial training. this mechanism gradually reduces the step size and increases the number of iterations of pgd maximization. one reviewer found the paper to be clear and competitive with existing work, but raised concerns of novelty and significance. another reviewer noted the significant improvements in training times but had concerns about small scale datasets. the final reviewer liked the optimal control formulation, and requested further details. the authors provided detailed answers and responses to the reviews, although some of these concerns remain. the paper has improved over the course of the review, but due to a large number of stronger papers, was not accepted at this time.", "accepted": 0}
{"paper_id": "iclr_2020_SJeItTEKvr", "review_text": "all reviewers agreed that this submission is still premature to be accepted to iclr2020. we hope the review comments are useful for improving your paper for potential future submission.", "accepted": 0}
{"paper_id": "iclr_2020_Byx55pVKDB", "review_text": "the paper investigates how the softmax activation hinders the detection of outofdistribution examples. all the reviewers felt that the paper requires more work before it can be accepted. in particular, the reviewers raised several concerns about theoretical justification, comparison to other existing methods, discussion of connection to existing methods and scalability to larger number of classes. i encourage the authors to revise the draft based on the reviewers feedback and resubmit to a different venue.", "accepted": 0}
{"paper_id": "iclr_2020_BJlbo6VtDH", "review_text": "this paper proposes a generalized way to generate sequences from undirected sequence models. overall, i believe a framework like this could definitely be a valuable contribution, but as reviewer 1 and reviewer 3 noted, the paper is a bit lacking both in theoretical analysis and strong empirical results. i dont think that this is a bad paper at all, but it feels like the paper needs a little bit of an extra push to tighten up the argumentation andor results before warranting publication at a premier venue such as iclr. id suggest the authors continue to improve the paper and aim to resubmit at revised version at a future conference.", "accepted": 0}
{"paper_id": "iclr_2020_H1l3s6NtvH", "review_text": "the paper studies how adversarial robustness and bayes optimality relate in a simple gaussian mixture setting. the paper received two recommendations for rejection and one weak accept. one of the central complaints was whether the study had any bearing on real world adversarial examples. i think this is a fair concern, given how limited the model appears on the surface, although perhaps the model is a good model of any local piece of a decision boundary in a real problem. that said, i do not agree with the strong rejection 1 in most places. the weak reject asked for some experiments. the revision produced these experiments, but im not sure how convincing these are since only one robust training method was used, and its not clear that its the best one could do among sota methods. for whatever reason, the reviewers did not update their scores. i am not certain that they reviewed the revision, despite my prodding.", "accepted": 0}
{"paper_id": "iclr_2020_Hke0oa4KwS", "review_text": "the paper proposes to model uncertainty using expected bayes factors, and empirically show that the proposed measure correlates well with the probability that the classification is correct. all the reviewers agreed that the idea of using bayes factors for uncertainty estimation is an interesting approach. however, the reviewers also found the presentation a bit hard to follow. while the rebuttal addressed some of these concerns, there were still some remaining concerns see r3s comments. i think this is a really promising direction of research and i appreciate the authors efforts to revise the draft during the rebuttal which led to some reviewers increasing the score. this is a borderline paper right now but i feel that the paper has the potential to turn into a great paper with another round of revision. i encourage the authors to revise the draft and resubmit to a different venue.", "accepted": null}
{"paper_id": "iclr_2020_Sklyn6EYvH", "review_text": "this paper that defines a residual learning mechanism as the training regime for variational autoencoder. the method gradually activates individual latent variables to reconstruct residuals. there are two main concerns from the reviewers. first, residual learning is a common trick now, hence authors should provide insights on why residual learning works for vae. the other problem is computational complexity. currently, reviews argue that it seems not really fair to compare to a bruteforce parameter search. the authors rebuttal partially addresses these problems but meet the standard of the reviewers. based on the reviewers comments, i choose to reject the paper.", "accepted": 1}
{"paper_id": "iclr_2020_Byl-264tvr", "review_text": "the authors propose an endtoend object tracker by exploiting the attention mechanism. two reviewers recommend rejection, while the last reviewer is more positive. the concerns brought up are novelty last reviewer, and experiments second reviewer. furthermore, the authors seem to overclaim their contribution. there indeed are endtoend multiobject trackers, see frossard  urtasuns work for example. this work needs to be cited, and possibly a comparison is needed. since the paper did not receive favourable reviews and there are additional citations missing, this paper cannot be accepted in current form. the authors are encouraged to strengthen their work and resubmit to a future venue.", "accepted": 0}
{"paper_id": "iclr_2020_BJxt2aVFPr", "review_text": "the paper proposes an iterative method that jointly trains the model and a scorer network that places a nonuniform distribution over data sets. the paper proposes a gradient method to learn the scorer network based on reinforcement learning, which is novel as to what the reviewer knows. there are several concernsquestions 1 the paper doesnt define the d_dev clearly. how is d_dev chosen? is it a subset of d_train? 2 in section 2.1, why smaller development set d_dev is much closer to the p_testx,y? p_testx,y is supposed to be not observed during training? 3 in eq 5, if d_dev is s subset of d_train, if theta is the minimal of j, it means the gradient at theta is 0. to calculate the gradient of j with respect to psi, by chain rule, it need to calculate gradient to theta first then theta to psi. if gradient of theta is 0, the product is also 0? so the psi will not be updated if d_dev is sufficiently similar to d_train ? 4 in section 2.3, it omits the second order hessian term. how does that influence the performance? 5 it mentions without significant computing overhead in abstract, which is not demonstrated elsewhere. 6 in the experiments, table 1, it seems the major improvement comes from retrain and tcs rather than dds? in figure 3, it is better to show the weights of an image without dds and comparing that with dds. 7 the paper contains many typos such as eqn.11 is not defined in the main paper, the eqn ?? appears in the appendix, tha minimizes etc. in general, the idea of the paper is natural and the results seem promising. i am looking forward to the reply to my questionsconcerns.  i have read the authors feedback. i think the clarity of both methodology and experiment does not reach the acceptance level and would maintain my current rating. this paper presents a reinforcement learning approach towards using data that present best correlation with a validation sets gradient signal. the broader point of this paper is that there is inevitably some distribution shift going from train to test set  and the validation set can be a small curated set whose distribution is closer to the testing distribution than what the training datasets distribution is. the problem setup bears relationship to several areas including domain adaptationcovariate shift problems, curriculum learning based approaches amongst others. one assumption that i see which needs to be understood more is equation 6  wherein, somehow, there is a markov assumption used to zero out the contribution of the scoring network on parameters unto previous time step. trying to understand the implications of this assumption how the performance varies withwithout this assumption would be instructive for understanding potential shortcomings of this framework. i think the paper is well written, handles an important question. that said, i am not too aware of recent work in this area to make a decisive judgement on this papers noveltycontributions. summary this paper introduces a simple idea to optimize the weights of a weighted empirical training distributions. the goal is to optimize the population risk, and the idea is to optimize a distribution over the training examples to maximize the cosine similarity between training set gradients and validation set gradients. the distribution over the training set is parameterized by a neural network taking as arguments the strengths  the method is quite simple.  the results appear to be strong, although i am less familiar with the nmt baselines. the imagenet results seem quite strong to me. weaknesses  i couldnt find a particularly clear description of the scoring networks architecture. given that it observes the whole dataset, this seems like a critical choice that could have a big impact on the complexity of this approach. at the very least, this should be clearly reported, and i recommend a more thorough investigation of this choice.  the authors report that their method takes 1.5x to 2x longer to run than the uniform baseline. yet, they ran all methods for the same number of steps  epochs. it seems to me that a fairer comparison might be letting all methods enjoy the same total budget measure roughly by wall time. questions  i didnt follow why the computation of the per example gradient grad lx_i, y_i, theta_t1 is so onerous. isnt that computed on line 5 already? the paper proposes an iterative learning method that jointly trains both a model and a scorer network that places a nonuniform weights on data points, which estimates the importance of each data point for training. this leads to significant improvement on several benchmarks. the reviewers mostly agreed that the approach is novel and that the benchmark results were impressive, especially on imagenet. there were both clarity issues about methodology and experiments, as well as concerns about several technical issues. the reviewers felt that the rebuttal resolved the majority of minor technical issues, but did not sufficiently clarify the more significant methodological concerns. thus, i recommend rejection at this time.", "accepted": 0}
{"paper_id": "iclr_2020_rylVTTVtvH", "review_text": "the paper proposes a tensorbased extension to graph convolutional networks for prediction over dynamic graphs. the proposed model is reasonable and achieves promising empirical results. after discussion, it is agreed that while the problem of handling dynamic graphs is interesting and challenging, the proposed tensor method lacks novelty, the theoretical analysis is artificial, and the empirical study does not cover enough benchmarks. the current version of the paper is not ready for publication. addressing the issues above could lead to a strong publication in the future.", "accepted": 0}
{"paper_id": "iclr_2020_B1eoyAVFwH", "review_text": "this paper considers how to create efficient architectures for multitask neural networks. r1 recommends weak reject, identifying concerns about the clarity of writing, unsupported claims, and missing or unclear technical details. r2 recommends weak accept but calls this a borderline case, and has concerns about experiments and comparisons to baselines. r3 also has concerns about experiments and baselines, and feels the approach is somewhat ad hoc. the authors submitted a response that addressed some of these issues, but the authors chose to maintain their decisions. the ac feels the paper has merit but given these slightly negative to borderline reviews, we cannot recommend acceptance at this time. we hope the reviewer comments help the authors to prepare a revision for another venue.", "accepted": 0}
{"paper_id": "iclr_2020_H1e31AEYwB", "review_text": "while there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at iclr in its present form. concerns raised include lack of sufficient motivation for the approach, and problems with clarity of the exposition.", "accepted": 0}
{"paper_id": "iclr_2020_Hkxvl0EtDH", "review_text": "this paper attempts to present a causal view of robustness in classifiers, which is a very important area of research. however, the connection to causality with the presented model is very thin and, in fact, mathematically unnecessary. interventions are only applied to root nodes as pointed out by r4 so they just amount to standard conditioning on the variable m. the experimental results could be obtained without any mention to causal interventions.", "accepted": 1}
{"paper_id": "iclr_2020_H1l_gA4KvH", "review_text": "the paper is not overly well written and motivated. a guiding thread through the paper is often missing. comparisons with constrained bo methods would have improved the paper as well as a more explicit link to multiobjective bo. it could have been interesting to evaluate the sensitivity w.r.t. the number of samples in the monte carlo estimate. what happens if the observations of the function are noisy? is there a natural way to deal with this? given that the paper is 10 pages long, we expect a higher quality than an 8pages paper reviewing and submission guidelines.", "accepted": 0}
{"paper_id": "iclr_2020_BkeaxAEKvB", "review_text": "while there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at iclr in its present form. concerns were raised as to the generality of the approach, thoroughness of experiments, and clarity of the exposition.", "accepted": 0}
{"paper_id": "iclr_2020_S1lRg0VKDr", "review_text": "the reviewers reached a consensus that the paper is preliminary and has a very limited contribution. therefore, i cannot recommend acceptance at this time.", "accepted": 0}
{"paper_id": "iclr_2020_HJlU-AVtvS", "review_text": "the authors develop a spectral analysis on the boolean cube for the neural conjugate kernel ck and tangent kernel ntk. the analysis sheds light into inductive biases of neural networks, such as whether they are biased to simple functions. this work contains rigorous analysis and theory which is useful for further discussions. however, the theory and insights do not feel complete. one important drawback is that the analysis is limited by the boolean cube setting; this also means that it is more difficult to link theory to practical scenarios. this has been discussed a lot during the rebuttal and among reviewers. empirical validation has attempted to deal with these concerns, but it would be useful to have this validation coming from theory, or at least have further relevant theoretical insights. this could happen by further building on the theorem provided in the rebuttal for eigenvalue behavior when d is large.", "accepted": 0}
{"paper_id": "iclr_2020_SJlRWC4FDB", "review_text": "this paper shows a case study of an adversarial attack on a copyright detection system. the paper implements a music identification method with a simple convolutional neural network, and shows that it is possible to fool such cnn with an adversarial learning. after the discussion period, two among three reviewers incline to the rejection of the paper. although the majority of the reviewers agree that this is an interesting problem with an important application, they also find many of their concerns remain unaddressed. these include the generality of the finding as the current paper is more like a proofofconcept that blackwhitebox attack can work for copyright system. the reviewers are also concerned that the technique solutionfinding is not novel as it is very similar to prior work in other domains e.g., image classification. one reviewer was particularly concerned about that the user study is missing, making it difficult to judge whether the quality of the modified audio is reasonable or not.", "accepted": 0}
{"paper_id": "iclr_2020_H1eWGREFvB", "review_text": "this paper proposes a new sampling mechanism which uses a selfrepulsive term to increase the diversity of the samples. the reviewers had concerns, most of which were addressed in the rebuttal. unfortunately, none of the reviewers genuinely championed the paper. since there were a lot of good submissions this year, we had to make decisions on the borderline papers and this lack of full support means that this submission will be rejected. i highly encourage you to keep updating the manuscript and to rebusmit it to a later conference.", "accepted": 0}
{"paper_id": "iclr_2020_rkxUfANKwB", "review_text": "the paper proposes all smiles vae which can capture the chemical properties of small molecules and also optimize the structures of these molecules. the model achieves significantly performance improvement over existing methods on the zinc250k and tox21 datasets. overall it is a very solid paper  it addresses an important problem, provides detailed description of the proposed method and shows promising experiment results. the work could be a landmark piece, leading to major impacts in the field. however, given its potential, the paper could benefit from major revisions of the draft. below are some suggestions on improving the work 1. the current version contains a lot of materials. it tries to strike the balance between machine learning methodology and details of the application domain. but the reality is that the lack of architecture details and some sloppy definitions of ml terms make it hard for readers to fully appreciate the methodology novelty. 2. there is still room for improvement in experiments. as suggested in the review, more datasets should be used to evaluate the proposed model. since it is hard to provide theoretic analysis of the proposed model, extensive experiments should be provided. 3. the complexity analysis is not fully convincing. some fair comparison with the alternative approaches should be provided. in summary, it is a paper with big potentials. the current version is a step away from being ready for publication. we hope the reviews can help improve the paper for a strong publication in the future.", "accepted": 0}
{"paper_id": "iclr_2020_BJlaG0VFDH", "review_text": "this paper proposes to apply regularizers such as weight decay or weight noise only periodically, rather than every epoch. it investigates how the nonregularization period, or period between regularization steps, interacts with other hyperparameters. overall, the writing feels somewhat scattered, and it is hard to identify a clear argument for why the nrp should help. certainly one could save computation this way, but regularizers like weight decay or weight noise incur only a small computational cost anyway. one explicit claim from the paper is that a higher nrp allows larger regularization. theres a sense in which this is demonstrated, though not a very interesting sense figure 4 shows that the weight decay strength should be adjusted proportionally to the nrp. but varying the parameters in this way simply results in an unbiased but noisier estimate of gradients of exactly the same regularization penalty, so i dont think theres much surprising here. similarly, section 3 argues that a higher nrp allows for larger stochastic perturbations, which makes it easier to escape local optima. but this isnt demonstrated experimentally, nor does it seem obvious that stochasticity will help find a better local optimum. overall, i think this paper needs substantial cleanup before its ready to be published at a venue such as iclr.", "accepted": 1}
{"paper_id": "iclr_2020_r1e0G04Kvr", "review_text": "this paper studies a problem of graph translation, which aims at learning a graph translator to translate an input graph to a target graph using adversarial training framework. the reviewers think the problem is interesting. however, the paper needs to improve further in term of novelty and writing.", "accepted": 1}
{"paper_id": "iclr_2020_rkgqm0VKwB", "review_text": "this paper presents an endtoend technique for named entity recognition, that uses pretrained models so as to avoid long training times, and evaluates it against several baselines. the paper was reviewed by three experts working in this area. r1 recommends reject, giving the opinion that although the paper is wellwritten and results are good, they feel the technique itself has little novelty and that the main reason the technique works well is using bert. r2 recommends weak reject based on similar reasoning, that the approach consists of existing components albeit combined in a novel way and suggest some ablation experiments to isolate the source of the good performance. r3 recommends weak accept but feels it is unsurprising that bert allows for faster training and higher accuracy. in their response, authors emphasize that the application of pretraining to named entity recognition is new, and that theirs is a methodological advance, not purely a practical one as r1 suggests and other reviews imply. they also argue it is not possible to do a fair ablation study that removes bert, but make an attempt. the reviewers chose to keep their scores after the response. given the split decision, the ac also read the paper. it is clear the paper has significant merit and significant practical value, as the reviews indicate. however, given that three expert reviewers  all of whom are nlp researchers at top institutions  feel that the contribution of the paper is weak in the context of the expectations of iclr makes it not possible for us to recommend acceptance at this time.", "accepted": 0}
{"paper_id": "iclr_2020_BJlJVCEYDB", "review_text": "this paper proposes a deep rl framework that incorporates motivation as input features, and is tested on 3 simplified domains, including one which is presented to rodents. while r2 found the paper wellwritten and interesting to read, a common theme among reviewer comments is that its not clear what the main contribution is, as it seems to simultaneously be claiming a ml contribution motivation as a feature input helps with certain tasks as well as a neuroscientific contribution their agent exhibited representations that clustered similarly to those in animals. in trying to do both, its perhaps doing both a disservice. i think its commendable to try to bridge the fields of deep rl and neuroscience, and this is indeed an intriguing paper. however any such paper still needs to have a clear contribution. it seems that the ml contributions are too slight to be of general practical use, while the neuroscientific contributions are muddled somewhat. the authors several times mentioned the space constraints limiting their explanations. perhaps this is an indication that they are trying to cover too much within one paper. i urge the authors to consider splitting it up into two separate works in order to give both the needed focus. i also have some concerns about the results themselves. r1 and r3 both mentioned that the comparison between the nonmotivated agent and the motivated agent wasnt quite fair, since one is essentially only given partial information. its therefore not clear how we should be interpreting the performance difference. second, why was the nonmotivated agent not analyzed in the same way as the motivated agent for the pavlovian task? isnt this a crucial comparison to make, if one wanted to argue that the motivational salience is key to reproducing the representational similarities of the animals? the new experiment with the random fixed weights is interesting, i would have liked to see those results. for these reasons and the ones laid out in the extensive comments of the reviewers, im afraid i have to recommend reject.", "accepted": 0}
{"paper_id": "iclr_2020_rklw4AVtDH", "review_text": "the paper introduces a variant of amsgrad optimisticamsgrad, which integrates an estimate of the future gradient into the optimization problem. while the method is interesting, reviewers agree that novelty is on the low side. the motivation of the approach should also be clarified. the experimental section should be made stronger; in particular, reporting convincing wallclock running time advantages is critical for validating the viability of the proposed approach.", "accepted": 0}
{"paper_id": "iclr_2020_HJg_ECEKDr", "review_text": "overview this paper introduces a method to distill a large dataset into a smaller one that allows for faster training. the main application of this technique being studied is neural architecture search, which can be sped up by quickly evaluating architectures on the generated data rather than slowly evaluating them on the original data. summary of discussion during the discussion period, the authors appear to have updated the paper quite a bit, leading to the reviewers feeling more positive about it now than in the beginning. in particular, in the beginning, it appears to have been unclear that the distillation is merely used as a speedup trick, not to generate additional information out of thin air. the reviewers scores left the paper below the decision boundary, but closely enough so that i read it myself. my own judgement i like the idea, which i find very novel. however, i have to push back on the authors claims about their good performance in nas. this has several reasons 1. in contrast to what is claimed by the authors, the comparison to graph hypernetworks zhang et al is not fair, since the authors used a different protocol zhang et al sampled 800 networks and reported the performance mean  std of the 10 judged to be best by the hypernetwork. in contrast, the authors of the current paper sampled 1000 networks and reported the performance of the single one judged to be best. they repeated this procedure 5 times to get mean  std. the best architecture of 1000 is of course more likely to be strong than the average of the top 10 of 800. 2. the comparison to random search with weight sharing here 3.92 error does not appear fair. the cited paper in table 1 is not the paper introducing random search  weight sharing, but the neural architecture optimization paper. the original one reported an error of 2.85  0.08 with 4.3m params. that paper also has the full source code available, so the authors could have performed a true applestoapples comparison. 3. the authors method requires an additional onetime cost for actually creating the fake training data, so their runtimes should be increased by the 8h required for that. 4. the fact that the authors achieve 2.42 error doesnt mean much; that result is just based on scaling the network up to 100m params. the network obtained by random search also achieves 2.51. as it stands, i cannot judge whether the authors approach yields strong performance for nas. in order to allow that conclusion, the authors would have to compare to another method based on the same underlying code base and experimental protocol. also, the authors do not make code available at this time. their method has a lot of bells and whistles, and i do not expect that i could reproduce it. they promise code, but it is unclear what this would include the generated training data, code for training the networks, code for the metaapproach, etc? this would have been much easier to judge had the authors made the code available in anonymized fashion during the review. because of these reasons, in terms of making progress on nas, the paper does not quite clear the bar for me. the authors also evaluated their method in several other scenarios, including reinforcement learning. these results appear to be very promising, but largely preliminary due to lack of time in the rebuttal phase. recommendation the paper is very novel and the results appear very promising, but they are also somewhat preliminary. the reviewers scores leave the paper just below the acceptance threshold and my own borderline judgement is not positive enough to overrule this. i believe that some more time, and one more iteration of reorganization and review, would allow this paper to ripen into a very strong paper. for a resubmission to the next venue, i would recommend to either perform an applestoapples comparison for nas or reorganize and just use nas as one of several equallyweighted possible applications. in the current form, i believe the paper is not using its full potential.", "accepted": 0}
{"paper_id": "iclr_2020_SyxjVRVKDB", "review_text": "this paper proposes a method to capture patterns of the so called off neurons using a newly proposed metric. the idea is interesting and worth pursuing. however, the paper needs another round of modification to improve both writing and experiments.", "accepted": 0}
{"paper_id": "iclr_2020_SkeXL0NKwH", "review_text": "the reviewers generally agreed that the novelty of the work was very limited. this is not necessarily a dealbreaker for a largely applied contribution, but for an applied paper, the evaluation of the actual application on edge devices is not present. so if the main contribution is the application, and there is no evaluation of this application, then it does not seem like the paper is really complete. as such, i cannot recommend it for acceptance.", "accepted": 0}
{"paper_id": "iclr_2020_ryxF80NYwS", "review_text": "this paper uses neural amortized inference for clustering processes to automatically tune the number of clusters based on the observed data. the main contribution of the paper is the design of the posterior parametrization based on the deepset method. the reviewers feel that the paper has limited novelty since it mainly follows from existing methodologies. also, experiments are limited and not all comparisons are made.", "accepted": 0}
{"paper_id": "iclr_2020_Ske5UANYDB", "review_text": "the authors show that data interpolation in the context of nearest neighbor algorithms, can sometime strictly improve performance. the paper is poorly written for an iclr audience and the added value compared to extensive prior work in the area is not clearly demonstrated.", "accepted": 0}
{"paper_id": "iclr_2020_rkgiURVFDS", "review_text": "the authors develop a certified defense for labelflipping attacks where an adversary can flip labels of a small number of training set samples based on the randomized smoothing technique developed for certified defenses to adversarial perturbations of the input. the framework applies to leastsquares classifiers acting on pretrained features learned by a deep network. the authors show that the resulting framework can obtain significant improvements in certified accuracy against targeted label flipping attacks for each test example. while the paper makes some interesting contributions, the reviewers had the following shared concerns regarding the paper 1 reality of threat model the threat model assumes that the adversary has access to the model and all of the training data so as to choose which labels to flip, which is very unlikely in practice. 2 limitation to least squares on pretrained features the only practical instantiation of the framework presented in the paper is on least squares classifiers acting on pretrained features learned by a deep network. in the rebuttal phase, the authors clarified some of the more minor concerns raised by the reviewers, but the above concerns remained. overall, i feel that this paper is borderline  if the authors extend the applicability of the framework for example relaxing the restriction on pretraining the deep features and motivating the threat model more strongly, this could be an interesting paper.", "accepted": 0}
{"paper_id": "iclr_2020_rygoURNYvS", "review_text": "the paper presents cubert code understanding bert, which is bertinspired pretrainingfinetuning setup, for source code contextual embedding. the embedding results are tested on classification tasks to demonstrate the effectiveness of cubert. this is an interesting application paper that extends existing models to source code analysis. the authors did a good job at motivating the applications, describing the proposed models and discussing the experiments. the authors also agree to share all the datasets and source code so that the experiment results can be replicated and compared with by other researchers. one major concern is the lack of strong baselines. all reviewers are concerned about this issue. the paper could lead to a good publication in the future if the issues can be addressed.", "accepted": 0}
{"paper_id": "iclr_2020_BklMDCVtvr", "review_text": "this work builds directly on mccoy et al. 2019a and add a rnn that can replace what was human generated hypotheses to the role schemes. the final goal of role is to analyze a network by identifying symbolic structure. the authors conduct sanity check by conducting experiments with ground truth, and extend the work further to apply it to a complex model. i wonder under what definition of interpretable authors have in mind with the final output figure 2  the output is very complex. it remains questionable if this will give some insight or how would humans parse this info such that it is useful for them in some way. overall, though this is a good paper, due to the number of strong papers this year, it cannot be accepted at this time. we hope the comments given by reviewers can help improve a future version.", "accepted": 0}
{"paper_id": "iclr_2020_ryg7vA4tPB", "review_text": "a somewhat new approach to growing sparse networks. experimental validation is good, focussing on imagenet and cifar10, plus experiments on language modelling. though efficient in computation and storage size, the approach does not have a theoretical foundation. that does not agree with the intended scope of iclr. i strongly suggest the authors submit elsewhere.", "accepted": 0}
{"paper_id": "iclr_2020_rJlVdREKDS", "review_text": "the paper introduces a novel way of jointly modeling annotator competencies and learning from imperfect annotations. reviewers were moderately positive. one reviewer mentioned carpenter 2002 and subsequent work. one prominent example of this line of work, which the authors do not cite, is httpswww.isi.edupublicationslicensedswmace  from 2013. i encourage the authors to cite this paper. in the discussion, the authors point out this type of work is not endtoend in their sense. however, theres, to the best of my knowledge, a relatively big body of literature on endtoend approaches that the authors completely ignore, e.g., 03. in the absence of a discussion of this work, it is hard to accept the paper. 0 httpslink.springer.comarticle10.1007s1099401354112 1 httpsieeexplore.ieee.orgstampstamp.jsp?arnumber7405343 2 httpwww.cs.utexas.eduatnnguyenacl17.pdf 3 httpsarxiv.orgpdf1803.04223.pdf", "accepted": null}
{"paper_id": "iclr_2020_HkxSOAEFDB", "review_text": "two reviewers are negative on this paper while the other one is slightly positive. overall, the paper does not make the bar of iclr and thus a reject is recommended.", "accepted": 0}
{"paper_id": "iclr_2020_SJlPOCEKvH", "review_text": "this work explores weight pruning for bert in three broad regimes of transfer learning low, medium and high. overall, the paper is well written and explained and the goal of efficient training and inference is meaningful. reviewers have major concerns about this work is its technical innovation and value to the community a reuse of pruning to bert is not new in technical perspective, the marginal improvement in pruning ratio compared to other compression method for bert, and the introduced sparsity that hinders efficient computation for modern hardware such as gpu. the rebuttal failed to answer a majority of these important concerns. hence i recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_S1xJFREKvB", "review_text": "this paper introduces a variant of nesterov momentum which saves computation by only periodically recomputing certain quantities, and which is claimed to be more robust in the stochastic setting. the method seems easy to use, so theres probably no harm in trying it. however, the reviewers and i dont find the benefits persuasive. while there is theoretical analysis, its role is to show that the algorithm maintains the convergence properties while having other benefits. however, the computations saved by amortization seem like a small fraction of the total cost, and im having trouble seeing how the increased robustness is justified. its possible i missed something, but clarity of exposition is another area the paper could use some improvement in. overall, this submission seems promising, but probably needs to be cleaned up before publication at iclr.", "accepted": 1}
{"paper_id": "iclr_2020_B1gXYR4YDH", "review_text": "this paper proposes a way to handle the hardnegative examples those very close to positive ones in nlp, using a distant supervision approach that serves as a regularization. the paper addresses an important issue and is well written; however, reviewers pointed put several concerns, including testing the approach on the stateofart neural nets, and making experiments more convincing by testing on larger problems.", "accepted": 0}
{"paper_id": "iclr_2020_HyljY04YDB", "review_text": "the paper introduces a new pooling approach laplacian pooling for graph neural networks and applies this to molecular graphs. while the paper has been substantially improved from its original form, there are still various concerns regarding performance and interpretability that remain unanswered. in its current form the paper is not ready for acceptance to iclr2020.", "accepted": null}
{"paper_id": "iclr_2020_Bkga90VKDB", "review_text": "this paper proposes to further distill token embeddings via what is effectively a simple autoencoder with a relu activation. all reviewers expressed concerns with the degree of technical contribution of this paper. as reviewer 3 identifies, there are simple variants e.g. endtoend training with the factorized model and there is no clear intuition for why the proposed method should outperform its variants as well as the other baselines as noted by reviewer 1. reviewer 2 further expresses concerns about the merits of the propose approach over existing approaches, given the apparently small effect size of the improvement let alone the possibility that the improvement may not in fact be statistically significant.", "accepted": 0}
{"paper_id": "iclr_2020_SklViCEFPH", "review_text": "this paper proposes a new training method for an endtoend contract bridge bidding agent. reviewers r2 and r3 raised concerns regarding limited novelty and also experimental results not being convincing. r2s main objection is that the paper has strong sota performance with a simple model, but empirical study are rather shallow. based on their recommendations, i recommend to reject this paper.", "accepted": 0}
{"paper_id": "iclr_2020_SylUiREKvB", "review_text": "the paper proposes a neural network architecture that uses a hypernetwork rnn or feedforward to generate weights for a network variational rnn, that models sequential data. an empirical comparison of a large number of configurations on synthetic and real world data show the promise of this method. the authors have been very responsive during the discussion period, and generated many new results to address some reviewer concerns. apart from one reviewer, the others did not engage in further discussion in response to the authors updating their paper. the paper provides a tweak to the hypernetwork idea for modeling sequential data. there are many strong submissions at iclr this year on rnns, and the submission in its current state unfortunately does not pass the threshold.", "accepted": null}
{"paper_id": "iclr_2020_HJewiCVFPB", "review_text": "this paper presents a method for improving optimization in multitask learning settings by minimizing the interference of gradients belonging to different tasks. while the idea is simple and wellmotivated, the reviewers felt that the problem is still not studied adequately. the proofs are useful, but there is still a gap when it comes to practicality. the rebuttal clarified some of the concerns, but still there is a feeling that a the main assumptions for the method need to be demonstrated in a more convincing way, e.g. by boosting the experiments as suggested with other mtl methods b by placing the paper better in the current literature and minimizing the gap between proofsunderlying assumptions and practical usefulness.", "accepted": 0}
{"paper_id": "iclr_2020_Hkl6i0EFPH", "review_text": "this paper addresses the problem of differential private data generator. the paper presents a novel approach called g_pate which builds on the existing pate framework. the main contribution is in using a student generator with an ensemble of teacher discriminators and in proposing a new private gradient aggregation mechanism which ensures differential privacy in the information flow from discriminator to generator. although the idea is interesting, there are significant concerns raised by the reviewers about the experiments and analysis done in the paper which seem to be valid and have not been addressed yet in the final revision. i believe upon making significant changes to the paper, this could be a good contribution. thus, as of now, i am recommending a rejection.", "accepted": 1}
{"paper_id": "iclr_2020_HylthC4twr", "review_text": "this paper studies twolayer graph convolutional networks and twolayer multilayer perceptions and develops quantitative results of their effect in signal processing settings. the paper received 3 reviews by experts working in this area. r1 recommends weak accept, indicating that the paper provides some useful insight e.g. into when graph neural networks are or are not appropriate for particular problems and poses some specific technical questions. in follow up discussions after the author response, r1 and authors agree that there are some over claims in the paper but that these could be addressed with some toning down of claims and additional discussion. r2 recommends weak accept but raises several concerns about the technical contribution of the paper, indicating that some of the conclusions were already known or are unsurprising. r2 concludes i vote for weak accept, but i am fine if it is rejected. r3 recommends reject, also questioning the significance of the technical contribution and whether some of the conclusions are wellsupported by experiments, as well as some minor concerns about clarity of writing. in their thoughtful responses, authors acknowledge these concerns. given the split decision, the ac also read the paper. while it is clear it has significant merit, the concerns about significance of the contribution and support for conclusions as acknowledged by authors are important, and the ac feels a revision of the paper and another round of peer review is really needed to flesh these issues out.", "accepted": null}
{"paper_id": "iclr_2020_B1gkpR4FDB", "review_text": "the paper proposes an approach to automatically tune the learning rate by using a statistical test that detects the stationarity of the learning dynamics. it also proposes a robust line search algorithm to reduce the need to tune the initial learning rate. the statistical test uses a test function which is taken to be a quadratic function in the paper for simplicity, although any choice of test function is valid. although the method itself is interesting, the empirical benefits over sgdadam seem to be minor.", "accepted": 1}
{"paper_id": "iclr_2020_Hyg5TRNtDH", "review_text": "the paper proposes a method called unsupervised temperature scaling uts for improving calibration under domain shift. the reviewers agree that this is an interesting research question, but raised concerns about clarity of the text, depth of the empirical evaluation, and validity of some of the assumptions. while the author rebuttal addressed some of these concerns, the reviewers felt that the current version of the paper is not ready for publication. i encourage the authors to revise and resubmit to a different venue.", "accepted": 0}
{"paper_id": "iclr_2020_r1xapAEKwS", "review_text": "this paper presents a method for merging a discriminative gmm with an ard sparsitypromoting prior. this is accomplished by nesting the ard prior update within a larger embased routine for handling the gmm, allowing the model to automatically remove redundant components and improve generalization. the resulting algorithm was deployed on standard benchmark data sets and compared against existing baselines such as logistic regression, rvms, and svms. overall, one potential weakness of this paper, which is admittedly somewhat subjective, is that the exhibited novelty of the proposed approach is modest. indeed ard approaches are now widely used in various capacities, and even if some hurdles must be overcome to implement the specific marriage with a discriminative gmm as reported here, at least one reviewer did not feel that this was sufficient to warrant publication. other concerns related to the experiments and comparison with existing work. for example, one reviewer mentioned comparisons with panousis et al., nonparametric bayesian deep networks with local competition, icml 2019 and requested a discussion of differences. however, the rebuttal merely deferred this consideration to future work and provided no feedback regarding similarities or differences. in the end, all reviewers recommended rejecting this paper and i did not find any sufficient reason to overrule this consensus.", "accepted": 0}
{"paper_id": "iclr_2020_H1eJAANtvr", "review_text": "this paper proposes an approach to handle the problem of unsmoothness while modeling spatiotemporal urban data. however all reviewers have pointed major issues with the presentation of the work, and whether the methods complexity is justified.", "accepted": 0}
{"paper_id": "iclr_2020_H1x9004YPr", "review_text": "with an average post author response score of 4  two weak rejects and one weak accept, it is just not possible for the ac to recommend acceptance. the author response was not able to shift the scores and general opinions of the reviewers and the reviewers have outlined their reasoning why their final scores remain unchanged during the discussion period.", "accepted": 0}
{"paper_id": "iclr_2020_SJeQGJrKwH", "review_text": "this work is interesting because its aim is to push the work in intrinsic motivation towards crisp definitions, and thus reads like an algorithmic paper rather than yet another reward heuristic and system building paper. there is some nice theory here, integration with options, and clear connections to existing work. however, the paper is not ready for publication. there were were several issues that could not be resolved in the reviewers minds even after the author response and extensive discussion. the primary issues were 1 there was significant confusion around the beta sensitivityfigs 6,7,8 appear misleading or at least contradictory to the message of the paper. 2 the need for x,y env states. 3 the several reviewers found the decision states unintuitive and confused the quantitative analysis focus if they given the authors primary focus is transfer performance. 4 all reviewers found the experiments lacking. overall, the results generally dont support the claims of the paper, and there are too many missing details and odd empirical choices. again, there was extensive discussion because all agreed this is an interesting line of work. taking the reviewers excellent suggestions on board will almost certainly result in an excellent paper. keep going!", "accepted": 0}
{"paper_id": "iclr_2020_BJe_z1HFPr", "review_text": "this paper offers likely novel schemes for image resizing. the performance improvement is clear. unfortunately two reviewers find substantial clarity issues in the manuscript after revision, and the ac concurs that this is still an issue. the paper is borderline but given the number of higher ranked papers in the pool is unable to be accepted unfortunately.", "accepted": 0}
{"paper_id": "iclr_2020_BkgeQ1BYwS", "review_text": "there is insufficient support to recommend accepting this paper. the authors provided detailed responses, but the reviewers unanimously kept their recommendation as reject. the novelty and significance of the main contribution was not made sufficiently clear, given the context of related work. critically, the experimental evaluation was not considered to be convincing, lacking detailed explanation and justification, and a sufficiently thorough comparison to strong baselines, the submitted reviews should help the authors improve their paper.", "accepted": 0}
{"paper_id": "iclr_2020_HJemQJBKDr", "review_text": "the paper seems technically correct and has some novelty, but the relevance of the paper is questionable. considering the selectiveness of iclr, i cannot recommend the paper for acceptance at this point. in more detail the authors propose a technique for estimating density rations between a target distribution of real samples and a distribution of samples generated by the model, without storing samples. the method seems to be technically well executed and verified. however, there was major concerns among multiple reviewers that the addressed problem does not seem relevant to the iclr community. the question addressed seemed artificial, and it was not considered realistic by r2 and also by r1 in the confidential discussion. r3 also expressed doubts at the usefulness of the method. furthermore, some doubts were expressed regarding clarity although opinions were mixed on that and on the justification of the modification of the vae objective to the continual setting.", "accepted": null}
{"paper_id": "iclr_2020_BylyV1BtDB", "review_text": "this manuscript proposes an approach for fair and robust training of predictive modeling  both of which are implemented using adversarial methods, i.e., an adversarial loss for fairness and an adversarial loss for robustness. the resulting model is evaluated empirically and shown to improve fairness and robustness performance. the reviewers and ac agree that the problem studied is timely and interesting, as there is limited work on joint fairness and robustness. however, the reviewers were unconvinced about the novelty and clarity of the conceptual and empirical results. in reviews and discussion, the reviewers also noted insufficient motivation for the approach.", "accepted": 0}
{"paper_id": "iclr_2020_H1lXVJStwB", "review_text": "all three reviewers, even after the rebuttal, agreed that the paper did not meet with bar for acceptance. a common complaint was lack of clarity being a major problem. unfortunately, the paper cannot be accepted in its current form. the authors are encouraged to improve the presentation of their approach and resubmit to a new venue.", "accepted": 0}
{"paper_id": "iclr_2020_Skx24yHFDr", "review_text": "this paper presents a neural topic model with the goal of improving topic discovery with a plsa loss. reviewers point out major limitations including the following 1 empirical comparison is done only with lda when there are many newer models that perform much better. 2 related work section is incomplete, especially for the newer models. 3 writing is unclear in many parts of the paper. for these reasons, i recommend that the authors make major improvements to the paper before resubmitting to another venue.", "accepted": 0}
{"paper_id": "iclr_2020_SylurJHFPS", "review_text": "the authors propose a novel metric to detect distributional discrepancy for text generation models and argue that these can be used to explain the failure of gans for language generation tasks. the reviewers found significant deficiencies with the paper, including 1 numerous grammatical errors and typos, that make it difficult to read the paper. 2 mischarcterization of prior work on neural language models, and failure to compare with standard distributional discrepancy measures studied in prior work kl, total variation, wasserstein etc.. further, the necessity of the complicated procedure derived by the authors is not welljustified. 3 failure to run experiments on standard banchmarks for image generation which are much better studied applications of gans and confirm the superiority of the proposed metrics relative to standard baselines. the reviewers were agreed on the rejection decision and the authors did not participate in the rebuttal phase. i therefore recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_HklsHyBKDr", "review_text": "nice start but unfortunately not ripe. the issues remarked by the reviewers were only partly addressed, and an improved version of the paper should be submitted at a future venue.", "accepted": 0}
{"paper_id": "iclr_2020_Skl3SkSKDr", "review_text": "this paper proposes a parametrisation of euclidean distance matrices amenable to be used within a differentiable generative model. the resulting model is used in a wgan architecture and demonstrated empirically in the generation of molecular structures. reviewers were positive about the motivation from a specific application area generation of molecular structures. however, they raised some concerns about the actual significance of the approach. the ac shares these concerns; the methodology essentially amounts to constraining the output of a neural network to be symmetric and positive semidefinite, which is in turn equivalent to producing a nonnegative diagonal matrix corresponding to the eigenvalues. as a result, the ac recommends rejection, and encourages the authors to include simple baselines in the next iteration.", "accepted": 1}
{"paper_id": "iclr_2020_rylrI1HtPr", "review_text": "this paper proposes to use the grey level cooccurrence matrix method glcm for both the performance evaluation metric and an auxiliary loss function for single image super resolution. experiments are conducted on xray images of rock samples. three reviewers provide comments. two reviewers rated reject while one rated weak reject. the major concerns include the lack of clear and detailed description, low novelty, limited experiment on only one database, unconvincing improvement over the prior work, etc. the authors agree that the limited experiment on one database does not demonstrate the generalization capability of the proposed method. the ac agrees with the reviewers comments, and recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_rJx8I1rFwr", "review_text": "this paper describes a new approach to metalearning with generating new useful examples. the reviewers liked the paper but overall felt that the paper is not ready for publication as it stands. rejection is recommended.", "accepted": 0}
{"paper_id": "iclr_2020_HyenUkrtDB", "review_text": "the paper proposes a new, stable metric, called area under loss curve aul to recognize mislabeled samples in a dataset due to the different behavior of their loss function over time. the paper build on earlier observations e.g. by shen  sanghavi to propose this new metric as a concrete solution to the mislabeling problem. although the reviewers remarked that this is an interesting approach for a relevant problems, they expressed several concerns regarding this paper. two of them are whether the hardness of a sample would also result in high aul scores, and another whether the results hold up under realistic mislabelings, rather than artificial label swapping  replacing. the authors did anecdotally suggest that neither of these effects has a major impact on the results. still, i think a precise analysis of these effects would be critically important to have in the paper. especially since there might be a complex interaction between the hardness of samples and mislabelings an mnist 1 that looks like a 7 might be sooner mislabeled than a 1 that doesnt look like a 7. the authors show some examples of real mislabeled sentences recognized by the model but it is still unclear whether downweighting these helped final test set performance in this case. because of these issues, i cannot recommend acceptance of the paper in its current state. however, based on the identified relevance of the problem tackled and the identified potential for significant impact i do think this could be a great paper in a next iteration.", "accepted": 0}
{"paper_id": "iclr_2020_Hkls_yBKDB", "review_text": "the authors consider the problem of program induction from inputoutput pairs. they propose an approach based on a combination of imitation learning from an autocurriculum for policy and value functions and alphago style tree search. it is a applied to inducing assembly programs and compared to ablation baselines. this paper is below acceptance threshold, based on the reviews and my own reading. the main points of concern are a lack of novelty the proposed approach is similar to previously published approaches in program synthesis, missing references to prior work and a lack of baselines for the experiments.", "accepted": 0}
{"paper_id": "iclr_2020_r1eCukHYDH", "review_text": "this work proposes a gan architecture that aims to align the latent representations of the generator with different interpretable degrees of freedom of the underlying data e.g., size, pose. reviewers found this paper wellmotivated and the proposed method to be technically sound. however, they cast some doubts about the novelty of the approach, specifically with respect to dmwgan and madgan. the ac shares these concerns and concludes that this paper will greatly benefit from an additional reviewing cycle that addresses the remaining concerns.", "accepted": null}
{"paper_id": "iclr_2020_SygeY1SYvr", "review_text": "the paper is interested in assessing the difficulty of popular fewshot classification benchmarks omniglot and miniimagenet. a clusteringbased metalearning method is proposed called centroid network, on which a metric is built gap between the performance of prototypical networks and centroid networks. as noted by several reviewers, the proposed metric critical for the paper is however not motivated enough, nor convincing enough  after discussion, the logic in the metric reasoning seems to remain flawed.", "accepted": 0}
{"paper_id": "iclr_2020_HJg_tkBtwS", "review_text": "the paper presents an approach to feature selection. reviews were mixed and questions whether the paper has enough substance, novelty, the correctness of the theoretical contributions, experimental details, as well as whether the paper compares to the relevant literature.", "accepted": 0}
{"paper_id": "iclr_2020_SkeKtyHYPS", "review_text": "this paper studies the effect of various data augmentation methods on image classification tasks. the authors propose the structural similarity as a measure of the magnitude of the various types of data augmentation noise they consider and argue that it is outperforms psnr as a measure of the intensity of the noise. the authors performed an empirical analysis showing that speckle noise leads to improved cnn models on two subsets of imagenet. while there is merit in thoroughly analysing data augmentation schemes for training cnns, the reviewers argued that the main claims of the work were not substantiated and the raised issues were not addressed in the rebuttal. i will hence recommend rejection of this paper.", "accepted": 0}
{"paper_id": "iclr_2020_rkeNqkBFPB", "review_text": "the manuscript proposes an autoencoder architecture incorporating two recent architectural innovations from the gan literature progressive growing  featurewise modulation, trained with the adversarial generatorencoder paradigm with a novel cyclic loss meant to encourage disentangling, and procedure for enforcing layerwise invariances. the authors demonstrate coarsefine visual transfer on generative modeling of face images, as well as generative modeling results on several large scale scene understanding lsun datasets. reviewers generally found the results somewhat compelling and the ideas valuable and wellmotivated, but criticized the presentation clarity, lack of ablation studies, and that the claims made were not sufficiently supported by the empirical evidence. the authors revised, and while it was agreed that clarity was improved, some reviewers were still not satisfied with the level of clarity the revision appeared at the very end of the discussion period, unfortunately not allowing for any further refinement. ablation studies were added in the revised manuscript, which were appreciated, but seemed to suggest that the proposed loss function was of mixed utility while stylemixing quantitatively improved, overall sample quality appeared to suffer. as the reviewers remain unconvinced as to the significance of the contribution and the clarity of its presentation, i recommend rejection at this time, while encouraging the authors to further refine the presentation of their ideas for a future resubmission.", "accepted": 0}
{"paper_id": "iclr_2020_HJxV5yHYwB", "review_text": "the paper considers planning through the lenses both of a single and multiple objectives. the paper then discusses the pareto frontiers of this optimization. while this is an interesting direction, the reviewers feel a more careful comparison to related work is needed.", "accepted": 0}
{"paper_id": "iclr_2020_H1lK5kBKvr", "review_text": "this paper proposes a semisupervised method for reconstructing 3d faces from images via a disentangled representation. the method builds on previous work by tran et al 2018, 2019. while some results presented in the paper show that this method works well, all reviewers agree that the authors should have provided more experimental evidence to convincingly demonstrate the benefits of their method. the reviewers are also unconvinced by how computationally expensive this method is or by the contributions of the unlabelled data to the performance of the proposed model. given that the authors did not address the reviewers concerns, and for the reasons stated above, i recommend rejecting this paper.", "accepted": 0}
{"paper_id": "iclr_2020_SJeQi1HKDH", "review_text": "the paper proposes a mechanism for obtaining diverse policies for solving a task by posing it as a multiagent problem, and incentivizing the agents to be different from each other via maximizing total variation. the reviewers agreed that this is an interesting idea, but had issues with the placement and exact motivations  precisely what kind of diversity is the work after, why, and what accordingly related approaches does it need to be compared to. some reviewers also found the technical and exposition clarity to be lacking. given the consensus, i recommend rejection at this time, but encourage the authors to take the reviewers feedback into account and resubmit to another venue.", "accepted": 0}
{"paper_id": "iclr_2020_SJgXs1HtwH", "review_text": "this paper proposes an application of capsule networks to code modeling. i see the potential in this approach, but as the reviewers pointed out, in the current draft there are significant issues with respect to both clarity of motivating the work, and in the empirical results which start at a much lower baseline than previous work. i am not recommending acceptance at this time, but would encourage the reviewers to clarify the issues raised in the reviews for future submission.", "accepted": 0}
{"paper_id": "iclr_2020_BJgRsyBtPB", "review_text": "the paper proposes a variant of the maxsliced wasserstein distance, where instead of sorting, a greedy assignment is performed. as no theory is provided, the paper is purely of experimental nature. unfortunately the work is too preliminary to warrant publication at this time, and would need further experimental or theoretical strengthening to be of general interest to the iclr community.", "accepted": null}
{"paper_id": "iclr_2020_H1eF3kStPS", "review_text": "this paper proposes a new graph hierarchy representation hag which eliminates the redundancy during the aggregation stage and improves computation efficiency. it achieves good speedup and also provide theoretical analysis. there has been several concerns from the reviewers; authors response addressed them partially. despite this, due to the large number of strong papers, we cannot accept the paper at this time. we encourage the authors to further improve the work for a future version.", "accepted": 0}
{"paper_id": "iclr_2020_HJx-akSKPS", "review_text": "this paper proposes a method called dynamic intermedium attention memory network diamnet to learn the subgraph isomorphism counting for a given pattern graph p and target graph g. however, the reviewers think the experimental comparisons are insufficient. furthermore, the evaluation is only for synthetic dataset for which generating process is designed by the authors. if possible, evaluation on benchmark graph datasets would be convincing though creating the ground truth might be difficult for larger graphs.", "accepted": 0}
{"paper_id": "iclr_2020_SkxHRySFvr", "review_text": "there is insufficient support to recommend accepting this paper. the reviewers unanimously criticize the quality of the exposition, noting that many key elements in the main development and experimental set up are not clear. the significance of the contribution could be made stronger with some form of theoretical analysis. the current paper lacks depth and insufficient justification for the proposed approach. the submitted comments should be able to help the authors improve the paper.", "accepted": 0}
{"paper_id": "iclr_2020_r1eU1gHFvH", "review_text": "this paper studies when hidden units provide local codes by analyzing the hidden units of trained fully connected classification networks under various architectures and regularizers. the reviewers and the ac believe that the paper in its current form is not ready for acceptance to iclr2020. further work and experiments are needed in order to identify an explanation for the emergence of local codes. this would significantly strengthen the paper.", "accepted": 0}
{"paper_id": "iclr_2020_SJlxglSFPB", "review_text": "this paper studies the problem of outofdistribution ood detection for semantic segmentation. reviewers and ac agree that the problem might be important and interesting, but the paper is not ready to publish in various aspects, e.g., incremental contribution and lessmotivatedconvincing experimental setupsresults. hence, i recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_B1eYlgBYPH", "review_text": "this paper presents a novel rnn algorithm based on unfolding a reweighted l1l1 minimization problem. authors derive the generalization error bound which is tighter than existing methods. all reviewers appreciate the theoretical contributions of the paper, particularly the derivation of generalization error bounds. however, at a higherlevel, the overall idea is incremental because rnn by unfolding l1l1 minimization problem le,2019 and reweighted l1 minimization candes,2008 are both known techniques. the proposed method is essentially a simple combination of them and therefore the result seems somewhat obvious. also, i agree with reviewers that some experiments are not deep enough to support the theory. for example, for overparameterization large model parameters issue, one can compare the models with the same number of parameters and observe how they generalize. overall, this is the very borderline paper that provides a good theoretical contribution with limited conceptual novelty and empirical evidences. as a conclusion, i decided to recommend rejection but could be accepted if there is a room.", "accepted": 1}
{"paper_id": "iclr_2020_r1xjgxBFPB", "review_text": "this work tackles the problem of catastrophic forgetting by using gaussian processes to identify memory samples to regularize learning. although the approach seems promising and wellmotivated, the reviewers ultimately felt that some claims, such as scalability, need stronger justifications. these justifications could come, for example, from further experiments, including ablation studies to gain insights. making the paper more convincing in this way is particularly desirable since the directions taken by this paper largely overlap with recent literature as argued by reviewers.", "accepted": null}
{"paper_id": "iclr_2020_r1xI-gHFDH", "review_text": "the paper proposed a general framework to construct unsupervised models for representation learning of discrete structures. the reviewers feel that the approach is taken directly from graph kernels, and the novelty is not high enough.", "accepted": 0}
{"paper_id": "iclr_2020_rJecbgHtDH", "review_text": "this paper considers the situation where a set of reinforcement learning tasks are related by means of a boolean algebra. the tasks considered are restricted to stochastic shortest path problems. the paper shows that learning goaloriented value functions for subtasks enables the agent to solve new tasks specified with boolean operations on the goal sets in a zeroshot fashion. furthermore, the boolean operations on tasks are transformed to simple arithmetic operations on the optimal actionvalue functions, enabling the zero short transfer to a new task to be computationally efficient. this approach to zeroshot transfer is tested in the four room domain without function approximation and a small video game with function approximation. the reviewers found several strengths and weaknesses in the paper. the paper was clearly written. the experiments support the claim that the method supports zeroshot composition of goalspecified tasks. the weaknesses lie in the restrictive assumptions. these assumptions require deterministic transition dynamics, reward functions that only differ on the terminal absorbing states, and having only two different terminal reward values possible across all tasks. these assumptions greatly restrict the applicability of the proposed method. the author response and reviewer comments indicated that some aspects these restrictions can be softened in practice, but the form of composition described in this paper is restrictive. the task restrictions also seem to limit the methods utility on general reinforcement learning problems. the paper falls short of being ready for publication at iclr. further justification of the restrictive assumptions is required to convince the readers that the forms of composition considered in this paper are adequately general.", "accepted": 1}
{"paper_id": "iclr_2020_SJxTZeHFPH", "review_text": "the paper investigates the effect of focal loss on calibration of neural nets. on one hand, the reviewers agree that this paper is wellwritten and the empirical results are interesting. on the other hand, the reviewers felt that there could be better evaluation of the effect of calibration on downstream tasks, and better justification for the choice of optimal gamma e.g. on a simpler problem setup. i encourage the others to revise the draft and resubmit to a different venue.", "accepted": 0}
{"paper_id": "iclr_2020_SJxmfgSYDB", "review_text": "main summary paper is about generating feature representations for set elements using weighted multiset automata discussion reviewer 1 paper is well written but experimental results are not convincing reviewer 2 well written but weak motivation reviewer 3 well written but reviewer has some questions around the motivation of weighted automata machinery. recommendation all the reviewers agree its well written but the paper could be stronger with motivation and experiments, all reviewers agree. i vote reject.", "accepted": 0}
{"paper_id": "iclr_2020_SkgOzlrKvH", "review_text": "this paper studies the impact of embedding complexity on domaininvariant representations by incorporating embedding complexity into the previous upper bound explicitly. the idea of embedding complexity is interesting, the exploration has some useful insight, and the paper is wellwritten. however, reviewers and ac generally agree that the current version can be significantly improved in several ways  the proposed upper bound has several limitations such as looser than existing ones.  the embedding complexity is only addressed implicitly, which shares similar idea with previous works.  the claim of implicit regularization has not been explored indepth.  the proposed mdm method seems to be incremental and related closely with the embedding complexity.  there is no analysis about the generalization when estimating this upper bound from finite samples. there are important details requiring further elaboration. so i recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2020_rJgjGxrFPS", "review_text": "this paper proposes to use pcs to replace the conventional decoder for 3d shape reconstruction. it shows competitive performance to the state of the art methods. while reviewer 3 is overall positive about this work, both reviewer 1 and 2 rated weak rejection. reviewer 1 concerns that important details are missing, and the discussion of results is insufficient. reviewer 3 has questions on the clarity of the presentation and comparison with sota methods. the authors provided response to the questions, but did not change the rating of the reviewers. the acs agree that this work has merits. however, given the various concerns raised by the reviewers, this paper can not be accepted at its current state.", "accepted": 0}
{"paper_id": "iclr_2020_BygkQeHKwB", "review_text": "in this paper the authors highlight the role of time in adversarial training and study various speeddistortion tradeoffs. they introduce an attack called boundary projection bp which relies on utilizing the classification boundary. the reviewers agree that searching on the class boundary manifold, is interesting and promising but raise important concerns about evaluations on state of the art data sets. some of the reviewers also express concern about the quality of presentation and lack of detail. while the authors have addressed some of these issues in the response, the reviewers continue to have some concerns. overall i agree with the assessment of the reviewers and do not recommend acceptance at this time.", "accepted": 0}
{"paper_id": "iclr_2020_SklSQgHFDS", "review_text": "the paper presents a method for intrinsically motivated exploration using successor features by interleaving the exploration task with intrinsic rewards and extrinsic task original external rewards. in addition, the paper proposes successor feature control distance between consecutive successor features as an intrinsic reward. the proposed method is interesting and it can potentially address the limitation of existing exploration methods based on intrinsic motivation. in experimental results, the method is evaluated on navigation tasks using vizdoom and deepmind lab, as well as continuous control tasks of cartpole in the deepmind control suite, with promising results. on the negative side, there are some domainspecific properties e.g., moderate map size with relatively simple structures, different rooms having visually distinct patterns, bottleneck states generally leading to better rewards, etc. that make the proposed method work well. in addition, offpolicy learning of the successor features could be a potential technical issue. finally, the proposed method is not evaluated against stronger baselines on harder exploration tasks such as atari montezumas revenge, etc., thus the addition of such results would make the paper more convincing. in the current form, the paper seems to need more work to be acceptable for iclr.", "accepted": 1}
{"paper_id": "iclr_2020_BklOXeBFDS", "review_text": "paper proposes a method for active learning on graphs. reviewers found the presentation of the method confusing and somewhat lacking novelty in light of existing works some of which were not compared to. after the rebuttal and revisions, reviewers minds were not changed from rejection.", "accepted": 0}
{"paper_id": "iclr_2020_r1xF7lSYDS", "review_text": "this paper presents several models for recognitionaware image enhancement. the authors propose to enhance the image quality in the presence of image degradation e.g., lowresolution, noise, compression artifacts as well as to improve the recognition accuracy in a joint model. while acknowledging that the paper is addressing an interesting direction, the reviewers and ac note the following potential weaknesses presentation clarity, limited technical contributions, insufficient empirical evidence. ac can confirm all the reviewers have read the rebuttal and have contributed to the discussion. all the reviewers and ac agree that the rebuttal was informative, and the authors have partially addressed some of the concerns e.g. additional experiments. r2 has raised the score from reject to weak reject. however, at this stage ac suggest the manuscript is below the acceptance bar and needs a major revision before submitting for another round of reviews. we hope the reviews are useful for improving and revising the paper.", "accepted": 1}
{"paper_id": "iclr_2020_r1e7NgrYvH", "review_text": "the idea of integrating causality into an autoencoder is interesting and very timely. while the reviewers find this paper to contain some interesting ideas, the technical contributions and mathematical rigor, scope of the method, and the presentation of results would need to be significantly improved in order for this work to reach the quality bar of iclr.", "accepted": null}
{"paper_id": "iclr_2020_SJeFNlHtPS", "review_text": "the paper shows how metalearning contains hidden incentives for distributional shift and how a technique called context swapping can help deal with this. overall, distributional shift is an important problem, but the contributions made by this paper to deal with this, such as the introduction of unittests and contextswapping, is not sufficiently clear. therefore, my recommendation is a reject.", "accepted": null}
{"paper_id": "iclr_2020_Hkx3ElHYwS", "review_text": "the paper propose a new quantizationfriendly network training algorithm called gq or dq net. the paper is wellwritten, and the proposed idea is interesting. empirical results are also good. however, the major performance improvement comes from the combination of different incremental improvements. some of these additional steps do seem orthogonal to the proposed idea. also, it is not clear how robust the method is to the various hyperparameters  schedules. for example, it seems that some of the suggested training options are conflicting each other. more indepth discussions and analysis on the setting of the regularization parameter and schedule for the loss term blending parameters will be useful.", "accepted": 0}
{"paper_id": "iclr_2020_HJeANgBYwr", "review_text": "this paper proposes a graph neural network based approach for scaling up imitation learning e.g., of swarm behaviors. reviewers noted key limitations in the discussion of related work, size of the proposed contribution in terms of model novelty, and evaluation  comparison to strong baselines. reviewers appreciated the author replies which resolved some concerns but agree that the paper is overall not ready for publication.", "accepted": 0}
{"paper_id": "iclr_2020_HyxgBerKwB", "review_text": "this paper introduces an approach for estimating the quality of protein models. the proposed method consists in using graph convolutional networks gcns to learn a representation of protein models and predict both a local and a global quality score. experiments show that the proposed approach performs better than methods based on 1d and 3d cnns. overall, this is a borderline paper. the improvement over state of the art for this specific application is noticeable. however, a major drawback is the lack of methodological novelty, the proposed solution being a direct application of gcns. it does not bring new insights in representation learning. the contribution would therefore be of interest to a limited audience, in light of which i recommend to reject this paper.", "accepted": 0}
{"paper_id": "iclr_2020_SJeWHlSYDB", "review_text": "this paper studies spread divergence between distributions, which may exist in settings where the divergence between said distributions does not. the reviewers feel this work does not have sufficient technical novelty to merit acceptance at this time.", "accepted": 0}
{"paper_id": "iclr_2020_HJeYSxHFDS", "review_text": "the paper extends gauge invariant cnns to gauge invariant spherical cnns. the authors significantly improved both theory and experiments during the rebuttal and the paper is well presented. however, the topic is somewhat niche, and the bar for iclr this year was very high, so unfortunately this paper did not make it. we encourage the authors to resubmit the work including the new results obtained during the rebuttal period.", "accepted": 1}
{"paper_id": "iclr_2020_r1nSxrKPH", "review_text": "the submission proposes a complex, hierarchical architecture for continuous control rl that combines hindsight experience replay, visionbased planning with privileged information, and lowlevel control policy learning. the authors demonstrate that the approach can achieve transfer of the different control levels between different bodies in a single environment. the reviewers were initially all negative, but 2 were persuaded towards weak acceptance by the improvements to the paper and the authors rebuttal. the discussion focused on remaining limitations the use of a single maze environment for evaluation, as well as whether the baselines were fair hac in particular. after reading the paper, i believe that these limitations are substantial. in particular, this is not a general approach and its relevance is severely limited unless the authors demonstrate that it will work as well in a more general control setting, which is in their future work already. thus i recommend rejection at this time.", "accepted": 0}
{"paper_id": "iclr_2020_rklfIeSFwS", "review_text": "this paper proposes a channel pruning approach based oneshot neural architecture search nas. as agreed by all reviewers, it has limited novelty, and the method can be viewed as a straightforward combination of nas and pruning. experimental results are not convincing. the proposed method is not better than stoa on the accuracy or number of parameters. the setup is not fair, as the proposed method uses autoaugment while the other baselines do not. the authors should also compare with related methods such as bayesnas, and other pruning techniques. finally, the paper is poorly written, and many related works are missing.", "accepted": 0}
{"paper_id": "iclr_2020_HkxcUxrFPS", "review_text": "the paper proposes to improve visual relation prediction by using depth maps. since existing rgb images do not contain depth informations, the authors use a monocular depth estimation method to predict depth maps. the authors show that using depths maps, they are able to improve prediction of relations between ground truth object bounding boxes and labels. the paper got relatively low scores with 3 initial weak rejects. after the revision and suggested improvements, one of the reviewers updated their score so the paper now has 2 weak rejects and 1 weak accept. the paper had the following weaknesses 1. the paper has limited technical novelty as it combines off the shelf components. the components also used different backbones resnet at some places, vggnet at others that were directly from prior work. was there any attempt to have an unified architecture? as the main novelty of the work is not in the model aspect, the paper needs to have stronger experiments and analysis. 2. more analysis on the quality of the depth estimation is needed. ideally, the work should provide some insight into whether some of the errors is due to having bad depth estimation? the depth estimation method used is from 2016, there are newer depth estimation methods now. would having better depth estimation give improved results? experiments that illustrates that method works well with predicted bounding boxes instead of ground truth bounding boxes will also strengthen the paper. 3. there was the question of whether the related yang et al. 2018 workshop paper should be included as basis for comparison. in the acs opinion, yang et al. 2018 is not concurrent work and should be treated as prior work. however, it is not clear whether it is feasible to compare against that work. the authors should attempt to do so and if infeasible, clearly articulate why that is the case. 4. as pointed out by r3, once there is a depth map available, it is also possible to compare against 3d methods such as those that operate on point clouds overall the paper had a nice insight by proposing the simple but effective idea of using depth information to help with visual relation prediction. still the work is somewhat borderline in quality. in the acs opinion, the main contribution and insight of the paper is of limited interest to the iclr community, and it would be more appreciated in a computer vision conference. the authors are encouraged to improve the paper with stronger experiments and analysis, incorporate various suggestions from the reviewers, and resubmit to a vision conference.", "accepted": 0}
{"paper_id": "iclr_2020_SyeRIgBYDB", "review_text": "the reviewers equivocally reject the paper, which is mostly experimental and the results of which are limited. the authors do not react to the reviewers comments.", "accepted": 0}
{"paper_id": "iclr_2020_HkgR8erKwB", "review_text": "this paper proposes pac_bayesian bounds for negative loglikelihood loss function. a few reviewers raised concerns around 1 distinguish their contributions better from prior work eg alquier. 2 confounders in their experiments. both reviewers agreed that the paper, as it is written, does not provide sufficient evidence of significance. in addition, experiments shown in the paper varies two things   parameters therefore expressiveness and potential generalizability and depth at each setting. as pointed out, this isnt right  in order to capture the effect, one has to control for all confounders carefully. another concerned raised were around theorem 2  that it contains datadistribution on the right hand side, which isnt all that useful to calculate generalization bounds we dont have access to the distribution. we highly encourage authors to take another cycle of edits to better distinguish their work from others before future submissions.", "accepted": 0}
{"paper_id": "iclr_2020_rJePwgSYwB", "review_text": "this article studies convergence of wgan training using sgd and generators of the form phiax, with results on convergence with polynomial time and sample complexity under the assumption that the target distribution can be expressed by this type of generator. this expands previous work that considered linear generators. an important point of discussion was the choice of the discriminator as a linear or quadratic function. the authors responses clarified some of the initial criticism, and the scores improved slightly. following the discussion, the reviewers agreed that the problem being studied is a difficult one and that the paper makes some important contributions. however, they still found that the considered settings are very restrictive, maintaining that quadratic discriminators would work only for the very simple type of generators and targets under consideration. although the article makes important advances towards understanding convergence of wgan training with nonlinear models, the relevance of the contribution could be greatly enhanced by addressing  discussing the plausibility or implications of the analysis in a practical setting, in the best case scenario addressing a more practical type of neural networks.", "accepted": 0}
{"paper_id": "iclr_2020_HkliveStvH", "review_text": "the paper proposes two methods for interactive panoptic segmentation a combination of semantic and instance segmentation that leverages scribbles as supervision during inference. reviewers had concerns about the novelty of the paper as it applies existing algorithms for this task and limited empirical comparison with other methods. reviewers also suggested that iclr may not be a good fit for the paper and i encourage the authors to consider submitting to a vision oriented conference.", "accepted": 0}
{"paper_id": "iclr_2020_H1xzdlStvB", "review_text": "the submission presents an approach to speed up network training time by using lower precision representations and computation to begin with and then dynamically increasing the precision from 8 to 32 bits over the course of training. the results show that the same accuracy can be obtained while achieving a moderate speed up. the reviewers were agreed that the paper did not offer a signficant advantage or novelty, and that the method was somewhat ad hoc and unclear. unfortunately, the authors rebuttal did not clarify all of these points, and the recommendation after discussion is for rejection.", "accepted": 0}
{"paper_id": "iclr_2020_BJlPOlBKDB", "review_text": "the author responses and notes to the ac are acknowledged. a fourth review was requested because this seemed like a tricky paper to review, given both the technical contribution and the application area. overall, the reviewers were all in agreement in terms of score that the paper was just below borderline for acceptance. they found that the methodology seemed sensible and the application potentially impactful. however, a common thread was that the paper was hard to follow for nonexperts on mri and the reviewers werent entirely convinced by the experiments asking for additional experiments and comparison to zhang et al.. the authors comment on the challenge of implementing zhang is acknowledged and its unfortunate that cluster issues prevented additional experimental results. while iclr certainly accepts application papers and particularly ones with interesting technical contribution in machine learning, given that the reviewers struggled to follow the paper through the application specific language it does seem like this isnt the right venue for the paper as written. thus the recommendation is to reject. perhaps a more application specific venue would be a better fit for this work. otherwise, making the paper more accessible to the ml audience and providing experiments to justify the methodology beyond the application would make the paper much stronger.", "accepted": 0}
{"paper_id": "iclr_2020_H1ls_eSKPH", "review_text": "the reviewers have provided thorough reviews of your work. i encourage you to read them carefully should you decide to resubmit it to a later conference.", "accepted": 0}
{"paper_id": "iclr_2020_rkeeoeHYvr", "review_text": "this paper proposes a method for generating text examples that are adversarial against a known text model, based on modifying the internal representations of a treestructured autoencoder. i side with the two more confident reviewers, and argue that this paper doesnt offer sufficient evidence that this method is useful in the proposed setting. im particularly swayed by r1, who raises some fairly basic concerns about the value of adversarial example work of this kind, where the generated examples look unnatural in most cases, and where label preservation is not guaranteed. im also concerned by the fact, which came up repeatedly in the reviews, that the authors claimed that using a treestructured decoder encourages the model to generate grammatical sentencesi see no reason why this should be the case in the setting described here, and the paper doesnt seem to offer evidence to back this up.", "accepted": 0}
{"paper_id": "iclr_2020_HygPjlrYvB", "review_text": "thanks for your feedback to the reviewers, which helped us a lot to better understand your paper. through the discussion, the overall evaluation of this paper was significantly improved. however, given the very high competition at iclr2020, this submission is still below the bar unfortunately. we hope that the discussion with the reviewers will help you improve your paper for potential future publication.", "accepted": 0}
{"paper_id": "iclr_2020_BkexaxBKPB", "review_text": "the general consensus amongst the reviewers is that this paper is not quite ready for publication. the reviewers raised several issues with your paper, which i hope will help you as you work towards finding a home for this work.", "accepted": 0}
{"paper_id": "iclr_2020_ryx4TlHKDS", "review_text": "this paper aims to study the effect of curvature correction techniques on training dynamics. the focus is on understanding how natural gradient based methods affect training dynamics of deep linear networks. the main conclusion of the analysis is that it does not fundamentally affect the path of convergence but rather accelerates convergence. they also show that layer correction techniques alone do not suffice. in the discussion the reviewers raised concerns about extrapolating too much based on linear networks and also lack of a cohesive literature review. one reviewer also mentioned that there is not enough technical detail. these issues were partially addressed in the response. i think the topic of the paper is interesting and timely. however, i concur with reviewer 2 that there are still lots of missing detail and the connection with the nonlinear case is not clear however the latter is not strictly necessary in my opinion if the rest of the paper is better written. as a result i think the paper in its current form is not ready for publication.", "accepted": null}
{"paper_id": "iclr_2020_SyxhaxBKPS", "review_text": "this paper studies mixedprecision quantization in deep networks where each layer can be either binarized or ternarized. the proposed regularization method is simple and straightforward. however, many details and equations are not stated clearly. experiments are performed on smallscale image classification data sets. it will also be more convincing to try larger networks or data sets. more importantly, many recent methods that can train mixedprecision networks are not cited nor compared. figures 3 and 4 are difficult to interpret, and sensitivity on the new hyperparameters should be studied. the use of best validation accuracy as performance metric may not be fair. finally, writing can be improved. overall, the proposed idea might have merit, but does not seem to have been developed enough.", "accepted": 0}
{"paper_id": "iclr_2021_UuchYL8wSZo", "review_text": "motivated by the importance of gameplay in the development of critical skills for humans and other biological species, this work aims to explore representation learning via gameplay in a realistic, high fidelity environment. inspired by childhood psychology, they propose a variant of hideandseek game called cache built on top of ai2thor, where one agent must place an object in a room such that another agent cannot find it, and demonstrate that the adversarial nature of the game helps the agents learn useful representations of the environment. they examine the difference in representations learned via such a dynamic, interactive adversarial gameplay approach, vs other more passive approaches involving static images. the paper is well written and motivated, and easy to follow. all reviewers agree that the paper will be a great contribution to the iclr community. i believe this is an important work, because not only does it challenge the traditional way of training many components of our systems passively via static image recognition models, it synthesizes ideas from various disciplines psychology, embodiment, ml and provides an excellent framework for future research. for these reasons im recommending we accept this work as an oral presentation.", "accepted": 1}
{"paper_id": "iclr_2021_YicbFdNTTy", "review_text": "this paper has generated a lot of great discussion and it presents a very different way of doing image recognition at scale compared to current state of the art practices. all reviewers rated this paper as an accept. this work is interesting enough that in my view it really deservers further exposure and discussion and an oral presentation at iclr would be a good way to achieve that.", "accepted": null}
{"paper_id": "iclr_2021_wb3wxCObbRT", "review_text": "the paper proposes a method to grow deep network architectures over the course of training. the work has been extremely well received and has clear novelty and solid experiment validation.", "accepted": 1}
{"paper_id": "iclr_2021_Mos9F9kDwkz", "review_text": "the reviewers unanimously agree that this paper is a strong accept; it makes important progress in developing our ability to query relational embedding models.", "accepted": 1}
{"paper_id": "iclr_2021_UH-cmocLJC", "review_text": "this paper studies how two layer neural nets extrapolates. the paper is beautifully written and the authors very successfully answered all the questions. they managed to update the paper, clarify the assumptions and add additional experiments.", "accepted": 1}
{"paper_id": "iclr_2021_pBqLS-7KYAF", "review_text": "the paper presents a nice analysis of the spectrum of a matrix that is obtained by applying nonlinear functions to a random matrix. the paper is mostly wellwritten, the result is novel and interesting, and has clear implications for ml problems like spectral clustering. so i would enthusiastically recommend the paper for acceptance at iclr. it would be important for authors to take into account reviewer comments. in particular, instantiating the theorems for simple mlcentric examples would be very useful.", "accepted": null}
{"paper_id": "iclr_2021_OthEq8I5v1", "review_text": "the paper introduces music, a method for unsupervised learning of control policies, which partitions state variables into exogenous and endogenous collections and maximizes mutual information between them. reviewers were uniformly positive, agreeing that the approach was interesting and wellmotivated, and the experiments convincing. some concerns were raised as to clarity, which were addressed through several revisions of the manuscript. i am happy to recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2021_2m0g1wEafh", "review_text": "this paper analyzes deep networks optimized using nonconvex noisy gradient descent. the main result shows that in a teacherstudent setting, the excess risk converges in a fastrate and is stronger than any linear estimators which include kernel methods. the paper also gives a convergence rate result that depends on some spectral gaps which can be very small but not on dimension. overall the paper is interesting. it should probably emphasize that the dependency on spectral gaps and the fact that they could be exponentially small on the convergence as the current abstract suggests efficient convergence.", "accepted": 1}
{"paper_id": "iclr_2021_sSjqmfsk95O", "review_text": "this paper received two clear accept, one accept, one borderline accept and one reject review. r4 identified that the paper falls short in discussing recent works from cvpr and eccv 2020 on the image inpainting and completion tasks which also tackle challenging scenarios in these tasks. the authors improve their related work section with these more recent works while pointing out that the task still remains unsolved and they propose an effective technique towards the solution. the meta reviewer recommends acceptance based on the following observations. the submission proposes a gan architecture for image inpainting using comodulation, which is similar to the weight modulation in stylegan2 but is conditioned on both the input image and the stochastic variable instead of only the stochastic variable. the main novelty of comodulation appears to be interesting as well as being generalisable to different tasks. the approach is shown to perform well in the image painting with largescale missing pixels and some imagetoimage translation tasks. furthermore a new metric pidsuids is proposed to evaluate the perceptual fidelity of inpainted images.", "accepted": 1}
{"paper_id": "iclr_2021_v9c7hr9ADKx", "review_text": "reviewers all agree on acceptance for this paper. the initial issues with clarity seem to have been addressed by the authors. the paper introduces a new transformerbased architecture for marl that enables variable input and output sizes, which is used to train the agent in a more general setting and on more diverse tasks for multitask training. the method also produces more interpretable agents. the paper shows results on the starcraft multiagent challenge not the full game of starcraft, but still a recognised and widely used multiagent benchmark. the method produces solid results both in terms of final training performance and zeroshot generalisation. although reviewers are generally supportive of this paper, they mention that the starcraft challenge used is somewhat simple only few units used, and that the transformerbased architecture may not be applied to domain which lack the proper structure.", "accepted": 1}
{"paper_id": "iclr_2021_VqzVhqxkjH1", "review_text": "the paper presents a new idea for detection of model stealing attacks. the new method generates fingerprint, i.e., adversarial examples that transfer to surrogate models extracted in model stealing attacks but not to reference models i.e., models obtained independently from the same data. if a model owner suspects that some model is stolen, fingerprints can be used for verifications of such claims. the papers contribution is novel and significant. it is the first practical tool, to my knowledge, suitable for a reliable characterization of stolen models. the empirical results are quite impressive demonstrating the detection of stolen models with an auc  1.0. some presentations issues have been addressed by the authors during the revision.", "accepted": null}
{"paper_id": "iclr_2021_MLSvqIHRidA", "review_text": "this paper reveals a novel interpretation of the wellestablished cd for energybased model training as an adversarial game through conditional nce. the paper could be potential impactful for the community of ebms. there are several points should be addressed in final version 1, based on such an interpretation, the number of steps becomes a tunable parameters, rather than in vanilla understaning in cdfamily the larger, the better in terms of approximation, by with more computation cost. 2, it is okay to stop the gradient when solving an adversarial game as the paper discussed. however, propagating the gradient through the component is also another choice, which leads to the algorithm proposed in 1. it will be interesting to discuss these in the paper. 1 sohldickstein, jascha, peter battaglino, and michael r. deweese. minimum probability flow learning. arxiv preprint arxiv0906.4779 2009.", "accepted": 1}
{"paper_id": "iclr_2021_opHLcXxYTC_", "review_text": "this paper advances the idea that recent influence estimation methods for supervised learning cannot be trivially applied to gans. based on hara et al.s method, the authors propose a novel influence estimation for gans, and an evaluation scheme based on popular gan evaluation methods, exploiting the fact that they are differentiable with respect to their input data. the paper demonstrates empirically that the proposed influence estimation method correlates to true influence. it also shows that removing harmful instances using the average loglikelihood, inception score, and frechet inception distance versions of the proposed metric improves the quality of generated examples. all reviewers were positive about the paper. r2 pointed out that it was wellwritten and appreciated the detailed analysis. they thought it thoroughly explained the similarities between it and the most closelyrelated recent work hara et al. and koh  liang. concerns expressed by the reviewer were the amount of samples needed to be removed to obtain a statistically significant result, lack of qualitative results, and an outdated baseline for anomaly detection. the reviewer also stated that they had some concerns with practical applicability and would like to see more gan metrics, like precision  recall. the authors added qualitative results to the paper which partially satisfied the reviewer. r1 also thought that the paper was wellwritten and contributed to the interpretability of gan training. like r2, they pointed out the lack of visual examples addressed in rebuttal, and asked for more insight into what kind of characteristics make a data point influential. they also requested that the authors add a metric that trades fidelity and diversity like pr. the reviewer originally felt that the paper was below the bar, because it was like a story without a satisfying conclusion. however, the authors responded with additional analysis which satisfied the reviewer, and they upgraded their score by two points. r3 also found the paper wellwritten and interesting, like the other reviewers. the reviewer raised some similar concerns as the other reviewers e.g. qualitative results, as well as the scalability of the method to relevant architectures, which i thought was surprising that the other reviewers didnt mention. the authors responded that they believe their method succeeded in improving diversity of the generated samples but not their visual quality. this is an important point. the additions in appendix d have addressed the main concerns of r1 and r2, as well as r3s concern about lack of visual analysis. r1 seems quite convinced now, and r2, though not changing their score, was already in favour of acceptance. it is an interesting finding that harmful instances seem to come from regions of distributional mismatch. i would like to see a fidelitydiversity tradeoff like pr added to a paper, and a discussion of this work in relation to devries et. al instance selection that appears to be similarly motivated though executed differently. i think one major thing holding back this paper is the scale of the experimental analysis gaussians  mnist; i hope the authors can scale the method in future work.", "accepted": null}
{"paper_id": "iclr_2021_qYda4oLEc1", "review_text": "post rebuttal, the reviewers all recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2021_0N8jUH4JMv6", "review_text": "the paper introduces convex reformulations of problems arising in the training of two and three layer convolutional neural networks with relu activations. these formulations allow shallow cnns to be training in time polynomial in the number of data samples, neurons and data dimension albeit exponential in filter lengths. these problems are regularized in different ways l2 regularization for two layers, l1 regularization for three layers, providing new insights into the connection between architectural choices and regularization. the paper also provides experiments showing convex training of neural networks on small datasets. pros and cons  the theoretical results show that globally optimal training of shallow cnns can be achieved in time fully polynomial, i.e., polynomial in the number of data samples, neurons and data dimension. this is significant theoretical progress, since the corresponding results for fully connected neural networks require time exponential in the rank of the data matrix. there is, however, an exponential dependence on the filter length or the rank of the patch matrix. in particular, the computational complexity is proportional to nkr_c3r_c, where n is the number of data points. while cnns do use relatively small filters, this becomes prohibitive even when r_c is a moderate constant. e.g., the experiments use filters of length 3. here, the comments of the reviewers about generalization may be appropriate; perhaps experiments that evaluate the performance of these networks in terms of generalization may show the disadvantages of using very small filters.  the work provides interesting and rigorous insights into the relationship between architecture and implicit regularization, with different network architectures leading to different regularizers l1, l2, nuclear. developing these insights for deeper architectures could lead to important insights even in situations where the convex relaxation is challenging to solve efficiently.  although the theoretical results require overparameterization, in the sense that strong duality holds when the number of filters is large relative to the number of data points, the authors convincingly argue that this degree of overparameterization is commensurate with, or even smaller than, the degree of overparameterization present in many experimentaltheoretical works in the literature.  the paper is mathematically precise and is written in a rigorous fashion, but is occasionally heavy on notation. the paper could be more impactful on empirical work on neural networks if it could provide more intuition about how the various forms of equivalent regularization arise from different architectures. all three reviewers express appreciation for the papers fresh insights into global optimization of shallow cnns and the connection between architectural choices and regularization. the ac recommends acceptance.", "accepted": null}
{"paper_id": "iclr_2021_Pbj8H_jEHYv", "review_text": "very good paper it proposes a novel parameterization of orthogonal convolutions that uses the cayley transform in the fourier domain. the paper discusses several aspects of the proposed parameterization, including limitations and computational considerations, and showcases it in the important application of adversarial robustness, achieving good results. the reviews are all very positive, so im happy to recommend acceptance. also, a big shoutout to the reviewers and to the authors for being outstanding during the discussion period. the reviewers engaged with the paper to a great depth, and the authors improved the paper considerably as a response. well done to all of you.", "accepted": 1}
{"paper_id": "iclr_2021_NeRdBeTionN", "review_text": "all four reviewers unanimously recommended for an acceptance four 7s. they generally appreciated that the proposed idea is novel and experiments are convincing. i think the paper tackles an important problem of evaluating gans, and the idea of using selfsupervised representations, as opposed to the conventional imagenetbased representations, would lead to interesting discussions and followups.", "accepted": null}
{"paper_id": "iclr_2021_kHSu4ebxFXY", "review_text": "this work proposes a method for generating candidate molecules using a novel fragmentbased mcmc proposal mechanism. pros  wellwritten paper  novel idea for an important application  very good empirical performance compared to the stateoftheart in multiobjective molecule generation  careful ablation studies cons  some details were missing runtime, experimental details and have been added to the revised version. the authors engaged in an extensive discussion with the reviewers and modified their paper to address the reviewer concerns. after discussions three reviewers recommend accepting the work and consider it a novel and useful contribution to the field. one reviewer reviewer 3 is not satisfied by the authors comments and has concerns about the work regarding asymptotic correctness of the sampling; fairness of the experimental comparison; and computational complexity. the authors provide detailed justifications for their choices. after looking at the discussion there are two factors 1. technical arguments regarding the correctness of the sampling method; the authors justify the correctness by known results for adaptive mcmc methods, and the argument is sound, and the area chair fully accepts the authors arguments as correct and applicable. 2. extend of the experimental evaluation and suitable baseline methods; this is partially subjective. the authors provide extensive experiments in their work and justify exclusion of certain methods in that they do not easily apply to the multiobjective setting. in addition, reviewer 3 demands a comparison of generated molecules per time, which is plausibly useful, however, none of the prior works have used such a metric in a consistent manner and it is clearly challenging to do so fairly as such metric would depend on specifics of the implementation and computer. the authors have updated their paper and added runtime information for their method. the area chair fully accepts the authors arguments and justification for the current experimental scope. in summary the area chair considers the remaining concerns by reviewer 3 as invalid; in particular, the authors have made extensive efforts to engage and educate the reviewer.", "accepted": 1}
{"paper_id": "iclr_2021_5jRVa89sZk", "review_text": "this paper studies the unlabeled entity problem in ner. specifically, performance degradation in training of ner models due to unlabeled entities. it analyzes the reason through evaluation on synthetic datasets and finds that it is due to the fact that all the unlabeled entities are treated as negative examples. to cope with the problem, it proposes a negative sampling method which considers the use of only a small subset of unlabeled entities. experimental results show that the proposed method achieves better performances than the baselines on realworld datasets and achieves competitive performances compared with the stateoftheart methods on wellannotated datasets. pros  the paper is clearly written.  the proposed method appears to be technically sound.  experimental results support the main claims.  the findings in the paper are useful for the field. cons  novelty of the work might not be enough. the authors have addressed some clarity and reference issues pointed out by the reviewers in the rebuttal. discussions have been made among the reviewers.", "accepted": 1}
{"paper_id": "iclr_2021_ZzwDy_wiWv", "review_text": "this paper proposes a new idea for performing knowledge distillation by leveraging teachers classifier to train students penultimate layer feature via proposing suitable loss functions. reviewers appreciate the simultaneous simplicity and effectiveness of the method. a comprehensive set of studies are performed to empirically show the effectiveness of the method. specifically, the proposed distillation method is shown to outperform stateoftheart across various network architectures, teacherstudent capacities, datasets, and domains. the paper is wellwritten and is easy to follow. all reviewers rate the paper on the accept side after the rebuttal and believe the new perspective this work provides on distillation and its simplicity to implement can lead it to gain high impact. i concur with the reviewers and find this submission a convincing empirical work, and thus recommend for accept.", "accepted": null}
{"paper_id": "iclr_2021_oZIvHV04XgC", "review_text": "this paper proposes a new online contextualized fewshot learning setting, with two associated datasets notably, including one obtained from trajectories within the realworld matterport3d reconstructions. a simple recurrent contextualized extension of prototypical networks is also proposed as a stronger baseline, demonstrating the need for incorporating such context. the reviewers all agreed that this is an interesting setting combining continual and fewshot learning, offering a more realistic problem that mirrors those that might be encountered by embodied agents. the authors provided very detailed rebuttals, answering some of the questions and concerns raised by the reviewers. in the end, all reviewers agreed that this paper would contribute a significant novel setting, and so i recommend acceptance. i encourage the others to include modifications related to some of the comments, such as strengtheningclarifying the setting including metrics, details of the method, etc.", "accepted": null}
{"paper_id": "iclr_2021_ee6W5UgQLa", "review_text": "the paper presents a new dataset for multimodal qa that is deemed interesting, relevant and well executed by all reviewers. multimodality in nlp qa included is an increasingly important topic and this paper provides a potentially impactful benchmark for research in it. all reviewers acknowledge that. we hence recommend to accept this paper as a poster. we recommend the authors to further improve the draft before camera ready by using the recommendations made by the reviewers with a particular focus on an extended discussion wrt prior work on vqa and other. the paper should also add more precisions on the licenses related to the images used in the dataset.", "accepted": 1}
{"paper_id": "iclr_2021_Ozk9MrX1hvA", "review_text": "this paper concerns data augmentation techniques for nlp. in particular, the authors introduce a general augmentation framework they call coda and demonstrate its utility on a few benchmark nlp tasks, reporting promising empirical results. the authors addressed some key concerns e.g., regarding hyperparameters, reporting of variances during the discussion period. the consensus, then, is that this work provides a useful and relatively general method for augmentation in nlp and the iclr audience is likely to find this useful.", "accepted": null}
{"paper_id": "iclr_2021_T1XmO8ScKim", "review_text": "this is a fairly technical paper bridging deep learning with uncertainty propagation in computations i.e. probabilistic numerics. it is well structured, but it could benefit from further improvements in readability given that there are only very few researchers that are experts in all subdomains associated with this work. given the above, as well as low overall confidence by the reviewers, i attempted a more thorough reading of the paper even if not an expert myself, and i was also happy to see that the discussion clarified important points. overall, the idea is novel, convincing and seems well executed, with good results. the technical advancements needed to make the idea work are fairly complicated and are appreciated as contributions, because they are expected to be useful in other applications too beyond irregular sampled data where uncertainty propagation matters.", "accepted": null}
{"paper_id": "iclr_2021_i80OPhOCVH2", "review_text": "the paper identifies the phenomenon of oversquashing in gnns and relate it to bottleneck. while this phenomenon has been previously observed, the analysis is new and insightful. the authors conclude that standard message passing may be inefficient in cases where the graphs exhibit an exponentially growing number of neighbors and longrange dependencies, and propose a solution in the form of a fullyadjacent layer. while the paper does not offer much methodologically, it is the observation of bottleneck that is of importance. we therefore believe that the criticism raised by some reviewers of the observation not being novel and the solution too simple rather unsubstantiated. the authors have well addressed these issues in their rebuttal. the ac recommends accepting the paper.", "accepted": 1}
{"paper_id": "iclr_2021_-Hs_otp2RB", "review_text": "this paper presents a hierarchical version of \u03b2tcvae that promotes disentanglement in the latent space and improves the robustness of vaes over adversarial attacks, without much degeneration on the quality of reconstructions. the analysis on the relationship between disentanglement and adversarial robustness is valuable and the method is new. the results look promising. the comments were properly addressed.", "accepted": null}
{"paper_id": "iclr_2021_9EKHN1jOlA", "review_text": "this paper proposes a method to quantify the uncertainty for rnn, which is an important problem in various applications. it provides results in a variety of domains demonstrating that the proposed method outperforms baselines. however, these experiments would benefit greatly from a comparison with sota methods for the specific tasks in addition to the considered baselines e.g. covariance propagation, prior network, and orthonormal certificates. the paper could also be improved by adding a theoretical justification to explain how the gumbel softmax function is able to capture the underlying data and model uncertainty.", "accepted": null}
{"paper_id": "iclr_2021_kDnal_bbb-E", "review_text": "in the context of constructing negotiation dialogue strategiespolicies, the authors explore the use of graph attention networks gats for determining the sequence of negotiation dialogue acts  specifically leading to a 1 hierarchical dialogue encoder via pooled bert  gru encoding  2 gat over dialogue strategiesacts many technical details around graph usage  3 gru decoder. while a relatively straightforward replacement relative to similar architectures with other structural encoders, they provide a sound endtoend training strategy that is shown to perform well on the buyerseller negotiation task via craigslistbargain dataset where they demonstrate sota performance.  pros   studying the pragmatics component of negotiation dialogue strategies has received recent interest and this seems a good milepost that demonstrates mainstream methodological approaches for this task i.e., this is a good baseline for future innovations  the paper is wellwritten in that it is easy to understand intuitively while having sufficient detail to understand the details.  the empirical results appear promising and meet the standard within this subcommunity  showing improvements with automatic and human evaluation.  cons   this builds on existing datasets, which are known to have undesirable properties e.g., automatic evaluation, small number of dialogue datasets, use of explicit dialogues acts, etc. while it still meets the standards of this subcommunity, it still isnt a completely convincing task.  while the use of gats is novel in this setting and they get it to work within the overall architecture, this is something that many people are likely trying at this time  so there isnt an exciting disruptive step here.  the empirical results, while satisfactory from a quantitative perspective, even in reading the appendices, it isnt clear that these are significantly better from a planning perspective or if it is just pattern recognition gains. evaluating along the requested dimensions  quality the underlying method is fairly straightforward and the authors incorporate uptodate gatrelated methods to get this to work in this setting. the empirical results are sound if predicated on the general quality in this subcommunity where you have the standard machine translation evaluation problem for meaning vs. lexical closeness. to mitigate, they use bertscore and human evaluation  which is at the higher end of what can be reasonably expected.  clarity the paper is written clearly overall, especially if considering the appendices where there is significant detail. related to empirical evaluation, it isnt easy to intuitively interpret the results, but this is again par for the course. additionally, i believe the authors did a good job responding to reviewer concerns.  originality while all of the reviewers agreed that the approach was novel in this setting, one of the reviewers explicitly pointed out that using gats in negotiation dialogues isnt that exciting  and i mostly agree. i view this as something that somebody would have done and will serve as a good baseline; although i think this subfield is going to need more datasets to continue progressing.  significance as stated above, it is a good baseline that i think many are likely thinking of as the tod community has been doing this for a bit now. however, it is done well. honestly, i agree with the reviewers that this is a somewhat borderline paper  mostly due to it being a fairly obvious idea and the nature of the subfield making it not entirely clear if the improvements are due to knowing the target performance while training or due to the methodological advance. personally, i am convinced, but it isnt totally clear. that being said, it is a wellwritten paper and i think the reviewer issues were sufficiently addressed. thus, i would prefer to see it accepted as i think it will be a strong methodological baseline for this problem which hopefully will accumulate more convincing datasets and standard evaluation.", "accepted": null}
{"paper_id": "iclr_2021_ULQdiUTHe3y", "review_text": "this paper considers a new setting of robustness, where multiple predictions are simultaneously made based on a single input. different from existing robustness certificates which independently consider perturbation of each prediction, the authors propose collective robustness certificate that computes the number of predictions which are simultaneously guaranteed to remain stable under perturbation. this yields more optimistic results. most reviewers think this is a very interesting work and the authors present an effective method to combine individual certificate. the experimental results are convincing. i recommend accept.", "accepted": 1}
{"paper_id": "iclr_2021_xjXg0bnoDmS", "review_text": "this paper studies the link between generalization behavior and flatness of the loss landscape in deep networks. specifically, the authors study two measures of flatness local entropy and local energy, and show that these two measurements are strongly correlated with one another. moreover they show via a careful set of numerical experiments that two previously proposed algorithms entropy sgd and replica sgd that optimize for local entropy tend to both find flatter minima as well as provide better generalization. despite the fact that the paper proposes no new models or algorithms, the experiments are compelling and provide nontrivial insights into predicting generalization behavior of deep networks, as well as solid evidence on the benefits of entropy regularization in sgd. the authors also seem to have satisfactorily answered the numerous initial concerns raised by the authors. overall, i recommend an accept.", "accepted": null}
{"paper_id": "iclr_2021_lvRTC669EY_", "review_text": "all the reviewers are in favor of accepting this paper, which demonstrates both theoretically and empirically the value of reward randomization in solving multiagent reinforcement learning problems. the rebuttal phase was crucial in improving the quality and evaluation of the submission. i am glad to recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2021_068E_JSq9O", "review_text": "the paper presents new contrastive based selfsupervised objective based on chi squared divergence that helps with mini batch sensitivity, training stability and improved downstream performance. an accept.", "accepted": 1}
{"paper_id": "iclr_2021_6zaTwpNSsQ2", "review_text": "this paper proposes a new approach to training networks with low precision called block minifloat. the reviewers found the paper well written and found that the empirical results were sufficient. in particular, they found the hardware implementation was a strong contribution. furthermore, the rebuttal properly addressed the comments of the reviewer.", "accepted": null}
{"paper_id": "iclr_2021_gIHd-5X324", "review_text": "the paper investigates the effect of soft labels in knowledge distillation from the perspective of samplewise biasvariance tradeoff. they observe that during training the biasvariance tradeoff varies samplewisely. and under the same distillation temperature setting, we distillation performance is negatively associated with the number of regularization samples. but removing them altogether hurts the performance the authors show empirical evidence of this. based on some observations about regularization samples, the authors propose the weighted soft labels to handle the tradeoff. experiments on standard datasets show that the proposed method can improve the standard knowledge distillation. pros. the paper is written clearly. through the review period the authors added additional experiments suggested by the reviewers and enhances experimental results. the experiment results are convincing and the authors have now added explanations on hyperparameter choices. the mathematical setting is now clear after incorporating reviewers comments. the missing related work as suggested by reviewers is added cons. comparison with results of zitong yang et al 20201 is missing. i thank the authors for incorporating the changes requested by reviewers. please add comparison with result of 1 in the final version. 1 rethinking biasvariance tradeoff for generalization of neural networks zitong yang, yaodong yu, chong you, jacob steinhardt, yi ma", "accepted": 0}
{"paper_id": "iclr_2021_ZsZM-4iMQkH", "review_text": "this paper suggests an extension of previous implicit bias results on linear networks to a tensor formulation and arguably weakens some of the assumptions of previous works e.g. loss going to zero is replaced with initialization assumptions. the reviewers were all positive about this work, saying it is clearly written and an original significant contribution. there were a few issues raised e.g. the novelty of the proof techniques and the authors responded. the reviewers did not clarify if this response satisfied these concerns, but did not change their positive scores. i will take this to indicate they still recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2021_5jzlpHvvRk", "review_text": "this paper received borderline scores but overall lean positive. the reviewers point out that the paper presents interesting new ideas and an effective solution to the problem of automatically searching for loss functions. the empirical results are convincing, although the baselines are not the strongest possible in terms of absolute performance. overall, the acs find that the paper has sufficient novelty and technical contribution to be accepted.", "accepted": null}
{"paper_id": "iclr_2021_8HhkbjrWLdE", "review_text": "after reading the authors response, all reviewers recommend accepting the paper. the authors provided an extensive response carefully considering all reviewers comments. after incorporating the feedback, the manuscript improved in terms of presentation, relation to the literature and empirical results. the paper is very well written and motivated. on top of the insightful analysis, experimental results are strong, obtaining comparable performance to that of a resnet18 on imagenet. r1 and r3 strongly support the paper while r2 and r4 consider it borderline. r2 raised questions about experimental details and reproducibility. while r2 did not comment, these concerns were very clearly addressed by the authors in the view of the ac. r4 was initially concerned with the novelty of the approach, but changed their mind after the authors response. the ac encourages the authors to further consider the feedback provided by the reviewer after the discussion period was over.", "accepted": 1}
{"paper_id": "iclr_2021_5NA1PinlGFu", "review_text": "the paper initially received a mixed rating, with two reviewers rate the paper below the bar and two above the bar. the raised concerns include the need for an autoregressive model for upsampling and the effect of batch sizes. these concerns were welladdressed in the rebuttal. both of the reviewers that originally rated the paper below the bar raise the scores. after consulting the paper, the reviews, and the rebuttal, the ac agrees that the paper has its merits and is happy to accept the paper.", "accepted": null}
{"paper_id": "iclr_2021_PObuuGVrGaZ", "review_text": "this paper studies the effect of label smoothing on knowledgedistillation. a previous work on this topic muller et al. has claimed that label smoothing can hurt the performance of the student model in knowledgedistillation. the rationale behind this argument is that label smoothing erases information encoded in the labels. this work shows that such claimed effect does not necessarily happen. specifically, by a comprehensive study on image classification, binary neural networks, and neural machine translation, the authors show that label smoothing can be compatible with knowledge distillation. however, they conclude that label smoothing will lose its effectiveness with longtailed distribution and increased number of classes. overall ratings of this paper are all on the positive side, and r2 finding this paper an important step toward understanding the interaction between knowledgedistillation and label smoothing. i concur with the reviewers about the importance of this research direction and i think this submission provides a reasonable empirical evidence to change our earlier perspectives. i recommend accept. while the paper specifically studies the effect of label smoothing on knowledgedistillation, i think providing a bigger context and reviewing some of the recent demystifying efforts on understanding knowledgedistillation could allow paper to communicate with a broader audience. i hope this can be accommodated in the final version.", "accepted": 1}
{"paper_id": "iclr_2021_RqCC_00Bg7V", "review_text": "the authors put a lot of effort in replying to questions and improving the paper to a point that the reviewers felt overwhelmed. pros  an interesting way of dealing with model bias in mpc  they successfully managed to address the most important concerns of the reviewers, with lots of additional experiments and insights  r3s concerns have also been successfully addressed by the authors, the review  score were unfortunately not updated cons  the only remaining point is that the simulations seem to be everything but physically realistic update at end of r1s review, which is probably a problem of the benchmarks and not the authors faults.", "accepted": null}
{"paper_id": "iclr_2021_9YlaeLfuhJF", "review_text": "this paper presents an approach for mitigating subgroup performance gap in images in cases when a classifier relies on subgroup specific features. the authors propose a data augmentation approach, where synthetically produced examples by gans act as instantiations of the real samples in all possible subgroups. by matching the predictions of original and augmented examples, the prediction model is forced to ignore subgroup differences encouraging invariance. the proposed method of controlled data augmentations as precisely called by r4 is relevant and wellmotivated, the theoretical justifications support the main claims, and the experimental results are diverse and demonstrate merits of the proposed approach. as rightly pointed out by r3, the appendices are also very thorough, and the code is organized well. in the initial evaluation, the reviewers have raised in unison concerns regarding overlapping subgroups per class, and an imbalance problem in the subgroups when training gans. there were also questions reg. theoretical justifications, and empirical evaluations of the baseline methods. the authors have addressed all major concerns in the rebuttal. pleased to report that based on the author respond with extra experiments and explanations, r2 has raised the score from 6 to 7. in conclusion, all four reviewers were convinced by the authors rebuttal, and ac recommends acceptance of this paper  congratulations to the authors! there is a colossal effort in the community addressing a goal similar to this work  learning invariant representations w.r.t. sensitive features by means of algorithmic fairness methods. r1 and r3 relate to it. when preparing the final version, the authors are encouraged to elaborate more on the discussioncomparison to fairnessbased methods, ideally including empirical evidence where possible where subgroups overlap, e.g. celeba. the ac believes this will strengthen the final revision and will have an even broader impact in the community.", "accepted": 1}
{"paper_id": "iclr_2021_Fmg_fQYUejf", "review_text": "the paper is presenting an important empirical finding. when the learning algorithms are initialized with the same point, the continual and multitask solutions are connected by linear and lowerror paths. motivated by this finding, the paper proposes a new continual learning algorithm based on path regularization. the paper received unanimously good scores. i agree with the reviews and recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2021_l0mSUROpwY", "review_text": "protein molecule structure analysis is an important problem in biology that has recently become of increasing interest in the ml field. the paper proposes a new architecture using a new type of convolution and pooling both on euclidean as well as intrinsic representations of the proteins, and applies it to several standard tasks in the field. overall the reviews were strong, with the reviewers commending the authors for an important result on the intersection of biology and ml. the reviewers raised the points of  weak baselines the authors responded with adding suggested comparison, which were not completely satisfactory  focus mostly on recent protein literature  the reliance of the method on the 3d structure. the ac however does not find this as a weakness, as there are multiple problems that rely on 3d structure, which with recent methods can be predicted computationally rather than experimentally. we believe this to be an important paper and thus our recommendation is accept. as the ac happens to have expertise in both 3d geometric ml and structural biology, heshe would strongly encourage the authors to better do their homework as there have been multiple recent works on convolutional operators on point clouds, as well as intrinsic representationbased ml methods for proteins.", "accepted": 1}
{"paper_id": "iclr_2021_LhY8QdUGSuw", "review_text": "this paper studies how layerwise representation and task semantics affect catastrophic forgetting in continual learning. it presents two findings 1. the higher layers contribute more to forgetting than lower layers, 2. intermediatelevel similarity between tasks causes the maximal forgetting. it also indicates that existing methods employ either feature reuse or orthogonality to mitigate forgetting. pros  the layerwise analysis of catastrophic forgetting and investigation of different mitigating forgetting methods are important and interesting.  the paper is wellmotivated and wellwritten.  the results can potentially help to suggest new approaches for developing and measuring mitigation methods. cons before rebuttal  the paper misses discussion on and takeaways from the findings.  how general are the findings? there is a different observation by kirkpatrick et al. 2017.  limited diversity of experiments, because the experiments are only done on image classification tasks with cifar10 and cifar100. the authors conducted more experiments and updated the paper with added explanations and results. the reviewers found the new evidence and arguments in the rebuttal to be convincing and the authors addressed most concerns. in summary, the findings from this paper will help researchers better understanding and addressing catastrophic forgetting, and will be of interest to the community. hence, i recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2021_KTlJT1nof6d", "review_text": "this paper applies spectral initialization and weight decay to neural nets with factorized layers. although these ideas have been extensively studied in other areas, formalizing and applying them to deep neural nets is of potential interest to the community. the simulation results are nice, especially the experiments on compression methods comparison to sparse pruning e.g. lottery tickets and transformers. i recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2021_1Jv6b0Zq3qi", "review_text": "the authors design a framework to estimate the uncertainties in the predictions of gradient boosting models, for both classification and regression. the framework contains several methods, some that use subsampling on data to calculate the estimation, and some that use subsampling on the trees within one single gradient boosting model i.e. virtual ensemble to calculate the estimation. the different methods reveal the tradeoff between faster calculation and good uncertainty estimation. the authors conduct extensive empirical study to demonstrate the validity of the designed framework. the reviewers agree that the paper is wellwritten on a very important topic of machine learning in practice. the authors have done a great job addressing the comments from the reviewers, including the comparison to random forest, and adding more motivating examples. the reviewers believe that the work marks a good starting point for addressing this important topic. nevertheless, the reviewers have some concerns that the results are promising but not impressive yet, and the performance of the virtual ensemble is a bit discouraging.", "accepted": null}
{"paper_id": "iclr_2021_kBVJ2NtiY-", "review_text": "first as a procedural point, the paper got 7, 7, 5, 5. anonreviewer3 gave it a 5, but seemed satisfied by the discussion and promised to raise their score. they did not do so, but i must interpret their last messages as indicating they now support the paper. anonreviewer2, the other 5, had some concerns that other reviewers seem to have helped address during rebuttal. they did not update their score, but were happy to leave their certainty low and defer to other reviewers recommendation. as such, although the average score looks low in the system, the paper is of an acceptable standard according to reviews. the paper adapts a method from tabular rl to deep rl, allowing as the title aptly says, agents to learn what to do by simulating the past. reviewers speaking in support of the paper found that the paper was clear and sound in its evaluation, providing interesting results and a useful and reusable method. it is my feeling that after discussion, the case for the paper has been clearly made, and in the absence of any strong objections from the reviewers, i am happy to go with the consensus and recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2021_Gu5WqN9J3Fn", "review_text": "description the paper presents a patchbased 3d representation of manmade shapes that can be computed with deep learning and used directly in existing cad applications. this representation is based off a deformable parametric template with coons patches. results in sketchbased modelling tasks shows comparable results with stoa strengths  the patchbased representation provide several advantages compact, sparse, interpretable, consistent and easily editable.  can infer the right template, and thus does not require manually created templates weaknesses  limited evaluation restrained to mostly sketchbased modelling, and missing evaluation against a few stoa methods the paper has introduced a very impactful new representation for 3d shapes and has strong technical novelty. i recommend, as reviewers have suggested, more indepth quantitative evaluation against other work and ablation studies", "accepted": 0}
{"paper_id": "iclr_2021_2AL06y9cDE-", "review_text": "thank you for your submission to iclr. the reviewers and i are in agreement that the work presents some interesting connections between closedloop control and stabilization of activations to an observed manifold. specifically, the idea of using optimal control dynamic programming techniques to compute optimal adjustments to ensure control on this manifold is an interesting one and may have other implications within deep networks. although the reviewers were convinced by the experiments on robustness, i remain a bit skeptical here. the results show that while the method marginally improves robustness to smallepsilon perturbations, the models are still quite nonrobust against the size perturbations frequently used in assessing adversarial robustness e.g., to eps8 perturbations on cifar10, where the best approach gets 11 accuracy against pgd attacks. it doesnt really matter how well a defense works against suboptimal attacks if pgd is able to decrease its accuracy this much, clearly the model is not very robust and it seems upon reading that only 20step pgd, with no restarts, was used as an attack, which is a fairly weak variant of pgd. furthermore, the approach didnt improve much upon pgdbased adversarial training when combined with it, either, overall suggesting that the impact on robustness is somewhat minor, and needs to be evaluated quite a bit more. while i dont believe these concerns are substantive enough to override the beliefs of all reviewers, i think that the authors could do a much better job of evaluating the actual robustness of these models following the advice of httpsarxiv.orgabs1902.06705. and if the resulting metrics are not as strong as hoped for, then it would be good to evaluate other possible benefits of the approach perhaps to random distribution shift? it seems a much more likely situation for there to be real gains?. thus, while i believe the paper has some interesting ideas, i think the authors should probably tone down some of the current claims of improving adversarial robustness unless they can provide a much more thorough evaluation.", "accepted": null}
{"paper_id": "iclr_2021__i3ASPp12WS", "review_text": "this paper presents a defense scheme for adversarial attacks, called selfsupervised online adversarial purification soap, by purifying the adversarial examples at test time. the novelty of this work is in its incorporation of selfsupervised representation learning into adversarial defense through purification via optimizing an auxiliary selfsupervised loss. this is done by jointly training the model on a selfsupervised task while it is learning to perform the target classification task in a multitask learning setting. compared with existing adversarial defense schemes such as adversarial training and purification techniques, soap has a lower computation overhead during the training stage. strengths  it is novel to incorporate selfsupervised learning for adversarial purification at test time.  soaps training stage based on multitask learning incurs low computation overhead compared with the original classification task. weaknesses  although the proposed adversarial defense scheme is computationally cheaper than the other existing methods during the training stage, it does incur some overhead during test time. this may be undesirable for some applications in which efficiency during test time is an important factor to consider.  the choice of a suitable selfsupervised auxiliary task is somewhat ad hoc. the performance varies a lot for different auxiliary tasks.  the experimental evaluation is only based on relatively small and unrealistic datasets even after new experiments on cifar100 have been added by the authors. it is said in the paper that soap can exploit a wider range of selfsupervised signals for purification and hence conceptually can be applied to any format of data and not just images, given an appropriate selfsupervised task. however, this claim has not been substantiated in the paper using nonimage data. despite some limitations and that some claims still need to be better substantiated, the paper presents some novel ideas which are expected to arouse interest for followup work in the adversarial attack and defense research community.", "accepted": null}
{"paper_id": "iclr_2021_lmTWnm3coJJ", "review_text": "this paper has been thoroughly evaluated by four expert reviewers and it had received one public comment. the authors provided extensive explanations and added technical updates to the contents of their submission in response to constructive critiques from the reviewers. even though some minor issues have not been fully resolved in the discussion between the authors and the reviewers, i consider this paper worthy of inclusion in the program of iclr 2021 since, albeit marginally, the apparent strengths outweigh its outstanding limitations.", "accepted": null}
{"paper_id": "iclr_2021_rgFNuJHHXv", "review_text": "this paper use group convolutional neural networks in both generators and discriminator of gans, and demonstrates advantages of this approach when training with a relatively small sample size. while the novelty is limited in the work as it simply applies gcnn for gans , i believe this application is interesting and the authors have applied it to many gan image synthesis applications conditional generation , pix2pix on various benchmarks, which gives evidence of the potential of gcnns in generative modeling. accept", "accepted": null}
{"paper_id": "iclr_2021_ECuvULjFQia", "review_text": "the paper proposes a new teacherstudent framework where the teacher network guides the student network in learning useful information from trajectories of a dynamical system. the proposed framework is inspired by the knowledge distillation method. the teacher learns what information should be used from the trajectories and distills this information for the student in the form of target activations. in a nutshell, the framework allows the student to interpolate between modelbased and modelfree approaches in an automated fashion. experimental evaluation on both the handcrafted and simulated tasks demonstrate the effectiveness of the proposed framework. the reviewers had borderline scores in their initial reviews and raised several questions for the authors. the reviewers appreciated the rebuttal, which helped in answering their key questions  i want to thank the authors for engaging with the reviewers during the discussion phase. the reviewers have an overall positive assessment of the paper, and believe that the proposed teacherstudent framework is novel and potentially useful for many realworld problems. the reviewers have provided detailed feedback in their reviews, and i would like to strongly encourage the authors to incorporate this feedback when preparing the final version of the paper.", "accepted": null}
{"paper_id": "iclr_2021_7I12hXRi8F", "review_text": "this paper proposes an approach to learn the causal structure underlying a dataset with acyclicity and other structure constraints, and then used the inferred structure to compute partial causal effects. the authors show that, on simulated data, the proposed method outperforms others in the literature. the manuscript also contains an analysis of realworld data that describes the causal effects of the lockdown of cities in the hubei province china to reduce the spread of covid19. overall, the reviewers think that this is a well structured and written paper. from a novelty viewpoint, the main contribution consists in formalising the causal contribution of mediators, as the method for computing the causal structure is based on a small modification to previous literature. the main concern raised by the reviewers were on the experimental evaluation. some of these concerns were addressed by the authors during rebuttal, whilst some on the number of nodes remained. we encourage the authors to consider these concerns in the final version of the manuscript.", "accepted": 1}
{"paper_id": "iclr_2021_Mu2ZxFctAI", "review_text": "the paper proposed weightedmocu, a novel objectiveoriented data acquisition criterion for active learning. the propositions are wellmotivated, and all reviewers find the analysis of the drawbacks of several popular myopic strategies e.g. elr tends to stuck in local optima; bald tends to be overly explorative interesting and insightful. reviews also appreciate the novelty of the proposed weighted strategy for addressing the convergence issue of mocubased approaches. overall i share the same opinions and believe the paper offers useful insights for the active learning community. in the meantime, there were shared concerns among several reviewers in the readability structure and intuition, lack of empirical results on more realistic active learning tasks, and limited discussion on the modeling assumptions. although the rebuttal revision does improve upon many of these points, the authors are strongly encouraged to take into account the reviews, in particular, to further strengthen the empirical analysis and discussions, when preparing a revision.", "accepted": null}
{"paper_id": "iclr_2021_jEYKjPE1xYN", "review_text": "the paper provides a new covariant approach to 3d molecular generation motivated by the desire handle compounds with symmetries. to this end, the method uses equivariant state representations for autoregressive generation, built largely from recently proposed covariant molecular networks comorant, and integrating such representations within an existing actorcritic rl generation framework simm et al. the selection of focal atom, element to add, and the distance are realized in an equivariant manner while the compound valuation remains invariant to rotation. the approach is clean and wellexecuted. the authors added additional experiments e.g., rmsd demonstrating stability of generated compounds to further reinforce the case for the method.", "accepted": 1}
{"paper_id": "iclr_2021_193sEnKY1ij", "review_text": "the approach explore the use of conditional risk minimization crm as a posthoc operation to amend a classifier decision by averaging a prior class hierarchy. the authors show that it is beneficial for ranking predictions without sacrifying top1 accuracy. the rebuttal period clarified some reviewers concern on paper presentation and experiments, and all reviewers recommend acceptance after the discussion period. although the approach is simple and directly revisits the use of crm for deep models, the ac considers that the contribution is meaningful, and that the proposed method provides predictions with good ranking and calibration properties. the paper also sheds light into interesting issues in stateoftheart methods integrating class hierarchies during training. the ac therefore recommends paper acceptance.", "accepted": 1}
{"paper_id": "iclr_2021_Pz_dcqfcKW8", "review_text": "this paper proposes an approach to unifying both fullcontext and streaming asr in a single endtoend model. techniques such as weight sharing, joint training and teacherstudent knowledge distillation are used to improve the training. the socalled dualmode asr is evaluated under the contextnet and conformer networks on librispeech and multidomain datasets. the performance is good. while the technical novelty is not overwhelmingly significant, all reviewers agree that it may have impact to the speech machine learning community as highperformance streaming asr is of great importance in realworld deployment of asr systems. the authors have meticulously addressed the reviewers comments and, in particular, changed the title from universal asr to dualmode asr as suggested by some of the reviewers. after the rebuttal, all reviewers are supportive on accepting the paper.", "accepted": null}
{"paper_id": "iclr_2021_EQfpYwF3-b", "review_text": "the paper proposes to use projective clustering to compress the embedding layers of dnn. this is a novel interesting idea which can impact the area of knowledge distillation. there were some concerns about the empirical study which was addressed to some extent by the authors during the rebuttal.", "accepted": 0}
{"paper_id": "iclr_2021_CF-ZIuSMXRz", "review_text": "this paper studies extensions of the scattering graph transform to spatiotemporal domains. by exploring several design choices for spatiotemporal wavelet filters, the authors provide a solid and broad study of such predefined represenatations, including stability analysis as well as extensive empirical evaluations. reviewers were generally favorable, and highlighted the importance of this method as providing a simple yet powerful baseline for spatiotemporal graph prediction tasks that requires no training. despite some concerns about lack of analysis of the empirical results, the ac believes this work will provide a valuable baseline for future research and therefore recommends acceptance as a poster.", "accepted": 1}
{"paper_id": "iclr_2021_KmykpuSrjcq", "review_text": "this paper proposes an extension to previous unsupervised feature learning work, with an emstyle latent variable model with momentum encoders. the paper is wellwritten and provides a nice read. it has been noted that it is easy to follow and provides good insights. on the experimental side, compared with moco, the proposed approach achieve noticeable improvements. one of the reviewers noted the easy reproducibility of the proposed approach. some reviewers noted some comparisons were lacking from the original manuscript, but the authors have update the draft to include those. as noted in the reviews, the field of ssl in vision is moving at a very quick pace, making it hard to clearly state what is the sota at time t. overall, most questions raised by the reviewers were properly addressed during rebuttal  and given the ratings, i suggest acceptance.", "accepted": null}
{"paper_id": "iclr_2021_8xLkv08d70T", "review_text": "i thank the authors for their submission and very active participation in the author response period. the paper is well written r3,r4, tackles a hard problem r4 in a novel way r4 with interesting and convincing results r2. r3 noted that an empirical comparison to poet would be appropriate. however, in my view the authors addressed these concerns in a satisfactory manner. it seems that r3 has not updated their assessment nor confirmed their current score based on the author response. i am therefore discounting the only review voting for rejection and am siding with r1, r2 and r4. thus, i recommend acceptance.", "accepted": 0}
{"paper_id": "iclr_2021_Zbc-ue9p_rE", "review_text": "this work improves deep generative models by applying langevin dynamics to sample in the latent space. the authors test their method under different configurations different loss functions and various generative models vae, flow, besides gan. experimental results demonstrate the benefits of the proposed method in different generative tasks. i tend to accept this solid work. i just have two suggestions 1 the authors should discuss the connections and the differences between the proposed method and the energybased methods like arbel et al., 2020 indepth; 2 it may be more suitable to replace wasserstein gradient flow with discriminator gradient flow in the title.", "accepted": null}
{"paper_id": "iclr_2021_hpH98mK5Puk", "review_text": "this paper introduces two regularizers that are meant to improve outofdomain robustness when used in the finetuning of pretrained transformers like bert. results with anli and adversarial squad are encouraging. pros  new method with concrete improvements in several difficult task settings.  new framing of adversarial generalization. cons  the ablations that are highlighted in the main paper body dont do a good job of isolating the specific new contributions. though the appendix provides enough detail that im satisfied that the main empirical contribution is sound.  reviewers found the theoretical motivation very difficult to follow in places.", "accepted": 1}
{"paper_id": "iclr_2021_Ig-VyQc-MLK", "review_text": "the paper analyses several approaches to pruning at initialization, compared to after training. there was a large gap in reviewers appreciation of the paper, but i think that the pros outdo the cons as the paper show a lot of insights overall. i recommend accepting the paper.", "accepted": 1}
{"paper_id": "iclr_2021_WAISmwsqDsb", "review_text": "this paper presents a novel method for generalpurpose supervised domain transfer that trains both generator and discriminator to compete in a minimax game in order to reconstruct data. this setup is meant to address a common issue in conditional gan setups they often ignore conditioning information. results are positive and span two very different tasks imagetoimage translation and silentvideotospeech reconstruction. overall reviewers were quite positive about this paper they found the method to be novel and wellmotivated, and after rebuttal, found experimental results to be sufficiently convincing. several concerns were brought up a lack of emphasis that the approach is in fact supervised, b need for comparisons with stronger or taskspecific baselines, c lack of description of experimental details for reproducibility, and d lack of discussion of ethical implications. all of these concerns were satisfactorily addressed by authors in rebuttal and reviewers unanimously vote for acceptance. i agree, and recommend this paper be accepted.", "accepted": null}
{"paper_id": "iclr_2021_Ldau9eHU-qO", "review_text": "the paper considers the problem of learning interpretable, lowdimensional representations from highdimensional multimodal input via weak supervision in a learning from demonstration lfd context. to mitigate the disparity between the abstractions that humans reason over and the robots lowlevel action and observation spaces, the paper argues for learning a lowdimensional embedding that captures the underlying concepts. the primary contribution of the paper is the ability to learn disentangled lowdimensional representations that are interpretable from weak supervision using conditional latent variable models. the paper was reviewed by three knowledgeable referees, who read the author response and discussed the paper. the paper considers a challenging problem in learning from demonstration, namely dealing with the disparity that exists between the ways in which humans and robots model and observe the world, a problem that is exacerbated when reasoning over highdimensional multimodal observations. as the reviewers note, the use of variational inference to learn lowdimensional interpretable representations from weak supervision is compelling. the primary concerns are that the contributions need to be more clearly scoped and that the experimental evaluation is a bit narrow. the authors make an effort to resolve some of these issues, in part through the inclusion of an additional experiment that considers pouring tasks. however, the extent to which this second task mitigates concerns about the narrow evaluation is not fully clear. the paper would be strengthened by the inclusion of experiments in a less contrived setting and one for which the concepts are not necessarily disjoint as well as a clearer discussion of the primary contributions.", "accepted": null}
{"paper_id": "iclr_2021_60j5LygnmD", "review_text": "while this paper would be significantly improved with experiments on real data, the reviewers all agreed that there is value in the ideas and simple experiments in this paper and all voted for acceptance after the discussion period. we encourage the authors to consider adding an experimental evaluation in more realistic settings e.g. with real data in the final version of the paper.", "accepted": 1}
{"paper_id": "iclr_2021_FX0vR39SJ5q", "review_text": "this paper makes an innovative change to the adjacency matrix definition in graph convolutional neural networks gcns kipf  welling, 2017. the change results in computationallyefficient isometric transformation invariance. there were a number of concerns raised by reviewers, and the author responses and revisions, and the subsequent discussion, resulted in most of these concerns being satisfactorily addressed. on reviewer continued to feel the paper was entirely theoretical and therefore not appropriate to iclr, but that opinion was not shared more broadly and is not held by the area chair.", "accepted": null}
{"paper_id": "iclr_2021_qrwe7XHTmYb", "review_text": "this paper is a study of neural network scaling, with models containing hundred of billions of parameters. to that end, the paper introduce a new module called gshard, consisting of annotations apis on how to split computations across accelerators, which is integrated in the xla compiler. this enables the training of models with hundreds billions of parameters. to scale efficiently to very large models, the paper proposes to use transformer networks, where every other feed forward sublayer is replaced by a sparse mixture of experts similar to shazeer et al. 2017. this model is then evaluated on a multilingual machine translation task, from 100 languages to english. on the one hand, i believe that the contributions of the paper are significant scaling to 600b parameters, and showing that this leads to better translation quality are important achievement. the analysis of transformer networks scaling could also have an important impact. finally i think that gshard and its integration in xla could be very valuable. on the other hand, i agree with some of the concerns raised by the reviewers, regarding the writing of the paper and the reproducibility. i found the paper not well written, and hard to identify the differences with previous work. as gshard is one of the main contribution, i would expect a better description of it in the main text compared to the moe which seems more incremental. regarding reproducibility, i do not think that the authors provided a good reason not to evaluate on standard benchmarks the test sets could be excluded from the train set through various deduplication heuristics. to conclude, i am leaning toward accepting the paper, but believe it is borderline. the reason is that the contributions are significant, and worth publishing. but i would not oppose a rejection based on the reproducibility and writing issues.", "accepted": 1}
{"paper_id": "iclr_2021_OmtmcPkkhT", "review_text": "the paper proposes multiplicative filter networks gabornet and fouriernet as functional approximations of deepnets. the proposed networks are a sequence of multiplications linear functions of sinusoidal or gabor filters. the authors show that in some cases the performance of proposed networks outperforms the existing deepnets using relu activations. this representation is notably simpler as well. moreover, compared to classical fourier approach, the proposed method scales to higher dimensions in practice as well. the downside of the paper is that it is not clear how to empirically use exponentially many fourier functions. moreover, proposed methods have more parameters, and the additional parameters are linear in size of the hidden layer. the paper is clearly written and the authors improved the quality of the paper and added additional experiments to support their claim through the review process and i appreciate that.", "accepted": 1}
{"paper_id": "iclr_2021_Ig53hpHxS4", "review_text": "this paper proposes an autoregressive flowbased network, flowtron, for tts with style transfer. it integrates the tacotron architecture with the flowbased generative model. extensive experiments are carried out in a controlled manner and the results show that the proposed flowtron framework can achieve comparable mos scores to the sota tts models and is good at generating speech with different styles. all reviewers consider the work interesting. there are concerns raised on technical details which mostly have been cleared by the authors rebuttal. the exposition also has been greatly improved based on the reviewers suggestions and questions. overall, this is an interesting paper and i would recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2021_qVyeW-grC2k", "review_text": "the paper attempts at providing a general benchmark for evaluatinganalysis of long range transformer models, consisting of a 6 evaluation tasks. the main goal of the paper is to remove conflating factors such as pretraining from model performance and keeping the benchmark accessible. all reviewers agreed that these are important positive aspects of the paper and the presented analysisresults are useful. while reviewers generally feel positive about the work, there are some critical concerns on how useful this benchmark is in practice, how generalizable are the results, and whether the benchmark is good at what is intended for. for example, the vanilla transformer model performs very well on all the proposed tasks, making me question on what we can actually learn about long range dependencies through this benchmark. in addition, most tasks are synthetic and all models fail on 1 of the 6 proposed tasks. therefore, i think lra should be viewed more as a tool for analysis, or as authors nicely put in their response, it should be viewed as a means to encourage hypothesis driven research instead of hillclimbing or sota chasing.. during discussion period with reviewers, while acknowledging the abovementioned issues, this strength was highlighted as a valuable contribution. therefore, given the general positive sentiment about the work, id recommend accept.", "accepted": null}
{"paper_id": "iclr_2021_MyHwDabUHZm", "review_text": "this paper proposes to use high dimensional representation for labels to strengthen the adversarial robustness of deep neural networks. experimental results demonstrate that the proposed method improve adversarial robustness. all reviewer agree that the authors propose an interesting idea and this direction deserves further exploration. on the other hand, the reviewers also raise a serious question there is a lack of explanation of why high dimensional representation of labels improve adversarial robustness. therefore, it is not clear if the proposed method can defend refined attacks tailored to such dimensional label representation. the authors are highly encouraged to conduct deeper analysis, especially on the robustness against finer attacks.", "accepted": 0}
{"paper_id": "iclr_2021_yUxUNaj2Sl", "review_text": "this work investigates the recently proposed hypothesis that enhanced shape bias improves neural network robustness to common corruptions. several interesting experiments are performed to better understand the contributing factors that lead to improved robustness of models trained with texture randomization. of particular note, the authors design a data augmentation strategy that verifiably increases the shape bias of model, but for which corruption robustness is not improved. reviewers agreed that this is an interesting counterexample to the shapebias hypothesis and improves our understanding of why stylization improves robustness. given the carefully designed experiments investigating an important topic i recommend accept.", "accepted": 1}
{"paper_id": "iclr_2021_xgGS6PmzNq6", "review_text": "this paper is devoted to dyadic fairness in representation learning. all the reviewers agreed that the contribution is novel, original and technically sound. however, all the reviewers agreed that the paper should be improved in terms of presentation  for two reviewers, presentationclarity issues were at the core of their weak rejects. the most positive reviewers highlighted that the problem is still understudied despite the flurry of work on fair machine learning in the last years and therefore the contribution deserves to be accepted. if there is room, this paper can be accepted as a poster.", "accepted": null}
{"paper_id": "iclr_2021_oyZxhRI2RiE", "review_text": "this paper proposes to pretrain contextual semantic parsing models on synthesized data using a small amount of additional supervised training data and grammarbased generalizations therefrom with two new training objectives column contextual semantics ccs, mapping text to database columns, and turn contextual switch tcs, to deal with the update semantics between turns. i thank the reviewers for their detailed engagement with this paper, and thanks the authors for their responsiveness in doing extra experiments and rewriting that made this paper better and the decision clearer. pros the authors did such a great job of summarizing the pros, that i think i can just copy their summary we are glad that the reviewers appreciate the novelty and the effectiveness of our proposed approach r5, find our experiments to be comprehensive and convincing by achieving sota on 3 out of 4 different tasks r1, r2, r3, r4, ablation studies and analysis to be informative and well done r2, r4, and think our paper is clearly written and easy to follow r1, r2, r3, r4. cons  a somewhat specific and ad hoc data synthesis solution  stronger pretrained contextual language models might beat assumed baselines or methods shown here r4, r5  the story is weak and should be better motivated through discussion of contextualization of interpretation in general the reviewers recommend accepting the paper, and i agree. however, it is perhaps not of the novelty, clarity, or impact size to qualify for more than a poster. r5 has a good point about how strong pretrained lms are a general tool and should be preferred to the extent they work in 2020, but i think they are too opinionated to suggest this is a reason for rejection. along with the other reviewers and the authors, i think it is most reasonable to accept work showing good progress using mediumsized pretrained lms  really we thought bert was big a couple of years ago!  and this work has comprehensive experiments with good results. i would encourage the authors  to say more about the alternative strategy of instead using a bigger pretrained lm, as has come out in the discussion on openreview, and the pros and cons of this approach though maybe the results with bart are the only fairly comparable data point  to strengthen the presentation by orienting the paper more around the importance of contextualization in interpreting dialog turns in conversational semantic parsing as opposed to the one turn nature of the original famous semantic parsing datasets. p.s. one typo i noticed in the revised paper while reading fours  four", "accepted": 0}
{"paper_id": "iclr_2021_g11CZSghXyY", "review_text": "this paper analyses the interaction between dataaugmentation strategies and model ensembles with regards to calibration performance. the authors note how strategies such as mixup and label smoothing, which reduce a single models overconfidence, lead to degradation in calibration performance when such models are combined as an ensemble. they propose a simple solution. the paper merits publication.", "accepted": 1}
{"paper_id": "iclr_2021_pHXfe1cOmA", "review_text": "this paper proposes hyperdynamics a framework that takes into account the history of an agents recent interactions with the environment to predict physical parameters such as mass and friction. these parameters are fed into a forward dynamics model, represented as a neural network, that is used for control. pros  addresses an important problem adapting dynamics models to new environments and provides strong baselines  well written and authors have improved clarity even further based on reviewers comments cons  i agree with the reviewer that it is currently unclear how well this will transfer to the real world  the idea of predicting physical parameters from a history of environment interactions is not not novel in itself although the proposed framework is, as far as i know. the authors should include related work along the lines of 1 this is just one paper that comes to mind, others exist 1 preparing for the unknown learning a universal policy with online system identification", "accepted": null}
{"paper_id": "iclr_2021_l0V53bErniB", "review_text": "the paper combines bilevel optimization and reverse modedifferentiation for flow estimation on networks. most reviewers think the idea of incorporating physical constraints is interesting and novel, and the experiments convincing.", "accepted": 0}
{"paper_id": "iclr_2021_NX1He-aFO_F", "review_text": "this paper is accepted, however, it could be much stronger by addressing the concerns below. the theoretical analysis of the proposed methods is weak.  as far as i can tell, the proposition has more to do with the compatible feature assumption than their method. furthermore the compatible feature assumption is very strong and not satisfied in any of their experiments.  sec 4.2 does not provide strong support for their method. r2 points out issues with their statements about variance and the next subsection argues from an overly simplistic diagram. the experimental results are promising, however, r3 brought up important issues in the private discussion  their implementation of sac systematically produces results worse than reported in the original paper they use a version of sac with automatically tuned temperature httpsarxiv.orgpdf1812.05905.pdf; 1a their sac gets average returns of 2.5k at 500k steps while the original implementation gets 3k at 500k steps; 1b their sac on halfcheetah 10k at 1m steps, original paper  11k at 1m steps; 1c the same applies to humanoid, there is no improvement with respect to the original sac;  their approach degrades performance on hopper.  they use nonstandard hyper parameters for sac. 0.98 instead of 0.99 for the discount and 0.01 instead of 0.005 for the soft target updates. that might be the main reason why their sac works worse than the original implementation.  the authors use the hyperparameters suggested for halfcheetahbulletenv for all continuous control tasks. for halfcheetah, however, the authors of the stablebaselines repository which this paper uses suggest to use the hyper parameters from the original sac paper httpsgithub.comaraffinrlbaselineszooblobmasterhyperparamssac.ymll48. nonetheless, the results for the unmodified sac reported in this work for halfcheetahhopperwalkerant are subpar to the original results, suggesting that the hyperparameters for halfcheetahbulletenv are suboptimal for these tasks. given the simplicity of the change and the promising experimental results with some caveats, i believe the community will find this paper interesting and will lead to followup work that can patch the theoretical gaps.", "accepted": 1}
{"paper_id": "iclr_2021_dOcQK-f4byz", "review_text": "the paper shows that standard transformers can be trained to generate satisfying traces for linear temporal logic ltl formulas. to establish this, the authors train a transformer on a set of formulas, each paired with a single satisfying trace generated using a classical automatatheoretic solver. it is shown that the resulting model can generate satisfying traces on heldout formulas and, in some cases, scale to formulas on which the classical solver fails. the reviewers generally liked the paper. while the transformer model is standard, the use of deep learning to solve ltl satisfiability is novel. given the centrality of ltl in formal methods, the paper is likely to inspire many followup efforts. there were a few concerns about the evaluation; however, i believe that the authors comments address the most important of them. given this, i am recommending acceptance. please add the new experimental results about outofdistribution generalization to the final version of the paper.", "accepted": null}
{"paper_id": "iclr_2021_lgNx56yZh8a", "review_text": "the paper presents a bayesian approach for classification able to adapt to novel classes given only a few labeled examples. the models combines a onevseach approximation of the likelihood combined with a gaussian process. this allows to resort to a dataaugmentation scheme based on polyagamma random variables. the paper is clearly written and combines existing techniques in a convincing manner; the experiments demonstrate better accuracy and uncertainty quantification on benchmark datasets. i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2021_Ec85b0tUwbA", "review_text": "the paper introduces new methods and building blocks to improve hyperbolic neural networks, including a tighter parameterization of fully connected layers, convolution, and concatenatesplit operations to define a version of hyperbolic multihead attention. the paper is well written and relevant to the iclr community. the proposed methods offer solid improvements over previous approaches in various aspects of constructing hyperbolic neural networks and also extends their applicability. as such, the paper provides valuable contributions to advance research in learning noneuclidean representations and hnns. all reviewers and the ac support acceptance for the papers contributions. please consider revising your paper to take feedback from reviewers after reubttal into account.", "accepted": 1}
{"paper_id": "iclr_2021_ESG-DMKQKsD", "review_text": "this paper uses an extension of hologan for few shot recognition and novel view synthesis. all but one reviewer gave a final rating of accept. these reviewers were concerned that the submitted version of this work had not adequately placed this work in context with prior art. however, during the discussion these concerns seem to have been addressed sufficiently. the most negative reviewer was not impressed by the quality of the generated images; however these are relatively new methods and the few shot recognition aspect of this work is also part of the contribution. accounting for all reviews and the discussion the ac recommends accepting this work as a poster.", "accepted": null}
{"paper_id": "iclr_2021_ZW0yXJyNmoG", "review_text": "the reviews were a bit mixed, with a general consensus towards acceptance. the authors were one of the first to extend lookahead to minimax optimization, and demonstrated its potential through thorough experiments. the theoretical results were not as strong or at least not very well presented. overall, the authors made interesting contributions and this work is of general interest to the iclr audience. please consider further polishing the draft according to the reviewers comments. the ac would also like to draw the authors attention to the following issues discovered in an independent assessment a as the reviewers mentioned, how lookaheadminimax addresses rotational dynamics is not clearly presented. the current justification is a bit handwaving and speculative. b please consider rewriting section 3. if there is some new results on the minimization problem, state the results in a theorem and include all assumptions clearly and precisely. this is also useful for other people to reference your result. as the authors themselves pointed out, this result falls quite short of explaining or motivation lookahead. c theorem 1, add e.g. in the citation before bertsekas, 1999. theorem 2, in its current form, is quite weak in two aspects a without checking its proof one can already see how to derive it in 1 line or 2. b if the base optimizer already converges, what is the point of having lookahead to converge as well? the potentially different convergence rate should be ones target here. it is certainly fine for the authors to not fully justify their proposed algorithm, as long as the authors hopefully are at least aware of the issues. d section 4 is a bit disappointing as one would have expected the authors to derive some qualitative results here also raised by some reviewers.", "accepted": 0}
{"paper_id": "iclr_2021_d-XzF81Wg1", "review_text": "this paper analyzes batchnorm through a series of experiments and shows that batchnorm improves training and generalization by preventing excessive growth of the activations of the previous to the last layer. the paper received mixed reviews. two reviewers find that the paper brings clear and important new contributions to the understanding of batchnorm, and appreciate the experimental evaluation, though some suggestions for improving the experiments were also provided. the other reviewer appreciated the contribution of regularizing against explosive growth in the previous to the last layer, but did not find the results to be very convincing as they can limit the learning rate. the authors responded that, contrary to the reviewers claim, the learning rate is not limited. overall, while there are some aspects that need improvement, and the authors should address those in the final version, this is a solid paper that brings an interesting contribution to iclr.", "accepted": 0}
{"paper_id": "iclr_2021_nEMiSX_ipXr", "review_text": "the paper considers new notions of adversarial accuracy and risk which are called genuine with an aim to fix issues with the existing definitions in the literature. a number of issues in the paper, including lack of motivation and intuition, and poor formalism were identified by the reviewers. the paper also fails to cite some of the previous literature that has identified similar issues. the authors have only responded to some of the questions raised by the reviewers.", "accepted": 0}
{"paper_id": "iclr_2021_dJbf5SqbFrM", "review_text": "the paper proposes transfer learning where the target domain data is evolving along time. they use both labeled and unlabeled data to learn domain and timeinvariant features based on a discrepancy measure they introduce. their proposed algorithm uses vae to learn such features. reviewers have mixed response, although the author feedback did help. the main limitation with the paper is that it does not seem to be aware of the very extensive literature on continuous domain adaptation. the related work only discusses papers on transfer learning, multisource domain adaptation, and continuous learning. but ignores papers on continuous domain adaptation which are much more related to this paper. the most recent of these that appeared in icml 2020 also attempts to learn time invariant features using adversarial methods. unfortunately, the reviewers seem to be also unaware of this literature 1. continuously indexed domain adaptation, hao wang, hao he, dina katabi, icml 2020 2. active adversarial domain adaptation 3. continuous domain adaptation using optimal transport 4. learning to adapt to evolving domains  neurips 2020 5. judy hoffman, trevor darrell, and kate saenko. continuous manifold based adaptation for evolving visual domains. in proceedings of the ieee conference on computer vision and pattern recognition, pages 867874, 2014. 6 massimiliano mancini, samuel rota bulo, barbara caputo, and elisa ricci. adagraph unifying predictive and continuous domain adaptation through graphs. in proceedings of the ieee conference on computer vision and pattern recognition, pages 65686577, 2019. 7 atsutoshi kumagai and tomoharu iwata. learning future classifiers without additional data. in thirtieth aaai conference on artificial intelligence, 2016.", "accepted": null}
{"paper_id": "iclr_2021_IohHac70h3R", "review_text": "the paper presents a new online convex optimization algorithm that uses percoordinate learning rates. the learning rates are changed over time using information coming from the gradients. a regret upper bound is proved and the algorithm is empirically validated on deep learning experiments. while the analysis is in principle correct, it does not seem to provide any advantage over the guarantees of similar algorithm, for example the mirror descent version adagrad with diagonal matrices. also, despite the intuition of the authors, the reviewers have found that the approach used in the analsysis is fundamentally bounded to give a worse guarantee than adagrad. overall, the theoretical contribution appears to be not sufficient. on the empirical side, the experiments failed to convince the majority of the reviewers that the algorithm has a significative gain over similar algorithms. more generally, this paper suffers from the same problem of many other similar papers there is a complete disconnect from the theory proven under restrictive assumptions convexity, bounded domains, no stochasticity and the experiments nonconvex functions, no projection on bounded domain, stochastic setting. unfortunately, the deep learning literature is full of such papers, but the community should strive to do better and substantially raise the quality of field. in this view, i strongly suggest to the authors to try to improve the theoretical contribution, for example, trying to prove a convergence guarantee of the gradients to 0, rather than focusing on regret upper bounds. such analysis would also suggest better ways to design new optimization algorithms better suited to nonconvex problems.", "accepted": 1}
{"paper_id": "iclr_2021_dmCL033_YwO", "review_text": "one referee supports acceptance, whereas three referees lean towards rejection. all referees agree that the idea introduced in the paper is interesting but find that the motivation and evaluation of the proposed aggregation functions could be significantly strengthened. the rebuttal addresses r1s concerns about novelty and unfair comparisons, r2s concerns about computational efficiency of the methods, r3s concerns about motivation of the proposed approach and some missing baselines, and r4s concerns about motivation. however, the rebuttal does not address the reviewers concerns related to improvements achieved by the proposed approach, statistical significance nor appropriate comparison with sota. i agree with the reviewers that the paper tries to address a relevant problem and proposes interesting ideas, which are worth exploring. however, after discussion, the referees agree that further work should be devoted to strengthen the contribution. i agree with their assessment and hence must reject. in particular, i would strongly recommend to follow their suggestions to either provide strong theoretical motivation to support the claims of the paper or work on a strengthened empirical evaluation, following ogb guidelines to report the std of the results and including a proper comparison with the state of the art.", "accepted": 0}
{"paper_id": "iclr_2021_foNTMJHXHXC", "review_text": "the paper is proposing risk extrapolation rex as a domain generalization algorithm. authors extends the distributionally robust learning to affine mixture of distributions from convex mixture. authors later uses variances instead of this extension and demonstrate various empirical and theoretical properties. the paper is reviewed by four expert reviewers and the reviewers did not reach to a consensus. hence, i also read the paper in detailed and reviewed it. in summary, reviewers argue the following  r2 main argument is the lack of justification of the claim rex could deal with both covariate and concept shift together. authors try to address this in their response. moreover, reviewer also argues in the private discussion that manuscript is not updated and authors did not address any of the issues during the discussion period.  r3 argues that similar to r2, dealing with covariate shift is not explained properly. reviewer is not persuaded that rex results in invariant prediction.  r1 and r4 largely positive about the paper. in the mean time, argue that organization of the paper is lacking and some of the material in the supplement is relevant and should be moved to the main text. r1 decreases their score due to the lack of reorganization during the discussion. the value of the paper is clear to me, the joint treatment of minimax perspective, domain generalization and invariances is definitely interesting and valuable. hence, the paper has merit to be published. however, the presentation is lacking significantly. the main contribution of the paper lies in table 1 but the invariant prediction property is not justified at all in the main text. hence, table 1 is not justified properly. authors discuss thm 12 in their response but they both are in the supplement. from reading only the main text, confusion of the reviewers are well justified. iclr guidelines clearly states that ...note that reviewers are encouraged, but not required to review supplementary material during the review process... it is authors responsibility to make the main paper self contained. even more worrisome is the fact that authors dismiss this concern in their response to r1 which eventually leads to r1 decreasing their score. hence, i decided to reject the paper since the presentation is subpar and authors did not persuaded reviewers that they can fix this presentation issue by the cameraready deadline. on the other hand, i think the paper can be really influential if it was written clearly. i suggest authors to revise the claims more precisely, extended the discussion on the claims and move the theorems to the main paper.", "accepted": 0}
{"paper_id": "iclr_2021_XMoyS8zm6GA", "review_text": "this paper aims to study the dimension of the class manifolds cm which are defined as the region classified as certain classes by a neural network. the authors develop a method to measure the dimension of cm by generating random linear subspaces and compute the intersection of the linear subspace with cm. all reviewers agree that this is an interesting problem and worth studying. however, there are major concerns. one question raised by several reviewers is that the goal of this paper is to analyze the dimension of the region that has the same output for the neural network; while the method and analysis are for a single datum. it is not clear if the obtained result is what the paper really aimed at. another issue is the experimental results are different from that of local analysis. the dimension estimated by using the method in this paper is much higher. based on these, i am not able to recommend acceptance. but the authors are highly encouraged to continue this research.", "accepted": 0}
{"paper_id": "iclr_2021_WW8VEE7gjx", "review_text": "overall, there were significant concerns about the motivation and experiments in this paper, and these were thought not to merit acceptance on their own. because of this, the reviewers started discussing the theory to see if that would justify acceptance. the reviewers were not able to find a clear advantage over existing approaches, nor sufficient motivation; also the presentation was found to be largely inaccessible. in the rebuttal there was a brief mentioning of background and possible implications, but they were hard to assess and the paper itself did not have such context nor was updated to have such context. for a future version, one recommendation could be to focus significantly more on context, motivation, and improvements over prior work. also, making the paper more selfcontained could help.", "accepted": 0}
{"paper_id": "iclr_2021__lV1OrJIgiG", "review_text": "the paper considers the problem of 2d pointgoal navigation in novel environments given access to an abstract occupancy grid map of the environment, together with knowledge of the agents state and the goal location typical of pointgoal navigation. the paper proposes learning a navigation policy in a modelbased fashion, whereby the architecture predicts the parameters of the transition function and then uses this learned transition function to plan the agents actions. the authors also describe a modelfree approach that extends a version of dqn to reason over the 2d maps. the paper was reviewed by four knowledgeable referees, who read the author response. the general problem of learning to navigate a priori unknown environments to reach a desired goal is an interesting problem that has received significant attention oflate in the learning community. in its current form, however, the paper does not adequately convey why this is a difficult problem that can not be solved using existing planning techniques or why it benefits from learning, particularly given access to an abstract map. these concerns apply more generally to pointgoal navigation, namely the assumption that the pose of the agent and goal are fully known throughout or the agentrelative pose of the goal and that there is no uncertainty in the agents motion. the practicality of these assumptions is unclear, and they are inconsistent with decades of research in robotics and robot learning, which addresses the more realistic setting in which there is uncertainty in pose and motion. the author response helps to clarify some of these questions, but it is still not fully clear why existing methods are insufficient for this task, whether they use traditional planning methods or are learned. revisiting the discussion of why this is a hard problem would strengthen the paper, as would a more thorough evaluation that compares against other baselines.", "accepted": 0}
{"paper_id": "iclr_2021_LuyryrCs6Ez", "review_text": "this paper was reviewed by 3 experts in the field. the reviewers raised their concerns on lack of novelty, unconvincing experiment, and the presentation of this paper, while the paper clearly has merit, the decision is not to recommend acceptance. the authors are encouraged to consider the reviewers comments when revising the paper for submission elsewhere.", "accepted": null}
{"paper_id": "iclr_2021_dN_iVr6iNuU", "review_text": "the paper tackles the qvalue overestimation problem by proposing a regularization technique to maximize diversity in representation space, preventing ensemble collapse, in order to improve the efficacy of techniques such as maxmin and ensemble qlearning. reviewers praised the originality of the method and the interesting connections drawn to economic theory, and seemed to agree that the method is somewhat effective. on the other hand, r3 pointed out that hyperparameters are tuned perdomain, the number of domains considered is small echoed by r4, and criticized the paper for failing to truly validate its central hypothesis experimentally echoed by r1. r4 raised the issue of unfair comparisons in between ensemble and nonensemble methods, while r1 raised a multitude of criticisms ranging from ahistorical attributions to confusing figures, which i will not exhaustively repeat here. based on the reviews, and the fact that the majority of reviewers concerns remain entirely unaddressed the authors only responded to r2, this manuscript is not a candidate for acceptance at this time.", "accepted": 0}
{"paper_id": "iclr_2021_NPab8GcO5Pw", "review_text": "the paper studies optimization landscapes arising the fitting of sparse linear networks to data. it argues that for scalar outputs, every local minimum is global, while for d  3 dimensional outputs, there can be spurious local minimizers. the paper also argues that similar results hold for deep networks. counterexamples on the existence of nonglobal local minimizers are constructed analytically and corroborated by probing the optimization landscape experimentally. pros and cons  network sparsification is an important practical problem, and there are relatively few theoretical guidelines on when and how sparsification can be achieved. in particular, results that help to explain why trained networks are often sparsifiable andor provide theoretical guarantees for sparsification algorithms would be significant.  reviewers raised concerns about the significance of the papers results. in particular, they found it difficult to connect the papers analysis of landscapes of linear networks to the question of when and why practically occurring networks can be pruned. they also had difficulty isolating new ideas in the mathematical analysis beyond previous works on the landscape of linear networks. finally, several reviewers expressed concerns about the extent to which the papers observations on linear networks generalize to nonlinear networks.  reviewers raised concerns about the clarity of the paper. the mathematical exposition is unclear in places conditions are not clearly stated, terminology is occasionally vague. moreover the papers handling of optimality conditions for constrained optimization problems is unclear e.g., the proof of theorem 6 uses the unconstrained optimality condition ellt x  0 in the inductive step, even though the inductive hypothesis pertains to a constrained problem.  the technical results make assumptions that are occasionally quite strong. for example, theorem 7 requires orthogonality of the data matrices, when restricted to indices where the weights are nonzero. as reviewers note, this assumption seems highly restrictive. overall, the paper addresses a topic that is important to the iclr community developing theoretical analyses of network sparsification. however, the significance of its results is not clear, and the technical exposition would need significant improvement to meet the bar for publication.", "accepted": 0}
{"paper_id": "iclr_2021_Bw7VC-DJUM", "review_text": "the paper presents an approach for weakly supervised pretraining for videos using textual information provided with web videos on youtube and instagram.  strength  the work shows strong results with relative small dataset and computational resources compared to other work in the area of selfweakly supervised learning for videos.  interesting ablations  main concerns  the authors dont discuss and compare to the weakly supervised work ghadiyaram et al. cvpr19 adequately. furthermore, the authors characterize the work incorrectly in their author response as detailed by r2. i agree to r2 here and like to highlight the concern is not that the method of ghadiyaram et al. cvpr19 being similar to this work but the leveltype of supervision.  limited novelty over prior work.  further concerns  some unclarities  the authors did not provide an updated revision of the pdf overall the paper received reject and borderline scores after author response and discussion with the strongest score 6 from r1 due to the concerns concerns listed above apart from the ability to work with small number of data. i think the missing comparison to ghadiyaram et al. cvpr19 which operates in a similar setting weights strongly and i recommend reject.", "accepted": 0}
{"paper_id": "iclr_2021_lXW6Sk1075v", "review_text": "all reviewers agreed that the novelty of the method was not at the level expected for publication, and also raised a number of technical concerns regarding the approach. there was no response from the authors on these issues, hence the reviewer consensus is that the paper is not ready for publication at this time.", "accepted": 0}
{"paper_id": "iclr_2021_GbCkSfstOIA", "review_text": "all reviewers agree that the writing is not precise. it does not help to find any novelty in the ideas, and the limited and too quickly described experiences are not convincing enough to forgive this problem. the authors chose not to oppose or comment on the detailed arguments provided by the reviewers. i agree with the reviewers in recommending the rejection of this paper.", "accepted": 0}
{"paper_id": "iclr_2021_pXmtZdDW16", "review_text": "this work develops an approach to embed random graphs some even with dependent edges, hence going beyond classical models such as erdosrenyi gn,p using gnns, and uses these to develop approximation algorithms for solving nphard scheduling problems, which typically involve some notion of minimizing weighted completion time or equivalently, the reward incentivizes early completion, where the age of a job is a linear function of time. this is then used to schedule multiple identical robots to solve a given set of spatiallydistributed tasks. the problems consideredmultirobot reward collection mrrc model vehiclerouting, rideshare etc., and are wellmotivated. this paper takes as motivation earlier work on structure2vec by dai et al. 2016 that uses gnns to approximately solve other nphard graph problems specifically, the random structure2vec developed here is used for an rl approach that learns nearoptimal solutions for the mrrc problems considered. while the papers contributions were appreciated in general, its clarity, the fact that the 1  1e bound of theorem 2 follows from classical work of nemhauser et al. 1978, and the fact that reallife examples were not considered, were considered weaknesses. the authors are encouraged to work on these aspects of the paper.", "accepted": null}
{"paper_id": "iclr_2021_5g5x0eVdRg", "review_text": "during the discussion phase, although the reviewers acknowledge superior empirical performance of the proposed method, they shared the two major concerns 1. lack of theoretical or empirical justificationproof for the key statement the current methods do not effectively maximize the mi objective because greedy sgd typically results in suboptimal local optima. 1. lack of comparisons with newer methods from e.g. eccv2020 etc. in particular, the first point is crucial. as the reviewers pointed out, since it is the main contribution and the key message of this paper, it should be carefully examined theoretically andor empirically. however, in its current state, there is no theoretical analysis, and empirical evaluation is not convincing. about the second point, although i think it cannot be a solo reason for rejection, at least it is better to cite and discuss it fo the completeness. overall, the contribution of this paper it not significant enough for publication. hence i will reject the paper.", "accepted": 0}
{"paper_id": "iclr_2021_pbXQtKXwLS", "review_text": "addressing the initialization issue in dnns is an important topic, and the proposed approach is found by the reviewers to be interesting. however, the reviewers feel that to clearly promote this research beyond the proof of concept phase, deeper investigation in multilayer architectures would be required. this would raise the significance of the paper. besides extending the study to deeper networks, the paper could also benefit from elaborate experiments to increase convincingness, in particular by addressing r4s concerns regarding robustness of performance e.g. on small dataset sizes. finally, the methodology is sound and the authors clarify the significance of the relu associated covariance; however, overall the paper does not offer significant technical advancements that could make up for the shortcomings in the areas discussed above.", "accepted": 0}
{"paper_id": "iclr_2021_J_pvI6ap5Mn", "review_text": "the paper presents a novel framework from transfer learning over gnns. experiments ought to better substantiate how structural differencessimilarities are measured, as well as relying on prior art to measure transferability success. a plan for incorporating structural features would also strengthen the present work.", "accepted": 0}
{"paper_id": "iclr_2021_5Spjp0zDYt", "review_text": "this paper investigates various pathologies that occur when training vae models. there was quite a bit of discussion including private discussion between the reviewers about the theory presented. particular concerns included for theorem 1, while the required conditions formalise the setting in which the learned likelihoods are poor, its unclear whether these particular conditions they are useful in practice or provided deep insight; for theorem 2, its relevance and importance was not necessarily clear. in general the results in these two theorems are closely related to known challenges e.g. that using the elbo to optimise parameters may lead to bias, without necessarily providing as much new insight as one might hope. i would note that all the reviewers included positive feedback as to the quality of the experiments, showing the impact of these pathologies on downstream tasks. however, as written much of the paper focuses on the theory  too many of the very interesting! figures and experimental results are relegated to the appendix.", "accepted": 0}
{"paper_id": "iclr_2021_UAAJMiVjTY_", "review_text": "the paper addresses the difficult problem of combining ilp in a metainterpretive framework with noisy inputs from a neural system. the essential idea is to use mil to efficiently search for constraints on the neural outputs eg z1  z2  z3  7, or z2 z3 as well as logic programs, with a score related to program complexity as well as probability of the best constraintsatisfying neural outputs. it is interesting work for the right audience but its clear from the reviews that the presentation was difficult for iclr readers, even ones with appropriate background. some potential weaknesses of the approach include 1  its unclear how scalable the mil framework is  presumably the intrinsic difficultly of the search means that programs and constraint sets must be small 2  its unclear how general the approach is beyond the digitsasseparateinputs setting of the two experimental studies, and its unclear how accurate the perceptual layer needs to be  mnist obviously being an example of a case where there is little noise with a modern classifier. 3  its unclear how constraints can in general be used to backprop any information to the underlying neural system, and without this the joint training seems to be quite limited. overall the paper is judged as inappropriate for iclr.", "accepted": 0}
{"paper_id": "iclr_2021_D2TE6VTJG9", "review_text": "this paper proposes a mathematical framework to theoretically understand and quantify the benefit of selfsupervision on the downstream tasks. the theoretical analyses in this paper are concrete and the authors conducted experiments to support their claims. however, the current version still has the following weaknesses.  this paper would benefit from incorporating the reviewers comments which was complained by multiple reviewers and the authors responses if any.  the authors need to make it clear in the abstract and introduction that 1 this paper only considers the reconstructionbased ssl, instead of the general ssl, and 2 addresses the discrepancy between the practice and the proposed framework.  in the postrebuttal phase, reviewer 2 pointed out lemma 3 seems much more meaningful after the clarifications. i also notice that r5 concerned about the discrepancy in downstream task setup and may doubt the mathematical framework in this paper. in fact, i agree with r5. there is indeed a gap between the proposed mathematical model and the practical ssl algorithms. there is still some work need the authors to complete, i.e., mathbbeyx_1approx mathbbeyx_1,x_2.", "accepted": null}
{"paper_id": "iclr_2021_G70Z8ds32C9", "review_text": "this paper received borderline scores, which makes for a difficult recommendation. unfortunately, two of the reviews were too short and thus were of limited use in forming a recommendation. that includes the highscoring one, which did not adequately substantiate its score. there is much to admire in this submission. reviewers appreciated the originality of this research, linking rate reduction optimization to deep network architectures  r1 the paper proposes a novel perspective  r4 the novelty of the paper is in that formulation of the feature optimisation is bakedin into a deep architecture  r5  i think the construction seems interesting and the rate reduction metric seems like a reasonable thing to optimize. i found the relationship of coding rate maximization to redunet to be quite clever  r3 short the innovative method allows the inclusion of a new layer structure named redunet reviewers also applauded the papers clarity, including r4 who raised their score to 6 based on satisfying clarity revisions from the authors  r1 the writing is good and easy to follow  r4 postdiscussion clarity is not an issues anymore  additional explanations provided by the authors and one more careful reading of the paper helped in understanding of all the aspects of the model  r2 short the paper is wellstructured. however, there were some core questions around how well the main significance claims of the paper are supported. the most indepth discussion on these topics is in the detailed thread with r5. in that thread there are many points discussed, but the two issues seem to be 1. whether the connection between redunet and standard neural net architectures is sufficiently substantiated so as to constitute an explanation for behaviors of those standard architectures, like cnns; and 2. whether the emergence of redunets group invarianceequivariance is surprising or qualitatively new. the first is much more central. on the first issue, r5 writes in summary fundamentally i think the authors propose a hypothesis that redunets explain dl models. however, the authors do not take meaningful steps towards validating this hypothesis. ... i would contrast this with, for example, the scattering networks paper httpsarxiv.orgabs1203.1513 which did an exceptional job of arguing for an ab initio explanation of convolutional networks. i find r5s perspective on this point to be compelling, in that the paper currently doesnt do enough to justify these main claims, either through drawing precise nontrivial mathematical connections or through experimental validation. the thread has a much more detailed and nuanced discussion. the second issue is not quite as central to the significance of the paper, but it was noted by multiple reviewers  r5 i may be missing something, but given the construction of redunet, i feel as though the emergence of a convolutional structure subject to translation invariance is not terribly surprising.  r4 finally, i am not sure if the result of obtaining a convnet architecture in redunet when translation invariance constraint is added the embedding is all that surprising.  r4 postdiscussion reading the exchange between the authors and r5 i am still not fully convinced that translation invariance property is all that surprising, but for me thats not a reason to reject. at the least, the paper as written hasnt yet convinced some readers myself included on these claims. as i mentioned at the start, this paper is borderline, but because i am largely aligned with r5s perspectives, i think this paper does not quite pass the bar for acceptance. i recommend a rejection, but i look forward to seeing a strengthened version of this work in the future. i hope the feedback here has been useful to bringing about that stronger version.", "accepted": 1}
{"paper_id": "iclr_2021_-gabSeMKO4H", "review_text": "this paper presents a way to use a translation memory tm to improve neural machine translation. basically the proposed model uses a ngram retrieve matching sentence or pieces and takes advantage of the useful parts using gated attention and copying mechanism. although the idea of leveraging tm in the context of nmt is not new, this work seems to be a fair contribution. my major concerns are the following 1. the retrieval part is not clearly presented, raising questions about complexities and the noise brought by the common words. the authors should give a better exposition on the ranking mechanism. 2. the experiments are not convincing enough since the proposed model is not compared to the sota and the competitive models described in the prior work. in conclusion i would suggest to reject this paper.", "accepted": 0}
{"paper_id": "iclr_2021_4zr9e5xwZ9Y", "review_text": "the paper proposes a new distributed training method for graph convolutional networks, using subgraph approximation. the reviewers raised multiple challenges, such as novelty, validity of experiments, and some technical issues. the authors did not respond to the reviewers comments. the ac agreed with the reviewers that the paper, in the current form, is not ready for publication.", "accepted": 0}
{"paper_id": "iclr_2021_c5klJN-Bpq1", "review_text": "the paper provides a neural generalization of decision trees with the idea of maintaining interpretability. the approach falls a bit short on theoretical grounds. for example, the main theorem portraying interpretability isnt properly defined and some definitions appear implicitly in the proof. the view of decision trees as a sequence of soft decisions appears to need to model how the full probability distribution over the nodes propagates at each depth. a much stronger case for interpretability rather than assuming that each t_i is interpretable should be made if this is kept as one of the main arguments for the architecture. interpretability of decision trees does not directly carry over to these models.", "accepted": 0}
{"paper_id": "iclr_2021_68747kJ0qKt", "review_text": "this paper analyzes dropout and shows it selectively regularizes against learning higherorder interactions. the paper received mixed reviews, with two in favor of rejection and one in favor of acceptance. specifically, while all reviewers find the intuitions and ideas in the paper adequateplausible, two reviewers didnt find sufficient evident that supports the conclusions. the reviewers provided very detail feedback, which the authors responded to, but it is apparent that some of the analysis needs to be reviewed again before the paper can be published.", "accepted": 0}
{"paper_id": "iclr_2021_NrN8XarA2Iz", "review_text": "this paper presents an approach to reward shaping in rl centred on the question of how to select between different shaping signals. as such this is an interesting research direction that could make important contributions in the area. generally the reviewers felt that the paper is too preliminary in its current form. there were several questions raised around problems with the technical formulation. it was also felt that the experiments could be more rigourous to fully validate the claims of the paper.", "accepted": 0}
{"paper_id": "iclr_2021_HfnQjEN_ZC", "review_text": "this paper initially received three negative reviews 4,4,4. the main concerns of the reviewers included limited methodological novelty and an oversimplistic experimental setup. the authors did not submit their responses. as a result, the final recommendation is reject.", "accepted": 0}
{"paper_id": "iclr_2021_HMqNjkBEqP4", "review_text": "this submission generated a lot of discussion. the main strengths of the paper  it is an interesting application of metalearning to 3d shape completion, and a novel one.  it appears to work well it is remarkable that the proposed model can reconstruct shapes as well as it does given a point cloud with only 50 input points. the main concern raised by two reviewers is that while the method is described using the language of metalearning, the proposed architecture ends up looking very similar to a variational autoencoder a point cloud encoder that outputs some distribution in a latent space, which is then sampled from to produce a code which drives an implicit surface decoder. the only difference appears to be that the proposed method uses a factored distribution in latent space, whereas a traditional vae uses a nonfactored one i.e. a single multivariate gaussian over all dimensions of the latent code. one reviewer engaged the authors in a discussion about this point, but the resulting conversation was not satisfactory. one interpretation of the authors response is that they are simply not aware of how a vae could be trained using variablesized point clouds as input which is quite possible using many standard point cloud processing networks. however, at other points, they do seem to grasp this, when they write that this flexible representation of the uncertainty i.e. the one proposed by the authors cannot be attained by the mere average or max pooling aggregation what a pointnet encoder would do. they even go on to provide an additional ablation study where they replace their factored probabilistic encoder with a deterministic meanmax pool encoder, and show worse results. unfortunately, they never compare against probabilistic variants of such encoders i.e. where the meanmax pool output is then used to compute a mean and variance. without seeing this comparison, the reviewers believe that this paper cannot be accepted, and i am inclined to agree. on a related note one reviewer pointed out an issue with unfair comparisons, in that baselines were trained on high density point clouds and evaluated on low density ones. the reviewer noted that these methods could be and should have been trained on point clouds of varying density. perhaps this relates to my hypothesis that the authors initially did not understand that training such encoders on variablesized point clouds was possible. in any event, in their rebuttals, they have reported some preliminary results from experiments which do this type of training, but these results are not conclusive. complete, conclusive results from these experiments would also need to be presented before this paper could be accepted.", "accepted": 0}
{"paper_id": "iclr_2021_OAdGsaptOXy", "review_text": "the authors propose to improve the lms ability on modelling entities by signalling the existence of entities and also allowing the model to represent entities also as units. the embeddings of the surface form and then entity unit are then added and passed through a layer to predict the next word. the paper evaluates on qa and conducts probing tasks and shows that such an entity modelling results in better performance. all reviewers have found the idea conceptually simple and novel. at the same time a number of concerns are raised, with the most important being the lack of understanding around which and how hyperparameters matter for this model and, most importantly, the confounder introduced to the model by the much larger size of parameters introduced by the embedding layers. while the authors comment that not all the parameters are used all the time, the size of the embeddings still count at the total size of parameters a model has. thus, without properly controlling for this e.g., have an another model where the extra embedding params are given to another part of the model, it is difficult to determine whether adding more parameters was the solution or adding more parameters for modelling the entities.", "accepted": 0}
{"paper_id": "iclr_2021_loe6h28yoq", "review_text": "some reviewers expressed concerns on soundness of the theory in the paper. specifically, theorem 3 does not seem to be correct. there are other concerns such as the significance of the theoretical contributions, little empirical value and existence of much stronger results. unfortunately the authors did not provide responses to the concerns raised by the reviewers.", "accepted": 0}
{"paper_id": "iclr_2021_Twm9LnWK-zt", "review_text": "this paper investigates the use of classconditional architectures in gans. it achieves this by employing neural architecture search nas on top of reinforcement learning. their main contribution is a flexible and safe search space; experiments are carried out on cifar10 and 100. standard performance results are augmented by diagnostic studies. this paper received a total of five reviews which, remarkably, yielded the same assessment the paper had merits but was marginally below the acceptance threshold. in general, the reviewers thought the idea was interesting and straightforward, experiments extensive and the paper was clearly written. the primary concerns brought up were novelty i.e. just cgan  nas?, r1 and r2, minimal performance gain r4, r5, unclear motivation r2, lack of comparisons  including to other nas for gan methods  r1,r3, limited to lowresolution datasets r2,r3, no reporting of time or space complexity r1,r3, unclear where improvement comes from  no control for capacity  r5. on the point about comparing to nas  gan works, the authors responded, stating that the nas  gan methods brought up by the reviewer were unconditional gan methods and pointed out that they made unconditional gan comparisons in the appendix. the authors also emphasized to multiple reviewers that the point of the paper was not to improve nas. interestingly, they also made a comment to r5 that the point of the paper was not to improve performance of cgans, but to improve understanding of them. the reviewers are unanimous in that this paper falls just below the bar for the reasons outlined above. following the discussion phase, i see no reason to overturn their recommendation. i hope that the authors can use the feedback from these reviews to improve this paper and resubmit it.", "accepted": null}
{"paper_id": "iclr_2021_ptbb7olhGHd", "review_text": "one reviewer is positive, but that review is not of high quality. the other reviewers agree that this paper is interesting, but has too many limitations to be accepted by a highly competitive venue such as iclr.", "accepted": 0}
{"paper_id": "iclr_2021_RcJHy18g1M", "review_text": "in this paper, a data mapping method to a latent space designed for outlier detection is proposed. outlier detection by latent space mapping has been extensively studied in the literature. unfortunately, this paper does not fully discuss the relation of the proposed method with a large amount of existing literature and lacks novelty.", "accepted": 0}
{"paper_id": "iclr_2021_AT7jak63NNK", "review_text": "the paper was evaluated by 4 knowledgeable reviewers and got mixed scores. while most reviewers appreciated the new intuitive approach to meta rl. there were severe concerns about algorithmic choices and the evaluations that led to a poor score from some reviewers. these concerns are summarized below  the motivation of experience relabeling for out of distribution samples is not clear r2  it is unclear why experience relabeling does not work for in distribution samples r2, r4  the reported performance is not a fair comparison as it is typically not known when a task is indistribution or outdistribution, so we would either have to take always experience relabeling or never or learn when do use which algorithm  the paper falls short in terms of evaluations r3, r4, in particular it remains unclear to me if mier can, under realistic circumstances. it is suggested to use more established benchmarks such as metaworld to evaluate the performance of mier. for the given reasons, i recommend that the authors do these corrections and go through another round of reviews at another conference.", "accepted": 0}
{"paper_id": "iclr_2021_53WS781RzT9", "review_text": "this work analyses the impact of minibatch size on the variance of the gradients during sgd, in the context of linear models. it shows an inverse relationship between the variance of the gradient and the batch size for such models, under certain assumptions. reviewers generally agree that the work is theoretically sound. however, all reviewers believe that the contributions of this work are limited. this concern was not adequately addressed during the discussion phase and led to the ultimate decison to reject.", "accepted": 0}
{"paper_id": "iclr_2021_EQtwFlmq7mx", "review_text": "all reviewers recommend rejection concerns were raised in terms of technical correctness, quality of presentation and the quality of experiments. there was no rebuttal. the ac agrees with the reviewers and recommends rejection.", "accepted": 0}
{"paper_id": "iclr_2021_1sJWR4y1lG", "review_text": "this paper provides a new perspective on deep networks by showing that npk is composed of base kernels and their dependence on the architecture is explicitized. it is further shown that learning the gates can perform better than random gates. while the paper provides interesting understanding neural networks, it is unclear what practical benefit can be drawn from it. on the architectures considered such as fc, resnet and cnn btw, it seems restricted to 1d, it will be important to show that such insights lead to new models or learning algorithms that improve upon the standard practice in deep learning or get very close to. it is debatable whether drawing such a nontrivial insight alone warrants publication at iclr, while nontrivial itself is a subjective judgement. i understand people differ in their opinions, and the ntk paper has been impactful. unfortunately since there are quite a few other papers that are stronger, i have to recommend not accepting this paper to iclr this time.", "accepted": 1}
{"paper_id": "iclr_2021_0p-aRvcVs-U", "review_text": "this paper proposes alphavil, a method for weighting the taskspecific losses in a multitask setting in order to optimize the performance on a particular target task. the idea is to first collect gradient updates for the model based on all the separate tasks, and then reweight those updates in order to optimize the loss on a heldout development set for the target task. in practice, this metaoptimization is performed with gradient descent. experiments on multimnist and several tasks that are part of glue and superglue show that alphavil is close in performance to a baseline multitask method and discriminative importance weighting. strengths  the idea is intuitively appealing. directly reweighting tasks as a metaoptimization step is straightforward and appears to not be proposed previously in the literature.  the paper is clear in its presentation. weaknesses  the reviewers agree that the main weakness is that the experimental results do not show that alphavil offers any substantial benefits over existing methods. on the multimnist task, while alphavil tends to have the highest mean performance, the difference is small less than a standard deviation. on the gluesuperglue tasks, it outperforms other methods on only 1 out of 10 experiments. there are also no confidence intervalsstandard deviations provided to assess the significance of the results.", "accepted": 0}
{"paper_id": "iclr_2021_MhTgnultR1K", "review_text": "although this paper tackles an important problem, all reviewers agree that it requires further work before it can be published. first, the paper would need to be polished in order to be easier to read. stronger experiments would also be needed in order to support the claims of the paper, e.g. by considering additional datasets and proper baselines. finally, an important concern about this paper is novelty and originality. it is not clear at this point that the contribution is substantial enough for a conference like iclr. addressing these points would significantly improve the paper.", "accepted": 0}
{"paper_id": "iclr_2021_oLltLS5F9R", "review_text": "four knowledgeable referees have indicated reject mainly because of limited motivation r1,r3,r4, limited insights on the proposed approach r1,r2,r3,r4, and inconclusive results r1,r2,r3,r4. the claims of the paper could have been strengthened by e.g. discussing currently missing experimental details r1, performing statistical significance tests r2,r4, and including comparisons to baselinespreviously introduced normalization strategies r2,r3. unfortunately, there was no rebuttal. the paper is therefore rejected.", "accepted": 0}
{"paper_id": "iclr_2021_xVzlFUD3uC", "review_text": "this paper studies the behavior of sgd for linear models fit with the squared euclidean loss. there are three main results the first result sec. 4 studies the behavior where instead of regularizing the objective, gaussian noise is added to the inputs. the main result is a sufficient condition for how the learning rate and noise can jointly change over time in order for sgd on the mse error with noisy input to asymptotically converge to the same solution as regular gradient descent without noisy input. the second result sec. 5 is slightly more general in that is considers the case where the noise can be an isotropic gaussian where the variance changes over time. again, a result is given for how the learning rate interacts with the data in order to asymptotically converge to the unregularized solution. this is first studied in thm 5.1 then assuming powerlaw decay in the noise in thm. 5.2. it should be emphasized that though these are asymptotic guarantees, the results give asymptotic rates of convergence. in my opinion this is a significant strength of the results that was not emphasized by the reviewers. the third result sec. 6 studies sgd for leastsquares linear models where the stochasticity is due to data subsampling only. the fraction of subsampled data may change over time. the primary sentiment from reviewers was that the mathematical complexity of the paper meant that they could not understand it or give a fair review. more on this below. for this reason, and because the overall reviews are somewhat borderline, i read the paper in detail. a specific concern raised by two reviewers was that the paper first presents a very general framework but then studies very restricted specific problems. some reviewers felt that the paper was very wellwritten, while others felt it was poorly written. there are no experiments. for my part, i mostly concur that the paper is wellwritten albeit quite technical. however, i agree with the concern from reviewers that the technical results all concern extremely restricted settings, and its not clear what value the extremely general setup brings. i also find the title of the paper a bit puzzling. for specifics of the results, the practical value of sections 4 and 5 is unclear. its wellknown that adding data noise is exactly equivalent to adding ridge regularization when doing linear regression. but ridge regularized linear regression would be a nonstochastic problem. so what is the value of studying the convergence rates in this case? the paper never makes this clear. i have concerns about the exponential convergence rate in thm. 6.1. the paper claims that an exponential convergence rate for sgd has been extensively studied. i do not believe this is true. in general sgd does not have an exponential convergence rate. there are modified methods like sag that achieve this on finite data sets, but thats not whats studied here. the paper cites two papers the first is bottou et al. 2018. this is a lengthy review, with no specific reference given. i am familiar with it and also spent time searching but could not find a specific result. ma et al. 2018 is also cited. this indeed gives an exponential convergence rate but assuming that at the optimum the loss for all datapoints is zero! no such assumption is made in the submitted paper, and the issue is not further discussed. this is cause for grave concern.", "accepted": null}
{"paper_id": "iclr_2021_oh71uL93yay", "review_text": "three of the reviewers are significantly concerned about this submission while r3 was positive during review. during discussion, r3 also agreed that there are concerns not only on experimental designs and results but also the proposed model. thus a reject is recommended.", "accepted": 0}
{"paper_id": "iclr_2021_Yj4mmVB_l6", "review_text": "this paper provides a unified view of some known methods for monotone operator inclusion problems like forwardbackwardforward fbf and ogda, and provides new convergence results for the stochastic version of a variant of fbf called fbfp. all reviewers initially recommended rejection. the rebuttal and the manuscript update addressed several concerns from the reviewers, though the general consensus after rebuttal was still that the paper lacked in significance for the iclr community. the ac thinks that the paper could make an interesting overview paper in a more optimization  theoretically minded venue.", "accepted": 0}
{"paper_id": "iclr_2021_OSynkDOWbk2", "review_text": "the paper proposes three discretization schemes for two firstorder optimization flows, and proves the convergence to the minimizers of the problem that the optimization flows approach. the methods are tested on the dnn training problem and show comparable performance. pros 1. the problem being studied, the discretization of optimization flows, is of interest to the community. 2. convergence guarantee is provided. cons 1. the theoretical analysis is somewhat preliminary, as the authors have admitted. there is a prescribed epsilon in the approximation error 23 that prevents the right hand side of 23 from approaching zero. the parameter eta, depending on the chosen accuracy epsilon, should be provided so that a user can implement the discretization schemes if she is interested. moreover, by specifying eta, it may be possible to compare the numbers of iterations to approach an epsilonsolution between the proposed discretization schemes and other optimization methods for solving the original optimization problem. by doing this, the motivation issue from reviewer 1 and the ac can be resolved. purely discretizing an optimization flow is of less interest to the machine learning community. 2. although the comparison on academic problem is obviously advantageous, the comparison on dnn training is only comparable or marginally better. the author responses resolved part of the challenges from the reviewers, but the key issues remained as communicated in confidential comments. since the final average score is below threshold, the ac decided to reject the paper.", "accepted": 0}
{"paper_id": "iclr_2021_GafvgJTFkgb", "review_text": "the authors study bias amplification zhao et al, 2017 and propose an improved metric for measuring it. the authors also discussed normative issues in bias amplification predicting a sensitive feature, and how to measure amplification when we do not have labels, or where labels correspond to uncertain future events. while the reviewers acknowledged the importance to study bias amplification, normative measures and social context, they raised several important concerns 1 limited technical contributions r3 and r4  see reviewers concerns that the metric is the only technical contribution; one possible suggestion is to propose a mitigation strategy based on the proposed metric similarly to zhao et al, 2017; 2 the usage of error bars because of the rashomon effect seems incomplete and almost trivial r3, r4, lacks a proper grounding r1  see two suggestions by r3 how to revise; these suggestions have not been discussed in the rebuttal; 3 empirical evidence lacks a controlled scenario of tuning the bias source to evaluate consistency suggested by r3 and is limited in the context of algorithmic fairness benchmarks r4  this has been partly addressed in the rebuttal; 4 normative contributions and broader discussions are oversimplified  see r3s comments and suggestions on how to better position the paper. among these, 3 and 4 did not have a major impact on the decision but would be helpful to address in a subsequent revision. however, 1 and 2 make it very difficult to assess the benefits of the proposed work and were viewed by ac as critical issues. a general consensus among reviewers and ac suggests, in its current state the manuscript is not ready for a publication. it needs technical strength, more empirical studies and polish to achieve the desired goal. we hope the detailed reviews are useful for revising the paper.", "accepted": null}
{"paper_id": "iclr_2021_RmB-zwXOIVC", "review_text": "this is a difficult borderline decision, with the reviewers evenly split in their final recommendation. overall, the authors provided good responses to the reviewer questions this was much appreciated. the reviewers requested additional ablations and explanations, which the authors provided. a prevailing concern is that the experimental evaluation, restricted to a few standard mujoco environments, does not really demonstrated a distinctive advantage for the proposed approach. in fact, one of the new ablations added raises concerns about the significance of the papers main technical contributions the lambda_f0 row added to table 3 shows very strong reward results, which apparently obviates a key aspect of the proposed approach. this work is interesting, and would like to see it published, but the current state of the evaluation does not support the significance of the main contribution. i think the authors need to expand their empirical evaluation, as suggested, to better highlight the effectiveness of the proposed approach over the lambda_f0 baseline. in the end, i think the authors would be better served by broadening the evaluation, isolating scenarios where the key proposal shows significant rather than marginal benefits, and publishing a more compelling version.", "accepted": 1}
{"paper_id": "iclr_2021_bW9SYKHcZiz", "review_text": "after the rebuttal phase, all reviewers give borderline scores leaning slightly positive, one of these noted in the comment rather than final review. while the reviewers recognize the potential merit of the contribution efficiency while preserving effectiveness, support for acceptance is not sufficient. the major concerns include novelty shared by multiple reviewers and the limited experimental settings.", "accepted": null}
{"paper_id": "iclr_2021_yN18f9V1Onp", "review_text": "this paper investigates how to deploy adaptive learning rates in multiagent rl marl. in particular, the learning rates are adaptively chosen based on which directions maximally affect the qfunction, and take into account the interplay and balance between the actors and the critics. the topic is certainly of great interest when designing fastconvergent marl algorithms. however, the reviewers point out the inadequacy and insufficiency of empirical gains in the reported experiments. also, largerscale experimental settings are needed in order to provide more convincing evidence about the practical benefits of the proposed scheme.", "accepted": 0}
{"paper_id": "iclr_2021_nxJ8ugF24q2", "review_text": "the paper proposes to train a rejection sampler in the latent space of a gan to learn disconnected data manifolds. reviewers raised concerns about some theoretical aspects of the method as well as about the lack of larger scale datasets imagenet in the experiments. authors responded to these concerns but some of them still remain including hatgammaz not guaranteed to be a probability distribution and lack of more convincing experiments. i still think the work is promising, and encourage the authors to revise and resubmit the paper addressing these points highlighted by the reviewers.", "accepted": 0}
{"paper_id": "iclr_2021_szXGN2CLjwf", "review_text": "this paper proposes a new variant of adaptive stochastic gradient method that has notable differences from adam, and claims the advantage of adaptive variance reduction. while the algorithm construction looks novel, there are several concerns by the expert reviewers on the theoretical results of the paper, including lack of clarity and its guidancerelevance to the practical performance. there are also questions on the practical merit of the paper pointing to in limitations on the numerical experiments. i recommend rejection of the paper in the current form, and hope the reviews can help the authors to improve it in both theoretical and empirical aspects.", "accepted": 0}
{"paper_id": "iclr_2021_ox8wgFpoyHc", "review_text": "the paper introduces some good ideas, but i dont think it is quite there in terms of a method to be recommended for publications. i think it is mostly reasonably written i do not agree with the comment of a complete rewrite but there are indeed some passages for improvement for instance, an equation as y  \u03c31q0t, x  \u03b5ht, x, section 2, needs comments, as the left hand side is discrete and the right hand side is continuous, unbounded. my main concern is the disregard for identification. some citations are unclear the secondtolast paragraph in section 4 cites a few papers in identification that have little to do with the problem here, which is proxybased. the papers cited dont even mention latent variables at all. as stated, the split in three sets of variables as suggested by figure 1 is just an idealization there is no reason at all they can be identified, and actually the theory where just zc is considered impose a lot of restrictions on when we can possible identify zc see e.g., miao et al. 2018, biometrika, httpsarxiv.orgpdf1609.08816.pdf . i know that some papers like louizos et al. play fast and loose with identification too, but at least their z_c structure they aim at has been studied elsewhere like the miao et al. paper, while here, like the zhang et al. paper cited, may be leading researchers to an unfruitful path. this, combined with the relative modesty of the novelty, is the primary reason for my recommendation. i do think the paper can be improved in a productive way by investigating it from the point of view of either i the theoretical justification for identification; ii or from a more empirical direction with much experimentation on the different ways the structured latent space is capturing confounding the target learning aspect of it is pretty much orthogonal to this.", "accepted": 0}
{"paper_id": "iclr_2021_8nXkyH2_s6", "review_text": "the paper studies the behavior of the intermediate relulike activations of trained neural networks and show empirically that the intermediate activation can be used as a hashing function for the examples with some key advantages, including almost no collisions and that there are desirable geometric properties i.e. can use kmeans, knn, logistic regression on these embeddings. pros  the experimental analysis was solid and thorough investigating the effects of model size, training time, training set, regularization and label noise. cons  an overall lack of novelty. it is already quite wellknown throughout the ml community that in many cases, using the intermediate embeddings serve as useful features to apply more classical methods such as knn and clustering. overall, the reviewers appreciated the solid and thorough investigation into the hashing properties of neural network activation patterns, which convincingly confirms some intuitions about the behavior of activation patterns in neural networks. however, the reviewers also agreed that there was no significant new finding. there have already been many studies on clustering and knn on the embeddings of a network. thus, the core novelty of the paper appears to be the finding that almost every linear region has at most one datapoint after training which does not seem too surprising given hanin  rolnick icml 2019; however, without further novel implications of this finding, the impact of the paper is limited.", "accepted": null}
{"paper_id": "iclr_2021_MY5iHZ0IZXl", "review_text": "this paper propose a method to explain the contextualization of bert by identifying a set of influence paths from the input to the output. although all reviewers give overall score 6, their comments are pointing to the negative direction. the following excerpts summarize the general sentiment of the reviews r2 overall, i incline toward rejecting. this paper provides an instrument to explain bert, but i have a hard time understanding the result itself influence paths or patterns. i also have a major concern with the final analysis and its findings. r4 i think the paper has improved substantially. the direction is exciting, but even with the updates i believe this paper needs quite some work to improve the presentation and clarity, which is why i have not updated my score. r1 overall, i think there are certainly interesting analyses that could come out of this work, but the current paper does not provide a clear enough contribution to be ready for publication. r3 it looks like the proposed method is largely based on lu et al, 2020; the major difference is the introduction of multipartite patterns, which basically expands the objects of influence analysis from paths to patterns partial paths. this doesnt look like a significant novelty.", "accepted": null}
{"paper_id": "iclr_2021_XoF2fGAvXO6", "review_text": "this paper proposes a weakly supervised model for numerical reasoning. after discussion with the reviewers it seems that it is already known that training nmns directly on drop is not successful and requires taking additional measures. past work nerd has resorted to using data augmentation, and this work encodes it directly to the model. this paper needs to show the advantages of their approach and that it generalizes better to other scenarios. other minor issues include a clarity fo writing b focus on a subset of questions c no evaluation on other numerical datasets d mild inaccuracies w.r.t prior work genbert", "accepted": 0}
{"paper_id": "iclr_2021_szUsQ3NcQwV", "review_text": "this paper proposes an attention based technique to focus on relevant entities in multiagent reinforcement learning. while the effectiveness of the proposed method is demonstrated on some tasks, there remain major concerns including the following 1. it is not sufficiently convincing that the proposed method performs well in more complex domains 2. novelty over agarwal et al. and maac is rather minor", "accepted": null}
{"paper_id": "iclr_2021_UoAFJMzCNM", "review_text": "this paper introduces a scalable method for fsp based on fbsde. the method is theoretically derived then applied on two problems, one simple but with many 1000 agents, and one with only 2 agents but partial observability. the main strength of this paper lies in the scalability and the time complexity of the proposed method. computing nash equilibriums for many agents is a difficult problem and this paper is interesting in this aspect. however, the reviewers point out several weak points to this paper. the difference with a previous work by han, hu and long needs to be highlighted. some parts of the paper are not clear, and too much of the important results are pushed into the appendix. maybe this work is not best fitted to a conference format, and should be submitted to a journal? another concern raised by the reviewers is that the experimental section does not show significant enough results, and that it is surprising to see a 2agents problem as an illustration of a method that is aiming at addressing scalability with respect to the number of agents. reviewers agree on rejection for this paper, although by a small margin. i therefore recommend rejection. i think that if the authors improve this paper by following the reviewer suggestions, it can be accepted in a future venue.", "accepted": 0}
{"paper_id": "iclr_2021_r1d-lFmO-cM", "review_text": "this paper has been evaluated by three expert reviewers, two of whom recommended rejection and one acceptance. two of the three reviews are particularly detailed and thorough. both point out a few points of conceptual issues that leave the reader confused. these key issues have not been addressed sufficiently in the rebuttal to result in changing the reviewers assessments. one major concern is lacking novelty of the work as presented, which limits its current utility to the iclr audience. i recommend a rejection.", "accepted": 0}
{"paper_id": "iclr_2021_apiI1ySCSSR", "review_text": "the paper compares transfer learning with finetuning and joint training and then proposes a new approach merlin. reviewers have pointed to the fact that merlin works in a setting that is different from normal transfer learning settings it assumes some target domain data is available during training. the authors acknowledge this and think it can still be a reasonable setting, but of course it makes comparisons more difficult. overall, while there are interesting analysis and results, the paper remains borderline and more work should be done to make it a good contribution, including significantly improving the presentation to make clear the distinction in settings. i therefore recommend to reject the paper.", "accepted": null}
{"paper_id": "iclr_2021_3eNrIs9I78x", "review_text": "this paper proposes a method to update the learning rate dynamically by increasing it in areas with higher sharpness and decreasing it otherwise. this would the hopefully leads to escaping sharp valleys and better generalization. authors further provide some related theoretical results and several experiments to show effectiveness of their models. all reviewers find the proposed method wellmotivated, novel and interesting. the paper is wellwritten and easy to follow. however, both theoretical results and empirical evaluations could be improved significantly 1 the theoretical results as is provides little to no insight about the algorithm and unfortunately, authors do not discuss the insights from the theoretical results adequately in the paper. see for eg. r1s comments about this. 2 given that the theoretical results are not strong, the thoroughness in empirical evaluation is important and unfortunately the current empirical results is not convincing. in particular, there are two main areas to improve a based on the appendix d, the choice of hyperparameters seem to be made in an arbitrary way and all models are forced to use the same hyperparameters. this way, the choice of hyperparameters could potentially favor one method over the other. a more principled approach is to tune hyperparameters separately for each method. b it looks like the choice of epochs has been made in an arbitrary way. for all experiments, it would be much more informative to have a figure similar to the left panel of fig. 4 but with much more epochs so that reader can clearly see if the benefit of salr would disappear with longer training or not. c based on the current results, salrs performance is on par with that of entropysgd on cifar100 and wp and there is a very small gap between them on cifar10 and ptb. i highly recommend adding imagenet results to make the empirical section stronger. the other option is to compare against other methods in finetuning tasks. that is, take a checkpoint of a trained model on imagenet and compare salr with other methods on several finetuning tasks. given the above issues, my final recommendation is to reject the paper. i want to thank authors for engaging with reviewers during the discussion period and adding several empirical results to the revision. i hope authors would address the above issues as well and resubmit their work.", "accepted": 0}
{"paper_id": "iclr_2021_TVbDOOr6hL", "review_text": "the authors suggest a vae model for causal inference. the approach is motivated by cevae louizos et al., 2017 which uses a vae to learn a latent representation of confounding between the treatment, target, and covariates. this paper goes beyond this approach and tries to design generative model architectures that encourage learning disentangled representations between different underlying factors of variation inspired by hassanpour  greiner 2019. the reviewers agreed that the topic will be of interest to a large group of readers. while the first version of the papers raised questions about the experimental design, several questions on the architecture design were addressed during the rebuttal period e.g., deeper architectures. other improvements were suggested and not adopted e.g., alternative methods to achieve better disentanglement. the ablation studies seem to suggest that some of the loss terms are not actually needed and that nonprobabilistic autoencoders beta0 also work well. we recommend aiming at improving the writing quality and coverage of more background material on the proposed architectures and causal factors.", "accepted": null}
{"paper_id": "iclr_2021_0MjC3uMthAb", "review_text": "the paper proposed a shotconditional form of episodic finetuning approach for fewshot image classification. there were a number of concerns raised, e.g., there lacks of sufficient comparison with sota baselines, the justification on the significance of shotaware approach is not entirely convincing, and incremental contributions in both novelty and improvements. while some of these issues were improved in the rebuttal, the revision remains not satisfied by the reviewers. overall, i think the paper has some interesting idea, but is still not ready for publication.", "accepted": 0}
{"paper_id": "iclr_2021_HW4aTJHx0X", "review_text": "the paper attempts at generating two types of summaries for scientific papers summary of contribution and summary of background context. most reviewers appreciated the motivation and found this research to be quite interesting and useful, however all reviewers had concerns regarding both executionpresentation of the ideas. while authors try to address some of the concerns, there are many clarifying questions and points raised by reviewers. addressing all these points requires rather a major revision. therefore the paper is not quite ready yet and would benefit from another iteration.", "accepted": 0}
{"paper_id": "iclr_2021_2G9u-wu2tXP", "review_text": "reviewers raised several concerns about the paper guided by unfounded heuristics as well as the artificiality of the tasks involved. rebuttal only answered a few of them and did not convince the reviewers which has been clearly stated in the response. we hope that the authors will improve the paper for future submission based on the reviews.", "accepted": 0}
{"paper_id": "iclr_2021_Qe_de8HpWK", "review_text": "this paper proposes a new quantum machine learning framework which is evaluated on the mnist dataset. while the paper was relatively well written, reviewers noted that most of the ideas are already well established and used in quantum machine learning community. thus it was not clear what novelty is provided relative to related work.", "accepted": 0}
{"paper_id": "iclr_2021_zsKWh2pRSBK", "review_text": "the paper argues that a successful backdoor attack on classifiers is connected with further fundamental security issues. in particular they demonstrate and not only an original backdoor trigger but also other triggers can be inserted by anyone with access to the classifiers. furthermore, the alternative triggers may appear very different from the original triggers, which confirms the claim in the papers title that such classifiers are fundamentally broken. the paper offers an interesting insight into the features of poisoned classifiers. however, such insight is diminished by the fact that the proposed attack requires a substantial manual interaction. the user must manually analyze the adversarial examples generated for robustified classifiers in order to determine the key parameters of alternative triggers. while manual intervention as such does not undermine the main observation of the paper, this makes an automatic exploitation of this idea hardly feasible and hence decreases the significance of the papers main result.", "accepted": null}
{"paper_id": "iclr_2021_VyENEGiEYAQ", "review_text": "the paper attempts to make transformers more scalable for longer sequences. in this regards, authors propose a clusteringbased attention mechanism, where only tokens attends to other tokens in the same cluster. this design reduces memory requirements and allows more information mixing than simple local windows. using the proposed approach, new stateoftheart performance is obtained on natural questions long answer, although marginal. however, reviewers raised numerous concerns. first, the novelty of the paper compared to prior work like reformer or routing transformer which also conceptually does clustering is not resolved. second, the claim that kmeans yields a more balancedstable clustering than lsh is not well established. finally, why clustering, i.e. attention between similar vectors is better than dissimilar or randomly chosen vectors or does is it even as expressive is not clear. thus, unfortunately i cannot recommend an acceptance of the paper in its current form to iclr.", "accepted": null}
{"paper_id": "iclr_2021_MBdafA3G9k", "review_text": "the paper considers the problem of learning to imitate behaviors from visual demonstrations, without access to expert actions. consistent with recent approaches, the proposed method uses a neural network to measure the similarity between visual demonstrations and the agents behavior, and employs this metric as a reward in rl. the primary contribution is the use of a recurrent siamese network that is trained to measure the distance between motions, as a means of better dealing with the challenges of imitation learning from a small number of as few as one noisy visual demonstrations. experiments on a variety of simulated domains show that the proposed approach achieves reasonable results. the paper was reviewed by four knowledgeable referees, who read the author responses and engaged in extensive discussion. the reviewers agree that learning to imitate behaviors from a small amount of noisy demonstrations is a challenging and important problem that is of significant interest. the proposed method nicely extends existing approaches to visual imitation learning, and the results reveal that the method performs well in a variety of continuous control domains. the reviewers raise several concerns regarding the clarity of the technical presentation and the sufficiency of the experimental evaluation. the authors have made a significant effort to address these concerns in their responses and updates to the paper, which the reviewers very much appreciate. however, some of the reviewers primary concerns regarding clarity and the thoroughness of the experimental evaluation remain. this work has the potential to make a really nice contribution and the authors are encouraged to take this feedback into account for any future version of the manuscript.", "accepted": 0}
{"paper_id": "iclr_2021_nzKv5vxZfge", "review_text": "i found the main algorithmic contributions to be interesting and of potential value to practitioners, as highlighted by reviewer 3. like several reviewers, i found the causal framing to be confusing, or at least not really to be framed in a framework like pearls the word confounding is thrown a few times in the manuscript, but there is no formal sense by which it is linked to what we commonly understand as confounding. we are still talking about what happens inside a predictive model a deterministic function, not what happens in the real world the authors are not alone as targets of my observation my point applies to a lot of the papers in the references, where the causal interpretation is hardly illuminating for those coming from a causal inference background, for instance. the reply to reviewer 2, for instance, cites 1, which is about granger causality and has little to do with pearls framework. despite its name, granger causality is a probabilistic concept or, at best, an idea for identifying noncausality with a very minimal causal basis besides the use of time ordering. a much more rigorous explanation of confounding in this papers context needs to be provided. that been said as helpfully highlighted by reviewer 3 and summarized without any need to resort to a causal framing, there are several positive contributions added here, which might be of interest to the iclr audience. the causal framing unfortunately gets in the way without adding clarity. in its present state, the paper is not yet ready for publication. we hope that the reviewer comments prove helpful for preparing a strong future submission.", "accepted": null}
{"paper_id": "iclr_2021_ARFshOO1Iu", "review_text": "this paper introduces a selftraining strategy for semisupervised learning for few shot sequence learning. it builds on ideas from an existing work on robust deep learning that adaptively reweights examples for learning to reduce impact of noisy examples, here the noisy examples are introduced to the student network training by the teacher network. two main novel points, one is on selectively constructing the validation set used for adaptive reweighting. another idea is to move from the sentence level reweighting to token level reweighting. the paper shows strong results suggesting the proposed method can effectively learn under fewshot learning. a primary concern from the reviewers is that the paper has limited novelty given that it primarily applies existing ideas to a slightly different problem. another concern is that the system consists of many components, each of the choices could have other viable options. the ablation studies indicate these components are useful compared to when removed, but fail to explore possible alternative choices. one of the questions is whether tokenlevel reweighting is necessary. it would have been nice to see an ablation study comparing against a baseline using sentencelevel reweighting.", "accepted": 0}
{"paper_id": "iclr_2021_IkYEJ5Cps5H", "review_text": "this paper proposed a new optimization framework for pruning cnns considering coupling between channels in the neighboring layers. two reviewers suggested acceptance and two did rejection. the main concerns of the negative reviewers are a limited novelty, b limited performance metrics and c limited baselines. the authors response did not fully clarify the reviewers concerns during the discussion phase, and ac also agrees that they should be resolved to meet the high standard of iclr. hence, ac recommend rejection. here is additional thought from ac. the authors propose oursc and ourscs. the latter is reported to outperform the former in terms of flops, but ac thinks the former may have merits in other more important performance metrics, e.g., the actual latency andor memory consumption on a target device. more discussions and results for this would strengthen the paper.", "accepted": null}
{"paper_id": "iclr_2021_akgiLNAkC7P", "review_text": "this paper introduces icrl, where the rl agent is supposed to maximize the reward under unknown constraints, which should be inferred from the expert demonstration. reviewers generally agreed that this is an interesting work, and potentially make rl to be applied to more general settings. however, they also would like to see more experimental results with baselines e.g. agents based on irl and also related constrained learning approaches to make the motivation behind the approach more convincing. i hope these concerns are addressed in the future work.", "accepted": 0}
{"paper_id": "iclr_2021_QM4_h99pjCE", "review_text": "the paper offers a direction for multagent rl that builds on results for actorcritic methods zhang, icml 2018, extending that work to address deterministic policies. the authors establish convergence under a number of assumptions. both onpolicy setting and offpolicy settings are treated. the reviewers point out several concerns and the consensus seems to be that, while the direction looks promising, the paper deserves further work.", "accepted": 0}
{"paper_id": "iclr_2021_WrNjg9tCLUt", "review_text": "the paper makes an attempt towards byzantine resilient federated learning, in the pressneece of backdoor attacks. the method presented combines a clustering step with a poison elimination step, and seems to be effective against a range of current attacks. both steps are a bit ad hoc in nature, and do not come with provable guarantees. moreover, the algorithms presented will have a big negative impact on personalization as several models may be incorrectly discarded during and fl round. the authors further point in their response that  no existing defense against backdoor attacks preserves the privacy of the clients data. this is in fact not true, as the differential privacy defense presented by the can you really backdoor fl paper is in fact fully respective of user privacy. at the same time, the work on backdoor attacks and defenses is reminiscent of the cat and mouse work in adversarial examples an attack comes out, then a defense claims to protect against it, then an attack that incorporates that defense can be made stronger, and so on. this is similar in the context of backdoor attacks. in fact, a recent work 1 proposes that detecting backdoors is in the general computationally unlikely, rendering the generality of the proposed algorithm questionable, and also suggest a set of attacks that seem very hard to defend against. it is fine that the authors do not reference this work as it was published just recently as the paper lacks significant algorithmic novelty, solid guarantees, and also is unclear whether it is universally sound, the overall contribution is limited. 1 wang et al. attack of the tails yes, you really can backdoor federated learning, neurips 2020 httpspapers.nips.ccpaper2020fileb8ffa41d4e492f0fad2f13e29e1762ebpaper.pdf", "accepted": 0}
{"paper_id": "iclr_2021_a9nIWs-Orh", "review_text": "this paper proposes a new mechanism, called hire, to improve the downstream performance of a pretrained transformer on nlp tasks. different from directly using the last layer of transformer, the proposed model allows the system to dynamically decide which intermediate layers to use based on the input through some sort of gating. the model is evaluated on glue, a benchmark for natural language understanding. my major concerns are the following 1. the gating mechanism on using intermediate sentence representation is not new, as pointed by some reviewers, although its implementation on transformers is still interesting. 2. the empirical part is not convincing enough a glue data set is relatively simple, the authors should try something more complex, bthe improvement over baseline is rather modest, which could be achieved with simpler modification. id suggest to reject this paper.", "accepted": 0}
{"paper_id": "iclr_2021_MpStQoD73Mj", "review_text": "this paper introduces a framework for automatic differentiation with weighted finitestate transducers wfsts, which would allow userspecified graphs in structured output prediction tasks and easy plugandplay of graphs through the composition operation demonstrated with variants of ctc. the authors demonstrated their framework on the ocr and asr domains, which are important application scenarios. all reviewers agree the work is useful and can potentially be significant. however, the reviewers think the paper needs more discussions of similarparallel work and the key differences from them, and clear description of the novelty in terms of either machine learning insights or algorithmic implementations. we understand that this may be an implementationheavy work, but the level of details provided in the current version does not convince the reviewers that the proposed approach is already efficient and can scale up. this could be shown by fair comparison with existing approaches e.g., hardcoded error backpropagation implementation with a fixed graph in runtime and accuracy.", "accepted": 0}
{"paper_id": "iclr_2021_e-ZdxsIwweR", "review_text": "quality i personally feel that the comment from reviewer1 regarding realworld is a minor but valid point. even after the rebuttal, the abstract seems to suggest that the proposed algorithm is effective to solve realworld challenges. maybe further rephrasing or explicitly stating that the experiments are in simulation might help to clarify this point. reviewer3 also raised valid points regarding the experimental evaluation and the use of just 3 seeds. given the struggle in reproducibility in rl and the shady experimental practices from even leading ai companies e.g., cherrypicking of seeds, it is paramount that experiments follow strong methodologies and good practices. as such, the experimental results presented in this manuscript should be strengthened.  clarity all the reviewers pointed out that the paper writing should be improved. although the authors significantly improved the manuscript during the rebuttal period. several reviewers suggested that the manuscript should be further polished before publication.  originality the two proposed approaches are novel to the best of the reviewers and my knowledge. two reviewers pointed out that the theoretical results should be explained more thoroughly and to clearly differentiate from prior work.  significance of this work the paper deal with an important and timely topic. although the work could be very impactful for realworld applications, there is no realworld application. hence it is difficult to gauge the significance of the work.  overall the paper does not feel quite ready for publication yet. a clearer presentation and extended experiments would certainly improve the quality of the manuscript.", "accepted": 0}
{"paper_id": "iclr_2021_Hr-cI3LMKb8", "review_text": "this paper proposes to employ affinity cycle consistencyacc for extracting active or shared factors of variation across groups. experiments shows how acc works in various scenarios. pros  the problem is important and relevant.  the paper is well written.  the proposed method is simple and effective. cons  the experimental section is weak it lacks an ablation to validate the contribution of acc and discussion on why the method works and the scalability of the proposed method to more complex cases.  the novelty is limited because the proposed acc is similar to previous work temporal cycle consistencytcc.  the paper missed some implementation details and could be difficult to reproduce without code provided. reviewers raised the concerns listed in cons. the authors conducted additional experiments and added more discussions on the experimental results in the revised paper. the authors also explained that acc is more general than tcc. however, the reviewers were not convinced by the rebuttal and kept their original ratings. due to the two main weaknesses  limited novelty and weak experimental analysis, i recommend reject.", "accepted": 0}
{"paper_id": "iclr_2021_sMEpviTLi1h", "review_text": "the authors propose two algorithms and their theoretical analysis for solving bilevel optimization problems where the inner objective is assumed to be strongly convex. the authors have greatly improved the paper to answer reviewer comments and three out of four reviewers have increased their scores. that said, given the large amount of new material added to this paper during the discussion phase, the program committee believes the paper requires a new round of reviews for a confident assessment. we encourage the authors to resubmit their work to a top conference such as icml.", "accepted": 0}
{"paper_id": "iclr_2021_pHgB1ASMgMW", "review_text": "the reviews were a bit mixed, with some concerns on the novelty and experimental evaluation. while the authors efforts during rebuttable were appreciated, the overall sentiment is that this work, in its current form, cannot be accepted to iclr yet. please consider revising your work based on the excellent reviews. some more comments from the acs independent assessment a further elaboration on the novelty is needed. currently the main message appears to be that if we combine two existing approaches at and entm or ls then we get better results. this is perhaps not too surprising and more elaboration on the significance would be appreciated. b more comparisons in the experiments, including the sota performances and alternative defenses some below. c the analysis in section 6 adds more confusion than clarification. it is clear that entm and ls would largely decrease m_f, but why would they also decrease the lipschitz constant even more sharply? if this explanation is useful, why not directly regularize the lipschitz constant and maximize the margin m_f? there is in fact a large body of work on this, see for example 1. formal guarantees on the robustness of a classifier against adversarial manipulation 2. parseval networks improving robustness to adversarial examples 3. l2nonexpansive neural networks 4. and the many references since.", "accepted": 0}
{"paper_id": "iclr_2021_-Qaj4_O3cO", "review_text": "this paper provides a method of encoding inputs to a spiking neural network snn using the discrete cosine transform dct. the goal is to create a more energy and time efficient means of doing inference with snns. the authors provide a description of the method, then show accuracy results on a variety of standard benchmarks. they also compare to a number of other methods for ann and snn based inference in the literature. altogether, they show that their method allows for accurate inference using fewer spikes than other approaches, which can potentially reduce the energy used for inference. the paper is fairly clearly written, and the results well articulated. the reviewers had a number of concerns, most notably related to questions of 1 clarity about the actual benefits of this approach, and 2 fair comparisons to other models. altogether, the authors did try to address the reviewers comments, and at least one reviewer increased their score. however, the actual scores for this paper remained very close to the acceptance threshold, and the first point was difficult to rebut without a lot more added to the paper. ultimately, this paper is using a classic signal processing strategy to improve snn run time, and the reviewers asked for some reason as to why that is desirablenovel. the authors answer was effectively that snns provide promise for lowenergy edge computing, and their method could make snns for edge computing even more efficient. this is potentially of interest for edge computing, but the paper could do a lot more to demonstrate that. specifically, some consideration of how this would actually operate on spiking chips or a more robust estimate of energy efficiency than that given at the end of section 4 would be required to make this paper a clear accept. notably, the paper does not demonstrate that this technique could be used to significantly reduce the energy requirements for spiking chips, relative to other snns, just that this is more energy efficient than anns, which is already known for other spiking neural network approaches. given this, and the scores relative to other papers at iclr, a reject is recommended. however, the ac notes that this was a difficult decision, and this paper was right at the threshold.", "accepted": null}
{"paper_id": "iclr_2021_uFkGzn9RId8", "review_text": "this paper presents a refreshening insight into the classical idea of using external memory for reinforcement learning agents that learn and act in partially observable environments. the authors investigate a number of different memory architectures ok, oak, kk and provide an insightful discussion on why we want to restrict the structure of the memory. reviewers generally appreciated the technical contribution of the paper, although not very convinced that this work will have a significant impact on future work. ac is also not sure about the conclusion drawn from the paper, where policies with external memory could have better sample complexity compared to rnnbased policies. bttt is computationally expensive, but it shall give better direction of which state to jump to, compared to the authors approach where the gradients are stopped at every timestep. so there should be pros and cons about this approach, and ac suspects that the sample complexity improvement actually comes from the fact that authors are explicitly limiting what can be stored in the memory, e.g. o or oa. this advantage can be broken in some other domains. ac admits that this is only a speculation at this point, but the motivation to use the external memory framework proposed in the paper needs to be more carefully investigated.", "accepted": null}
{"paper_id": "iclr_2021_vrCiOrqgl3B", "review_text": "the paper proposes a novel approach to detect outliers using optimal transport. the authors prove a very interesting relation between outlier robust ot and solving ot with a thresholded loss. numerical experiments show that the proposed approach indeed work for outlier detection. the paper had mixed reviews and the comments and changes from the authors were appreciated. the comments about recent and contemporary references were not taken into account in the final decision following iclr guidelines. one major concern that appeared during discussion was the fact that one important claimed contribution is the ability to perform outlier detection, the proposed method is never evaluated or compared to the numerous existing outlier detection methods. it works on a toy example and seem to provide a robust way to train a robust gan but the experiments are very limited. also the claim from the authors that the method scales are not really true. the proposed approach requires solving an exact ot of complexity on3logn, while one can use an approximated entropic solver on the thresholded loss it does not solve the robot problem anymore and the relations between the problem does not exist anymore in this case or are more similar to uot. the concerns detailed above and the limited novelty of the contributions most of the formulations proposed in the paper are already existing in the literature suggest that the paper in its current iteration is too borderline for being accepted in a selective venue such as iclr. the method and the relations uncovered are interesting and the ac encourages the authors to continue work on the proposed method and provide more detailed experiments illustrating and comparing the method to baselines for outlier detection.", "accepted": 0}
{"paper_id": "iclr_2021_ku4sJKvnbwV", "review_text": "this work applies collocation, a well known trajectory optimization technique, to the problem of planning in learned visual latent spaces. evaluations show that collocationbased optimization outperforms shooting via cem planet and shooting via gradient descent. pros  i agree with the reviewers that this idea makes sense, and will very likely be built on in future work  the authors have very actively addressed most comments of all reviewers that engaged in discussion cons  i agree with the reviewers that this is a very simple and straightforward application of collocation methods to the visual latent space domain. furthermore, the chosen tasks are fairly simplistic, metaworld has a variety of tasks, most of which are more complex than the reaching and pushing task that were chosen for this manuscript.  even with all the updates, the evaluation is still very shallow. i agree with the reviewers that obtaining results for both settings a visual mpc with pretrained or even ground truth dynamics model b in the modelbased rl setting, for which the model is being learned, is important. while the authors have added some of these experiments, a detailed discussion of how the results change from a to b is missing. furthermore, when using collocation in this mbrl setting, how should dynamics constraints be enforced should they even be enforced when the model is still really bad?. how does the comparison between collocation and shooting fare when you use denseshaped rewards for the sawyer tasks? many questions come to mind, some of which that have been raised by the reviewers, and my main point is that simple idea  indepth analysis of some of these questions would have created a stronger contribution.  alternatively, real system experiments would have increased the significance of this work.  i dont see any direct references of gradientbased visual latentspace planning shooting, but related work on this does exist. in my opinion, a simple straightforward idea is no reason to reject a paper. however, currently, the reader does not learn when collocation should be considered over other trajectory optimization methods, when attempting to plan in a learned visual latent space. and what some of the main remaining challenges are. because of this i lean towards recommending reject, and would encourage the authors to deepen their analysis of collocation in visual latent space.", "accepted": 0}
{"paper_id": "iclr_2021_aa0705s2Qc", "review_text": "the paper proposes a modification to the deepmind control suite to measure generalization with respect to visual variation. the authors run baseline experiments against their new benchmark and discover, unsurprisingly, that rl agents learning from visual observations overfit to spurious details of the observations. reviewers generally found the work to be clearly written, and the experimental analysis to be thorough and well done, though concerns about the rather simple nature of the visual augmentations persisted even after updates and author rebuttals. there were also concerns that by focusing only on soft actor critic in the experiments. 3 of 4 reviewers felt the work met the acceptance bar, albeit only marginally. the dissenting reviewers concerns centered on clarity many specific issues appear to have been remedied, the relatively limited nature of the augmentations, and the fact that reviewers were not given access to the code. while the submission has potential, improvements needed are not minor, and given the short process, we can only accept papers as is, rather than expecting certain changes. please take the reviewers comments into consideration as you revise and resubmit to a future venue.", "accepted": null}
{"paper_id": "iclr_2021_PGmqOzKEPZN", "review_text": "this paper discusses the likelihood ratio estimation using the bregman divergence. the authors consider the trainloss hacking, which is an overfitting issue causing minus infinity for the divergence. they introduce nonnegative correction for the divergence under the assumption that we have knowledge on the upper bound of the density ratio. some theoretical results on the convergence rate are given. the proposed method shows favorable performance on outlier detection and covariate shift adaptation. the proposed nonnegative modification of bregman divergence is a reasonalbe solution to the important problem of density ratio estimation. the experimental results as well as theoretical justfication make this work solid. however, there are some concerns also. the paper assumes knowledge on the upper bound of density ratio and uses a related parameter essentially in the method. assuming the upper bound is a long standing problem in estimating density ratio, and it is in practice not necesssarily easy to obtain. also, there is a room on improvements on readability. although this paper has some signicance on the topic, it does not reach the acceptance threshold unfortunately because of the high competition in this years iclr.", "accepted": null}
{"paper_id": "iclr_2021_KjeUNkU2d26", "review_text": "the paper proposes an approach to definingtackling the question of separating style and content of images, and introduces a novel way to learn representation that disentangle these aspects of images. i think it offers some new ideas. the reviewers were split on the evaluation. among the chief concerns with the initial submission were a problematic formulation of the objective, missing comparisons and analysis, and questions about novelty of the architecture in particular w.r.t. adain. i think the rebuttalrevision have addressed these fairly well. i do agree with r2 that some flaws remain, in particular the analysis could be more thoroughcomplete, and the paper could then be stronger.", "accepted": 0}
{"paper_id": "iclr_2021_21aG-pxQWa", "review_text": "the paper introduces an approach to counterfactual fairness based on data preprocessing, and compare it to other two counterfactual fairness approaches on the adult and compas datasets. the reviewers are in agreement that, in its current state, the paper should not be accepted for publication at the venue. their main concerns are around the metric used to measure fairness, and these were not resolved during the discussion. the reviewers would have also appreciated more experiments on realworld datasets to get a more comprehensive comparison of the methods. finally, discussion and comparison with other methods to achieve counterfactual fairness from the literature were limited.", "accepted": 0}
{"paper_id": "iclr_2021_uV7hcsjqM-", "review_text": "this is a nice paper using contrastive learning for code representation. the idea is to generate variations on unlabeled source code using domain knowledge by creating equivalent version of code. improvements over baselines on two multiple tasks are shown. while some of the reviewers liked the and r4 should have responded, none of the reviewers found the paper exciting enough to strongly recommend its acceptance.", "accepted": 0}
{"paper_id": "iclr_2021_E4PK0rg2eP", "review_text": "this paper studies a problem setup of parameterefficient transfer learning for largescale deep models. the approach consists of learning a diff vector with a sparsity constraint and then pruning the vector using magnitude pruning. a group penalty is also introduced to enhance structured sparsity. the main motivation is that for each new task, we only need to add a few parameters based on a pretrained model without finetuning it. the proposed approach possesses technical soundness and shows empirical efficacy for the studied problem setup. during the rebuttal and discussion phases, two of the reviewers raised two major concerns based on which they strongly disagreed with acceptance  the problem setup is not elaborated sufficiently and falls short of plausibility. an approach targeting at efficiency should either improve inference speed or reduce storage cost. unfortunately, neither advantage has been well approached.  the technical novelty is somewhat incremental, given the rich previous work on residual adapter, network reparameterization, and network compression pruning, sparsity etc.. ac read the paper and agreed that, while the paper has some merit such as a better model for the particular problem setup, the reviewers concerns are reasonable and need to be addressed in a more convincing way. for example, try to study a practical application in which the proposed approach is essential and useful for efficiency enhancement.", "accepted": 1}
{"paper_id": "iclr_2021_2KSsaPGemn2", "review_text": "this paper extends the idea of successor representations. typically the reward is compute linearly on top of states in this setting but the authors relax it to have a quadratic form. bf pros 1. a novel formulation of the successor representation where the reward does not follow the linearity assumption 2. the idea of using a second order term for the reward branch is interesting and could have meaningful implications for learning and exploration. bf cons 1. all authors agree that the experimental results do not clearly validate the advantage of this method. more work is needed to establish the effects of using this particular reward structure on a wide variety of tasks 2. both r2 and r4 were unconvinced of the limitations of the linearity assumptions in the original successor representation formulation  especially in the case when the state is represented by a nonlinear function approximator. the ideas presented in this paper are quite interesting and promising. but more experimental work is needed to show the benefits of this approach.", "accepted": 0}
{"paper_id": "iclr_2021_eyXknI5scWu", "review_text": "while the reviewers found parts of the paper interesting, the main concern about this paper was lack of novelty and marginal improvements obtained by the proposed methods.", "accepted": 0}
{"paper_id": "iclr_2021_VMAesov3dfU", "review_text": "dear authors, thank you very much for submitting this very interesting paper. this work analyzes the effect of gradient descent training on the compositionality of the learned model. their main argument is that gd tries to use the redundant information in the data and, as a result, it doesnt generalize well. the paper then tries to show that theoretically and empirically with some simple experiments. there is a general consensus among all the reviewers that this paper is not suitable for publication at iclr. the authors do not entirely address most of the concerns raised by the reviewers during the rebuttal. if the authors improve the clarity of the paper, making some of the propositions and theories more concrete and grounded in experiments as well, i would recommend them to resubmit this paper to a different venue since the premise of the paper is important and interesting. some of the reasons  the paper claims that the gradient descent can not ignore the redundant information without providing sufficient empirical results. though the part that is not clear to me whether if it is a credit assignment or an optimization problem. i agree with r1 that it is not clear what type of new insights from the proofs.  as r1 mentions, this papers claim seems too strong and not supported by experiments.  r2 finds part of the paper unclear and thinks that some of the papers propositions and theories are either trivial or wrong. the rebuttal doesnt seem to be doing a good job in terms of addressing those concerns.  r4 also is confused with the paper thinks that some of the theories are incorrect.", "accepted": 0}
{"paper_id": "iclr_2021_Kz42iQirPJI", "review_text": "the paper proposes a sequential metalearning method over fewshot sequential domains, which meta learns both model parameters and learning rate vectors to capture taskgeneral representations. reviewers raised many insightful and constructive comments. the main themes are as follows  the problem setting needs further motivation and clarifications, to make it more realistic and applicable.  the novelty is relatively weak, e.g. the approach is too simple, and learning the learning rate is a common trick.  the method needs great effort for better presentation and justification. the current presentation simply lists several equations in a dense way without detailed explanation. some main claims such as mitigating catastrophic forgetting are not elaborated extensively. ac scanned through the paper and agreed with the reviewers main points. authors rebuttal in general did not address these concerns to the satisfaction. for example, even after revision, the readability of this paper is not good enough. the authors are encouraged to perform a thorough revision.", "accepted": 0}
{"paper_id": "iclr_2021_49V11oUejQ", "review_text": "this paper studies efficient robust training. the key idea is to use backward smoothing as an advanced random initialization to improve a models adversarial robustness. the approach is sound, well grounded, and quite logical. results demonstrate the effectiveness. however, there exists some limitations 1 andriushchenko  flammarion, 2020 gives a better and more fundamental explanation on how to address fast adversarial training. 2 backward smoothing does not generalize to standard adversarial training. in other words, it only works for kl divergence loss rather than crossentropy loss, and it seems that backward smoothing does not address the fundamental problem of fast adversarial training. 3 if we compare the performance of fast trades and backward smoothing since backward smoothing intends to improve fast trades, there is always a tradeoff between clean accuracy and adversarial robustness, e.g., table 4 and table 8. 4 randomized smoothing is helpful for onestep adversarial training and randomized smoothing in general seems to be orthogonal to the proposed method. moreover, 2step pgd training can perform similarly well to backward smoothing while being much simpler conceptually. in the end, i think that this paper may not be ready for publication at iclr, but the next version must be a strong paper if above limitations can be well addressed.", "accepted": null}
{"paper_id": "iclr_2021__zHHAZOLTVh", "review_text": "overview this paper introduces a maximum mutual information method for helping to coordinate rl agents without communication. discussion some reviewers leaned towards accept, but i found the two reviewers recommending rejecting to be more convincing. recommendation this is an important research topic and im glad this paper is focusing on the problem. hopefully the reviews will help improve a future version of this paper. i agree that this is a new way of using mutual information, but it seems more like a small improvement rather than a very significant step forward. in addition, i think the setting needs to be better motivated. this is a centralized training with decentralized execution ctde setting, and this paper helps the agents coordinate. in ctde, the agents work in the environment and then pool their information to train before deploying on the next episode. i dont understand why, e.g., in multiwalker, agents would not be able to communicate while walking, can communicate after they succeed or drop the object the episode ends, and then cannot communicate once the next episode starts.", "accepted": 0}
{"paper_id": "iclr_2021_Utc4Yd1RD_s", "review_text": "this paper first examines a multidomain separation phenomenon, where different types of adversarial noise lead to different running statistics, and then introduces gated batch normalization gbn, a building block for deep neural networks that improves robustness against multiple perturbation types. gbn consists of a gated subnetwork and a multibranch bn layer, and each bn branch handles a single type of perturbation. results were reported on mnist, cifar10, and tinyimagenet. though the idea and methodology are valid and of interest, some concerns regarding the experimental section remain after rebuttal discussions.", "accepted": null}
{"paper_id": "iclr_2021_uvEgLKYMBF9", "review_text": "this paper develops a smoothing procedure to avoid the problem of posterior collapse in vaes. the method is interesting and novel, the experiments are well executed, and the authors answered satisfactorily to most of the reviewers concerns. however, there is one remaining issue that would require additional discussion. as identified by reviewer 1, the analysis in section 3 is only valid when the number of layers is 2. above that value, it is possible to construct models where the elbo has a reasonable value, but the smoothed objective behaves catastrophically. thus, the scope of the analysis in section 3 deserves further discussion. given the large number of iclr submissions, this paper unfortunately does not meet the acceptance bar. that said, i encourage the authors to address this point and resubmit the paper to another or the same venue.", "accepted": 0}
{"paper_id": "iclr_2021_3Wp8HM2CNdR", "review_text": "this paper received borderline recommendations 5, 5, 6, 7 but even the two slightly more positive reviewers were lukewarm r1 and r2. while the reviewers acknowledged the heavy computational requirements to do an applestoapples comparison with existing baselines, they remain underwhelmed with the lack of experiments. i agree with their criticism; even though the proposed idea seems promising, without comprehensive experiments, it is difficult to judge the significance of this work. r1 commented after the discussion period that an earlier version of this paper actually had imagenet results. r4 made excellent suggestions to improve the paper further. the authors are strongly encouraged to incorporate them into their future submission. i am copying r1s comment below in case it is invisible to the authors after the notification. sorry for the late update  i have read the rebuttal earlier. i would like to keep my acceptance rating but after the rebuttal i am fine either way. the paper first appeared in march on arxiv, so indeed it is a concurrent work actually an earlier work compared to byol or swav. we have actually tried to reproduce the results in the paper a while back but it did not go well could not reproduce it, but this time the submission also includes the code. while i havent run it, i trust the results are reproducible maybe there are some tricks that i am not aware of. regarding running experiments on toy examples  i can understand that this research is resourceconstrained for imagenet, but the earlier draft actually had some results on imagenet 60 top1 accuracy see appendix of httpsarxiv.orgpdf2007.06346v1.pdf, and for some reason this submission removed that. so this is not a positive sign. overall, my experience for cifar vs imagenet is that it is easier to make things work on cifar, while it is much harder to do so on imagenet. so maybe some trials are indeed done by the authors, but they choose to not report it in the submission for some reason. on the other hand, one can argue that results on toy datasets are good enough contributions for an early develop of something and they are just not ready for larger and more challenging datasets yet. therefore, this paper is quite a struggle. i hoped to see a betterthanthis submission as this paper actually had all the time from march to october to improve its quality of experiments actually even for imagenet, one can to dozens of cycles on it during this time, but it did not for some reason.", "accepted": null}
{"paper_id": "iclr_2021_awOrpNtsCX", "review_text": "this paper proposes a new kind of cnn that convolves on deformable regions and cooperates with the poisson equation to determine the deformable regions. experiments on texture segmentation look promising. pros 1. the paper is well written and easy to follow. 2. the idea is interesting and the reviewers liked it. 3. the experiments on texture segmentation are promising. cons 1. actually, convolution on nonrectangular region is not new, in contrast to the authors claim and reviewers belief, although the authors may argue that the mechanisms of determining the region for convolution are different and the cnns are used for different tasks. see, e.g., jifeng dai, haozhi qi, yuwen xiong, yi li, guodong zhang, han hu, yichen wei deformable convolutional networks. iccv 2017 764773 and other papers by the same first author. so the ac would discount the novelty of the paper. 2. as most of the reviewers commended, the experiments on texture segmentation were insufficient. although extra experiments were added thank the authors effort on doing this, the reviewers actually still deemed that they were not convincing enough, e.g., should compare with more stateoftheart methods. reviewers 24 both confirmed this issue in confidential comments. although reviewer 1 increased hisher score, the final average score is still below the threshold. so the ac decided to reject the paper.", "accepted": null}
{"paper_id": "iclr_2021_Kr7CrZPPPo", "review_text": "the paper tackles a major problem of supervised ml, that of the minimisation of the risk of a set of classifiers. this problem has received attention in numerous work over the past decades, much of which spans the formal aspects of the problem. the paper tackles the problem from a diversity standpoint. my main concern is, for such a problem and exhaustive formal and experimental sota, one cannot just evacuate any formal understanding of a contribution to future work authors reply to r2. the argument is then a victim of its own content, ending up in a sloppy vocabulary where speculation and intuition are called forward as justification to the calls for rigorous r1 and theoretical understanding r2  answer to r2. i am confident the authors can find formal merit to their contribution, but this needs to be addressed. r1  r4 hint on avenues to understand the contribution.", "accepted": 0}
{"paper_id": "iclr_2021_QZaeLBDU03", "review_text": "the paper proposes a new gametheoretic model, bayesian stackelberg markov game bsmg, for designing defense strategies while accounting for the defenders uncertainty over attackers types. the paper also proposes a learning approach, bayesian strong stackelberg qlearning bssq, to learn the optimal policy for bsmgs. it is shown that bssq converges to an equilibrium asymptotically. experimental results are provided to demonstrate the effectiveness of bssq in the context of web application security. overall this is an interesting approach and an important direction of research. however, the reviewers raised several concerns, and there was a clear consensus that the paper is not yet ready for publication. the specific reasons for rejection include the following i the experimental results are not presented with sufficient clarity, no statistical significance tests are performed, and the choice of baselines is weak; ii the contributions are not sufficiently broad, the learning process described in the paper is unclear, and the framework requires a strong assumption of knowing the attackers distributions. i want to thank the authors for actively engaging with the reviewers during the discussion phase. the reviewers have provided detailed feedback in their reviews, and we hope that the authors can incorporate this feedback when preparing future revisions of the paper.", "accepted": 0}
{"paper_id": "iclr_2021_PghuCwnjF6y", "review_text": "the contributions of this paper are twofold 1 datasets of tasks are provided, and 2 based on the datasets and hyperparameter lists on the datasets, a transfer learning approach for hyperparameter optimization hpo is proposed. many reviewers positively evaluated the idea and approach discussed in this paper. however, the common concern of multiple reviewers and area chair is that it is not clear whether the provided datasets and their hyperparameter lists are generally applicable to other practical problems. since there is no discussion on how the datasets are constructed, it is not clear whether they have generally or not. in addition, the comparison with existing hpo approaches is not sufficiently made, and it is not clear whether the performance of the proposed method is advantageous over existing methods. overall, although the idea is very interesting and potentially useful, i cannot recommend the acceptance in its current form due to the lack of evidence on its generality.", "accepted": 0}
{"paper_id": "iclr_2021_qf6Nmm-_6Z", "review_text": "the paper proposes a new method to learn representation and community structure of a network jointly. the reviewers agree that the paper contains some interesting ideas but they raise also some important concerns. for example  even after considering the authors rebuttal, the paper seems not too novel. in particular the results seem a bit incremental over vgraph.  the notion of community is not formalized by the authors neither in the paper or in the rebuttal. the paper would benefit greatly by having such formal definition overall, the paper is interesting but it does not meet the high acceptance bar of iclr", "accepted": null}
{"paper_id": "iclr_2021_t4EWDRLHwcZ", "review_text": "the reviewers generally like the paper, in particular the scalability of the proposed approach. the author response and revised version clarified some questions of the reviewers, however, it didnt fully mitigate their concerns.", "accepted": null}
{"paper_id": "iclr_2021_nQxCYIFk7Rz", "review_text": "while there was some interest in the analysis, the consensus view was that the original treatment was not sufficiently wellmotivated, and the revision was too dissimilar from the original submission for it to be evaluated for publication in this years iclr.", "accepted": 0}
{"paper_id": "iclr_2021_1OCwJdJSnSA", "review_text": "this paper investigates the problem of unsupervised domain adaptation and proposes a framework based on a specific type of disentangled representations learning. the paper is well written and the proposed method seems plausible. however, according to reviewers 3 and 4, the proposed framework does not seems to be sufficiently different from existing ones, and the empirical results do not seem convincing enough. please also double check in c3, whether t and s should be marginally independent or conditionally independent conditioning on x.", "accepted": 0}
{"paper_id": "iclr_2021_CNA6ZrpNDar", "review_text": "this paper studies the decision boundaries of shallow relu network using the formalism of tropical geometry. its main takeaway is to provide a new interpretation of the lottery ticket hypothesis in terms of network pruning strategies that preserve certain geometric structure. reviewers were appreciative of the clarity of the exposition, and the novel perspective on interesting and elusive phenomena such as the lottery ticket hypothesis. on the other hand, they also expressed some doubts about the significance of some aspects of the theory such as proposition 1 and corollary 1, as well as the computational considerations required to elevate the analysis to largescale architectures from applications. ultimately, and after taking into consideration all the reviewing discussions, the ac believes that this submission is not yet ready for publications, but it is in a trajectory to become an important piece of work. in particular, the ac encourages delving deeper into the tropical network pruning. additionally, the authors might want to discuss breaking the curse of dimensionality with convex neural networks, bach17 in the related work, since this is the first instance the ac is aware of where the connection between zonotopes and shallow relu networks is established.", "accepted": null}
{"paper_id": "iclr_2021_vOchfRdvPy7", "review_text": "this paper examines adversarially trained robust models, and finds that accuracy disparity is higher than for standard models. the authors introduce a method they call fair robust learning using lagrange multipliers to minimize overall robust error while constraining the accuracy discrepancy between classes. in discussion, consensus was reached that the observations and approach are interesting but the paper is not yet ready for publication. the main concern is that it is not clear if the class accuracy disparity is due to adversarial training, or simply due to lower accuracy in general. please see reviews and public discussion for further details.", "accepted": null}
{"paper_id": "iclr_2021_k9EHBqXDEOX", "review_text": "although the reviewers found the paper wellwritten that analyzes a relatively popular algorithm td0 version of a3c, there are concerns regarding the novelty of the convergence results given those for a2c, the comparison of the results with those for a2c, and the sufficiency of the experiments. although the authors addressed some of these issuescomments during the rebuttals, it seems none of the reviewers is excited about the paper and there still exist concerns regarding the novelty of the results and how they are compared with those in the literature. i would suggest that the authors take the reviewers comments into account, have a more comprehensive discussion about the relation of their results with those in the literature twotime scale algorithms, and prepare their work for future conferences.", "accepted": null}
{"paper_id": "iclr_2021_pAJ3svHLDV", "review_text": "the paper has good contributions to a challenging problem, leveraging a fasterrcnn framework with a novel selfsupervised learning loss. however reviewer 4 and other chairs in calibration considered that the paper does not meet the bar for acceptance. the other reviewers did not champion the paper either, hence i am proposing rejection. pros  r1 and r3 agree that the proposed model improves over related models such as monet.  the value of the proposed selfsupervised loss connecting bounding boxes and segmentations is well validated in experiments. cons  r4 gives good suggestions that may be useful to reach a broader readership, namely introducing more of the concepts used in the paper., e.g. stick breaking, spatial broadcast decoder, multiotsu thresholding so it becomes more selfcontained. r4 also suggests improving the writing more generally.  r4 still finds the proposed method quite complex yet derivative after the rebuttal.  all reviewers complain about lack of experiments in real data, but the authors did revise their paper and add some coco results in the appendix. these could be part of the main paper in a future version.", "accepted": 0}
{"paper_id": "iclr_2021_SP5RHi-rdlJ", "review_text": "this paper compresses neural networks via so called sparse binary neural network designs. all reviewers agree that the paper has limited novelty. experiments are only performed on small datasets with simple neural networks. however, even with toy experiments, results are very weak. there is no comparison with the sota. numerous related works are missed by the authors. besides, the paper is poorly written, and there are misleading notations.", "accepted": 0}
{"paper_id": "iclr_2021_NgZKCRKaY3J", "review_text": "the paper proposes an adjustment to the ece metric to make it less biased in the small sample case by including the assumption that the confidence output by a classifier is monotonic with the true correctness probability. the main idea is to successively make finer bins until a nonmonotonicity is observed. the paper is interesting, but the magnitude of the contribution would be just enogh for a short paper if such a track existing in iclr. reviewers have raised concerns about the discrepancy between their revised ece formula and the algorithm accompanying it, although that has been fixed through the author feedback phase. another concern is that for a paper whose core technical contribution is a revised metric for measuring calibration, a more thorogh empirical study over larger datasets is required.", "accepted": 0}
{"paper_id": "iclr_2021_412_KkkGjJ4", "review_text": "this paper presents work on scene graph grounding under weak supervision. the reviewers appreciated the consideration of this task and formulation of a solution for it. however, concerns were raised over the importance of this weaklysupervised grounding task, how it addresses challenges in previous methods, the empirical evaluation, insights obtained, motivation, and clarity of exposition. after reading the authors response, the subsequent discussion and reconsideration resulted in a sense that while the task is new, the overall contribution and remaining questions over empirical evaluation mean the paper is not yet ready for publication at iclr.", "accepted": 0}
{"paper_id": "iclr_2021_XG1Drw7VbLJ", "review_text": "the reviewers appreciate the steps taken to combine continual learning with fewshot learning, this is an interesting intersection with many potential applications. however, the reviewers generally outlined a number of concerns with the benchmark and paper in its current form. they largely feel that this benchmark doesnt differentiate itself well enough from other incremental learning benchmarks, nor does it experiment with a wide enough variety of settings additional episode configurations, more fslcl approaches. as such, it is difficult to determine at this point what major insights can be gained from this benchmark. i understand that there is a tradeoff computation is limited, and there is merit in keeping things simple. however, the general consensus is that more work needs to be done in order to fully realize the potential of this benchmark.", "accepted": 0}
{"paper_id": "iclr_2021_9D_Ovq4Mgho", "review_text": "a majority of the reviewers find the paper lacks novelty and provides an insufficient discussion of the stateoftheart in knowledge distillation and student teacher training to warrant publication. the approach is quite narrow to the application domain and the paper does not provide novel insights on how to chose a good network. a subset of the experiments performed on an internal data set with random traintestsplits do not evaluate a realistic transfer setting.", "accepted": 0}
{"paper_id": "iclr_2021_zfO1MwBFu-", "review_text": "this paper presents a representation method for time series data in the sequential vae, where the global feature z and local features s are better disentangled. the intuition behind learning z is to maximize the mutual information between z and input x, while minimizing the mutual information between z and s. the second mutual information is estimated with a discriminator in the drt framework. overall, the methodology can be seen as reasonable applications of the disentanglement principle to sequential data. the authors have shown that z and s learned in this way is better disentangled as compared to betavae. in the end, the reviewers feel that while there is good intuitionstechnical ingredients, the derivations in section 3 are not very smooth, and several approximationschoices are not very carefully justified e.g., choice of alpha, choice of drt vs. other mi estimators, and perhaps stronger baselines than betavae can be used. the reviewers rate this paper to be borderline.", "accepted": null}
{"paper_id": "iclr_2021_avBunqDXFS", "review_text": "this paper proposes a semisupervised setting to reduce memory budget in replaybased continual learning. it uses unlabeled data in the environment for replaying which requires no storage, and generates pseudolabels where unlabeled data is connected to labeled one. the method was validated on the proposed tasks. pros  the semisupervised continual learning setting is novel and interesting.  the proposed approach is memory efficient, since it does not need exemplars to replay past tasks. cons  the scale of experiment is small. it lacks evaluation in real world environment.  the novelty is limited, because it is a combination of existing technologies pseudolabeling, consistency regularization, outofdistribution ood detection, and knowledge distillation.  the comparison might not be fair due to different settings. the authors addressed the fairness and scalability with additional experiments and leave some suggestions of reviewers for future work. r3 had a concern on the error propagation of pseudolabels which i also share. the authors agreed that this is a challenge for all cl methods. in summary, the reviews are mixed. all reviewers agree that the semisupervised continual learning setting is novel and interesting, and some have concerns on scalability and novelty of the method which i also share. so at present time i believe there is much room for the authors to improve their method and experiments before publication.", "accepted": 0}
{"paper_id": "iclr_2021_xJFxgRLx79J", "review_text": "this paper received divergent scores one strong negative and three positives. the positive reviews praise the clear intuitionmotivation and strong empirical performance, while the negative review considers the proposed approach adhoc with limited novelty. i read the paper myself and found myself leaning more towards the negative score. in more details i think the paper proposed a cleverlyengineered solution to employ two separate rnns to model two different subsets of the users active v.s. inactive. to combat overfitting, the authors proposed some tricks 1 use mf to learn a better initialization and 2 tie some of the parameters together. the ablation study shows that both are quite useful, and not too surprisingly when you have two powerful rnns and work hard to make sure they dont overfit, they perform better than a single rnn. as i mentioned earlier, this whole approach is quite cleverlyengineered and executed. but its not clear to me if this is something that the iclr community can benefit from maybe except a relatively small proportion. i believe this paper can find a bigger audience in venues like kdd whose deadline is coming up. furthermore, the authors presented some theoretical analysis to justify their intuition, which to me feels a bit forced. to start, a big assumption is that the optimal user embeddings are very concentrated, which is a rather strong assumption and i hardly believe it will hold in practice what can even be considered as concentrated in highdimensional space?. following that, the theorem implies that the initial embedding for the inactive users should be the expected optimal embedding, but then later in the paper this point seemingly got completely ignored and instead the authors just proposed to learn a common initialization. additionally, the theorem suggests that there is an optimal threshold, but later in the paper again this point got completely ignored. i know the theory makes assumptions and builds on simple cases. but my point is that you dont need theorem to show me that two rnns can perform better than one when trained properly which, i admit, is nontrivial and is the main contribution of the paper. there has been a lot of discussion around the ml community about the trend of making your paper look mathy and i dont think this is a good thing. minor comment in mf, the objective wrt each embedding is convex, but the whole optimization is not jointly convex and it is not likely that you can get to the global optimum. it is relatively insensitive to initialization though comparing with neural nets.", "accepted": 0}
{"paper_id": "iclr_2021_JzG0n48hRf", "review_text": "this paper presents a method to improve the calibration of neural networks on outofdistribution ood data. the authors show that their method can be applied posthoc to existing methods and that it improves calibration under distribution shift using the benchmark in ovadia et al. 2019. however, reviewers felt that the theoretical justification for why this works is unclear see detailed comments by r1 and r4, and some of the choices are not welljustified. revising the paper to address these concerns with additional theoretical andor empirical justifications should improve the clarity and strengthen the paper. i encourage the authors to revise and resubmit to a different venue.", "accepted": 0}
{"paper_id": "iclr_2021_hKps4HGGGx", "review_text": "this paper presents an inferencesoftmax cross entropy isce loss, a modification to the widely adopted softmax cross entropy sce loss, to achieve better robustness against adversarial attacks. the original submission had critical issues on motivation, theoretical analysis and experiments. although the authors provided a revised version, it needs another round of thorough examination before publishing.", "accepted": 0}
{"paper_id": "iclr_2021_xrLrpG3Ep1X", "review_text": "the paper is proposing a domain generalization method based on the intuition that an invariant model would work for any split of trainval. hence, the method uses adversarial trainval splits during training. the paper is reviewed by three expert reviews and none of them championed the paper to be accepted. i carefully checked the reviews and the authors response and agree with the reviewers. specifically  r1 argues that the paper is not ready for publication. also argues the optimization problem is only a motivation as it is not directly solved. this is an important issue and it needs to be addressed in a conclusive manner.  r2 argues empirical studies do not show the value of trainval splitting. i partially disagree with this issue but it is clear that more qualitative and quantitative study is needed to properly justify the proposed method.  r3 argues the contribution is not enough for publication. the paper is clearly novel but the contribution and novelty is not presented in a clear manner. moreover, the empirical study does not complement the novelty. hence, i disagree with the comment. overall, i believe the paper proposes an interesting idea. however, the presentation and empirical studies need to be improved significantly. i recommend authors to address these issues and submit to the next conference.", "accepted": null}
{"paper_id": "iclr_2021_s0Chrsstpv2", "review_text": "the overall impression on the paper is rather positive, however, even after rebuttal, it still seem that the paper requires further work and definitely a second review round before being ready for publication. thus, i encourage the authors to continue with the work started during the rebuttal to address the reviewers comment, which although moved in the right direction would still benefit from further work. especially, i believe the experiments could be significantly improved by for example bringing some results to the main paper. moreover, a more thorough comparison theoretically and empirically with previous work would increase the impact of the paper.", "accepted": 0}
{"paper_id": "iclr_2021_YjXnezbeCwG", "review_text": "this paper improves the waitk based simultaneous nmt by training on an adaptive waitm policy with a controller determining the lag for sentence pair. the controller is trained with rl to minimize the loss on a validation set. the overall model is reasonable, which is well presented. i however have the following two concerns 1. there is a clear mismatch between traininginference strategies, which raises two problems 1. the motivation the authors tried to explain that in discussion, but it is not convincing enough 2. the title is misleading since there is no future information to use during inference 2. the experiments is not convincing enough in that a the improvement over baseline is modest, and b comparison to adaptive waitk and other strong baseline is insufficient in conclusion i would suggest to reject this paper.", "accepted": 0}
{"paper_id": "iclr_2021_zcOJOUjUcyF", "review_text": "the paper investigates an active learning strategy for speeding up the convergence for ssl deep learning algorithms. when the ssl objective could learn a good approximation of the optimal model, the proposed method efficiently converges to the result with a few queries. the main idea is that when the eigenvalues of the ntk are large, the convergence rate is faster. the proposed algorithm maximizes the smallest eigenvalue of the ntk. an empirical investigation is also reported. the reviewers appreciated the general idea, but questioned about the actual execution of this paper in terms of both experimental comparison and lack of supportive theoretical results. i would like to encourage the authors to consider improving their paper along one of these two lines. unfortunately, as it currently stands, this paper is not ready for publication.", "accepted": null}
{"paper_id": "iclr_2021_QnzSSoqmAvB", "review_text": "there is a pretty good consensus that this paper should not be accepted at iclr. the reviewers do not seem think that extending muzero to nondeterministic muzero constitutes a significant advance. three reviewers give clear rejects with scores 3, 4, 5 all with good confidence 4. a fourth reviewer gave a score of 6, i.e., borderline accept. while the fifth reviewer recommends, he does not seem to be very confident and did not step in to champion the paper. the program committee decided that the paper in its current form does not meet the acceptance bar.", "accepted": 0}
{"paper_id": "iclr_2021_sebtMY-TrXh", "review_text": "the authors propose to use volume coding to enable uniform sampling from an implicit latent space to be used together with a autoregressive language model. all the reviewers find this approach interesting, but all found that the submission would be much stronger with more thorough evaluation. in particular, i noticed that the reviewers wanted to see how the proposed ariel works in comparison to e.g. vae on a more diverse set of benchmarks, since the choice of two datasets, one synthetic and one small, narrowdomain, is somewhat limited largely due to their relative simplicity. furthermore, the reviewers were unsure whether various evaluation metrics the authors have used are exhaustive nor appropriate to demonstrate the efficacy of ariel or to put the proposed approach correctly in the context of other approaches. i agree with the reviewers on both of these points. im thus recommending this manuscript be rejected, and strongly recommend the authors give a bit more thoughts on how to demonstrate the effectiveness of the proposed approach in the context of other approaches and the problem of sentence generation which is the main problem the authors claim to tackle, as the title directly suggests. with a better planned experiment and analysis, i believe the authors efforts will have significant impact.", "accepted": 0}
{"paper_id": "iclr_2021_heFdS9_tkzc", "review_text": "the paper studies distantly supervised relation extractiondsre in distributed settings. though dsre has been studied in centralized setting it has not been studied in distributed platform. this paper leverages the federated learning setup for this problem and proposes to use lazy mil for this purpose. the paper identifies the main challenge as label noise but does not attempt to characterise the severity of the problem visavis the centralised setup. though intuitive but a formal approach would have helped in understanding the importance of the derived results better.", "accepted": 0}
{"paper_id": "iclr_2021_RayUtcIlGz", "review_text": "this paper discusses a method to updateoptimise invertible matrices via lowrank updates. the key property of the proposed method is that it keeps track of the matrix inverse and its determinant through the optimisation with updates that are much cheaper to compute than a direct inversiondeterminant computation. while the method of performing lowrank updates for invertible matrices itself has already been extensively studied in the literature as pointed out by reviews, this work focuses after extensive revision on the properties of this update method. since the updates may leave the manifold of invertible matrices, a numerical stabilisation step was introduce whereby updates that produce illconditioned matrices are rejected during optimisation. rankone updates allow for fast update of matrix inverse and determinants. so this is particularly interesting when applied to normalising flows, as it allows for cheaper computation of the logdetjacobian terms. the novelty of this approach is rather limited as also pointed out by r2. the experiments and, in particular, the application to normalising flows are interesting, wellexecuted. it is not clear if there are advantages of the method in other domains where logdetjacobians are not necessary relative to existing literature.", "accepted": null}
{"paper_id": "iclr_2021__Tf6jEzbH9", "review_text": "the paper addresses the task of contextagnostic learning and presents an algorithm to solve the problem while assuming the ability to sample objects and contexts independently. it is reported a theoretical ground proposing to decompose factors contributing to the classification risk in context bias and object error. the method makes use of only one synthetic sample in training, still being able to generalize well. the paper received contrasting reviews, 2 positive 7 and 6 and 2 below threshold 5 and 5. r2, r3 and r4 raised similar issues, especially regarding the experimental validation, which is the main shortcoming of the work addressing simple datasets only, no comprehensive comparative analysis only in relation to baselines vanilla sgd but not in relation to stateoftheart methods, possibly slightly revised to accomplish the experimental protocol proposed in this work authors claim that the originality of the work do not allow a proper comparison with soa method asis. indeed, i deem r3s rating 6 a bit overestimated given the provided comments. ac does not see an issue the use of only one sample and no info about target, rather, itd be interesting to know if the proposed method could use more than one sample in order to make the comparison with soa methods fair, while assessing performance in comparative terms with soa, to give value to the method also in relation to performance. unfortunately the rebuttal did not lead an increase of the ratings, nor to better comments. after the rebuttal, r2 and r4 still remained below threshold; r1 was also not changing idea, remaining positive, and r3 did not react after rebuttal. overall, the ac deems this paper containing interesting contributions, but it is not sufficiently ready to be accepted at iclr mainly because the experiental validation is not showing a fully convincing evaluation of the proposed approach see above.", "accepted": null}
{"paper_id": "iclr_2021_jYVY_piet7m", "review_text": "this paper proposes a new method to combine nonautoregressive nat and autoregressive at nmt. compared with the original iterative refinement for nonautoregressive nmt, their method first generates a translation candidate using at and then fill in the gap using nat. all of the reviewers think the idea is interesting and this research topic is not wellstudied. however, the empirical part did not convince all the reviewers. the revised version and response is good; however, it still does not solve some major concerns of reviewers.", "accepted": null}
{"paper_id": "iclr_2021_9vCLOXwprc", "review_text": "reviewers found the new framework interesting. however, reviewers are unsatisfied with empirical evaluations. more experiments and discussion are needed.", "accepted": 0}
{"paper_id": "iclr_2021_GVNGAaY2Dr1", "review_text": "this paper proposes a method for collaborative multiagent learning and adhoc teamwork. the paper includes extensive empirical results across multiple environments including one of known outstanding high difficulty and repeatedly performs favourably in comparison to a suitable set of state of the art methods. the proposed method is motivated by theoretical analysis, which was considered interesting but its connection to the method in the initial paper was weak. overall, there are remaining concerns which have not been fully addressed in the discussion phase. the authors responses and discussion with the reviewers should be utilised to improve the materials presentation and to clarify the theoryempirical connection in future revisions of the paper.", "accepted": null}
{"paper_id": "iclr_2021_LFs3CnHwfM", "review_text": "this paper proposes to solve the fuel optimization problem in hybrid electric vehicles using reinforcement learning. the work is interesting, but the reviewers consider it lacks novelty and it has different concerns on the assumptions of the modeling. the paper is quite difficult to follow.", "accepted": 0}
{"paper_id": "iclr_2021_arNvQ7QRyVb", "review_text": "the reviewers enjoyed reading about an interesting take on lifelong learning, encapsulating an em methodology for selecting a transfer configuration and then optimizing the parameters. r3 made valid concerns regarding comparison with previous, recent work. r2 also would prefer to see more thorough experiments ideally in settings where multiple tasks exist, as also commented by r4. during the rebuttal phase the authors made a good effort to run additional experiments which cover the related work aspect better. these experiments and the overall paper were discussed extensively among reviewers after the rebuttal phase. in the discussions, the reviewers agreed that an interesting idea can be publishable even if it does not achieve sota results in all scenarios, as long as it brings new perspectives and shows at least comparable results. however, in the particular case of this paper, there exist remaining concerns regarding the usefulness and applicability of the method. specifically, the paper could benefit from a more convincing demonstration about how the method can scale e.g. r3 and r4s comments, especially since training time and model capacity are important factors to consider for practical continual learning scenarios. furthermore, it is not clear how the proposed method can be used in combination with other machine learning tools within a continual learning application, for example by leveraging modern deep architectures or by complementing existing adaptive knowledge approaches as discussed by r3. although the opinions of the reviewers are not fully aligned, this borderline paper seemed to lack an enthusiastic endorsement by a reviewer to compensate for the concerns discussed above and the relatively weak experimental results. therefore i recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2021_4I5THWNSjC", "review_text": "the reviews were largely split in the beginning. some of the concerns are firmly addressed, e.g. new results evaluating the actual latency in real hardware, and one reviewer raised the score from 5 to 6. however, another reviewer is not fully convinced by the response and decided to keep the original score of 3 clear rejection. there are mainly two issues here. one is about the novelty of the method. the reviewer asked about the novelty and difference from condconvdynamicconv, however the authors emphasized the techniques to successfully train conditional computation models in general. as the focus of the paper is the proposal of the new claimed as better method, i have to say it is missing the points. the authors could have organized the storyline of the paper as best practices for training conditional computation models or something like that, if that is the true contribution the paper. another issue is about the advantage over the condconv method. in the newly added results, basisnet does not show clear advantage in terms of accuracyspeed tradeoff without early exiting. although the authors simply state that it is infeasible to do early termination on condconv, the reason is not clear. indeed, one can easily try layerlevel early exiting as done in branchnet for example, if not the modellevel early exiting assumed for the basisnet. base on the discussion above, i conclude that the paper has to clarify many issues before publication and thus recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2021_0rNLjXgchOC", "review_text": "this paper studies different properties of the top eigenspace of the hessian of a deep neural network and their overlap. it raised quite a lot of discussion, which finally went in not very constructive way. the reviewers generally agree that the paper has potential, but the actual contribution is limited. pros  the idea that top eigenspaces between different models have high overlap is interesting  the explanation that these structures can be explained by kroneckerproduct approximation of the hessian. cons  the connection to pacbayes is unclear and seems artificial.  many of the related work is missing  the models and datasets are too simple, and general conclusions can not be made on such kind of models. much more testing is needed to verify the claims, including stateofthe art architectures and datasets.", "accepted": 0}
{"paper_id": "iclr_2021_dak8uQE6BOG", "review_text": "this paper proposes a new network architecture that implements higher order multivariate polynomials mvp. they show that mvp generalizes well to different types of conditional variables, and can be applied to a broad range of tasks. however, unifying discrete and continuous conditions and network without activation function are both well studied in literatures. the inappropriate discussions on the prior works and the advantages of the proposed method over prior approaches are not clearly justified. although outperforming sota is not necessary, the compared methods need to be well chosen which can provide convincing evidence on why mvp is needed under common settings.", "accepted": null}
{"paper_id": "iclr_2021_3GYfIYvNNhL", "review_text": "this paper proposes the cscore, which is the aggregation of a consistency profile that measures perinstance generalization. naive computation of the cscore is expensive and thus requires an approximation. the paper then uses the cscore to analyze several image benchmarks and their learning dynamics. while the reviewers found the experiments to be welldone, their primary concern was over the novelty and ultimate usefulness of the cscore. as r1 and r4 point out, the cscore correlates with other known measures such as accuracy and training speed. the authors claim this is a contribution. in turn, it is hard to tell if the cscore is a true metric of interest or a recapitulation of what is already known. no reviewer was in favor of acceptance.", "accepted": 0}
{"paper_id": "iclr_2021_tq5JAGsedIP", "review_text": "the paper is concerned with learning representations for timevarying graphs which is an important problem that is relevant to the iclr community. for this purpose the authors propose a new method to extend skipgram with negative sampling to higherorder tensors with the goal to perform an implicit tensor factorization of timevarying graphs.the proposed approach shows promising experimental improvements compared to previous methods. reviewers highlighted also the tasks considered in the paper as well as the theoretical and qualitative analysis as further positive aspects. however, there exist still concerns regarding the current version of the manuscript. in particular, reviewers raised concerns regarding the novelty of the approach sgns, its extension to higherorder tensors, as well as the connection to pmi have been studied in the literature. as such the new technical contributions are limited. reviewers raised also concerns regarding the scalability of the method and its applicability to large graphs. the revised version addresses this concern to some extent by showing experiments on midsized graphs with 20005000 nodes. while this clearly improves the paper, i agree with the majority of the reviewers that the manuscript requires an additional revision to iron out the points raised in this round of reviews. however, the presented results are indeed promising and id encourage the authors to revise and resubmit their work considering the reviewers feedback.", "accepted": 0}
{"paper_id": "iclr_2021_tw60PTRSda2", "review_text": "this paper is a computational linguistic study of the semantics that can be inferred form text corpora given parsers which are trained on human data are used to infer the verbs and their objects in text. the reviewers agreed that the work was well executed, and that the experiments comparing the resulting representations to human data were solid. the method employed has little or no technical novelty in my opinion, not necessarily a flaw, and its not clear what tasks beyond capturing human data representations could be applied to again, not a problem if the goal is to develop theories of cognition. the first draft of the work missed important connections to the computational linguistics literature, where learning about affordances for verbs referred to as selectional preferences has long been an important goal. the authors did a good job of setting out these connections in the revised manuscript, which the reviewers appreciated. the work is well executed, and should be commended for relating ideas from different subfields in its motivation and framing. but my sincere view is that it does not meet the same standards of machinelearning or technical novelty met by other papers at this conference. it is unclear to me what the framing in terms of affordance adds to a large body of literature studying the semantics of word embeddings, given various syntactically and semanticallyinformed innovations. it feels to me like this work would have been an important contribution to the literature in 2013, but given the current state of the art in representation learning from text and jointly learning from text and other modalities, i would like to have seen some attempt to incorporate these techniques and bridge the gap between the notion of affordance in textverbs selectional preference and gibsons notion of object affordance what you can do physically with an object in experiments and modelling, not just in the discussion. such a programme of research could yield fascinating insights into the nature of grounding, and the continuum from the concrete, which can be perceived and directly experienced, to the abstract, which must be learned from text. i encourage the authors to continue in this direction. an alternative is to consider submitting the current manuscript to venue where the primary focus is cognitive modelling, and accounting for human, behavioural data, and where there is less emphasis on the development of novel methods or models. for these reasons, and considering the technical scope of related papers in the programme, i cannot fairly recommend acceptance in this case.", "accepted": null}
{"paper_id": "iclr_2021_L5b6jUonKFB", "review_text": "this paper received 1 weak accept, 1 accept, and 1 weak reject. all reviewers questioned the motivation for continuous spacetime with respect to biological vision. obviously, discrete approximations used in machine vision are approximations but it is not clear from the paper or the authors response that this severely limits the ability of deep nets to predict neural data in ways that their continuous nets would not. in addition, i have to confess that i did not really understand the argument made by the authors in their revision. in any case, the burden should be on the authors to go beyond general statements and to really demonstrate that the proposed models provide actual insights for neuroscience since the performance in terms of machine vision on cifar10 is underwhelming the authors have to find a low data regime and even then the reviewers stated that the baselines used are not strong baselines, the reduction in the number of parameters is quite small relative to methods for actually reducing the number of parameters. clearly, the work has potential as noted by the reviewers. the authors suggest that dcns can be used to model the temporal profiles of neuronal responses, which are known to not be constant even when the experimental stimuli are static images for example, spatial frequency responses change over time in response to stationary gratings frazor et al., 2004. similar observations are made for the contrast response function albrecht et al., 2002. such temporal profiles cannot be simulated in conventional cnns.  this sounds like an interesting set of neuroscience data that the authors could be indeed leveraging to demonstrate the benefit of their approach. my recommendation would be to add those in a revision of this paper which will significantly strengthen the work. i would add that the concepts of temporal and spatial continuity are independent and the authors should consider studying them separately to provide more indepth analyses and convincing results. as it stands, the paper has clear potential but it does not make a sufficient contribution to either ml or neuroscience and hence, i recommend the paper to be rejected.", "accepted": null}
{"paper_id": "iclr_2021_j6rILItz4yr", "review_text": "adversarial training is usually done on the image space by directly optimizing the pixels. this paper suggests the adversarial training over intermediate feature spaces in the neural network. the idea is very simple. the authors have done extensive experiments to justify its performance. but the performance gain though this idea seems to be marginal. further, the layer to conduct the adversarial training can be optimized within the framework, which aligns with the general automl idea. the new version lalfa has been well introduced, but unfortunately, the practical result can be very straightforward, that is just to select the final layer. the more important alfa hyperparameters that would most benefit from automatic tuning are not sufficiently treated. there have been extensive discussions between the authors and the reviewers. after incorporating the reviewers comments, the paper will have a good chance to be accepted at another venue.", "accepted": 0}
{"paper_id": "iclr_2021_vttv9ADGuWF", "review_text": "the paper provides a simple prediction procedure to defend against rectangular patch attacks, and also a method to obtain some random estimates of the certified robustness of the method. the simplicity of the method is certainly appreciated. on the other hand, there are a number of issues preventing the acceptance of this paper. the main problem is that the paper deals with a randomized predictor, yet the certification guarantee developed for deterministic predictors is applied. this leads to several problems, starting from the target being undefined to unfair comparisons. while the authors made an attempt to address this in the rebuttal, more work is needed to properly settle this issue.", "accepted": 0}
{"paper_id": "iclr_2021_Q1aiM7sCi1", "review_text": "this paper is overall clearly written, and the proposed approach of performing clustering on the space of persistence diagrams can be a significant contribution. however, during the discussion, the reviewers share the concern about insufficient empirical evaluation. in particular, datasets are limited and lacombe et al. 2018 is not included as a comparison partner, although the authors had a chance to include it in the author response phase. since this point is crucial, i will reject the paper. addressing these points will largely improve the paper, and also reviewers put a great effort to give detailed reviews for the paper. so i hope the authors take the reviews into consideration for further revision of the paper.", "accepted": 0}
{"paper_id": "iclr_2021_cvNYovr16SB", "review_text": "the authors propose a particlebased entropy estimate for intrinsic motivation for pretraining an rl agent to then perform in an environment with rewards. as the reviewers discussed, and also mentioned in their reviews, this paper bears stark similarity to work of 5 months ago, presented at the icml 2020 lifelong ml workshop, namely, a policy gradient method for taskagnostic exploration, mutti et al, 2020mepol. what is novel here is the adaptation of this entropy estimate to form an intrinsic reward via a contrastive representation and the subsequent demonstration on standardized rl environments. the authors have added a comparison to mepol, and in these experiments, apt outperforms this method, sometimes by some margin. unfortunately this work does not meet the bar for acceptance relative to other submissions.", "accepted": null}
{"paper_id": "iclr_2021_T4gXBOXoIUr", "review_text": "the proposed convirt learns representations of medical data from paired image and text data. while the paper addresses a relevant problem, the reviewers agree that the method has limited novelty. two reviewers find and that the experiments are not convincing. one reviewer notes that the paper does not compare to the stateoftheart methods for the tasks.", "accepted": 0}
{"paper_id": "iclr_2021_0Zxk3ynq7jE", "review_text": "the addresses openset recognition, namely, detecting anomalous samples that belong to classes not observed during training. it has been shown that existing methods fail on openworld images. the current paper shows empirically that performance can be greatly improved if based on lowdimensional features. reviewers had grave concerns about the novelty of the approach and the logic behind the workflow. they found merit in the paper but chose to retain their scores after reviewing the rebuttal. as a future recommendation, it would be useful to provide more evidence about what component of the method or workflow are novel and what makes them work well.", "accepted": 0}
{"paper_id": "iclr_2021_qFQTP00Q0kp", "review_text": "this paper presents a general selfsupervised time series representation learning framework. the organization is good, and the architecture is well motivated. however, the paper has limited novelty, and is a straightforward application of ideas in selfsupervised learning literature. experimental results are not entirely convincing. the used dataset is smallscale that makes the task simple. a more thorough comparison with recent related work is needed. the presentation is also sometimes hard to follow.", "accepted": null}
{"paper_id": "iclr_2021_GSTrduvZSjT", "review_text": "dear authors, the paper contains many interesting and novel ideas. indeed, tuning stepsize is very time and energyconsuming, and deriving and analyzing new adaptive algorithms has not only theoretical benefits but, more importantly, is a key when training more complicated ml models. the paper contains many weaknesses as noted by reviewers. i know that you have addressed many of them one of the reviewers is still concerned about the other issues involving theorem 1 and the assumption of the bounded preconditioner. he thinks the preconditioner bound is troublesome. in the overparameterized regime, he would expect the gradients to become near zero as the algorithm converges, which would actually cause the preconditioner to not be bounded below. it seems that the analysis might actually improve if the authors abandoned amsgradadam and instead just considered sgd for which the preconditioner assumption is not an assumption but just a property of the algorithm. thank you", "accepted": null}
{"paper_id": "iclr_2021_RrSuwzJfMQN", "review_text": "i thank the authors and reviewers for the discussions. reviewers raised major concerns regarding the significance of the results and experiments. given all, i think the paper needs more work before being accepted. i encourage authors to address comments raised by the reviewers to improve their paper.  ac", "accepted": 0}
{"paper_id": "iclr_2021_8znruLfUZnT", "review_text": "the paper proposes a deep learning approach to blind image denoising based on deep unrolling. in particular, the proposed network is derived from convolutional sparse coding algorithms, which are unrolled, untied across layers and learned from data. the paper proposes a frequency domain regularization scheme in which the filters consist of a single analytically defined lowpass filter and a large collection of filters which are constrained to reside in the midtohigh frequency ranges. it also proposes to tie the thresholds in the softthresholding stages of the learned network to estimates of the noise variance, making the proposed scheme more robust to variations in the noise level. pros and cons  having a single lowpass dictionary atom reduces redundancy and potentially coherence in the learned dictionary. this type of regularization may also reduce the timedata required to learn.  using noise estimators and a noise adaptive threshold renders the model more robust to variations in the noise level. this is important, since in most denoising applications the noise level is not known apriori. as the reviewers note, the idea of tuning thresholds in an unrolled sparse coding method based on the noise level is not a novelty of the paper; the novelty here is coupling this with a waveletbased estimate of the noise level.  all three reviewers raise concerns regarding the novelty of the work compared to existing convolutional sparse codingbased neural networks. the structure of the network is similar; the main difference is the frequency restriction for learned atoms, which is enforced by prefiltering the learned atoms with a highpass filter.  the paper is not entirely clear in its motivation and argumentation. reducing the coherence of the learned dictionary makes sense from the perspective of certain worst case results from sparse approximation. however, the coherence is a worst case quantity; moreover, certain approaches to coherence control e.g., using large stride control coherence at the expense of the expressiveness of the dictionary, and hence may not actually improve its ability to provide sparse reconstructions of natural signals. the proposed frequency domain regularization is a sensible approach to controlling coherence, since lowfrequency atoms will tend to be highly coherent, but would benefit from a crisper analytical motivation.  reviewers found the experiments lacking in some regards. in particular, the paper only evaluates its proposals on synthetic experiments with gaussian noise. while this is in line with some previous work on deep learning based denoising, more extensive and realistic experiments would have bolstered the papers argument. overall, the paper makes a sensible proposal regarding the adaptivity to unknown noise levels, and introduces a potentially useful frequencydomain restriction on the learned filters in a csc network. however, the reviewers did not find that the paper made a clear argument for the significance of these proposals, and raised other concerns regarding the clarity and experiments. the consensus of the reviewers is to recommend rejection.", "accepted": 0}
{"paper_id": "iclr_2021_P63SQE0fVa", "review_text": "this paper proposes a deep reinforcement learning approach for solving minimax multiple tsp problem. their main algorithmic contribution is to propose a specialized graph neural network to parameterize the policy and used a clipped idea to stabilize the training. unfortunately, the reviewers remain to be unconvinced by the experiments after the rebuttal and the writing need to be significantly improved. also, it would be worthwhile to study how the proposed method can generalize to other problems.", "accepted": 0}
{"paper_id": "iclr_2021_ZHADKD4pl5H", "review_text": "the rationality of the proposed method, especially its implementation detail, is challenged by the reviewers. additionally, the experimental part and the writing of the paper should be improved. according to the feedback of the reviewers, i dont think this work is qualified enough at its current status.", "accepted": 0}
{"paper_id": "iclr_2021_1-Mh-cWROZ", "review_text": "this is a promising idea, without enough empirics to substantiate its potential utility, and also with a lack of clarity on the importance of the outlined task itself foldbased rather than structurebased conditional sequence generation. there remain concerns about the lack of a more comprehensive comparison to methods for structuretosequence e.g. ingraham was added during the revision but only in a limited capacity, or easy generalizations of them, and about the quality of some of the presented results. additionally, the concern about sensitivity to rotational invariances, and related issues wrt the fixedsize cubic grid were not satisfactorily addressed. as a sidenote, the quality of the manuscript in terms of scholarliness of presentation was overall lacking.", "accepted": null}
{"paper_id": "iclr_2021_SVsLxTfHa1", "review_text": "this paper investigates how to align word senses across languages. this has not been studied much as past work has primarily considered aligning word embeddings across languages. the paper is well written and well motivated. unfortunately the empirical results are not very strong. the baselines are somewhat low and the gains are modest the excuse that it is difficult to train bertsized models in academia is acknowledged. overall, there is not enough support for acceptance at such a competitive venue as iclr.", "accepted": 0}
{"paper_id": "iclr_2021__MxHo0GHsH6", "review_text": "this paper proposed a method to train quantized supernets which can be directly deployed without retraining. a main concern is that there is limited novelty. the proposed method looks like a combination of wellknown techniques. experimental results are promising. however, it is not clear if the comparisons are fair and if all the methods are using the same setup. it is desirable to have additional analysis and ablation studies. the writing can also be improved.", "accepted": 0}
{"paper_id": "iclr_2021_1UtnrqVUeNE", "review_text": "in this paper, the authors use a gp classifier to detect if the output of a nn classifier has been decided correctly. the gp takes as input the original input vector x and the output of the nn, i.e. the calibrated posterior probabilities given by the nn. it uses that as an input vector for the gp classifier to decide if the sample was correctly decided. the output of the gp will serve as confidence in the output of the nn. the results are comparablesuperior with the stateoftheart and the authors have repeated the experiments with over 125 different datasets. the reviewers of this paper were all cautiously positive about the paper, but all of them pointed towards the reduced novelty of the paper. also, none of the reviewers were willing to champion this paper as a musthave at iclr 2021. for my reading of the paper, i would tend to agree with the reviewers comments. also, i find that using the same nn, rather shallow, with the same configuration for all the datasets seems rather limited. given that this method is independent of the underlying classifier and that the databases used are low dimensions and a low number of training examples, i would have liked to see what a random forest or a gp can accomplish. also, i would have used bigger nns that can be trained to overfit the sigmoid outputs for classification of higher accuracy. i believe that having a diversity of underlying classifiers is more relevant than having 125 datasets. we need to find the best classifier or ensemble and then apply the different mechanisms for estimating if the output is the correct one. otherwise, the proposed method might only be workable for this specific nn configuration. in the tables, it can be hinted that this might be happening, as about 80 of the cases mcp and red are indistinguishable in the auroc values. also, for all of these datasets a gp could be used as an underlying classifier, and given the premises of this paper, the authors could check how well calibrate a gp classifier is. also, there has been considerable work on calibrating nns when they are trained to overfit. comparing with those methods should be straightforward, as they provide more information than just a confidence score. this is probably the most influential paper httpsarxiv.orgabs1706.04599 1000 references, but there are some recent papers too. finally, if the goal is to use a gp to detect if the classification done by the nns is accurate, using a gp might be an overkill, as the complexity of the gp, especially for large datasets might end up being larger than the underlying classifier.", "accepted": null}
{"paper_id": "iclr_2021_m4PC1eUknQG", "review_text": "well, this paper has achieved something remarkable in this review process the initial scores came in at fairly low scores 4, 5, 3, 6. however, as the discussions  rebuttals went back and forth, the reviewers were able understand and see the merits of the proposed methodology. namely, the setting of l2e learning to exploit, which makes use of a novel method called opponent strategy generation, to quickly generate very different types of opponents to play against. one more pertinent component is the use of mmd maximum mean discrepancy regularization which can remove the necessity of dealing with task distributions, and does a better job in creating diverse opponents. having understood the technical approach, three of the reviewers decided to substantially increase their scores. r4 increased 46, r5 increased 56, r3 increased 34, while r2 held steady with a score of 6. it was also good to see empirical favorable results compared to other baseline methods l2e had the best return against unclear opponents, such as rocks opponent and nash opponent. without any reviewer arguing strongly for acceptance, the program committee decided that the paper in its current form does not quite meet the bar, and also that it would benefit from another revision.", "accepted": 0}
{"paper_id": "iclr_2021_B5bZp0m7jZd", "review_text": "quality while the paper presents an interesting approach, reviewer 2 raised relevant questions about the assumption of the theoretical justification that needs to be thoroughly addressed. moreover, as noted by reviewer4, the quality of the paper would also benefit from a more clear connection to existing modelbased reinforcement learning literature, besides pan et al.. for example, how much of the proposed approach and results can be applied in other algorithms?  clarity while the paper is generally well written and only minor suggestions from the reviewers should be implemented.  originality the proposed approach is a small but novel improvement over existing algorithms to the best of the reviewers and my knowledge.  significance of this work the paper deal with a relevant and timely topic. however, it is currently very difficult to gauge the significance of this work, and it unclear if the results can be extended beyond toy benchmarks and to other rl algorithms. several reviewers suggested additional experiments to strengthen the paper.  overall this paper deal with an interesting topic and presents new interesting results. however, the current manuscript is just below the acceptance threshold. extending the experimental evaluation and improving the clarity of the paper would crucially increase the quality of the paper.", "accepted": 0}
{"paper_id": "iclr_2021_kJVVgJ-yCq", "review_text": "this paper proposes an approach for improving multitasklearning by providing a way of incorporating task specific information. pros 1 all reviewers agreed that the paper is clearly written 2 interesting to see a single model for ast, sts speechtospeech translation and mt cons 1 the work is not adequately compared with related work some important references are also missing  the authors did perform some additional experiments with t5 and pointed out some drawbacks but this needs to be explored a bit more. 2 the answers about scalability are not very convincing and need more empirical results. overall, none of the reviewers were very positive about the paper and felt that while this is a good first attempt, more work is needed to make it suitable for acceptance.", "accepted": 0}
{"paper_id": "iclr_2021_MbM_gvIB3Y4", "review_text": "overview the paper tries to answer which mutual information mi objective is sufficient for representation learning repl in reinforcement learning rl. three common objectives are considered forward, state, and inverse. the paper shows that only the forward objective is sufficient for learning, i.e., sufficient for learning of optimal policyvalue function. the authors also demonstrate this phenomena using empirical experiments. quality, clarity, originality and significance all the reviewers believe this paper is novel in terms of methodology, i.e., evaluate the sufficiency of the repl in terms of down stream tasks. however, there is a lack of clarity in the experiment sections. the authors have provided more details in the rebuttal phase. the reviewers also have concerns that this paper may be too far from typical experimental settings to have a real impact on the field. an unofficial review pointed out there is a mistake in the proof of the paper. the authors later also confirmed the flaw and claimed it is fixed. recommendation the paper is indeed interesting and novel. however, the impact to the practice community might not be significant. that being said, the paper should warrant publication eventually. however, the authors changed large amount of text about the proofs before and after rebuttal, which also introduced some additional typos, confusions, and also technique sloppiness or flaws. the reviewers are concerned about this. overall i believe that the paper is not in a state to be published yet.", "accepted": null}
{"paper_id": "iclr_2021_UQz4_jo70Ci", "review_text": "all three reviewers initially recommended reject. the main concerns were 1 weak technical contribution and insight r1, r2, r3, r4; 2 incremental novelty another variation of siamfc r1, r2, r3; 3 unconvincing experiment results against missing sota r1, r2, r3; the authors response did not assuage these concerns.", "accepted": 0}
{"paper_id": "iclr_2021_lDjgALS4qs8", "review_text": "the paper proposes to explain the representation for layeraware neural sequence encoders with multiordergraph mog. based on the mog explanation, it further proposes graphtransformer as a graphbased selfattention network empowered transformer. as commented by the authors, a main purpose of graphtransformer is to show an example application of the mog explanation. during the discussion period, after reading the paper and checking the code, the ac had raised a serious concern there is a big gap between the mog motivation and the actual implementation. the ac had urged the referees to take a careful look at the implementation details, in particular, lines 524561 in the attached code supplementfairseq0.6.2_halfdim_gate  fairseq  models transformer.py. the ac had made the following comments to the referees whether the performance gain of graphtransformer over transformer is due to the mog explanation is highly unclear. there is no direct evidence, such as appropriate visualization, to support that. in a highlevel description, instead of using a usual skip connection that would combine beforex and x, the actual implementation is to 1 define increamental_x  x  beforex, 2 let increamental_x attend on beforex to produce x1, let beforex attend on increamental_x to produce x2, and let increamental_x attend on increamental_x to produce x3, 3 combine beforex, x1, x2, x3 in a certain way to produce the layer output. reviewer 2 responded to the acs concern after examining the transformer.py and section 2  3, we cannot understand why the output of selfattentions could be regarded as mog subgraphs? the authors did not explain the connection. in their code, the graph transformer seems to just utilize 3 multihead attentions line 539541 in their encoder. using mog to interpret the outputs of three attentions line 539541 is not very convincing. the link is weak. we agree with your comments. to summarize, the link between the actual implementation in the code and all the mog explanations is quite weak, and the technical novelty of the actual implementation is not strong enough for an iclr publication. therefore, the ac recommends reject.", "accepted": null}
{"paper_id": "iclr_2021_rryJiPXifr", "review_text": "the reviewers appreciate the idea of hyperparameter planning and the thorough experimentation. some concerns remain regarding the comparison between this method and slowfast that require to be addressed. also, the scope of the paper that targets hyperparameter optimization networks for action recognition specifically, may be too narrow for an iclr audience.", "accepted": null}
{"paper_id": "iclr_2021_f_GA2IU9-K-", "review_text": "this work proposes a nondecreasing quantile functional form for distributional rl, and secondly propose using the distributional error as a means of exploration. the experimental results are very exciting. the paper, however, needs further work before acceptance the reviewers raised concerns about theorem 1 a full proof is not included nor written convincingly during discussion, and while several encouraging experiments were added during the discussion to the paper addressing the reviewers concerns, they fell short understandably, given the time available. thus on this basis, i recommend rejection at this time, but think it likely that with these adjustments the paper will be accepted in future.", "accepted": 0}
{"paper_id": "iclr_2021_XOuAOv_-5Fx", "review_text": "this work proposes a novel metric for measuring calibration error in classification models. pros  novel calibration metric addressing limitations of previously used metrics such as ece cons  limited experimental validation on cifar10cifar100 only  unclear impact beyond proposing a new calibration metric  unclear value of using the proposed uce metric for regularization and ood detection all reviewers appreciate the aim of the work to produce a calibration metric that addresses shortcomings of commonly used existing metrics such as expected calibration error ece, which is known to be sensitive to discretization choices. however, all reviewers remain in doubt whether the proposed metric uncertainty calibration error, uce is truly a better metric of calibration than previous proposals. this doubt comes from two sources 1. limited experiments that do not convincingly show the usefulness of uce; and 2. interpretability of uce not being as intuitive to the reviewers. the experiments also use uce as regularizer but the benefit of doing so over simple entropy regularization is not clear. overall the work is wellmotivated and written and the proposed uce measure is interesting. however, the reviewers remain unconvinced of the claimed benefits and the potential impact for measuring or improving calibration.", "accepted": 0}
{"paper_id": "iclr_2021_sHSzfA4J7p", "review_text": "three reviewers have reviewed this paper and they maintain their findings after the rebuttal. the reviewers are mainly concerned about the novelty several highlyrelated papers exist and well as the technical contribution more theoretical developments are needed. therefore, this paper in its current form cannot be accepted.", "accepted": null}
{"paper_id": "iclr_2021_tqc8n6oHCtZ", "review_text": "the paper attempts to reduce computational cost of transformer models. in this regard, authors generalizer powerbert by proposing a variant of dropout that reduces training cost by randomly sampling a fraction of the length of a sequence to use at each layer. further, a sandwich training method is used which trains a spectrum of randomly sampled model between the largest and the smallest size model. at test time, the best length configuration that balances the accuracy and latency tradeoff via evolutionary search is used. the reviewers found the general idea interesting, but raised a number of concerns. first, proper baselines should be used and related works be discussed. in particular, the method is built on top of powerbert, yet it does not directly compare with it, and there was no good response when pointed out by a reviewer. second, as the paper employs many tricks some new some from prior work, but does not do any ablation studies to show how each of those contributes to the final accuracy gains. finally, to showcase benefit compared to prior works in terms of computational cost a proper evaluation methodology and actual speedups for batch size 1 inference should be provided. thus, an improved evaluation would benefit the paper a lot and paper in its current form is not ready for publication.", "accepted": 0}
{"paper_id": "iclr_2021_48goXfYCVFX", "review_text": "there is a broad consensus that this paper explores an interesting and novel problem space. nonetheless, in their initial assessment, the reviewers pointed to a few limitations of the paper including lack of strong baselines, lack of an ablation study, and weaker results according to the hit10 metric. the authors provided an improved version of their paper as a response. the new paper added two baselines, is better written, and justifies some of the hit10 results basically, the metric is biased for this task. after discussion, the reviewers find that the contribution of the current manuscript falls short of the acceptance threshold. in particular, the reviewers find that 1 this contribution is for a specific domain of recommender systems, an area of interest, but perhaps only relevant to a subset of the iclr community; 2 while more recent baselines helped, there has been lots of more recent work on collaborative filtering models for recommender systems over the last few years the widedeep baseline is from 2016; 3 since some of the usual recommender systems metric does not seem appropriate here, why not suggest new ones or propose a slightly different evaluation protocol; 4 the proposed model is useful, but somewhat incremental given prior work. all in all, while any of these limitations on their own might not have been sufficient to warrant rejection, i find that their combination does. given the interest in this new task, i do strongly encourage the authors to pursue their work. i also find that the qualitative study propsoed by reviewer 4 could add another interesting angle to this paper i also imagine that it might not be that easy to carry out.", "accepted": 0}
{"paper_id": "iclr_2021__FXqMj7T0QQ", "review_text": "as the reviewer confidence of the reviews of 2.8 or lower, i made a full and detailed pass of submission as it was requested by the pcs. the paper studies how to parallelization of mcts affects its performance and provides the analysis of the excess regret  how much error we incur by parallelizing as opposed to singlethreaded execution. the submission however does not give convincing arguments on why existing parallel methods perform empirically under their sequential counterparts. this would be particularly useful for the settings described in the submission, which are only resembling the actual parallel solutions. furthermore, the paper analysis of a cumulative regret in which it counts the errors made for the exploration  but the setting here is a mcts search, planning, when we access an oracle a model of the environment and we should be rather interested in the sample complexity, studied for example in httpswww.cis.upenn.edumkearnspaperssparsesamplingjournal.pdf that gave early results and many follow ups until or we should study how good the final policy it  and in your parallel setting which is indeed very timely, how small the excess error of this final policy is. or you only focus on a single action recommended by mcts and you study some pureexploration measure bestarm identification, simple regret, epsbai, ...  from the theoretical side modeling practice, i dont see the theoretical framework provided to be suited with practical considerations that an mcts is facing. specifically the need of condition 1 for the consequence of theorem 1 to hold is very worrying. briefly it states that if the value and the counts are not the same as reference method, then the results obtained from the that parallel executions would not match the results reference method. while this true, it simply tells us that it would be good if the parallel threads came up with the same value as the reference method  the main issue that this does not tell the practitioner how to assure the condition states in eq 8. it would be much more interesting to characterize that, i.e., under which conditions in particular, that the practitioner can influence or at least verify prior to the execution is the property stated in the eq. 8 satisfied. theorem 2 bring additional concerns. a book of munos on mcts httpshal.archivesouvertes.frhal00747575v4document has section 2.3 devoted why vanilla uct has poor performance. there is work by kocsis and szepesv\u00e1ri studying under which conditions uct can have favorable regret  there is at least a discussion missing why theorem 2 would be able to bypass known hardness for uct for higher depths, and if not i question its utility. finally, from finite time result excess i would expect more finite time lessons learned beyond regret term that converges to zero as n increases.", "accepted": null}
{"paper_id": "iclr_2021_bLGZW0hIQpO", "review_text": "this paper consider an important problem counterfactualregret cfr minimization, and proposes a new algorithm to solve this problem. reviewers raised many questions and concerns that the authors chose not to answer. we can only recommend rejection", "accepted": 0}
{"paper_id": "iclr_2021_TTnPcO6kK5", "review_text": "the paper presents a new variant of the stochastic heavy ball method with coordinatewise stepsizes. they prove a regret upper bound in the online convex optimization setting and validate the algorithm on few deep learning tasks. the reviewers found the paper severely lacking on many aspects. in particular, the formulation appears not motivated at all, the regret upper bounds relies on an unverified assumption of boundedness of the iterates, the momentum parameter must decrease exponentially over time. note that it is known how to analyze the momentum algorithm under much more general conditions. the empirical evaluation was also judged not sufficient, with only 2 datasets one of them being mnist. overall, the paper was judged not suited for publication at iclr.", "accepted": 0}
{"paper_id": "iclr_2021_RCGBA1i5MF", "review_text": "reviewers were concerned about the technical novelty because the twostage sampling strategy is similar to bbn and the decoupling of features and classifiers. rebuttal addressed some concerns about the experiments, but reviewers major concerns remained.", "accepted": null}
{"paper_id": "iclr_2022_QuObT9BTWo", "review_text": "this paper develops a preferenceconditioned approach to approximate the pareto frontier for multiobjective combinatorial optimization moco problems with a single model thus dealing with the thorny problem that there can be exponentiallymany paretooptimal solutions. it appears to provide flexibility for users to obtain various preferred tradeoffs between the objectives without extra search. the basic idea is to use endtoend rl to train the single model for all different preferences simultaneously. the technical soundness and practical performance are strong. this works approximation guarantee depends on the ability to approximately solve several weighted singleobjective problems. this may be challenging due to the nphardness of the latter. however, this limitation seems to also apply to other endtoend learningbased approaches. one area where the novelty is somewhat limited is that the paper borrows some number of ideas from neural singleobjective optimization. the contribution overall seems noteworthy for hard multiobjective problems.", "accepted": 1}
{"paper_id": "iclr_2022_oiZJwC_fyS", "review_text": "the submission introduces an algorithm for structured pruning of fully connected relu layers using ideas from tropical geometry. the paper begins with a very accessible overview of key concepts from tropical geometry, and shows how relu networks can be thought of as tropical polynomials. it gives an efficient kmeansbased algorithm for pruning units in a way that approximately minimizes the hausdorff distance between certain polytopes. experiments show that the method outperforms other methods based on tropical geometry and is competitive with sota methods from a few years ago. i think the reviewers, authors and i all agree on the following points tropical geometry is a mathematical topic not commonly used in our field and for which it is difficult to find expert reviewers notice that most of the citations arent from ml venues. the paper is wellwritten, and the authors have taken pains to present the required concepts in an accessible way. nobody has raised any concerns about correctness. while this isnt the first pruning method that uses tropical geometry, the algorithm is novel and interesting. its expensive, but not unreasonably so. the experiments are a proofofconcept they use small networks by todays standards, and the baselines arent the current sota. the average scores are slightly below the usual cutoff. the reviewers are concerned about whether this method is useful, given that is based on different principles from current methods and cant quite compete with current sota. but my own sense is that this is a paper that wed like to have at iclr. it gives a clear, accessible introduction to tropical geometry and demonstrates its usefulness for practical deep learning. it demonstrates competitiveness with fairly strong baselines, which is all we should expect from methods that havent benefited from years of hillclimbing on the same handful of ideas. i recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2022_-llS6TiOew", "review_text": "the authors address a very important question pertaining to the relevance of morphological complexity in the ability of transformer based conditional language models. through extensive controlled experiments using 6 languages they answer as well as raise very interesting questions about the role of morphologysegmentationvocab size which mat spawn more work in this area. all the reviewers were positive about the paper and agreed that the paper made significant contributions which would be useful to the community. more importantly, the authors and reviewers engaged in meaningful and insightful discussions through the discussion phase. the authors did a thorough job of addressing all reviewer concerns and changing the draft of the paper accordingly. i have no hesitation in recommending that this paper should be accepted.", "accepted": 1}
{"paper_id": "iclr_2022_3PN4iyXBeF", "review_text": "the paper presents a quite rigorous analysis of approximate implicit differentiation with warm starts applied to strongly convex upper levelstrongly convex lower level and nonconvex upper levelstrongly convex lower level bilevel optimization algorithms in a very general yet also very practical framework. they allow for stochastic errors in the algorithms solving the upper and lower level problems, making their work practical and applicable to real problems in machine learning hyperparameter optimization, while analyzing in a way that is agnostic towards which algorithms are specifically used for the lower and upper level problems. three out of four reviewers were rather positive of the paper scores 6, 6, 8. one reviewer was very negative score 3. to my knowledge, the authors have convincingly answered all the points raised by the reviewer. unfortunately, the reviewer did not follow up. similarly to reviewers, i found sections 13 to be extremely wellwritten and to give a nice overview of the field. section 4 had slight clarity issues dense notation that were addressed in the revision. reviewer 6zlq partially proofread proofs. overall, i recommend acceptance as a poster, as this paper is advancing stochastic implicit differentiation and should be of interest to many at the iclr conference.", "accepted": 1}
{"paper_id": "iclr_2022_RCZqv9NXlZ", "review_text": "most of the reviewers think this paper is clearly a valuable addition to iclr based on the convincing theoretical analysis and extensive experimental results. please refer to reviewerss review for more detailed discussions of the pros and cons of the paper.", "accepted": 1}
{"paper_id": "iclr_2022_zzk231Ms1Ih", "review_text": "the paper takes a creative step in the theory of tournaments, and it seems plausible that this could lead to interesting followups. the reviewers made many excellent comments and i highly encourage the authors to take all of them into account in the revision, it will make the paper much stronger.", "accepted": 1}
{"paper_id": "iclr_2022_TYw3-OlrRm-", "review_text": "this paper studies the problem of training tiny networks, by proposing a new training method called network augmentation netaug. the main challenge for training tiny networks lies in underfitting, which data augmentation and dropout etc. regularizations may suffer from for tiny networks. to overcome this hurdle, the proposed method first embeds or augments the tiny network as a subnet into a larger network, mostly by enlarging the width; then the gradients from the larger network are used as additional or auxiliary supervision. with this training strategy, the tiny model can perform better than the conventional training scheme on imagenet and several downstream tasks. the proposed method is simple to implement and complementary with other techniques such as knowledge distillation and pruning. while there are lots of works studying how to improve the accuracy of large models, there are relatively fewer works focusing on the tiny network training. despite that there are existing works sharing a similar idea of netaug for large model training, which slightly hurts the novelty of this work, the majority of reviewers still like the idea and suggest to accept the paper.", "accepted": 1}
{"paper_id": "iclr_2022_aBsCjcPu_tE", "review_text": "thank you for your submission to iclr. this paper presents a technique for image synthesis based on stochastic differential equations and a diffusion model. this looks to be a very nice idea with good results. after discussion, the reviewers converged and all agreed that the paper is ready for publicationthe most negative reviewer raised their score after the author rebuttal, from a weak reject to weak accept. the rebuttal clearly and concisely addressed several concerns of the reviewers. im happy to recommend accepting the paper.", "accepted": 1}
{"paper_id": "iclr_2022_o_HsiMPYh_x", "review_text": "the authors propose a simple method to estimate the accuracy of a classifier on an unlabeled dataset given an indistribution validation set. in extensive experiments the authors show that the proposed method is significantly more accurate than previous methods and other baselines. the reviewers are quite consistent in their judgement, just the weighting of the different aspects is different. after the rebuttal four out of five reviewers recommend acceptance. strong points  simplicity of the method  strong experimental results for various tasks and domain shift problems weak points  there is no clear theoretical statement when the method is supposed to work  the discussion in section 3.1 is pretty obvious and seems a bit like a waste of space whereas the motivation for the actual method is very short while i agree with the reviewers that there is little theoretical justification for the method, the strong experimental results on various datasets, tasks and different domain shifts make this paper interesting for a large audience. thus this paper is a nice contribution to iclr and i recommend acceptance. however, i strongly recommend to the authors to add more motivation in section 4 and add a limitation section where the cases are discussed where the method is definitely not working. section 3 is pretty obvious and could be significantly shortened or integrated into the limitations section. one case which is highly relevant for this limitations section is the provable asymptotic overconfidence of neural networks which is discussed in hein et al, why relu networks yield highconfidence predictions far away from the training data and how to mitigate the problem, cvpr 2019 this would definitely lead to a failure of the presented method as all predictions would get a score above the threshold. i would also assume that the method would predict high accuracy values for outofdistribution tasks which are semantically similar e.g. training on cifar10 and then using cifar100 as unlabeled dataset. in that context it would be interesting to evaluate oodaware classifiers using atc such as discussed in hendrycks et al, deep anomaly detection with outlier exposure, iclr 2019 also it would be helpful to understand better the influence of the classifier performance on the original task on the performance of atc on unlabeled data.", "accepted": 1}
{"paper_id": "iclr_2022_q7n2RngwOM", "review_text": "in this paper, the authors proposed a method for causal inference under limited overlap  an important and understudied complication. the authors propose to recover a prognostic score using a variational autoencoder, and thereby map a higher dimensional set of covariates with limited overlap to a lower dimensional set where overlap holds, and such that ignorability is maintained. the paper was reviewed quite favorably by reviewers, and the authors updated the manuscript to address specific issues raised by reviewers.", "accepted": 1}
{"paper_id": "iclr_2022_mHu2vIds_-b", "review_text": "this paper integrates model ensembles with randomized smoothing to improve the certified accuracy. the methodology is motivated theoretically by showing the effect of model ensemble on reducing the variance of smooth classifiers. moreover, it proposes an adaptive sampling algorithm to reduce the computation required for certifying with randomized smoothing. extensive experiments were conducted on cifar10 and imagenet datasets. the strengths of the paper are as follows  in terms of significance of the topic, the problem tackled in the paper is significant and highly relevant.  the motivation of using model ensemble is clearly illustrated via a figure and well justified with theoretical analysis.  algorithmically, the paper proposes adaptive sampling and kconsensus algorithms to reduce the computational cost, making the method more practical.  experimentally, the paper exhibits competitive results against several frameworks for training smooth classifiers and on several datasets.", "accepted": 1}
{"paper_id": "iclr_2022_64trBbOhdGU", "review_text": "this paper proposes a new approach to solve mixed discretecontinuous action rl problems, based on embedding actions into a latent space so that standard continuous control algorithms like td3 can be applied. experiments over standard discretecontinuous benchmarks demonstrate the superiority of the proposed approach vs. existing baselines. there is overall a strong consensus of all reviewers towards acceptance, especially after the discussion period where the authors were able to submit several revisions addressing most of the questions and concerns raised in the original reviews, in particular w.r.t. the quality and relevance of the results. i believe this submission could be improved along two axes though 1. as pointed out by some reviewers, the current environments are somewhat simple. a more realistic robotic task for instance could be a good fit for such an algorithm. that being said, as the authors pointed out, this may require custom development due to the lack of existing public environment with the proper setup. 2. as a reviewer mentioned, there is no related work section, and although previous work is discussed in the introduction, i consider that it remains limited, and more previous work should have been discussed. here are some pointers regarding relevant work i am aware of  hierarchical approaches for reinforcement learning in parameterized action space httpsarxiv.orgabs1810.09656  neural ordinary differential equation value networks for parametrized action spaces httpsopenreview.netforum?id8wkd467b8h  improving action branching for deep reinforcement learning with a multidimensional hybrid action space httpsipsj.ixsq.nii.ac.jpejindex.php?actionpages_view_mainactive_actionrepository_action_common_downloaditem_id199976item_no1attribute_id1file_no1page_id13block_id8  distributed reinforcement learning with selfplay in parameterized action space httpscgdsss.github.iopdfsmc21_0324_ms.pdf  discrete and continuous action representation for practical rl in video games httpsarxiv.orgabs1912.11077  multipass qnetworks for deep reinforcement learning with parameterised action spaces httpsarxiv.orgabs1905.04388 in particular, i believe the last one mpdqn should have been one of the baselines, since it is supposed to be an improvement over the pdqn algorithm that is one of the baselines used here. i encourage the authors to try and incorporate it for the final version at the very least, it should be cited. in spite of these concerns, i still recommend acceptance since the combination of action space embedding with mixed discretecontinuous actions is novel and nontrivial, and the empirical validation is convincing enough in its current state.", "accepted": 1}
{"paper_id": "iclr_2022_rw1mZl_ss3L", "review_text": "thank you for your submission to iclr. this paper presents a straightforward but reasonable approach to slightly improving the performance of largebatch training via adversarial training. the basic approach is to apply small epsilon adversarial training, shown to help performance in smallbatch settings, but accelerate the method using stale parameters to allow for parallel computation of the perturbations. this speeds up adversarial training while improving performance, enough to enable it to be more effective than existing techniques for this largebatch setting. the reviewers are not entirely in agreement about this paper, but i personally found the objections of the reviewer to be fairly generic, and not really addressing the core contributions of the paper. however, i also felt that the overall contribution of this work seems somewhat incremental, using a notparticularlyunexpected result that we can use stale gradients for this form of adversarial training to achieve moderate speedup in what ultimately seems like one point in hyperparameter space. that all being said, though, clearly the authors are working within standard benchmark frameworks, and simple algorithms here are indeed a positive rather than a negative. so i am inclined to slightly recommend the paper for acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_YeShU5mLfLt", "review_text": "verifying robustness of neural networks is an important application in machine learning. the submission takes on this challenge via the interval bound propagation ibp framework and provides a theoretical analysis on the training procedure. they establish, in the large network with case, that the certification via ibp reflects the robustness of the neural network. despite the tensions between the changing architecture and the required accuracy, the results are insightful. the ac recommends the authors to revise the paper, correcting the significant amounts of typos and improve the presentation for its final version.", "accepted": 1}
{"paper_id": "iclr_2022_w60btE_8T2m", "review_text": "this paper proposes a spanning treebased graph generation framework for molecular graph generation, which is an interesting problem. the treebased approach is efficient and relatively effective in molecular graph generation tasks, and the empirical results are convincing. there were some concerns during the initial reviews, but all of them have been addressed during the discussion phase. thus, i recommend this work be accepted.", "accepted": 1}
{"paper_id": "iclr_2022_cw-EmNq5zfD", "review_text": "the paper proposes a new pipelineparallel training method called wpipe. wpipe works on a very high level by replacing the twobuffer structure of pipedream2bw with a twopartitiongroup structure, allowing resources to be shared in a similar way to pipedream2bw but with less memory use and less delays in weight update propagation across stages. the 1.4x speedup it achieves over pipedream2bw is impressive. in discussion, the reviewers agreed that the problem wpipe tries to tackle is important and that the approach is novel and interesting. but there was significant disagreement among the reviewers as to score. a reviewer expressed concern about the work being incremental and difficult to follow. and while these were valid concerns, and the authors should take note of them when revising their paper, i do not think they should present a bar to publication, both based on my own read of the work and also in light of the fact that other reviewers with higher confidence scores did not find novelty to be a disqualifying concern. as a result, i plan to follow the majority reviewer opinion and recommend acceptance here.", "accepted": 1}
{"paper_id": "iclr_2022_f9MHpAGUyMn", "review_text": "a new method for dynamic token normalization in vits both within and across tokens is introduced in the paper. as noted by the reviewers, the proposed method is technically sound, with a clear and solid motivation. the main raised concerns included the lack of experiments using larger models, unclear reason for the accuracy gains, and lack of experiments on other tasks beyond classification, such as detection and segmentation. the authors response was strong, clarifying other questions and providing additional experiments, for example, showing the effectiveness of the method on object detection, and when applied to larger models or architectures that explicitly model local context. two reviewers recommend borderline rejection, but they did not participate in the discussion nor updated their reviews after the author response. the ac considers that their concerns were adequately addressed by the rebuttal, and agrees with the other two reviewers that the paper passes the acceptance bar of iclr. the authors should carefully proofread the paper for the final version.", "accepted": null}
{"paper_id": "iclr_2022_4-D6CZkRXxI", "review_text": "this paper studies modelbased rl in the setting where the model can be misspecified. in this case, mle of model parameters is a not necessarily a good idea because the error in the model estimate compounds when the model is used for planning. the authors solve this problem by optimizing a novel objective, which takes the quality of the next state prediction into account. this paper studies an important problem and this was recognized by all reviewers. its initial reviews were positive and improved to 8, 8, 6, and 6 after the rebuttal. the rebuttal was comprehensive and exemplary. for instance, one concern of this paper was limited empirical evaluation. the authors added 5 new benchmarks and also included stabilizing improvements in their original baselines. i strongly support acceptance of this paper.", "accepted": 1}
{"paper_id": "iclr_2022_fR-EnKWL_Zb", "review_text": "the paper proposes an efficient attention variant inspired by quadtrees, for use in vision transformers. when applied to several vision tasks, the approach leads to better results andor less compute. the reviews are all positive about the paper, after taking into account the authors feedback one reviewer forgot to update their official rating, apparently. they point out that the idea is reasonable and the empirical evaluation is thorough and convincing, with good gains on several tasks and datasets. overall, i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_pjqqxepwoMy", "review_text": "this paper studies the problem of using oracle information thats only available during training in rl. the key contributions are 1 a variational bayesian approach that models the oracle observation as latent variables; and 2 a mahjong environment for benchmarking rl with oracle guiding. the novelty of the proposed approach is limited, but reviewers find the problem intriguing and agreed that its a reasonable application of bayesian approach to rl with latent oracle information. in addition, the mahjong environment could benefit the community and spur new work in this direction. therefore, i recommend this paper to be accepted as a poster.", "accepted": 1}
{"paper_id": "iclr_2022_USC0-nvGPK", "review_text": "this paper proposes a new approach to graphbased active learning, using the query whether the predictions made by the current model are correct or not. although the theoretical underpinnings of the proposed approach are a bit weak, the problem formulation that is newly proposed in this paper makes sense from a practical point of view, and the paper makes a simple and interesting proposal that would be worth sharing with the community.", "accepted": 1}
{"paper_id": "iclr_2022_OUz_9TiTv9j", "review_text": "this paper presents a method, called zest, to measure the similarity between two supervised machine learning models based on their model explanations computed by the lime feature attribution method. the technical novelty and significant are high, and results are strong. reviewers had clarifying questions regarding experiments and suggestions to add experiments, which involve additional domains text and audio and different families of classifiers, and more contexts based on prior literatures. these were adequately addressed by the authors. overall, this paper deserves borderline acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_JPkQwEdYn8", "review_text": "this work receives mostly positive rates. most reviewers agree that the use of bayesian attention to neural processes is novel, and its interpretation is interesting. since the reviewer tbta requests a substantial revision of the submission and fortunately authors feedback is thoroughly satisfactory, we highly recommend the authors to prepare for a significantly improved cameraready version that clarifies most of reviewers concerns.", "accepted": 1}
{"paper_id": "iclr_2022_fILj7WpI-g", "review_text": "this paper proposes perceiver io, a general neural architecture that handles general purpose inputs and outputs. it operates directly in the raw input domains, and thus does away with modality specific architecture components. the paper contains extensive experiments showing the capabilities of this architecture in different domains. the paper received very positive reviews from all reviewers. some concerns included a need for additional details such as a missing task from glue, flops comparisons to past works, nomenclature for the versions of perceiver io, etc. these concerns were well addressed by the authors. others concerns by reviewers were the lack of experiments in a multi task setting. however, it was acknowledged by the authors and reviewers that this is an open area of research and is a good fit for future work. given this high quality submission, strong reviews and a very positive discussion amongst authors and reviewers, i recommend accepting this paper.", "accepted": 1}
{"paper_id": "iclr_2022_g1SzIRLQXMM", "review_text": "this paper experiments with what is required for a deep neural network to be similar to the visual activity in the ventral stream as judged by the brainscore benchmark. the authors have several interesting contributions, such as showing that a small number of supervised updates are required to predict most of the variance in the brain activity, or that models with randomized synaptic weights can also predict a significant portion of the variance. these different points serve to better connect deep learning to important questions in neuroscience and the presence of the paper at iclr would create good questions. the discussion between authors and reviewers resulted in a unanimous vote for acceptance, and the authors already made clarifications to the paper. i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_Zf4ZdI4OQPV", "review_text": "the paper shows that the transfer attack is query efficient and the success rate can be kept high with the zerothorder scorebased attack as a backup. experiments show stateoftheart results. pros  simple method based on a simple idea.  state of the art performance. cons  proposal is a straightforward combination of two methods, and therefore technical contribution is marginal.  the threat model is easy surrogate can be trained on the same datasets and use the same loss function and questionable. most of the experimental evidence shows that the research for this threat model is almost saturated and the problem seems almost solved. this paper got a borderline score with reviewers concerns above. i agree with the authors that the simplest method is best among those performing similarly, but the threat setting considered might be not very realistic as the authors admitted. i see the proposed method a kind of egg of columbus in a negative sense. namely, the authors found a shortcut to win a game that was created and adopted by the community. perhaps this paper would give an impact on the small community and would make the community change the game. but to give an impact to a general audience, the authors should convince that there are some situations where the analyzed thread model is realistic and therefore the proposed method is really useful. or, the authors could adjust the thread model to be more realistic. serious discussion on the thread model would be a big plus to the marginal technical contributions. after discussion with sac, and pc, our conclusion is that this paper effectively tells the community that the benchmark they are using is too simple, which alone is worthwhile publishing because this may move the community forward even if the community is small.", "accepted": null}
{"paper_id": "iclr_2022_XJiajt89Omg", "review_text": "this paper proposes a new timevarying convolutional architecture stgnn for dynamic graphs. the reviewers were positive about the presentation and detailed theory, especially on the stability analysis. the shared criticism was on experimental validation synthetic datasets that the reviewers did not find appealing. the ac believes that while the lacking validation concerns are legit, there is a lack of sophisticated dynamic graph benchmarks in the community yet, so the authors did their best effort to test their method. we thus recommend to accept the paper.", "accepted": 1}
{"paper_id": "iclr_2022_TpJMvo0_pu-", "review_text": "in recent years, artificially trained rnns have been used for studying systems and behavioral neuroscience in terms of their learned representations, dynamics, computation, and the learning process itself. this paper contributes to further identify learning principles that may be revealed by curricula. the proposed approach for finding signatures of different loss functions is a novel and interesting idea which is very well fit for the neurooriented iclr audience and has potential impact in other fields.", "accepted": 1}
{"paper_id": "iclr_2022_k7-s5HSSPE5", "review_text": "the paper presents a domain adaptation approach based on the importance weighting for unsupervised crosslingual learning. the paper first analyzes factors that affect crosslingual transfer and finds that the crosslingual transfer performance is strongly correlated with feature representation alignments as well as the distributional shift in class priors between the source and the target. then the paper designs an approach based on the observations. pros  the paper is well written and the proposed approach is well motivated.  the analysis about which factors affect crosslingual transfer is interesting and provides some great insight. cons  as the reviewer pointed out, the experiments for verifying the proposed approach are relatively weak. overall, the paper presents nice insights to connect crosslingual transfer with domain adaptation. all reviewers lean to accept the paper and i also found the paper is in general interesting.", "accepted": null}
{"paper_id": "iclr_2022_EHaUTlm2eHg", "review_text": "this paper presents a new reinforcement learning algorithm for pomdps that specifically deals with the credit assignment problem. the proposed algorithm consists in using at each timestep t of a training trajectory the subsequent future trajectory that starts at time t1 as additional inputs to the policy and value networks. instead of using the trajectories directly, two rnns are used to encode the trajectories into latent two variables that are then given as inputs to the policy and value networks. a key novel contribution of this work is the use of zforcing to help the rnns learn the relevant information. since future trajectories are not available during testing, a prior network is trained to predict the latent variable given a state. during testing, the latent variable is sampled from the network. empirical experiments on simple simulated environments show that the proposed algorithm outperforms several baselines. key issues raised by the reviewers include the complexity of the proposed algorithm, the fact that several interesting results are in the appendix rather than the main paper, and the weakness of certain baselines. the authors responses helped clarify these issues, and additional experiments such as a comparison to a dqn with nstep value updates were performed and added to the paper. the reviews are updated accordingly. in summary, the paper contains several novel ideas in the context of learning in partially observable environments. it is not entirely clear similar effects of the proposed algorithm can be obtained by using simpler tricks, but the evidence provided by the authors supports the claim that the algorithm outperforms several sota techniques in the context of pomdps.", "accepted": 1}
{"paper_id": "iclr_2022_ibrUkC-pbis", "review_text": "the reviewers were split about this paper on one hand they would have liked to see more experiments on different problem settings on the other they appreciated the elegance of graph encoding methods and current results. after going through the paper and discussion i have voted to accept for the following reason the additional experiments and discussion posted during the rebuttal phase have addressed many of the main concerns of the reviewers i.e., training time, message passing figure, discussion on encoding and sat solvers. the only remaining one i see is the request for additional experiments which i dont think is grounds for rejection current results are comprehensive and an additional experiment i think would not alter the main conclusions. i urge the authors to take all of the reviewers changes into account if not already done so.", "accepted": 1}
{"paper_id": "iclr_2022_bCrdi4iVvv", "review_text": "this paper analyzes the extent to which parameterized layers within a cnn can be replaced by parameterfree layers, with specific focus on utilizing maxpooling as a building block. after the author response and discussion, all reviewers favor accepting the paper. the ac agrees that its empirical results open a potentially interesting discussion on network design.", "accepted": 1}
{"paper_id": "iclr_2022_PTRo58zPt3P", "review_text": "the paper presents an approach to predict relations between node pairs in heterogeneous graphs, with application to recommendation and knowledgebase completion. the authors approach is to compute similarities between subgraphs that are neighborhoods of nodes where the relation holds or not to score a relation. the authors use graph neural networks to scores these subgraphs. the type of subgraphs that are considered are pairs of nodes, 3 and 4 cyles to make inference and training tractable. the paper lies in the stream of work that combines logical reasoning and neural network, even though in that particular instance it mostly combines graph mining techniques and neural networks. the reviewers unanimously liked the presentation of the paper and the high empirical performance. the rebuttal addressed most of the remaining concerns.", "accepted": 1}
{"paper_id": "iclr_2022_e2Lle5cij9D", "review_text": "the authors provide a convexification for the gan training via integral probability metrics induced by twolayer neural networks. the exposition relies on the convexification tools recently proposed by the pilanci et al., and provides interesting insights to follow up in the future.", "accepted": 1}
{"paper_id": "iclr_2022_baUQQPwQiAg", "review_text": "to address the problem of unauthorized use of data, methods are proposed to make data unlearnable for deep learning models by adding a type of errorminimizing noise. based on th fact that the conferred unlearnability is found fragile to adversarial training, the authors design new methods to generate robust unlearnable examples that are protected from adversarial training. in addition, considering the vulnerability of errorminimizing noise in adversarial training, robust errorminimizing noise is then introduced to reduce the adversarial training loss. the authors have tried to respond to reviewers comments along with adding more experiments. overall, this manuscript finally gets three positive reviews and one negative review, where the possible vulnerability or robustness of errorminimizing noise against simple image processing operations was not verified. in comparison with other manuscripts im handling that got consistent positive comments, this manuscript is still recommended to be accepted poster with a further study of robustness under simple image transformations in the final version.", "accepted": 1}
{"paper_id": "iclr_2022_ZSKRQMvttc", "review_text": "there is consensus in the reviews that this paper convincingly demonstrates strong acceleration of policy learning using differentiable simulation in tasks involving contactrich dynamics. the authors are encouraged to explore where the smoothness assumptions made in the simulator actually transfer to real robots. the paper may be further strengthened through more complex benchmarks involving contact rich manipulation.", "accepted": 1}
{"paper_id": "iclr_2022_iEvAf8i6JjO", "review_text": "the submission addresses the problem of whether or not to update weights for a previous task in continual learning. the approach is to specify a trust region based on task similarity and update weights only in the direction of the tasks that are similar enough to the current one. the paper was on the balance well received 34 reviewers recommended acceptance, 2 with scores of 8 and complemented for its simple but effective approach, and good discussion of related literature. the submission attracted a reasonable amount of engagement and discussion between reviewers and authors, which should be taken into account in the final version of the paper.", "accepted": 1}
{"paper_id": "iclr_2022_EBn0uInJZWh", "review_text": "the authors argue that offline metarl methods do not learn from poor data well, by demonstrating that combo a singletask offline rl method outperforms focal an offline metarl method only when the training data is good. they use this to motivate a method called merbo, which involves 1 learning a metadynamics model with proximal metarl and 2 updating a policy with real and synthetic data using a method called rac, which is equivalent to combo but with an added kl regularizer against the behavior and metapolicy, and 3 updating the metapolicy used by rac. the authors prove that under certain assumptions, the resulting policy will outperform both the metapolicy and the behavior policies. the authors compare the method to existing offline metarl methods and demonstrate that 1 the use of a dynamics model is important, 2 the addition of the behavior cloning regularizer is important, and 3 the use of the dynamics model is important to perform well on heldout offline rl tasks. this paper studies an interesting question of offline meta rl and presents promising experimental results. the ablations do a good job of demonstrating that each component of the method contributes to the success of the overall method. one concern with the paper is that the motivation for the method is rather unclear. this lack of clarity is due in part because the paper does not make specific, falsifiable claims. for example, one question posed is, why does the proximal metarl method in eq. 5 perform poorly in offline metarl, even with conservative policy evaluation? the paper continues to say that, a poor metapolicy may have negative impact on the performance and that, following the metapolicy may lead to worse performance. it is always possible that something may lead to worse performance, and this does not directly motivate the conclusion that, it is necessary to balance the tradeoff between exploring with the metapolicy and exploiting the offline dataset, in order to guarantee the performance improvement of new offline tasks. it would be great for the authors to provide direct evidence that this tradeoff is necessary. similarly, the results in figure 1 do not justify the claim that, clearly, existing offline metarl fails to generalize equally well over datasets with varied quality. figure 1 only looks at one specific offline metarl algorithm and on one environment. making such a general claim would require much more extensive evidence, and i do not see how, even if the claim were true, that would directly motivate the need to develop a method that will strike the right balance between exploring with the metapolicy and exploiting the offline dataset. in short, the paper does not provide evidence that striking this balance is the main issue that must be addressed. another concern with the paper is that it is a relatively complex combination of existing components combo, proximal meta rl, behavior cloning regularization, and so the onus is on the paper to demonstrate exceedingly good results to justify the complexity of the method. although the results are positive, the method only mildly improves over focal, and the paper would be strengthened by comparing to macaw and borel, as those methods have been shown to perform well in these environments. another concern is that it is unclear how alpha is tuned or can be tuned in practice. the authors state that, it is worth noting that different tasks can have different values of \u03b1 to capture the heterogeneity of dataset qualities across tasks but in practice, choosing a separate \u03b1 for each task seems undesirable. if \u03b1 was tuned for this method but no hyperparameters were tuned for baselines, this would also be concerning as this would bias the results in favor of merpo. there are some unjustified, or at least confusing claims, such as because tasks are trained on offline datasets, value overestimation fujimoto et al., 2019 inevitably occurs in offline metarl overestimation, as far as i know, is only an issue with valuebased methods. we study a more general offline metarl problem. i do not see how the problem statement in this paper is more general than the offline metarl problem present in past papers. learnt dynamics models not only serve as a natural remedy for task structure inference in offline metarl, but also facilitate better exploration of outofdistribution stateactions by generating synthetic rollouts evidence that learnt dynamics model help specifically because they facility better exploration is not provided. our results also provide a guidance for the algorithm design in terms of how to appropriately select the weights in the interpolation i do not see how the theoretical results can practically guide appropriate choosing alpha. in the end, it seems like alpha still had to be chosen empirically. lastly, i have a clarification question can the method be applied to new, nonoffline metarl tasks? one limitation of the approach is that rac seems to depend on having a behavior policy to regularize against, making it impossible to use the resulting policy for online metarl. the paper does not provide compelling evidence that their method addressed a critical problem, and the experiments make it difficult to know if the method proposed really outperforms current metaoffline rl method since details on tuning and important comparisons were not included. moreover, the overall method is rather complex and potentially limited to the pureoffline rl unless i am mistaken evaluation scenario, making the practicality of the method questionable. edit based on the rebuttal, ive increased my score up to a 6. this paper proposes a modelbased metarl approach to offline learning over a distribution of mdps offline metarl. the proposed method is inspired by combo and in fact is a direct extension of it to the multitask setting. the extension involves metalearning a modelinitialisation or prior that can be rapidly adapted to new offline rl tasks. they further add a proximal rl policyimprovement operator, where the prior policy is metalearned over the task distribution. they establish theoretical guarantees for their method and conduct a careful empirical investigation to motivate its design, while establishing that it provides additional benefits compared to relevant baselines. this paper is motivated by what the authors refer to as an inherent tradeoff between exploiting the data in the offline dataset, and exploring outofdistribution states by means of a model. empirically, they motivate this tradeoff by noting that the quality of the dataset has a large impact on the efficacy of offline metarl approaches, and that singletask approaches to offline rl can be superior if dataquality is high. to allow outofdistribution exploration, they metalearn an initialisation for a statetransition model via standard supervised metalearning approaches. this initialisation is then used to rapidly adapt the model to a given dataset. the main part of the paper revolves around how to metalearn a policy that can act as a regularizer in a policyimprovement step. they demonstrate empirically that if taskadaptation is regularised only towards the metalearned policy, then performance can be surprisingly poor in the even that the metapolicy does not correspond to useful behaviour on the new task. instead, the authors propose to regularize policyimprovement updates both towards the metalearned policy and towards the behaviour policy implicitly defined in the offline dataset. while the authors make a good case for their design choices, my main issue with this paper is that it complicates the presentation unnecessarily, which obfuscates connections to prior works. as far as i can tell, the latter regularizer towards the behaviour policy corresponds to combo, but this is not made clear until several pages later in the experimental section. hence, the gist of this paper is to extend combo to the multitask setting by metalearning, while also metalearning a prior policy for proximal policy updates. this could be made much clearer, which would not only help place the contribution in proper context but also make the paper much more readable. a main strength of this paper is the careful motivation of each component of the proposed method. the authors demonstrate theoretically that both these regularisers are important for policy improvement, clearly motivating the need for a metalearned prior as well as regularisation towards the datasets behaviour policy. this observation holds also empirically, as the authors demonstrate that their method is at least as good as using only one of the two regularisers under various assumptions of dataquality. they also demonstrate that learning a model is critical for performance, and that their proposed method is better or on par with established offline metarl baselines. overall, while i think the presentation of the method can be made much simpler and clearer, i believe this paper presents interesting findings for offline rl and has a strong proposal for an offline metarl algorithm. i recommend acceptance of this paper. main strengths of the paper are  compelling motivation of algorithm  careful analysis of its design  competitive performance main weaknesses are  overly complicated presentation of the algorithm post rebuttal ive read other reviews and the authors rebuttal and maintain my recommendation. targeting offline metareinforcement learning, the work proposes a modelbased method called merpo, with conservative value evaluation and individual policy improvement method with the tradeoff between the metapolicy and the behavior policy influence. the main algorithm merpo includes an initialization step and a twoloops metalearning approach. the initialization step learns the model of the metamodel and dynamics for each task. with the fixed estimations of the models of tasks, the proposed merpo will alternatively update the policies for each task and the metapolicy. the main contribution claimed in this paper is that the proposed method is a more robust method with the design of a regularization term involving the behavior policy, which improves the metalearning policy when the behavior policy is actually better than the current metapolicy. theoretical guarantees are displayed and experiments are conducted to show the performance of merpo. advantage the writing of the paper is clear and easy to follow. the proposed merpo outperforms the stateofart baselines and is intuitively explained well. the experiments show most of the performance of the components in the proposed algorithms, modelbased vs modelfree, involving behavior policy or not, the influence of the alpha, etc. disadvantageproblems under concern 1. about the important regularization parameter alpha. 1 will an adaptive alpha_t be better? since the quality of the metapolicy pi_c will be different during the learning process. 2 do we need to use different alpha for each task when the behavior policy property of each task is different? this scenario seems didnt been discussed enough? 2. how to choose alpha without information on the performance of the behavior policy and metapolicy? do we need to try it out? 3. there are some undefined or unclear notations, such as what is rho_n in equation 5. the main contribution of this paper is well claimed and verified. with a small revision, the algorithm can achieve better performance on the examined tasks in this paper. this paper considers the challenge in offline metarl problems, particularly, the difficulty with balancing a learned policy between exploring ood stateactions by following the metapolicy and exploiting the offline dataset. to solve the problem, it starts with proximal metarl approach in the online setting, points out the problem of a degraded metapolicy in the offline setting due to lack of feedback from online interactions and proposes to regularize the task policy with additional penalty from deviating from the behaviour policy. it also integrates the modelbased offline rl method for single tasks combo and offline metalearning for task specific dynamics into its final form merpo. this paper provides theoretical analysis on its advantage over behaviour policy and the meta policy. extensive experiments with ablation show the empirical performance meets its expected behaviour. this paper proposes a new modelbased offline metarl algorithm under the actorcritic approach. it is a nice addition to existing algorithms for the study of metarl problem. the difficulty of balancing the exploration with metapolicy and exploitation with offline dataset is an interesting observation, and the proposed solution is a reasonable remedy. the main idea and the corresponding algorithm are well presented. the paper starts with a clear motivation in the introduction. after explaining the background, it points out the limitation of applying online metarl approach to the offline setting in section 4.2.1, and proposes its solution to introduce another regularisation around behaviour policy. the full algorithm include many components, metalearning for dynamics, model based policy optimisation, conservative qlearning, regularisation with behaviour and metapolicy. i am glad that the authors explain each component clearly and how they interact with each other. the solution to add a second regularisation to taskspecific policy seems simple and intuitive, but it is nice to show the proposed method does improve the performance both theoretically and empirically. the main theory results look reasonable but i havent checked the detailed derivation in the appendix. the experiments results are convincing and the ablation study is very helpful to show the contribution of each component. i have a few questions about the algorithm design  cql already encourages the q function, and consequently the learned policy, to stay close to the behaviour policy. why do you think its not enough and we need another explicit regularisation to pull the policy close to the behaviour policy?  instead of regularising taskpolicy to behaviour policy in eq 7, we can also consider regularising the metapolicy to behaviour policy in order to prevent the quality of the metapolicy to decrease. could the authors comment on this option?  selecting a proper alpha value is the key to the proposed algorithm. unfortunately the range provided by theorem 1 is not computable and it seems hard to find an appropriate value according to the available offline dataset of each new task in practice. while the authors show 0.4 is a good value for all the experiments, i doubt if we can trust it in general. hoping a constant value of alpha to work for all is counterintuitive because the whole paper is arguing that we should balance the two regularisation according to the quality of the dataset and the metapolicy. so we should adjust the interpolation coefficient accordingly. the paper has a good motivation and provides a reasonable solution to the problem in offline metarl. theoretical and empirical results support the advantage of the proposed algorithm over recent baseline algorithms. summary this paper proposes a novel offline modelbased metarl approach called merpo. merpo combines conservative value iteration with proximal rl policy iteration. the proposed method is novel despite having some similarities to approaches like combo. the paper compares against it in the experiments. the paper provides both empirical and theoretical justification for the proposed approach. final thoughts overall, i think the authors did a pretty good job at addressing the reviewers concerns. overall, i think this is an interesting contribution to the iclr community. the reviewers were all positive about this paper. for the cameraready version of the paper, i would recommend the authors to go over the reviewers concerns again and make sure that those concerns are addressed in the paper too as they did in the rebuttal. some captions are pretty short; for example, see the captions of figure 6 and figure 7. i would recommend the authors add more description to the captions in the cameraready version of this paper.", "accepted": 1}
{"paper_id": "iclr_2022_1wVvweK3oIb", "review_text": "this is a borderline paper. the most enthusiastic reviewer does not have much confidence in the score. the other reviewers think the paper has some value after the rebuttal, but also feel there is little technical novelty. the proposed applications of the approach are interesting. after reading the reviews, rebuttal, and the paper, i agree that there is little technical novelty. the idea of adding nodelabel noise to a gnn to improve gnn expressiveness dates back to murphy et al., 2019 and has been also explored by dasoulas et al., 2019, vignac et al., 2020, loukas, 2020 among others one of which is suggested by a reviewer this literature is entirely missing from the paper. the paper has some novelty in proposing a regularization method for tackling the nodelevel noise by augmenting the loss function with a denoising term. the oversmoothing justification is not properly investigated whether the proposed solution really solves the issue in practice. if there is space in the borderline decision boundary, this paper could be a worthwhile inclusion. dasoulas, g., santos, l.d., scaman, k. and virmaux, a., 2019. coloring graph neural networks for node disambiguation. arxiv preprint arxiv1912.06058. loukas, a., 2020. how hard is to distinguish graphs with graph neural networks?. arxiv preprint arxiv2005.06649. vignac, c., loukas, a. and frossard, p., 2020. building powerful and equivariant graph neural networks with structural messagepassing. arxiv preprint arxiv2006.15107. murphy, r., srinivasan, b., rao, v. and ribeiro, b., 2019, may. relational pooling for graph representations. in international conference on machine learning pp. 46634673. pmlr.", "accepted": 1}
{"paper_id": "iclr_2022_AIgn9uwfcD1", "review_text": "this paper is proposed to address neural network pruning at initialization with the help of metagradients considering the highorder relations between loss and optimization of trainable subnetwork. the paper is well organized and written with the clear logic. the discussions of related works, as well as their limitations, are comprehensive. to verify the proposed method, the authors have tested it on various benchmarks with different settings. overall, the metalearning idea for model pruning is relatively new, which may bring more inspirations to the community.", "accepted": 1}
{"paper_id": "iclr_2022_9Vrb9D0WI4", "review_text": "this paper studies constructing text2text transformer models that are good at zeroshot task generalization via multitask learning over a diverse set of nlp tasks. one main contribution of the work is to create prompt templates for various nlp tasks that are of different task formats such that all tasks can be framed into text2text learning format and that is natural to the pretrained t6 model. the paper conducts extensive experiments to demonstrate the promising zeroshot generalization ability of such multitask learner. strength  important problem setup that has broad applications  extensive experiments to validate the claims  useful resources are developed for the problem weakness  good to study the effect of using different combination of training tasks on the downstream zeroshot generalization, which can shed some light on the usefulness of upstream tasks  justification of true zero shot learning capability would require further experiments on analyzing the data overlap between mtl datasets and also t5 pertaining task data and the unseen task data.  some more discussion on the task split and categorization will be helpful.", "accepted": 1}
{"paper_id": "iclr_2022_CpTuR2ECuW", "review_text": "the paper addresses coordination improvement in the marl setting by learning intristic rewards that motivate the exploration and coordination. the paper is theoretically founded and the empirical evaluations back up the claims. during the rebuttal the carried out an impressive amount of work. they provided several additional studies and substantially improved the presentation, addressing all of the reviewers requests. although not all the reviewers responded to the authors, the authors response was taken into the account when recommending the decision. minor  the authors should comment on the learning intristic rewards with evolution faust et al, 2019 httpsarxiv.orgabs1905.07628", "accepted": 1}
{"paper_id": "iclr_2022_1zwleytEpYx", "review_text": "the paper provides theoretical bounds for imitation learning with rewards algorithm from wang et al. 2019. the boundsproofs are highly novel and a very interesting contribution to the community, even though they are a lot more conservative than what is observed in practice. all reviewers agree on this point. it is laudable that the authors also additionally provide an experimental evaluation. after the revision and the discussion, quite a few of the reviewers are still not 100 convinced about them, on the one hand as they would have liked to see more tasks, and on the other hand due to concerns about the reward relaxation i.e., doesnt match the assumptions in the theorems any longer which is required for experiments on standard benchmarks. in the final answer the authors provide evidence that there is no big discrepancy, which is good enough given that there dont seem to be any alternatives to get around this issue, except removing the experimental section altogether, which would be undesirable. please clearly point out those limitations of the experiments in the paper and also incorporate this evidence.", "accepted": null}
{"paper_id": "iclr_2022_Ve0Wth3ptT_", "review_text": "this paper proposes a decompositionbased explanation method for graph neural networks. the motivation of this paper is that existing works based on approximation and perturbation suffer from various drawbacks. to address the challenges of existing works, the authors directly decompose the influence of node groups in the forward pass. the decomposition rules are designed for gcn and gat. further, to efficiently select subgraph groups from all possible combinations, the authors propose a greedy approach to search for maximally influential node sets. experiments on synthetic and realworld datasets verify the improvements over existing works. during their initial responses, reviewers suggested that the authors experiment with more baselines and also clarify some of the technical details. the authors revised their manuscript to address several of these comments. so, i am tentatively assigning an accept to this paper.", "accepted": 1}
{"paper_id": "iclr_2022_pFyXqxChZc", "review_text": "this paper proposes a theoretically sound and practically effective method to compress quantized gradients and reduce communication in distributed optimization. the method is interesting and worth publication.", "accepted": 1}
{"paper_id": "iclr_2022_kG0AtPi6JI1", "review_text": "the problem setting studied in this paper is an extension of the problem setting of multidomain learning, where domain information is missing in training. this is an interesting and practical problem setting. however, regarding technical novelty, the contributions are relatively limited. specifically, first, the overall idea is an extension of an existing multidomain learning method rebuffi et al. 2017 by replacing the domain indicators with learnable gates. second, the idea of introducing learnable gates is borrowed from some previous works. third, the sparse activation is also based on an existing work of sparsemax. though the authors claim sparsemax or sparse activation was only used in the nlp domain, this does not increase the technical novelty by applying sparsemax to the cv domain. to be fair, the combination of the aforementioned techniques to solve the socalled latent domain learning problem looks reasonable, while the technical contribution is not significant. overall, i feel slightly positive about this work and recommend a weak acceptance it could be considered for publication only if there is space.", "accepted": 0}
{"paper_id": "iclr_2022_Vjki79-619-", "review_text": "the paper presents interesting new results for pruning random convolutional networks to approximate a target function. it follows a recent line of work in the topic of pruning by learning. the results are novel, and the techniques interesting. there are some technical issues that are easy to fix within the camera ready timeline see comments of reviewers below. i would also suggest refining the title of the paper the lottery ticket hypothesis has an algorithmic component too, which clearly is not covered by existence results.", "accepted": 1}
{"paper_id": "iclr_2022_mfwdY3U_9ea", "review_text": "this paper introduces a novel approach for out of distribution detection that generates scores from a trained dnn model by using the fisherrao distance between the feature distributions of a given input sample at the logit layer and the lower layers of the model and the corresponding mean feature distributions over the training data. the use of fisherrao distance is novel in the context of ood, and the empirical evaluations are extensive. the main concerns of the reviewers were the limitations of the gaussianity assumption used in computing the fisherrao distance and the use of the sum of the fisherrao distances to the classconditional distributions of the target classes rather than the minimum distance. these concerns were addressed satisfactorily in a revision. in terms of technical novelty, experimental evaluation and novelty, the paper is above the bar of acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_AjGC97Aofee", "review_text": "this paper receives positive reviews. the authors provide additional results and justifications during the rebuttal phase. all reviewers find this paper interesting and the contributions are sufficient for this conference. the area chair agrees with the reviewers and recommends it be accepted for presentation.", "accepted": 1}
{"paper_id": "iclr_2022_rS9-7AuPKWK", "review_text": "the main contribution is a way of analyzing the generalization error of neural nets by breaking it down into bias and variance components, and using separate principles to analyze each of the two components. the submission first proves rigorous generalization bounds for overparameterized linear regression motivated in a general sense by the ntk; there are settings where this improves upon existing bounds. it extends the case to a matrix recovery model, showing that its not limited to the linear regime. finally, experimental results show that the risk decomposition holds empirically for neural nets. the numerical scores would place this paper slightly below the cutoff. the reviewers feel that the paper is well written and have not identified anything that looks like a critical flaw. they have a variety of concerns, mostly centered around whether the results apply to practical situations. specifically, theyre worried about 1 the theory not applying directly to neural nets, 2 the highnoise setting being less relevant for modern deep learning, and 3 whether theres a realistic situation where it improves over past bounds. regarding 1, the theory covers not only the linear regime, but also the nonlinear matrix recovery regime; combined with the empirical results, this seems pretty solid by the standards of a dl theory paper. regarding 2, even though the most common benchmarks indeed have low label noise, the highnoise regime still seems worth understanding after all, wed like our nets to work in domains like medicine. i havent dug deeply enough to properly evaluate 3, but the author response seems believable to me. overall, the paper strikes me as creative and wellexecuted. regardless of whether the theory is easily extendable to neural nets, this seems like an interesting paper that can be built on in future work. i recommend acceptance.", "accepted": null}
{"paper_id": "iclr_2022_edONMAnhLu-", "review_text": "the paper proposes an interesting and wellmotivated improvement of sharpness aware minimization. overall the ac and reviewers are satisfied by the author feedback in improving the solidity and rigor of the theoretical results. the points made by the authors in response to the reviewers initial concerns are essential, especially those regarding interpretation of corollary 5.2.1, making the proofs rigorous, and fixing the potential for crude convergence bounds. it is therefore critical that the authors incorporate them into their manuscript.", "accepted": 1}
{"paper_id": "iclr_2022_WH6u2SvlLp4", "review_text": "the proposed method for set representation learning with an application to mete learning is wellmotivated and reasonable. reviewers original concerns about novelty and technical presentation have been well explained and addressed in the revision. if some theoretical analysis can be provided regarding the proposed method, it would make this work stronger. in summary, a positive recommendation is given here.", "accepted": 1}
{"paper_id": "iclr_2022_Dup_dDqkZC5", "review_text": "this paper studies the problem of motion prediction for multiple agents in a scene using transformerbased vae like architecture. the paper received mixed reviews initially which generally tended towards borderline acceptance. all reviews appreciated extensive experiments but had some clarifications and requests for ablations. the authors provided a strong rebuttal that addressed many of the reviewers concerns. the paper was discussed and all the reviewers updated their reviews in the postrebuttal phase. reviewers unanimously agree that the paper should be accepted. ac agrees with the reviewers and suggests strong acceptance. the authors are urged to incorporate reviewers comments in the cameraready.", "accepted": 1}
{"paper_id": "iclr_2022_WLEx3Jo4QaB", "review_text": "this paper addresses the scale issue in graph neural networks by proposing a condensation approach that produces a small synthetic graph from a large original graph such that gnns trained on both graphs have comparable performance. reviewer ctj2 had concerns with novelty they claimed the proposed method was close to gradient matching. however, they admitted that the graph setting was new. they suggested some clarity and experimental improvements. reviewer r5cv made a similar comment w.r.t. the similarity to gradient matching. though overall they were more positive than r5cv and thought the idea was interesting and results were compelling. reviewer xqrk like the others, argued that the paper lacked technical innovation in terms of technical contribution. they pressed for a complexity or runtime analysis. reviewer pegb found the idea and problem intriguing though felt the solution fell short. they offered many suggestions for improving the quality of the experiments and the analysis. in the discussion period, reviewer pegb raised their score, thanking the authors for answering their questions. they felt that the additional experiment for nas was relevant and cleared up a key doubt. reviewer ctj2 updated their score as well but stated it was critical that the author release the code for reproducing the new experimental results. i think that with most reviewers now on the accept side of the fence, i am more inclined to recommend acceptance because i do not see any critical flaws. i think that ctj2s request for code is reasonable and strongly suggest that the authors do so.", "accepted": null}
{"paper_id": "iclr_2022_qsZoGvFiJn1", "review_text": "the paper proposes a framework for object detection on lidar scans, with query of scene feature extracted offline from previous traversals. overall there is good agreement among reviewers, with three recommending accepting the paper and one marginally accepting it  to me the authors satisfactorily addressed most aspect raised in reviewing.", "accepted": 1}
{"paper_id": "iclr_2022_DTkEfj0Ygb8", "review_text": "this paper addresses an important issue of automl systems, specifically their ability to cold start on a new problem. some of the reviewers initially had concerns about the experimental validation and the theoretical foundations of the method, but during the discussion phase the authors addressed concerns extremely well. the authors already included most of the feedback of reviewers, further strengthening the paper.", "accepted": 1}
{"paper_id": "iclr_2022_DfUjyyRW90", "review_text": "the reviewers agree that the proposed method to create a more robust representation of a task for modelbasedrl is interesting and has significant merits. after some revision, more critical reviewers improved their ratings of the paper, such that there is unanimous agreement that the paper can be accepted to iclr.", "accepted": 1}
{"paper_id": "iclr_2022_HndgQudNb91", "review_text": "overall, this paper receives positive reviews. the reviewers find the technical novelty and contributions are significant enough for acceptance at this conference. the authors rebuttal helps address some issues. the area chair agrees with the reviewers and recommend it be accepted at this conference.", "accepted": 1}
{"paper_id": "iclr_2022_0DLwqQLmqV", "review_text": "this paper proposes a novel benchmark for neural architecture search methods, which consists of 25 different combinations of search spaces and datasets. the main motivation is that existing nas benchmarks, such as nasbench201, consider very small search space and few datasets, such that conclusions drawn with them do not generalize to unseen settings with different search spaces and datasets. the authors first describe the 25 different combinations of the search space and tasks for the given benchmark, and then conduct an extensive empirical study of existing nas methods and performance predictors with the proposed benchmark, to show that architectures and hyperparameters found with the popular benchmarks do not generalize to other settings, which is consistent with their assumption.  all reviewers were initially positive about the paper, and remained positive throughout the discussion period. the reviewers found the paper wellmotivated, and the proposed benchmark useful, as they agree with the need of introducing a single, unified framework that can validate a nas method under diverse settings, since existing benchmarks only consider specific datasets and search spaces. however, the reviewers were also concerned with the weak technical novelty reviewer 2xvd, and that the work lacks deeper insights that could guide the community towards better methods reviewer gku7. i also agree with the authors and the reviewers on the necessity of having a unified benchmark that incorporates all different settings considered in the previous benchmarks, and find the extensive empirical study of existing nas methods useful. however, i find the work as rather technically weak as mentioned by r2xvd, since the authors spent too much time describing and showing the limitations of existing benchmark methods, while what is more important for benchmarks, is to justify how the proposed benchmark can evaluate the performance of different methods in a fair manner, while being representative of the practical settings. in short, the authors need to justify their design choices. yet, the 25 settings proposed in the paper seem to have been arbitrarily chosen, and it is not clear if having a good performance on this benchmark is indeed a fair evaluation, or wellreflects how the nas method will perform in practice. the proposed benchmark also does not really consider a novel search space or setting that have been overlooked in the past either, and does not provide much insights on the problem, as mentioned by reviewer gku7. thus, although i recommend an acceptance for its practical value acknowledged by the reviewers, the authors need to put a considerable amount of effort in revising the paper, and if this were a journal submission, the paper may need to undergo a major revision. most importantly, as described, the authors should justify their design choices as well as whether evaluating a model on the benchmark yields fair and representative results, focusing more on describing the proposed benchmark itself.", "accepted": 1}
{"paper_id": "iclr_2022_7IWGzQ6gZ1D", "review_text": "this work extends the successor feature framework by focusing on the question of which policies should be learned in order to get the best generalization performance. the reviewers all agree that the question being addressed is interesting and important. one concern raised by two of the reviewers is that the work is rather incremental, providing a relatively small extension from the work of barreto et al. nevertheless, the authors have provided a convincing rebuttal, resulting in an increase in score of two of the reviewers. hence, i recommend acceptance. i do want to ask the authors to carefully read the postrebuttal point mentioned by reviewer 3qck about clarifying the unsupervised rl setting.", "accepted": null}
{"paper_id": "iclr_2022_MkTPtnjeYTV", "review_text": "this paper studies the memorization power of relu neural networks and obtains sharp bounds in terms of parameters. the writing is very clear and the results very interesting.", "accepted": 1}
{"paper_id": "iclr_2022_HbtFCX2PLq0", "review_text": "the paper introduces a procedure to control the churn i.e. differences in the predictive model due o retraining using distillation. this is a strong paper, with novel technique which is clearly presented, and is backed by sound theory. the experimental results were also deemed extremely convincing by reviewers tj4g and pzbb. reviewer nqfu raised a question about the similarities between churn reduction and domain adaptation. the authors have addressed this by pointing out similarities to their work but also noting that, in the settings mentioned by the reviewer, alternative approaches such as completely retraining the model might be more appropriate. this part of the rebuttal is convincing. reviewer tj4g has pointed out several points of improvement, to which the authors have responded adequately. all in all, this paper is ready for and deserving of acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_cZAi1yWpiXQ", "review_text": "the paper shows a causal perspective to the adversarial robustness problem. based on a causal graph of the adversarial data creation process, which describes the perceived data as a function of content and style variables, the authors identified that the spurious correlation between style and label is the main reason for adversarial examples. based on this observation, they propose a method to learn models for which the conditional distribution of label given style and image does not vary much when attacked. experiments on mnist, cifar10, and cifar100 datasets show that the proposed method is better than two baselines, madry and trades. overall, the paper contains interesting ideas and tackles an important problem. due to some concerns regarding the clarity and motivation of the paper, we strongly recommend the authors take the reviewers comments to heart and incorporate their thoughts in preparing the cameraready version of their manuscript.", "accepted": 1}
{"paper_id": "iclr_2022_c87d0TS4yX", "review_text": "description of paper content the paper provides a framework to develop a family of algorithms that decompose rewards into linear combinations of several reward channels. the value functions per channel are estimated in a new space using an invertible function transformation, f. the framework encompasses several previously published algorithms, including log qlearning. conditions are provided for acceptable choices of f. convergence to the optimal q function in the tabular case is proven for a special learning update. summary of paper discussion all review scores were above the acceptance threshold. overall, the reviewers found the idea interesting, the theoretical results satisfying, and the writing and presentation clear. initial concern about the directedness of the experiments in showing the usefulness of this particular theoretical framework to explain performance improvements was allayed when some of the results in the paper e.g. reward density in atari skiing were reemphasized. generally, all reviewers felt that this was a nice, thorough contribution with the demerit that the framework lacked a killer application experimentally.", "accepted": null}
{"paper_id": "iclr_2022_HMJdXzbWKH", "review_text": "the paper analyzes a variant of the qlearning algorithm with two modifications online target learning otl, and reverse experience replay rep. otl is essentially the same as using the target network. rep is a new modification of er, which instead of randomly selecting samples from the buffer, replays them in the reverse order. most reviewers are positive about this paper, so i am going to recommend acceptance. there are, however, several concerns that have been raised by the reviewers. as the authors have not revised the paper during the discussion period, my acceptance recommendation is under the good faith expectation that the authors make a serious effort in improving their work based on the reviews. some of the concerns are  the intuition of why rep breaks the correlation is not clear enough. this has been brought up several times by the reviewers.  what are the technical differences in the analysis compared to previous work such as zou et al., 2019?  the kappa appearing in assumption 4, and showing up in the error bounds, can be dimension dependent. please clarify this and its effect on the results.  much of the paper is in the appendix. it helps if the authors can include more about the proof technique in the main body of the paper.  describe the relation between the error in the value function vs. the performance of its greedy policy.", "accepted": 1}
{"paper_id": "iclr_2022_tFgdrQbbaa", "review_text": "after carefully reading the reviews and rebuttal, i believe this work is of sufficient quality for acceptance. understanding continual learning from a theoretical stand point is a very important topic. i find that one of the main issue raised by reviewers was about the exact meaning of continual learning, and whether what the authors studied was more akin to sequential learning. while i dont mind the term sequential learning, and is quite descriptive of the work as well, i disagree that the considered setup is not continual learning.", "accepted": 1}
{"paper_id": "iclr_2022_DhzIU48OcZh", "review_text": "this paper introduces a prompting technique for eliciting factual knowledge from frozen pretained transformer lms. the key idea is to modify the embeddings produced by the embedding layer before they are passed to the first attention layer and the paper investigates several different design choices. the reviewers all agree that the paper tackles an important problem with interesting methods, that it is well written and has strong results. the main concerns, raised by reviewer jddf, were about clarifying the connections to the robust optimization literature and evaluating on ood relations. the former has been addressed in the revised version. while the latter point remains valid, i find that the paper in its current state has enough useful experiments and analysis to warrant publication. the authors have clarified most of the other points raised by the reviewers in their rebuttal.", "accepted": 1}
{"paper_id": "iclr_2022_O50443AsCP", "review_text": "reviewers are positive overall  the is a general consensus towards acceptance. reviewers viewed the simplicity, novelty, and effectiveness of the propose pretraining approach as strengths. further, reviewers praised the draft as very clearly written, and viewed experimental ablations as relatively indepth  e.g. two reviewers found the additional analysis of impact of data size to be valuable. a few concerns about additional ablations and claims were brought up, but all were adequately addressed in author response.", "accepted": 1}
{"paper_id": "iclr_2022_aYAA-XHKyk", "review_text": "this paper received a majority vote for acceptance from reviewers and me. i have read all the materials of this paper including manuscript, appendix, comments and response. based on collected information from all reviewers and my personal judgement, i can make the recommendation on this paper, acceptance. here are the comments that i summarized, which include my opinion and evidence. research motivation and problem this paper is well motivated by the agnostic of cpe assumption that the support of the positive data distribution cannot be contained in the support of the negative data distribution. to tackle this problem, the authors built an auxiliary probability distribution such that the support of the positive data distribution is never contained in the support of the negative data distribution. technical contribution the technical part is simple and clear. the regrouping idea is also easy to implement. the theoretical justification is a good complement of the proposed recpe algorithm. recpe does not affect its base if the assumption already holds the authors employed the synthetic datasets to verify this point. this is a plus. experimental results the authors demonstrated their recpe algorithm can be used as a booster on seven base pu classifiers. presentation the presentation has been much improved with the guidance of one reviewer. but i found two extra minor ones. 1 pu learning  pu learning at the beginning of the second paragraph on page 1. 2 two typos in 9 real word datasets. on page 7  9 realworld dataset., where the footnote should be placed after the period. layout 1 too many lines in table 1. it is suggested to remove the horizontal lines among the same dataset, 2 appendix should go after the main manuscript, rather than a separate file. no objection from reviewers was raised to again this recommendation.", "accepted": 1}
{"paper_id": "iclr_2022_8hWs60AZcWk", "review_text": "summary authors present an approach to improve the robustness of vision transformers by mapping standard tokens into discrete tokens that are invariant to small perturbations. method is applied to a variety of backbone architectures and evaluated on a range of out of distribution forms of imagenet test set. significant performance gains are measured across many of these tasks. pros  novel, simple, effective approach  general approach applicable across model variants, complimentary to other methods to improve robustness.  comprehensive study, evaluated on many imagenet robustness benchmarks  well written overall cons  biggest issue 3 reviewers point out concerns about validity of claims that vit architecture is more reliant on local patterns and less on global context. this seems mostly a semantic issue around conjectures about why the method works  it does not invalidate the value of the new approach or its solid results. authors have responded to reviewer concerns by changing wording in paper to relax the claims, specifying shape information rather than global information. they have also added experiments to measure shape bias, as defined in prior art, to backup these claims.  paper missing baselines of data augmentation strategies. authors have responded by including such comparative experiments.  paper is missing ablation studies on changing the type of codebook. authors have responded by including multiple variations of codebooks, and varying the codebook size. this paper was a close call based on the reviews. however, in ac opinion, the critiques have been adequately addressed by the authors. this is confirmed by adding an extra expert reviewer to the pool, who agreed with some earlier critiques, and was satisfied with the changes and additional experiments presented by the authors. ac recommendation is to accept.", "accepted": 1}
{"paper_id": "iclr_2022_3YqeuCVwy1d", "review_text": "this paper proposes to use anderson acceleration on minmax problems, provides some theoretical convergence rates and presents numerical results on toy bilinear problems and gans. after the discussion, the reviewers agreed that this paper makes a nice contribution to iclr. some concerns were originally expressed in terms of incrementality of the theoretical results with respect to previous work kcys, gbhu, but the authors have well clarified their contributions in the discussion, and have updated their manuscript accordingly. there were also initial concerns about the related work coverage, but this was also properly addressed in the rebuttal, with additional experimental comparisons as well as extended related work section, as well as an additional convergence result for convexnonconcave problems.", "accepted": 1}
{"paper_id": "iclr_2022_7gWSJrP3opB", "review_text": "this paper studies the dependency of sgd convergence on order of examples. the main observation of the paper is if the averages of consecutive stochastic gradients converge faster to the full gradient, then the sgd with the corresponding sampling strategy will have a faster convergence rate. for different sampling strategies, sampling with replacement has slower convergence in stochastic gradient, where sampling without replacement can converge faster. the paper also proposes two new algorithms that can improve convergence rates in some interesting settings. the reviewers find the analysis clean and the new algorithms are interesting. there is some concern on the dependency on n or d for the faster rate, which should be discussed more clearly in the final version of the paper. overall this is a solid contribution to the example selection problem.", "accepted": 1}
{"paper_id": "iclr_2022_rJvY_5OzoI", "review_text": "in this paper, the authors investigate a multitask rl actorcritic technique, where a single actor is used while multiple critics are trained one per task, where each task corresponds to a different reward function. experiments on several environments demonstrate that this method works quite well in practice. all reviewers found the proposed approach sensible and effective, in spite of its simplicity. the main concerns were  lack of novelty although this is indeed not a particularly original idea, the specific instantiation in the actorcritic setup is novel and well motivated  some confusing  unconvincing experimental results after receiving this feedback, the authors were able to upload a new revision that addressed the main concerns  focusing on the multistyle aspect when this is essentially a multitask algorithm although i agree that framing it as a specific case of multitask learning would make sense and would probably make more appealing to multitask rl researchers, i do not consider this to be a major issue in spite of being a relatively straightforward paper, i believe it is good to have strong empirical evaluation of such basic techniques disseminated to the research community, and i thus recommend acceptance, in accordance with reviewers recommendations after the discussion period.", "accepted": 1}
{"paper_id": "iclr_2022_bYGSzbCM_i", "review_text": "this paper opens the area of adversarialattack research on streaming data e.g., realworld settings such as selfdriving cars and robotic visual tasks for a robot. for instance, online adversaries can focus their attack on a small subset of the streamedonline data, but still cause much damage to downstream models. this work highlights the need for stateful defense strategies. connections to online algorithms and the ksecretary problem are made, along with improvements to some onlinealgorithms work of albers and ladewig. overall, the attack model introduced is important, and the bridge to online algorithms would be useful for the iclr community. i also believe this topic lends diversity to the typical set of iclr papers.", "accepted": 1}
{"paper_id": "iclr_2022_T8wHz4rnuGL", "review_text": "the paper addresses the problem of inconsistent gradients in multitask learning, proposing ways to handle both their magnitude nd direction. gradient directions are aligned by introducing a rotation layer between the shared backbone and taskspecific branches. reviewers appreciated the technical approach, higlighting the novelty of the rotation layers in this context. the empirical evaluations are systematic fair and insightful, and the presentation is polished. reviewers unanimously supported accepting the paper.", "accepted": 1}
{"paper_id": "iclr_2022_RLtqs6pzj1-", "review_text": "summary the goal of this work is to reduce the costs of inference in ensembled models by ensembling sparse models. the paper also aims to reduce the costs of training these ensembles as well. the proposed techniques dst and edst each these goals, respectively.  discussion as noted by the reviewers, the paper is interesting and timely. the authors provided significant clarifications in the response that satisfied the reviewers concerns. there is still significant room to revise the remaining points and polish the text of the paper for the cameraready i highly recommend proofreading from an individual who is not an author on the paper; there are still typos in the revised edits  recommendation. i recommend accept, due to the strengths above and the reasonably scoped remaining work to do going into the cameraready.", "accepted": null}
{"paper_id": "iclr_2022_vEZyTBRPP6o", "review_text": "the paper makes a significant contribution in the rather sparse and challenging field of convergence analyses of actorcritic style algorithms, under the linear mdp structural assumption, showing that there is a natural bias towards being highentropy. as one of the reviewers points out, although it is unlikely that the strategy actually proposed is amenable to implementation, the paper nevertheless provides a clean and novel analysis of convergence of learning by eschewing the usual mixing time type assumptions often found in the theoreticallyoriented rl literature. based on this strength of the paper, i am glad to recommend its acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_pWBNOgdeURp", "review_text": "the paper used the koopman operator theory to explain and guide the dnn pruning. all the reviewers deemed that such a viewpoint is novel but at different levels. however, the paper still had some issues, including unclear technical details, vagueoverselling statements, being computation and memory expensive, etc. the paper finally got 4 marginally above threshold one being of low confidence, making it on the borderline. the ac read through the paper and agreed that the koopman operator theory brings new perspective to dnn pruning, with potential for other analysis of dnns. although the paper is imperfect and not strong, it does not have severe problems either and the issues pointed out by the reviewers could be easily fixed except the scalability issue, which can be left as future work. in order to encourage new ideas, the ac recommended acceptance.", "accepted": null}
{"paper_id": "iclr_2022_X0nrKAXu7g-", "review_text": "this paper extends randomized leastsquare value iteration rlsvi, which is a method for explorationexploitation tradeoff that is suitable for linear fa, to the deep rl setting. a key component is using hypermodels of dwaracherla et al. iclr, 2020 to generate the weights of last layer of the dnn. this generates a learnable randomness required in an rlsvilike procedure. the paper provides some theoretical results regarding hypermodels, and provides extensive experiments to show that their method is a competitive one in solving exploration problems. the majority of the reviewers are positive about this work. the concerns include the incremental nature of this work and the empirical results. in my opinion, the algorithmic contribution is reasonable, but somehow incremental. the theoretical results are minor, but acceptable. the empirical results are extensive, though they have some shortcomings. i explain the algorithmic and empirical contributions below algorithmic contribution similar formulation has been done by dwaracherla et al. but that work does not consider the rl setting, and instead focuses on the bandit setting. this work provides such an extension. a straightforward application of hypermodels does not work for drl, but some simple, yet crucial, tricks needs to be applied to make it work. the trick is to use hypermodel to generate the weights of the last layer, instead of all layers. although this is simple, the fact that it enables the method to work for drl paper is significant. empirical results the empirical results are quite extensive. there are two main issues with them though a many experiments on the atari suite are terminated after 20m samples. this is shorter than usual. b the experiments are only repeated for 3 runs seeds  except one, which used 5 runs. the authors answer for a is that they have a limited compute budget, and running for 200m samples would cost them about 20k. also they argue that 20m samples is enough to show the benefit in a better exploration method, as shown by some other papers. after some inquiries, it seems that this 20k value has the correct order to run the experiments on a cloud maybe within a factor of 2 or 3. given this prohibitive cost, i am willing to accept that 20m samples might be sufficient for proving the main points of this paper. i am not giving a large weight to this in my evaluation. the main concern for me, however, is having only 3 independent runs of the algorithms, especially given that the issue under study is the efficiency of explorationexploitation tradeoff, and that the proposed method has a lot of randomness builtin. this is the main weakness of the empirical results in my opinion, and not the 20m samples issue. for instance, figure 3 humannormalized score over 56 environments in atari 2600 suite does not have any confidence interval information. and figure 4 has some shaded areas around the curves, but it is not clear whether it is standard deviation, standard error, or some other quantification of uncertainty. even though this is a borderline paper, i recommend acceptance of this work under the expectation that the authors should improve their empirical studies. in particular, i recommend much larger number of independent runs seeds, maybe around 10 or so, with proper information about the uncertainty of the estimates. if running this for all games is prohibitive, showing the performance with more runs on a subset of the games is sufficient. also i encourage the authors to consider neurips 2021 paper deep reinforcement learning at the edge of the statistical precipice.", "accepted": 1}
{"paper_id": "iclr_2022_FlwzVjfMryn", "review_text": "multiobjective learning is an increasingly important topic. this paper presents a method for better finding parts of the pareto frontier through a new method to estimate the distance to the frontier and use this proxy to refine the state space partition. the reviewers found this paper interesting and compelling and generally well written. the reviewers also thought the work could be further improved by better clarifying in the text where the proposed approach might fail, and what properties of the domain are needed, and also to better situate this paper within the related work, potentially including additional experimental comparisons. the authors provided detailed responses to the proposed questions and the authors are encouraged to ensure that these suggestions and discussions are well represented in the revised version.", "accepted": 1}
{"paper_id": "iclr_2022_RhB1AdoFfGE", "review_text": "this paper received 4 quality reviews, with the final rating of 8 by 2 reviewers, and 6 by the other 2 reviewers. all reviews recognize the contributions of this work, especially its superior performance. the ac concurs with these contributions and recommends acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_KJggliHbs8", "review_text": "in this submission, the authors presented a framework giant for selfsupervised learning to improve lm by leveraging graph information. reviewers agree that the method is somewhat novel, the partial theoretical analysis is interesting, and the evaluations are strong. we thank the authors for doing an excellent job in rebuttal which cleared essentially all the questions reviewers initially raised.", "accepted": 1}
{"paper_id": "iclr_2022_w4cXZDDib1H", "review_text": "the paper introduces an object detection method that integrates vision and detection transformers through a novel reconfigured attention module ram. among other questions, the reviewers raised concerns about fair comparison with baselines, limited novelty of the ram module, completeness of experiments, and missing details. the rebuttal adequately addressed these concerns with clarifications and additional experiments. r1 remained unconvinced that a simple modification to yolos could not be devised to improve the speed similar to the proposed method, but stated heshe wouldnt argue strongly for rejection. while this is a legitimate concern, the ac agrees with r2 and r3 that the paper has enough merits to be accepted at iclr, as the results are strong and are likely to have significant practical value.", "accepted": 1}
{"paper_id": "iclr_2022_M6M8BEmd6dq", "review_text": "the paper presents a new framework of synthesizing differential private data using deep generative models. reviewers liked the significance of the problem. they raised some concerns which was appropriately addressed in the rebuttal. we hope the authors will take feedback into account and prepare a stronger camera ready version.", "accepted": 1}
{"paper_id": "iclr_2022_nKWjE4QF1hB", "review_text": "this paper modifies the alphazero algorithm to generate proof tree size heuristics and shows empirical improvements over standard search algorithms. this is an interesting distinction that might lead to algorithms with distinct play styles and a deeper understanding of the games that we apply our agents to. the two positive reviewers felt that it was a solid contribution, worthy of publication. there were some questions regarding the clarity of the writing that were addressed in the discussion phase. the two reviewers that gave lower scores felt that the paper did not do a sufficient job motivating the work and distinguishing itself from the literature. ultimately, i agree with the positive reviewers, and it is my opinion that the revised version is acceptable for publication.", "accepted": 1}
{"paper_id": "iclr_2022_gbe1zHyA73", "review_text": "the paper proposes a method for hybrid modelbasedml learning, where a model is decomposed into an interpretable parametric prior and a neural net residual. in this case, the prediction error minimization does no identify the parametric component, and an alternating optimization method is proposed to augments prediction error loss with componentspecific losses. empirical and theoretical results are obtained. initial questions of several reviewers were addressed.", "accepted": 1}
{"paper_id": "iclr_2022_RAW9tCdVxLj", "review_text": "the initial reviews for this paper were somewhat diverging, however the paper did not receive any significant negative criticism to push it towards below the acceptance threshold. the reviewers have found some minor issues about the paper. following the reviewer recommendations, the meta reviewer recommends acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_ahi2XSHpAUZ", "review_text": "this paper received 5 quality reviews. the rebuttal and discussion were effective and addressed many concerns from the reviewers, after which most reviewers increase their ratings of this paper. the final rating is 6 from 4 reviewers, and 8 from 1 reviewer. the ac concurs with the positive recommendation from the reviewers and recommends acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_qqdXHUGec9h", "review_text": "this paper considers the socalled partiallabel learning problem and proposes a class activation map that is better at making accurate predictions than the model itself on selecting the true label from candidate labels. the authors investigate the approach in experimental results on four benchmark image datasets. the reviewers appreciated the simplicity of the approach and its effectiveness in practice. the reviewers raised questions how to apply the approach to another weakly supervised learning problem such as semisupervised learning and whether the approach is an identificationbased strategy. the reviewers also raised several questions asking for more details. the authors submitted responses to the reviewers comments. after reading the response, updating the reviews, and discussion, the reviewers who took part in the discussion considered that their questions have been well addressed and that the authors responses basically provided the answers to the questions. the feedback provided was already fruitful and the final version should be already improved. accept. poster.", "accepted": 1}
{"paper_id": "iclr_2022_C03Ajc-NS5W", "review_text": "this work introduces an autoregressive flow model that generates molecular geometries by placing one atom at the time. in order to preserve the e3 invariance of the density, successive atom locations are sampled relative to already placed atoms in a coordinate system described by distance, angle and torsion. the paper is overall wellwritten and experimental results are compelling.", "accepted": 1}
{"paper_id": "iclr_2022_3HJOA-1hb0e", "review_text": "this paper introduces a method to determine which precision to use for the weights, as well as a quantisation method using hysteresis to improve performance with lowprecision weights, including 4bits. reviewers tend to agree that the two points presented are useful and can have a large impact on the field. generally, reviewers pointed out that motivations, notations and experimental studies could be improved. this has been partly addressed by the authors. i recommend to accept this paper for iclr 2022.", "accepted": 1}
{"paper_id": "iclr_2022_t8O-4LKFVx", "review_text": "in this paper, a new learning scheme for minimizing the confidence set by conformal prediction is proposed. most of the reviewers agree that the idea is interesting and novel. this is an important contribution to trustworthy ml, with theoretically sound considerations and thorough experimental validation.", "accepted": 1}
{"paper_id": "iclr_2022_TW7d65uYu5M", "review_text": "this paper proposes to synthetize virtual outliers by sampling from lowlikelihood regions of the feature space of a class conditional distribution, in order to make more robust predictions via a regularization loss term. in the reviewing phase certain criticisms were raised by reviewers namely that i the paper was not clear w.r.t. its goal, motivation and position in the literature of ood detection for bounding boxes; ii details about the energybased formulation and covariance definitions and iii experimental setting and metric used were missing. during rebuttal the authors answered to all the above criticisms up to a satisfying extent and were able to increase two reviewers scores. the paper is accepted conditioned on the fact that the cameraready includes the additional details and discussions that arose in the comments with a specific emphasis on properly framing and limiting the motivation of ood detection for openset object detection and it is expected to properly cite the literature of the more general ood detection task as discussed in the comments.", "accepted": 1}
{"paper_id": "iclr_2022_5JdLZg346Lw", "review_text": "the paper proposes a new method to learn ot maps, and reframes it in the gan literature. the initial method works when computing maps between equal dimensions, through duality and an identity 10  11, amply discussed in the reviewing process. lemma 4.1 provides the main result. while the discussion right below on the fact that several functions nonot maps might maximize that criterion is not completely satisfactory, the result provides an interesting characterization. the second contribution adds a method to compute ot maps between spaces of unequal dimensions. overall the contribution sounds a bit adhoc, and one wonders whether this does really work comments such as we add small gradient penalty gulrajani et al., 2017 on potential \u03c8\u03c9 for better stability. the penalty in not included in algorithm 1 to keep it simple. are strange and point to instability but the overall creativity and new ideas in the paper seem to have garnered enough support from reviewers to push for an accept.", "accepted": 1}
{"paper_id": "iclr_2022_uYLFoz1vlAC", "review_text": "all reviewers agreed this was a very strong submission it was clearly written, was theoretically and experimentally interesting, and had excellent motivation. a clear accept. authors youve already indicated that youve updated the submission to respond to reviewer changes, if you could double check their comments for any recommendation you may have missed on accident that would be great! the paper will make a great contribution to the conference!!", "accepted": 1}
{"paper_id": "iclr_2022_vh-0sUt8HlG", "review_text": "this paper presents a light weight hybrid model using both convolutions and transformer layers resulting in models with lower computational cost and good performance. reviewers find the paper interesting and agree that the paper did a good job in presenting convincing experimental results. there were questions about role of different components in the proposed model, which authors addressed in the response with additional ablation studies. one reviewer expressed concerns about lack of theoretical foundations for the proposed approach. however they also agree that the paper presents a good and useful experimental study. i think overall the paper has good contributions that others can build on in the future and recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_81e1aeOt-sd", "review_text": "this paper presents a study of onpolicy data in the context of modelbased reinforcement learning and proposes a way to ameliorate the resulting model errors. this is a timely and interesting contribution, and all reviewers agree on the quality of the manuscript. please incorporate all the remaining feedback from the reviewers. minor comment there might be interesting points of contact between this work and the concept of objective mismatch httpsarxiv.orgabs2002.04523", "accepted": 1}
{"paper_id": "iclr_2022_A3HHaEdqAJL", "review_text": "this paper provides generalization bounds for metalearning based on a notion of taskrelatedness. the result is natural and interestingintuitively, when tasks are similar, then metalearning algorithms should be able to utilize all data points across all tasks. the theoretical contribution is novel, and the results also provide more practical insight into the performances of some models.", "accepted": 1}
{"paper_id": "iclr_2022_mQxt8l7JL04", "review_text": "this paper proposes an extension to learning a representation it motivates, proposes and evaluates a new regularizer term that promotes smoothness via enforcing the representation to be geometrypreserving isometry, conformal mapping of degree k. comparisons with a standard vae and fmvae chen et al. 2020 are shown and experiments are provided on celeba with several different attributes as target classification tasks. the paper has received extensive reviews and the authors have successfully answered most of the concerns raised, mostly regarding comparisons to other techniques that try to introduce a regularization based on the properties of the jacobian of the decoder network. the appendix has been extended as a result of the rebuttal and the paper could be accepted. notes i find the formulation based on the notion of the isometric decoder somewhat surprising as the encoder is a key object of interest that controls the nature of the representation. the authors should clarify the assumption 3 in 3.3 better by the consideration of potentially dimz  dimx, how the isometry of the decoder effects the encoder, additionally, for the latent space flattening an ablation using svd merely a linear mapping for icdot could be considered. reviewer zghs has noted that they raise their grade to 6 in their comment, but this is still not currently reflected.", "accepted": 1}
{"paper_id": "iclr_2022_fXHl76nO2AZ", "review_text": "the paper proposed an imputation free method to handle missing data by learning an input encoding matrix using rl with the prediction error as rewardpenalty signal. reviewers appreciate the interesting setup where rl is used to deal with missing data, and the method being imputation free. three out of four reviewers reviewer he3p, azsy, and 4cb5 have raised concerns on the complexity of the proposed method, but it seems like all the reviewers see the strength of the work outweigh the weakness.", "accepted": 1}
{"paper_id": "iclr_2022_06Wy2BtxXrz", "review_text": "this paper presents a conditional variational autoencoder cvae approach to solve an instance of stochastic integer program sip using graph convolutional networks. experiments show that their method achieves high quality solutions with high performance. it holds merit as an interesting novel application of cvaes to the ml for combinatorial optimization literature, as well as for the nice empirical results which show a very nice improvement. two reviewers had a concern that the contribution is a bit narrowly focused toward milpfocused journal rather than a generalpurpose ml conference since the core contribution is the novel application. on the other hand, they believe that combinatorial optimization has received growing interest from the ml community in recent years. all three reviewers vote for borderline accept of this paper. the authors have addressed some of reviewers concerns, hence some reviewers increased their scores throughout the discussion phase.", "accepted": null}
{"paper_id": "iclr_2022_qwULHx9zld", "review_text": "the reviewers overall were quite happy after the rebuttal phase, in which the authors considerably improved the presentation quality and addressed reviewer concerns, and recommended acceptance. the reviewers agreed that while the theory was short and relied on various possibly restrictive assumptions and maybe was largely an improvement in constant factors, it extended prior work some of which was in iclr and was interesting and motivated the experiments which were notably faster than existing methods.", "accepted": 1}
{"paper_id": "iclr_2022_7fFO4cMBx_9", "review_text": "meta review for variational neural cellular automata this paper proposes a generative model, a vae whose decoder is implemented via neural cellular automata nca. the authors show that this model performs well for reconstruction, but they also show that the architecture has some robustness properties against damage during generation. experiments were conducted on 3 datasets mnist, noto emoji, and celeba, and while experimental results were great on mnist, the method was less performant so on the other two datasets, although there is clear evidence that the model can learn to generate meaningful images. for the robustness experiments, the authors show that vnca is robust to perturbations occlusions and show that the model has a reasonable degree of robustness even without ever seeing any perturbations at training time. all authors agree that this model is an improvement over neural cellular automata, and that the approach is interesting and the results are sound and even useful. initially, there were concerns that ncas were simply convolutional neural networks the connection is already known, and not the point of the paper, and issues with comparison with baselines for damage reconstruction tasks, but these were addressed by the authors which the reviewers have acknowledged, and have improved their scores. the authors have also responded to the concerns of reviewer cp9d, and due to the lack of response from cp9d, i assessed the authors response myself and find that they do address the concerns in particular, they removed claims of superresolution, and improved the clarity of the work. with that in mind, the score of 5 is viewed as a score of 6 from my perspective giving this work effectively an average score of 6. after my assessment of the paper and reviews, i agree with reviewer kwgv, as they have summarized the work in their original review  the authors propose a variational neural cellular automaton, which learns to generate images by iterating the transition rule.  the paper is interesting, with good results, and a good fit for iclr.  the paper solves an interesting problem on the topic of neural cellular automata.  there are some doubtslimitations that i have asked the authors to address mainly concerning the architecture of the model.  there are some missing references and details that would help the readers to get a better sense of the subject. crucially, kwgv have acknowledged that the authors have improved the paper significantly after the reviews, and they have addressed all questions and comments that they raised especially with regards to the last 2 points, and kwgv has subsequently championed the work with a score of 8. with the increased scores from kwgv and anwx in mind, and also with what i view as an increased score of 6 from cp9d in the lack of response from the reviewer, the authors have addressed the concerns in my judgement, my conclusion is that this is a nice work that bridges ncas with generative models, and i think the work will be a useful addition to the growing literature in this space. i will recommend it for acceptance at iclr 2022 as a poster.", "accepted": 1}
{"paper_id": "iclr_2022_oapKSVM2bcj", "review_text": "all reviewers agree that this paper is a useful and valuable contributions to ml engineering.  insightful analysis .. highly user friendly operator design  useful and i can see it having large adoption in the community of scientific computing ...   personally i tend to buy these advantages of einops ... however, there is a lack of solid empirical study to validate the effectiveness and efficiency of the design  a useful and appealing new coding tool. the negative reviewers appear fixated on the true observation that the paper does not look like a conventional iclr paper, thati it reads like a technical blog, and lacks rigour. i belive it is fair and measured to state that these reviews may be considered to exhibit aspects of gatekeeping requiring more mathiness that does not help the paper, or more rigour through user studies that are in fact less valuable than the reviewers own observations i could see myself..., i tend to buy.... this is a paper about design, not about models or algorithms although the algorithmic work is good. it is about the design of tools that we all use, and about the decisions and thought processes that led to that design. a reviewer decries many nonrigorous claims. these are claims about the usability of existing systems, and mostly appear in the discussion and footnotes, as the authors note in rebuttal. of course, one could have run user studies to back up each claim, but i am just as convinced by the examples shown in the paper. it matters not to me what some users corralled into a user study thought. it matters what i and my colleagues will think, and i am now sure to recommend einops to colleagues. i would not have met it had the paper not been submitted to iclr, and hence i am certain it should be accepted, so more can see that we care not just about mathiness, but actually enabling progress in our field. the job of a conference like iclr is to expose researchers and practitioners in machine learning to ideas and techniques that may advance their research and practice. programming, and the translation of mathematical ideas to efficient computer code, are fundamental to all of machine learning, and hence programming models are very much suitable for presentation to an iclr audience. i am certain that this paper, and the technology it describes, are more important to iclr readers than knowing that if module a is cotrained with module b, then combined with compression c, the sota on some arbitrary benchmark is increased by 0.31  1.04. reviewer grmh says there is no code, but the code has been in the open for three years; it is an accident of our misapplication of the principles of blind review that the reviewer felt they could not search for the code, and that the authors felt they could not bring to bear the evidence that three years of realworld usage have brought. reviewers say the work is just an extension of einsum, while noting that the extension is useful and nontrivial. yes, it is an extension, and the papers examples show how it yields more compact code that is also more readable and maintainable. i could add more examples, but in short, i tend to side with the authors response at almost every point. at the same time, the final version of the paper has been strengthened by this dialectic, and i expect further strenghtening through exposure to the iclr community. to the authors listing 1 is useful, but should be in an appendix. instead, add examples of ellipses on p5, and show more inline examples in general. the paper would be strengthened by another pass over the english  after the decision is made i would be happy to volunteer to help.", "accepted": 1}
{"paper_id": "iclr_2022_wENMvIsxNN", "review_text": "this paper introduces a new technique for discovering closedform functional forms ordinary differential equations that explain noisy observed trajectories xt where the label xt  fxt, t is not observed, but without trying to approximate it. the method first tries to approximate a smoother trajector xhatt, then relies on a variational formulation using a loss function over functionals c_j_j, defined in terms of an orthonormal basis g_1, , g_s of sampling functions such that the sum of squares of all the c_j approximates the theoretical distance between fx and the solution fx. these sampling functions are typically chosen to be a basis of sine functions. the method is evaluated on several canonical odes growth model, glycolitic oscillator, lorenz chaotic attractor and compared to gaussian processesbased differentiation, to splinebased differentiation, regularised differentiation, and applied to model the temporal effect of chemotherapy on tumor volume. reviewers found that the paper was wellmotivated and easy to follow ebvj, well evaluated ebvj, offering new perspectives to symbolic regression 79ft. reviewer vag3 had their concerns addressed. reviewer zddy had concerns about the running time a misunderstanding that was clarified and the lack of comparison to a simple baseline consisting in double optimisation over f and xhat0 using neural odes the authors have added a neural ode baseline but were in disagreement with zddy and 79ft about their limitations. reviewers engaged in a discussion with the authors, and the scores are 6, 6, 8, 8. i believe that the paper definitely meets the conference acceptance bar and would advocate for its inclusion as a spotlight in the conference.", "accepted": 1}
{"paper_id": "iclr_2022_6q_2b6u0BnJ", "review_text": "the paper investigates what we can learn from _suboptimal_ demonstrations for imitation learning. it suggests that we can learn about the structure of the environment by finding a factored dynamics model including a latent action space. it demonstrates both theoretically and empirically that this information can reduce sample requirements for downstream il. the reviewers praised the simplicity of the method including its minimal assumptions, the theoretical analysis, and the breadth of the experimental validation. the authors were helpful during the discussion period, and addressed any questions or concerns the reviewers raised. overall, this is an interesting idea and a wellexecuted paper.", "accepted": 1}
{"paper_id": "iclr_2022_rhOiUS8KQM9", "review_text": "this paper proposes an adaptive tree search algorithm for nmt models with nondecomposable metrics and shows its efficacy against strong baselines. this is an interesting contribution towards overcoming the performance caps introduced by the uncontrolledfor biases of beam search, and it speaks to a growing community interested in decoding beyond greedy surprisal minimisation. the initial reviews brought to light a number of concerns that in my view are well addressed in the rebuttal and in the current version of the manuscript. one of the key issues was a confusion caused by the use of the term nonautoregressive to refer to the intractability of the metric  objective function of certain models. this use clashed with the more standard use in mt, which refers to a tractable factorisation of a joint probability by means of strong conditional independence assumptions. the confusion is easy to address and in no way compromises the thoroughness of the empirical section. the authors are aware of the confusion and how to resolve it, and they have acknowledged the need to pick a less ambiguous term. id like to recommend this for acceptance, but i urge that the authors do not ignore the confusion caused by autononauto regressive and the missing literature that came up in the discussion with reviewer i2pz i understand the discussion happened too late for the manuscript to be updated, but i trust this can be done for the final version.", "accepted": 1}
{"paper_id": "iclr_2022_d5SCUJ5t1k", "review_text": "this paper presents work on openworld object detection. the main idea is to use fixed percategory semantic anchors. these can be incrementally added to when new data appear. the reviewers engaged in significant discussion around the paper with many iterations of improvements to the paper. initial concerns regarding zeroshot learning were addressed, as were remarks on presentation and claims. in the end the reviewers were split on this paper. i recommend to accept the paper on the basis of the semantic topology ideas and the thorough experimental results. the remaining concern centered around the evaluation protocol used in the paper, which follows that in the literature e.g. joseph et al. cvpr 21. while this is not a fatal flaw, it is an issue with how this genre of methods is evaluated. it would be good to add discussion to the final paper to highlight this as an opportunity for future work in the field to address. specifically, as a reviewer noted after detecting unknown objects in t1, the hypothetical annotation process provides boxes for all objects of some new classes instead of only for those that have been correctly detected localized and marked unknown.", "accepted": 1}
{"paper_id": "iclr_2022_45L_dgP48Vd", "review_text": "the paper tackles the problem of detecting anomalies in multiple timeseries. all the reviewers agreed that the methodology is novel, sound and very interesting. initially, there were some concerns regarding the experimental evaluation, however, the rebuttal and subsequent discussion cleared up these concerns to some extent and all reviewers are eventually supporting or strongly supporting acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_MP904TiHqJ-", "review_text": "the authors first consider a mean field two player zero sum game and consider quasistatic wasserstein gradient flow dynamics for solving the problem. the dynamics is proved to be convergent under some assumptions. finally, the authors provide a discretization of the gradient flow and using this proposes an algorithm for solving minmax optimization problems. they use this algorithm for gans as the main example. experimental results claim that the algorithm outperforms langevin gradient descent especially in high dimensionas. this paper sits right at the border. but subsequent to the author response, one of the reviewers has updated the score and seems more positive about the paper. in view of this, i am leaning towards an accept.", "accepted": null}
{"paper_id": "iclr_2022_MIX3fJkl_1", "review_text": "the authors propose a new framework of population learning that optimizes a single conditional model to learn and represent multiple diverse policies in realworld games. all reviewers agree the ideas are interesting and the empirical results are strong. the meta reviewer agrees and recommends acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_IwJPj2MBcIa", "review_text": "this paper identifies a limitation with current attention in transformers where they scoring with querykey pairs is strongly tied to retrieving the value and proposes a more flexible configuration that subsumes the previous setup but provides more flexibility. the authors shows this leads to improvements in various settings. overall, all reviewers seem to agree there is interesting insight and results in this paper and it merits publication. also the discussion helped stress important points regarding weight sharing and more. one concern is that the model was not evaluated on standard nlpvision datasets i assume alluding to gluesupergluesquad, etc., and authors seem to hint that pretraining this is an issue for them computationally. this leaves open whether this indeed can and should replace the standard attention mechanism across the board, but is still very worthy of publication.", "accepted": 1}
{"paper_id": "iclr_2022_9RUHPlladgh", "review_text": "this paper presents a through study of generalization in visual representation learning. it compares in distribution generalization to out of distribution generalization using a comprehensive benchmark. the paper received very positive reviews from all reviewers. reviewers agreed that the paper has several strengths it is very well written, the presented benchmark is very useful and the analysis is thorough. one concern that was brought up by the reviewers was that a majority of the presented findings are expected and in a sense, known to the community. the authors have addressed this concern by pointing out that their findings are more fine grained than past works and that their proposed benchmark is a stepping stone towards measuring general robustness. i must note that in spite of this concern, all reviewers have maintained their strong acceptance scores. i agree with the reviewers. this paper makes a strong contribution to this important problem via its benchmark and analysis, which future works can build off of, and hence i recommend acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_dQ7Cy_ndl1s", "review_text": "this paper considers generalization of polynomial networks. it gives a characterization of the rademacher complexity as well as lipschitz constants for polynomial nets. inspired by the theoretical results, the paper also proposed regularization schemes that empirically improves accuracy and robustness. most reviewers found the theoretical results to be interesting but there are some concerns about the mismatch between the upperbound in theory and used in practice, which was partially addressed in the response. there are some more concerns about the experiments but many of them are addressed in the new version. overall although polynomial networks are not popular in practice, this paper provides some interesting theoretical results.", "accepted": 1}
{"paper_id": "iclr_2022_nBU_u6DLvoK", "review_text": "the paper presents an approach for spatiotemporal representation learning using transformers. it introduces a particular architecture design, which shows an impressive computational efficiency. the reviewers agree that the experimental results are strong, and unanimously recommend the paper for acceptance. the reviewers find their concerns regarding the details of the approachsetting address after the authors response. we recommend accepting the paper.", "accepted": 1}
{"paper_id": "iclr_2022_OqcZu8JIIzS", "review_text": "previous approaches to modelbased offline rl require carefully tuning the tradeoff between model return and uncertainty. the authors propose an approach that produces a diverse pool of policies on the pareto front of this tradeoff. on the d4rl offline rl benchmark, p3 outperforms competing approaches when the experience is collected with low or medium return policies. before the rebuttal, reviewers identified the following primary concerns  the experimental evaluation of p3 uses many policy evaluations to select the policy, which results in an unfair comparison with existing methods.  p3 underperforms existing methods on some datasets. why? overall, reviewers were satisfied by the response and raised their scores accordingly. the authors responded by including a modification of p3 that uses fqe to select the policy for evaluation, resolving the first concern. the authors explain that p3 underperforms on high return datasets because it splits its updates across the pool of policies. the authors state, we believe and in theory it does hold that p3 can achieve the same performance of uwac on highquality datasets, if provided with more computational budget. i suggest that the authors conduct at least one experiment to verify this claim. the proposed idea is interesting and the revisions the authors have made resolved the primary concerns from reviewers, so i recommend acceptance. the reviewerauthor discussion has many substantial points that i recommend the authors integrate into the revision.", "accepted": 1}
{"paper_id": "iclr_2022_-ApAkox5mp", "review_text": "the paper considers the setting of bilevel optimization and proposes a quasinewton scheme to reduce the cost of jacobian inversion, which is the main bottleneck of bilevel optimization methods. the paper proves that the proposed scheme correctly estimates the true implicit gradient. the theoretical results are supported by numerical experiments, which are encouraging and show that the proposed method is either competitive with or outperforms the jacobian free method recently proposed in the literature. even though the reviews expressed some initial concerns regarding the empirical performance of the proposed method, the authors adequately addressed those concerns and provided additional experiments. thus, a consensus was reached that the paper should be accepted.", "accepted": 1}
{"paper_id": "iclr_2022_AwgtcUAhBq", "review_text": "summary from reviewer uzt5 this paper analyzes adversarial domain learning dal from a gametheoretical perspective, where the optimal condition is defined as obtaining the local nash equilibrium. from this view, the authors show that the standard optimization method in dal can violate the asymptotic guarantees of the gradientplay dynamics, thus requiring careful tuning and small learning rates. based on these analyses, this paper proposed to replace the existing optimization method with higherorder ordinary differential equation solvers. both theoretical and experimental results show that the latter ode method is more stable and allows for higher learning rates, leading to noticeable improvements in transfer performance and the number of training iterations. all reviewers appreciated the contributions of this paper and recommended acceptance. while the methods themselves are not novel, the game perspective applied to this problem appears to be and the use of higherorder solves yield interesting theoretical and empirical improvements.  additional comments  1 for the comparison vs. game optimization algorithms figure 3, it would be nice to normalize the xaxis so that one epoch yields comparable computational cost among the different methods as rk4 and rk2 is much more expensive than eg or gd per minibatch. given that eg had such bad performance there, it would not change the conclusions; but the current scaling is still quite misleading. same comments for figure 2. 2 note that modern approaches for stochastic extragradient recommend to use different stepsizes for the extrapolation step and the update step see e.g. hsieh et al. neurips 2020 explore aggressively, update conservatively stochastic extragradient methods with variable stepsize scaling i suspect that much bigger stepsizes could be used in this case while maintaining convergence, and this version should be added to figure 3. 3 in related work  twoplayer zerosum games  note that gidel et al. 2019a provided all their convergence theory and methods for stochastic variational inequalities and thus it also applies to threeplayer games, unlike seems to be implied by this paragraph. in particular, all the algorithms they investigated extraadam amongst others could also be applied to dal. while i can see that the specifics of the objective in dal might be different than for gan optimization, it would be worthwhile to acknowledge these alternative approaches more clearly, and i encourage the dal community to investigate their performance more exhaustively for dal than what was done in this paper.", "accepted": 1}
{"paper_id": "iclr_2022_J1rhANsCY9", "review_text": "this is a borderline paper which elicited much discussion. the paper proposes to extract features from pretrained networks through kernel functions. it develops the idea of fisher kernels for neural networks calling it nfk. the methodology applies to both supervised and unsupervised setting. the paper shows that proposed kernel has low rank structure and serves as the basis for developing an algorithm for computing the kernel on large datasets. the idea of extending fisher kernels, their efficient computation, and investigating their usage in both supervised and unsupervised are some of the key strengths of the paper. the reviewers though appreciative suggested 1 several new experiments, 2 inclusion of more background work related to power method and 3 have more technical discussion clarifying the contributions related to background. the authors during rebuttal tried to incorporate most of the suggestions in the revised draft. since there was consensus on the novelty, the detailed discussions, and the results of the additional experiments, one could potentially accept this paper if there is space. the results will be interesting will be those who investigate the interplay of kernel methods and deep networks.", "accepted": null}
{"paper_id": "iclr_2022__4D8IVs7yO8", "review_text": "this paper proposes a simple approach to improve the robustness of training a sparsely gated mixtureofexperts model, which at a high level simply consists in training initially as a dense gated model, to better warm start a final phase of sparse training. results are presented to highlight the potential benefits of this approach. the authors have provided a detailed response and updated results, in response to the reviews. each reviewer has also responded at least once to the author response. despite that engagement, all reviewers are leaning towards rejection though there is one reviewer with a rating of 6, they regardless state that im confident this will make a great resubmission at a future venue, indicating they actually support rejection. the reviewers point out that the proposed method is not really novel, pointing to an existing recent paper. even without that prior work, i would also argue that the proposed approach is conceptually straightforward and has benefits that were fairly predictable and not particularly surprising. given the generally lukewarm reception from the reviewers, i think there is a legitimate concern to be had here about this works potential for impact. though the review process has definitely improved the papers manuscript since its submission, i unfortunately could not find a reason to dissent from the reviewers consensus that this submission is not ready to be published. therefore recommend it be rejected at this time.", "accepted": null}
{"paper_id": "iclr_2022_JHXjK94yH-y", "review_text": "the idea of having two policies with opposing strategies, one aiming to maximize a notion of surprise whereas the other tries to minimize it, is an interesting one. however, even after the author rebuttal, all reviewers have lingering concerns about the evaluation protocol. in addition, there are remaining questions about the bonuses used; there are concerns that these only work for very specific domains. for these reasons, im recommending rejection. i encourage the authors to carefully read the concerns of the reviewers about evaluation and consider using a different evaluation protocol for a future version of this work.", "accepted": 0}
{"paper_id": "iclr_2022_QbFfqWAEmMr", "review_text": "this paper proposes a novel method for improving domain generalization based on the idea of learning different subspaces for each domain. authors provide theoretical analysis related to their proposal and further evaluate their proposed method on a subset of domainbed benchmark. strong points  the paper is wellwritten.  the proposed method is novel.  authors provide theoretical analysis in support of their proposal.  the theoretical results seem to be correct.  empirical evaluation shows that the proposed method improves over baselines on a subset of datasets included in the domainbed benchmark. weak points  the complexity of the theoretical results makes it very difficult for the reader to get any intuition about the underlying mechanisms at play.  the theoretical analysis is disconnected from the proposed algorithm. it is hard to see how one could end up proposing such an algorithm following the theoretical results. i suggest that authors would consider reorganizing the paper with less emphasis on the theoretical part, perhaps simplifying the theoretical results and pushing the rest to appendix.  the empirical evaluation can be improved significantly. domain generalization is a very wellestablished area at this point. wilds is a carefully designed and wellknown benchmark and showing improvement in that benchmark would be very convincing but unfortunately authors do not discuss or even refer to it. they instead report their results on a subset of datasets used in domainbed benchmark. the domainbed benchmark is less challenging than wilds but even following domainbed closely and reporting the 3 evaluation metrics on all 7 datasets would have been satisfying. however, authors only report the results on 3 datasets. reporting the results on a diverse group of datasets is particularly important in the case of domain generalization because we know that many methods are able to show improvements on a few datasets but it is challenging to beat the baselines on a significant majority of datasets. final decision rationale this is a borderline paper. on one hand, the proposed method is interesting and novel. on the other hand, the theoretical contributions are very limited and the empirical evaluation is not strong enough for acceptance. given that all weak points mentioned above can be addressed, i recommend rejection and i sincerely hope that authors would strengthen their paper by addressing them before resubmitting their work.", "accepted": null}
{"paper_id": "iclr_2022__xxbJ7oSJXX", "review_text": "the authors propose the resource constrained offline rl problem where the offline dataset contains extra features that are not available online. the goal is to use these extra features to improve performance during deployment. they propose a simple modification to td3bc in the continuous control setting and a simple modification to cql in the discrete setting. they evaluate their proposed approaches on d4rl, rcd4rl a novel dataset that they introduce for resource constrained offline rl, atari, and a proprietary reallife ads problem. initial reviews identified the following concerns  while the exact problem is novel, the idea of having access to privileged features at training time that are not available at deployment has been explored in supervised learning and online rl. the reviewers were not clear how considering the offline rl setting interacts specifically with the privileged features to produce an interesting setting.  the baseline simply trains on the limited feature set. unsurprisingly, using the extra features can improve performance. in light of the previous point, reviewers asked for more substantial baselines, suggesting bc on the teacher and predicting the missing features as some possibilities.  the set of tasks was too limited. the authors provided a substantial response  experiments on ads data  experiments on atari with cql as the base algorithm  additional baselines on rcd4rl halfcheetahv2 datasets bc on teacher and predictive  additional analysis i commend the authors on the hard work they did preparing this response. it is quite substantial and does improve the paper significantly. however, reviewers and i still have a number of concerns  the additional baselines are appreciated, however, the results are mixed. the additional baselines are a step in the right direction, but they need to be evaluated beyond a single dataset. it is hard to evaluate the results without reasonable baselines. i agree and think that even though the specific problem is novel, the idea of transfer learning is not, so it is reasonable to require that we have more extensive baselines. furthermore, while the authors argue that their method has an edge on the more practical dataset, that is based on a very limited evaluation. probing this further is important.  the cql modification is quite different than the td3bc modification. the performance of the modification for cql is not significantly better than cql. what should we make of this?  for the ads dataset, all hyperparameter settings except transfer0, 1 show the same performance. this seems surprising as even transfer0.1, 0.9 shows no difference. finally, transfer0, 1 beating transfer1, 0 710 times is not statistically significant. at this time, the paper is not ready for publication, but the paper is moving in the right direction and i encourage the authors to submit a revised version to a future venue.", "accepted": null}
{"paper_id": "iclr_2022_rS9t6WH34p", "review_text": "the paper develops a method for decomposing 3d scenes into objects by coupling nerf decoders to representations produced by a slotbased encoder. after the discussion phase, reviewer ratings are mixed with three on either side of abovebelow threshold, and one higher but low confidence accept score. drawbacks include limited novelty, as stated by reviewer 8uah the unsupervised decomposition of the scene, which is somehow incremental, given that its achieved by applying the slotbased approach of locatello et al. 2020 to nerfs. reviewer bamb likewise mentions this issue. reviewer vrrk some of its contributions are on improving nerfs while the decomposition part is rather marginal. reviewer vrrk also raises concerns about lack of experiments on real data since the proposed method is based on nerf, how well does it work with real photographs? the ac agrees with the marginal rating of the reviewers and is particularly concerned with overall novelty of the proposed pipeline and question of applicability beyond simulated data. more work seems required to solidify an experimental case on real images.", "accepted": 1}
{"paper_id": "iclr_2022_2cpsEstmH1", "review_text": "this paper proposes to create an explanation space to describe the relationships between input data and prototypes and also between the prototypes themselves. it constructs such a space suing vaes and conducts experiments to validate the effectiveness and interpretability of the method. strengths  the proposed method is interesting and intuitive weakness  novelty of the idea is limited  missing experiment comparison with some important previous work  some claims are not well supported by the empirical results", "accepted": 1}
{"paper_id": "iclr_2022_rbPg0zkHGi", "review_text": "the submission considers a new acquisition function for active learning. the method considers the sensitivity of the prediction for a given datapoint with respect to parameter perturbations. points with the largest variance under these perturbations are selected for labelling. the method is simple and the empirical results are reasonable. some weaknesses are the clarity of writing, and somewhat limited experimental comparisons. the discussion was useful and helped improve the clarity. additional experiments also helped improve the paper, although some reviewers still felt the experimental comparisons were lacking, including using entropy as a baseline acquisition function. despite these improved scores, the overall average score remains below threshold im afraid. i feel this is a useful paper, but perhaps needs a little more polishing in the writing and some additional experiments. as such, it just falls short of the acceptance threshold.", "accepted": 0}
{"paper_id": "iclr_2022_eZ-xMLuKPc", "review_text": "the reviews are of good quality. the responses by the authors are commendable, but iclr is selective and reviewers still believe that the research would be better as two separate papers one about the problem and solution from an ml perspective, and the other about the application to surgery. papers that provide a new method in the context of a single application domain run the risk of making a contribution to neither, and of being evaluated by reviewers who are not experts in both.", "accepted": 0}
{"paper_id": "iclr_2022_m7zsaLt1Sab", "review_text": "this paper proposes a theory for understanding the context representation in pretrained language models. the strengths of the paper, as identified by reviewers, are in the importance of an attempt to explain contextualization in language models, and in the novelty of using the category theory to model the connection between contexts and their representations. however, all the reviewers identify several major weaknesses, including flawedincoherent definitions of concepts in the proposed theory and insufficient experimental results. although the authors rebuttal put a great deal of effort to address raised concerns, all five reviewers agree and provide very detailed justifications along with suggestions for improvements that the work is not yet ready for publication.", "accepted": 0}
{"paper_id": "iclr_2022_UTTrevGchy", "review_text": "this paper proposes infomax termination critic imtc, a new approach for learning option termination conditions with the aim of discovering more diverse options. imtc relies on a scalable approximation of the gradient of a mutual information objective with respect to the termination function parameters. reviewers liked the motivation and the simplicity of the approach. while there were some initial concerns regarding the similarity of imtc and vic, the authors did a good job of clarifying the differences and providing additional results in the rebuttal. while two reviewers raised their scores based on the rebuttal, this left reviewers split on whether to accept or reject the paper. given that the papers main contributions are evaluated empirically i based my decision on the strength of the evaluation. the main claim in the paper is that imtc significantly improves the diversity of the learned options when combined with intrinsic control methods like vic and rvic. the main supporting evidence of this claim is a visualization of the option policies and termination probabilities reached by vic and rvic. there are several issues with this comparison  this is a poor visualization of the kind of option diversity the paper aims to obtain. given that mutual information based objectives used by vic, rvic and imtc aim to optimize diversity in the final states reached by the options, visualizing the distribution of final states or the trajectories produced by the options is more meaningful.  the vic and rvic baselines are evaluated with a fixed option termination probability of 0.1 which biases the comparison in favor of imtc because imtc is able to choose when and where to terminate while vic and rvic with random termination get to control neither. using fixed option duration with mibased option discovery methods like vic, diayn and rvic is more standard and is known to produce options with very clear terminal state clusters which are wellseparated for different options. fixed option duration allows vic and rvic precise control of where they will terminate since option duration is fixed, hence it should have been included in the comparison.  as mentioned in point 2 above, it is well established that vic tends to learn options with wellclustered end states, especially in simple gridworld domains like in figure 3 see vic, diayn and rvic papers. the authors seem to obtain different qualitative results raising questions. overall, i dont think the qualitative experiments show that imtc is able to improve the diversity of options discovered by vic or rvic due to issues with how the experiments are done random option duration for vic and rvic and how the results are presented visualizing action probabilities instead of final states. given these concerns and the split among the reviewers i recommend rejecting the paper in its current form.", "accepted": null}
{"paper_id": "iclr_2022_6j9YOwh8itH", "review_text": "this paper presents work on action anticipation. the reviewers appreciated the message passing based method. however, concerns were raised regarding novelty, effectiveness, presentation, empirical results, and magnitude of impact for iclr. the reviewers considered the authors response in their subsequent discussions but felt the concerns were not adequately addressed. based on this feedback the paper is not yet ready for publication in iclr.", "accepted": 0}
{"paper_id": "iclr_2022_Lv-G9XqLRRy", "review_text": "the paper proposes a technique to efficiently retrain a model when a small number of classes are required to be removed. reviewers in general like the paper, but the key issue is motivation for the problem. the motivating examples in the rebuttal are not very good because a. authors do not provide any evidence that such situations are critical or commonplace, b. the data points that are available for retraining might be very biased. a more careful grounding of the work would be important to motivate the iclr community and the ml community in general to further study this problem. but for now, unfortunately the paper does not seem ready for publication at iclr.", "accepted": null}
{"paper_id": "iclr_2022_OhytAdNSzO-", "review_text": "this paper explores strategies for scaling vision transformers that can be transferable across hardware devices and vit variants. while it presents some interesting observations as well as a useful practical guide, multiple reviewers expressed major concerns over the novelty and significance of the methods and findings. besides novelty and significance, there are also some concerns about comparison with existing work as well as clarity of the presentation.", "accepted": 0}
{"paper_id": "iclr_2022_3r034NfDKnL", "review_text": "the authors study the emergence of systematic generalization in neural networks. the paper studies a timely topic and presents a set of concrete results. for example, reviewer zgrw emphasizes that a key strength of the paper is constructing simple datasets where systematicity emerges. i think indeed it is valuable, as systematicity is sometimes poorly defined and understood, so building a theoretical testbed might be very helpful. however, the reviewers found important issues, which the rebuttal was unable to address. perhaps the key issue raised e.g. by reviewer 9qcy is that results do not clearly generalize to more practically relevant settings. what is somewhat missing is a clear set of guidelines or implications for how to improve systematicity in more practically relevant neural networks. based on this and other issues raised by the reviewers, unfortunately, i have to recommend rejecting the paper. thank you for your submission, and i hope that the review process will help you improve the work.", "accepted": 0}
{"paper_id": "iclr_2022_u6sUACr7feW", "review_text": "this paper proposes the use of the determinantal point process to introduce the diversity in the prosodic features, including intonation, stress, and rhythm, in text to speech synthesis. the proposed approach is certainly new, but the experimental support is of critical importance for this work. one of the major points of discussion was the reliability of the experimental results. in the original submission, the mean opinion score mos of the proposed approach was inferior to the baseline. the authors updated the experiments, which significantly more than the confidence interval lowers the mos of a baseline. this however makes the experimental results questionable.", "accepted": 0}
{"paper_id": "iclr_2022_34mWBCWMxh9", "review_text": "this paper proposed a spatial smoothing layer for cnns which is composed out of a feature range bounding layer referred to as prob and a bluring layer referred to as blur. an empirical analyses shows that the proposed layer improves the accuracy and uncertainty of both deterministic cnns and bayesian nn bnns approximated by mcdropout. the paper further provides theoretical arguments for the hypothesis that bluring corresponds to an ensemble and represents the proposed method as a strategy to reduce the sample amount during inference in bnns. reviewers valued the extensive theoretical as well as practical analyses. however, the theoretical analysis should still be improved. first of all, the the proposed technique is motivated in the context of bnns, which is not very strongly supported. second, the argument that the smoothing layer is an ensemble is based on the observation that it has some properties ensembles have as well 1 they reduce feature map variances, 2 filter out highfrequency signals, and 3 flatten the loss landscape. but two things sharing the same properties do not need to be the same thing. moreover, the proofs of the prepositions stating the properties are difficult to follow and may contain some flaws. furthermore, the paper is not well selfcontained and highly depends on the appendix. given these, the paper can not be accepted in its current state. a future version could improve over the current manuscript by making the theoretical statements and proofs more clear. another option would be to analyze the contribution without connecting it to a bayesian setting and ensembles, and instead focus on showing that the proposed smoothing layer has those good properties, doing detailed empirical studies, and showing that cnn components like global average pooling and relu  bn are special cases of the propose method.", "accepted": 1}
{"paper_id": "iclr_2022_J8P7g_mDpno", "review_text": "the reviewers were generally split on this paper. on the one hand, reviewers generally appreciated the clear presentation, discussion, and explanations, and the experiments. on the other hand, most reviewers commented on the lack of comparative evaluation to other works, including works that are related conceptually. while the authors have a potentially reasonable argument for omitting such comparisons, in the balance i do not believe that the reviewers were actually convinced by this. particularly when the novelty of the contribution is not crystal clear, such comparisons are important, so i am inclined to not recommend acceptance at this point though i acknowledge that the paper is clear borderline and could be accepted.", "accepted": null}
{"paper_id": "iclr_2022_dHJtoaE3yRP", "review_text": "the paper proposes nafs nodeadaptive feature smoothing, which constructs node representations by only using smoothing without parameter learning. the authors first provide a formulation for the smoothing operator. they then define oversmoothing distance to assess how much a node is close to the stationary state. finally, they use the oversmoothing distance to calculate a smoothing weight for each node. experiments are conducted to verify the efficacy. strength  the paper tackles the problem of oversmoothing, which is a wellknown issue in gnn.  the solution appears to be effective.  the paper is generally clearly written. weakness  the novelty and significance of the work might not be enough. aspects of the contributions exist in prior work.  additional experiments have been conducted during the rebuttal. the reviewers appreciate the efforts. after rebuttal reviewer shxg increased the score accordingly. reviewer w2qg says given the concerns proposed by the other reviewers, i adjusted my score. reviewer ym4p says i read the rebuttal and slightly increased my score.", "accepted": null}
{"paper_id": "iclr_2022_THMafOyRVpE", "review_text": "the paper propose a fully online metalearning foml method which extend maml for continual learning in a fully online learning without requiring the knowledge of the task boundaries. experiments show that foml was able to learn new tasks faster than several existing online learning methods on rainbowmnist, and cifar100 datasets. there are a few major concerns from reviewers. one concern is about the lack of clarity on the problem statement the authors cast the problem as metalearning that must be done in a fully online setting, but it requires to store all the training data in a buffer storing all the training data seen so far, which contradicts to the principle of online learning. another major weakness is the poorly written literature survey, which missed to cite a large body of related work in continual learning and onlinemetalearning such as online continual learning, taskfree continual learning, continual learning without task boundaries, etc. these should at least be discussed carefully if not fully compared in the empirical studies. also experiments are quite weak in both settings, datasets and rather outofdate baselines. finally, there also lacks of theoretical justification or analysis. therefore, the paper is not recommended for acceptance in its current form. i hope authors found the review comments informative and can improve their paper by addressing these review comments carefully in future submissions.", "accepted": null}
{"paper_id": "iclr_2022_G7PfyLimZBp", "review_text": "the paper considers the difference between gd and adam in terms of implicit bias. it considers a specific distribution and architecture where the two algorithms converge to different solutions while perfectly fitting the training data. the authors highlight the fact that this happens while adding regularization, which does not happen in the linear case. the reviewers found some of the insights and analysis interesting. however, they also had reservations about the impact of the results given that it is known that gd and adam have different implicit biases, and that the distribution appears specifically crafted towards showing this effect for the architecture studied. in future versions, the authors are encouraged to better motivate the chosen distribution, use more standard neural architecture e.g., standard relu, and provide more explanation about the role of regularization in their result.", "accepted": 0}
{"paper_id": "iclr_2022_gmxgG6_BL_N", "review_text": "this work has generated a lot of discussion between authors and reviewers and among reviewers. overall it is reported that the results on eeg are not conclusive and directly relevant for this field. besides the theoretical contribution is not reported as a strong point of the work and the comparison with alternative baseline methods is judged too limited. for all these reasons the paper cannot be endorsed for publication at iclr this year.", "accepted": 1}
{"paper_id": "iclr_2022_dHd6pU-8_fF", "review_text": "this paper presents an adaptive gradient method for neural net training inspired by lbfgs. all of the reviewers recommend rejection. they raise concerns about the amount of novelty, the clarity of the writing, and the experimental comparisons. i encourage the authors to take the reviewers comments into account and improve the submission for the next cycle.", "accepted": 0}
{"paper_id": "iclr_2022_ArY-zkyHI_l", "review_text": "this paper proposes a new ensemble training method for improving adversarial robustness to multiple attacks e.g., ell_2, ell_1 and ell_infty. specifically, authors adopt the recent multiinput multioutput mimo ensemble architecture for computational efficiency. then, the authors construct the adversarial examples using the outputs of multiple attacks simultaneously. with these examples, standard adversarial training is conducted on mimo ensemble. all reviewers are on the negative side. ac agrees with reviewers concerns on limited novelty and insufficient empirical evaluation. ac also thinks that the improvement is not that significant compared to the existing method, especially concerning the realworld dataset. overall, ac recommends rejection.", "accepted": 0}
{"paper_id": "iclr_2022_By5Uwd_xzNF", "review_text": "this manuscript presents an approach to handling abstract visual analogy tasks where panels of drawings are shown with a missing entry. one of a number of candidate drawings must be chosen to complete the panel. reviewers brought up several concerns 1. the task performed was made considerably easier by providing additional annotations at training time. this was not the case in the original task in prior work that the manuscript builds on. no convincing explanation was provided as to why this change is critical to accommodate the manuscripts contributions. 2. a key feature of the approach, the adaptive modular design, does not seem to contribute much. the authors rightly point out this may be a limitation of current benchmarks. reviewers were sympathetic to this view but that leaves the manuscript in a tough spot a central contribution cannot be evaluated. what is even worrisome is that without evaluating the effectiveness of the adaptive design we cannot know if it is working at all. what if adaptivity is required for some future analogy tasks but it turns out that this approach, despite seeming to be adaptive, falls short? 3. another contribution, the multitask encoder does not seem to provide much value in ablation experiments. the manuscript would be improved if this feature was removed or its usefulness was demonstrated. a number of smaller issues were also brought up by reviewers. throughout the responses to reviewers the authors highlight that their central contribution is incorporating a structure mapping prior the central contribution of our method is introducing a structure mapping prior .... i would like to draw the authors attention to the fact that they had to remind 3 of the 4 reviewers to focus on this rather than another aspect of the work. that clearly indicates that the manuscript and work needs a shift in focus. i would suggest that authors double down on their structural mapping prior, eliminate all other features which turned out to be controversial or impossible to evaluate, and demonstrate the utility of their idea in two domains, i.e., including another domain. this would really highlight the core contribution. unfortunately, what may turn out to be a good idea, the structural mapping prior, is lost among many other complexities. i hope the authors are not discouraged and that we see this line of work again in the future.", "accepted": 0}
{"paper_id": "iclr_2022_Yr_1QZaRqmv", "review_text": "the reviewers identified missing comparisons to existing baselines deep rl and other treebased rl methods as well as simplistic experiments as the main limitations of the paper. while the authors could address some of the issues raised by the reviewers, the missing comparisons and too simple experiments remain. i therefore agree with most of the reviewers that the paper can not be published at its current state.", "accepted": 0}
{"paper_id": "iclr_2022_e0TRvNWsVIH", "review_text": "in this paper, the problem of identifying a lowdimensional latent space for highdimensional bayesian optimization bo is considered. in particular, the authors focus on the problem of collision, where different points in the original space become identical in the latent space, and propose a regularization method to avoid this problem. latent space identification for highdimensional bayesian optimization is an interesting and the authors approach sounds reasonable. however, many reviewers pointed out that the discussion and results in the paper do not provide sufficient evidence for the authors claims. therefore, we have to conclude that the paper cannot be accepted at this time.", "accepted": 0}
{"paper_id": "iclr_2022_7xzVpAP5Cm", "review_text": "this paper proposes the algorithm which they call deosgd, which is a combination of the ideas of the generalized deo scheme, denoted by deo, to facilitate exploration section 3.1, adoption of stochastic gradient descent sgd in the exploration chains i.e., those chains except the one with the lowest temperature section 4, and use of adaptive tuning of learning rates section 4.2. the proposal is applied experimentally in section 5 to demonstrate superiority of the proposal over existing approaches. the initial review scores of the four reviewers were one positive and three negatives. most reviewers positively evaluated the proposal, including the proposal of deo and its theoretical analysis, as well as its empirical usefulness in deep learning for a computervision task. on the other hand, some reviewers showed concern about soundness of the proposal. upon reading the reviews and the author responses, as well as the paper itself, i think that this paper lacks a clear statement on its objective.  what does uncertainty approximation mean? the paper title would imply that the objective of the proposal in this paper is for uncertainty approximation, but i could not find any concrete description on what it exactly is.  sampling versus optimization the methods of langevin dynamics, or more generally markovchain montecarlo methods, have been used for two distinct purposes sampling and optimization. in any case fast relaxation towards equilibrium would be of practical importance. for sampling purposes it is also important to assure that the stationary distribution of the markov chain corresponds to the target distribution in langevin dynamics the target distribution would be the canonical ensemble defined by the energy ucdot and the temperature tau. for optimization purposes, however, the assurance of the stationary distribution to be equal to the target distribution would be less of concern. it seems that the authors interest would be in optimization rather than in sampling, but it is not clearly stated.  soundness issue as reviewer mbau pointed out, deo does not have a guarantee of convergence to the target distribution. i thought that if the objective of this paper would be in optimization rather than in sampling, the existence of approximation already in deo might be thought of as a minor problem, as the proposal already has other approximations introduced in section 4. the authors claim that this problem does not affect the main body of the paper, but i feel that it would affect the overall organization of the paper, as the current organization seems to presume that approximation only resides in the adoption of the sgdbased exploration kernels with deterministic swap. in any case, this problem has been acknowledged by the authors themselves, as well as reviewer ofjx. in particular, the detailed discussion between the authors and reviewer mbau has been very fruitful in clarifying technical subtleties in this manuscript, including the soundness issue mentioned above. at the same time, it would imply that this paper still has room for improvement. an additional point i would like to mention is that this paper is not really selfcontained, in the sense that several key notions and quantities are not defined or only defined in the supplementary materials tildeu is not explicitly defined at all, the terms swap time and round trip time are defined in appendix a.5, sigma_p in corollary 1 and lemma 2 is defined in appendix a.1. all these weaknesses make me to think that another round of revision would be appropriate to properly judge the quality of this paper, whereas there is no such option within the review procedure of iclr. i therefore cannot recommend acceptance of this paper at least in its current form. minor points page and line numbers refer to the revised version  abstract, line 5 given sufficient many p chains would be better phrased as given p chains, as the bigo notation usually assumes the largep asymptotic.  in several places, there are periods after figure and table, which are not needed.  page 3, line 32 in lemma 2 there is apparently no such term found as the second quadratic term. it should appear only after having assumed the equiacceptancerejection rates in equation 4, so that the sum becomes proportional to p.  theorem 1 the maximal round trip time should certainly be the minimal round trip time.  is the ceiling function. t  , the round trip time  table 1 i did not really understand what nonasymptotic  asymptotic mean, as the bigo notation used here should by definition be asymptotic.  corollary 1 the optimal number of chains  page 4, line 34 the abbreviation sgld is not defined in this paper.  page 4, line 36 similarly to  equation 6 the sign of the last term should be .", "accepted": 1}
{"paper_id": "iclr_2022_rxF4IN3R2ml", "review_text": "this paper proposes a number of improvements to the previouslypublished transformerbased mqforecaster model for multihorizon forecasts on time series data. they show strong empirical improvements in terms of accuracy and excess forecast variability on a large proprietary dataset, as well as four public datasets. concerns were raised about the relatively incremental changes to the mqforecaster model this work is based on, lack of ablations on public data and, relatedly, inability to reproduce results on the proprietary data.", "accepted": null}
{"paper_id": "iclr_2022_R-I5CUDOAp7", "review_text": "the paper proposed a sketching algorithm for empirical risk minimization erm for linear regression and classification. the technique is based on lsh with nonstandard hash functions. the reviews indicate that the paper is well written and easy to follow. however, there are several concerns raised regarding its quality. a major one regards the novelty of the paper. mtpw the technique novelty is limited since previous work coleman  shrivastava, 2020 has used lsh to approximate kernel density estimation on streaming setting., 7lqm my review of the theoretical results and data structure design is that the results are believable and seem correct, but lack technical novelty. other than using nonstandard hashing functions, what distinguishes the storm sketch from the race sketch? an additional concern is a claim of weak experimental evidence. there seems to be a need for more thorough experiments isolating different components rather than the system as a whole, and in addition the bottom line results provide only a slight lift over a naive baseline 7lqm the experiments suggest that using the storm estimator is only slightly better than returning the mean of your data.. whether it is the case that the techniques should be improved or that these concerns could be addressed by improving the presentation of the paper, the conclusion is that the paper now is not ready to be published.", "accepted": 0}
{"paper_id": "iclr_2022_lvM693mon8q", "review_text": "reviewers have all agreed that this paper studied an important problem and made valuable contributions. the goal is to reduce the communication costs of federated learning where the data are stored in different parities based on subsets of features. the paper developed the theory to show guaranteed convergence and provided empirical evaluations to validate the theory. on the other hand, compared with existing literature, reviewers feel that the novelty of this submission appears limited and the improvements seem to be incremental. reviewers appreciate the authors efforts in conducting the detailed rebuttals and providing an improved manuscript. we hope the authors would continue to improve the paper based on reviews, when they prepare for their future submission.", "accepted": null}
{"paper_id": "iclr_2022_8uqOMUHgW4M", "review_text": "in this paper, the author present a method for learning a shared latent space between the fmri activity of multiple individuals processing the same stimulus. the method consists of an autoencoder with a single encoder and subjectspecific decoders which is specifically regularized to decouple common and shared representations. this paper generated a lot of discussion between the reviewers and the authors, as well as between the reviewers. in light of these discussion, i cannot recommend acceptance at this point, as the paper is not ready. the main concerns were 1 about how the results and improvement are evaluated statistically, 2 that the baselines chosen were not strong enough and did not include existing approaches neural or nonneural and relatedly 3 that the paper was not framed correctly within the existing literature on finding shared spaces between participants, which would help with determining and understanding the novelty of the proposed approach. some other smaller points were made by the reviewer can also strengthen the paper for a future submission in a neuroscience or machine learning venue.", "accepted": 1}
{"paper_id": "iclr_2022_YRDlrT00BP", "review_text": "this paper propose a way to make minibatch optimal transport mot more efficient by computing an optimal assignment in the ot sens and us this assignment to compute instead a hierarchical ot loss bombot  that can be used instead of the mot loss. the authors discuss how the equivalent ot plan with bombot is much more sparse, and how the proposed approach is actually not biased when the number of minibatches krightarrow infty . numerical experiments show that the proposed method allows a gain in performances in applications such as generative modeling, domain adaptation, color transfer and approximate bayesian computation. the paper originally got borderlinenegative scores from the reviewers. while the reviewers acknowledged that the idea is interesting, they had some concerns about the theoretical results strength, some missing baselines and discussions in the numerical experiments. the authors did a detailed reply that clarified some problems. the new numerical experiments with muot were also greatly appreciated by the reviewers but they also raised some questions about the paper. some concerns detailed below about the comparison with mot appeared during the reviewers discussion. despite the new information, the reviewers reached an agreement that this paper is interesting but needs more work and another round of reviews before acceptance. for theses reasons the ac recommends a rejection for this paper. more details and suggestions below  while it is clearly not the objective of the paper a discussion about the proximity of the average plan to the exact ot plan is interested. also a short numerical experiments showing that the bombot average plan is closer to the exact plan than mot would be a good illustration of the better performance of bombot. this seems more important for the paper than the color transfer experiments that is kind of a toy problem.  after checking the definition in the paper and discussion between reviewers it appeared that the comparison with mot is a bit unfair due to the reformulation of the problem in 1. indeed in the usual formulation, k pairs of independent minibatches are used and the ot is done on those pairs a sum of k ot not on all the possible pairwise permutation as in definition of mot in equation 1. in other words in mot the batches are supposed to be independent which is not the case in the proposed formulation it is equivalent in the population case though. it means that in practical application, for the same computational complexity k2 ot computed, mot actually uses k2m independent samples on each distribution whereas the bombod and the mot defined in equation 1  use km samples . by implementing mot as in 1 they actually prevent mot to explore the dataset as its original formulation does. this means that all the experiments should be done either with the original mot implementation of both the original and 1 in addition to bombot. the proposed method will proably work better but the current experiment do not allow this fair comparison.  the theoretical result need more discussion and justification. for instance mot converges to its population value in om12n12k12 that is independent from the dimensionality d, but the authors prove the concentration of bombot in of om12n1d which is clearly a problem for large d. also the dependence on k of the convergence would be important since bombot is well defined is true only in the population case where k is large. note that the claim that it is well defined and hence better is also a bit dubious because it is well defined for kinfty, which is also the case for mot when minfty. both m and k large will lead to not practical optimization problems so they are comparable except that mot converged to the true ot plan when mrightarrow infty which is not the case for bombot.  while the contribution of the paper in indeed a methodological method and does not require to be state of the art on all applications the numerical experiments should be improved. first as discussed above the comparison with mot is actually unfair an do not correspond to what in done in practice where all mini batches are independent. mot should be implemented with k2 truly independent minibatches.  second , the authors use approximate w2 on two of the gan dataset and fid on the third. this is problem because approximate w2 is not defined in the paper. fid is the standard performance measure and should be used for all dataset.  third the novel experiments comparing also raises a lot of questions. muot is far better than bombot suggesting that unbalanced ot can compensate for the limits of mot far better than bombot itself. yes there is a slight increase in performance for ebombuot over muot but is is so small 0.08  that it is hard to find them significant, especially since we have no variance. this result that is provided only for da application actually suggest that the competitor of bombot is muot and not mot so it should also be part of the comparison in the other experiments. the authors talk in their replay about the limits of muot but stating that the experiences are not done in the paper is not an excuse for evaluating this clear competitor on other problems and showing numerically these limits.  finally in the current version of the paper puts a lot of things in the annex that make the paper clearly not self content. some experiments could go in annexsupp for instance the color transfer to make place for more details in the main paper. note that it is not one of those comments above that lead the the reject decision but the sum of them that clearly show that the paper needs more work.", "accepted": 0}
{"paper_id": "iclr_2022_GU11Lbci5J", "review_text": "i agree that reviewer vxer was confrontational and abusive, especially in the response to the authors rebuttal, and believe that some form of sanction or reprimand is appropriate. that said, i do think that performance should be evaluated on both convergence rate and generalization. figure 3 does suggest some improvement on generalization for deep versions of resnet without batch normalization. the three less offensive reviewers all indicated weak acceptance. one reviewer pointed out the weakness of only getting positive results on deep versions of resnet with batch normalization removed. results on transformers, where adam is typically used, would be more compelling. this is my primary issue with the paper. it has not demonstrated improvement in the standard practice of resnet with batch normalization and has not presented experiments on transformers. the theoretical analysis is not aimed at explaining why the improvement is only observed on deep resnets with batch normalization removed or why l2 regularization seems to be of no value when batch normalization is present. i understand that these are very difficult questions. the paper has no champion and i am personally concerned about the significance of the contribution.", "accepted": 0}
{"paper_id": "iclr_2022_lgOylcEZQgr", "review_text": "this paper tackles a smallbatch online unsupervised learning problem, specifically proposing an online unsupervised prototypical network architecture that leverages an online mixturebased clustering algorithm and corresponding em algorithm. special features are added to deal specifically with the nonstationary distributions that are induced. results are shown on more realistic streams of data, namely from the roamingrooms dataset, and compared to existing selfsupervised learning algorithms including ones based on clustering principles e.g. swav. overall, the reviewers were positive about the problem setting and method, but had some concerns about hyperparameters hyzm, cvrn, ljvy and motivation for the specific setting where the method excels compared to other methods not designed for such a setting hyzm, cvrn, i.e. smallbatch setting, where it is not clear where the line should be drawn in terms of batch size and memory requirements with respect to performance differences between the proposed approach and existing selfsupervised methods. importantly, all reviewers had significant confusions about all aspects of the work ranging from lowlevel details of the proposed method to the empirical setting and evaluation including for competing methods. after a long discussion, the authors provided a large amount of details about their work, which the reviewers and ac highly appreciate. however, in the end incorporating all of the feedback requires a major revision of the entire paper. even the reviewers that were more on the positive side cvrn and ljvy mentioned it would be extremely beneficial for this paper to be significantly revised and go through another review. since so many aspects were confusing, it is not clear to the ac that the underlying method, technical contributions, and other aspects of the works had a sufficient chance to be evaluated fairly, given that much of the review period was spent on clearing up such confusion. in summary, while the paper is definitely promising and tackles an important area for the community, it requires a major revision and should go through the review process when it is more clearly presented. as a result, i recommend rejection at this point, since it is not ready for publication in its current form.", "accepted": 0}
{"paper_id": "iclr_2022_fUhxuop_Q1r", "review_text": "this work proposes to study the generalization capabilities of rl algorithms using contextual decision processes cdps. cdps allows to study generalization similar to how we are used to studying generalization in supervised learning, and can separate the generalization capabilities of a learned agent wrt observation, state and action space. this proposed measure for generalization is used in an extensive study on grid world domains to evaluate existing algorithms that aim to improve generalization. strengths this manuscript is well written and the work is well motivated a novel perspective and way of measuring generalization of learned agents an empirical study that compares existing algorithms on how well they generalize in observation, state, action spaces weaknesses some clarity issues existed missing links to existing literature, experimental details empirical study is out of necessity limited to small scale grid worlds no deeper analysis of the results, why do algorithms perform the way they do from this novel perspective of generalization, which makes it hard to understand how one could choose an algorithm for larger scale settings which dont allow for this type of analysis rebuttal the authors updated the paper to improve the parts that were unclear, and had an extensive discussion with reviewers on the intuition of the results and converging on takeaways. unfortunately, this intuition and takeaways have not been added. summary while i understand the authors wish to not speculate on intuition, i agree with the reviewers that without experimentally supported takeaways the provided analysis is incomplete. understanding why each algorithm achieves the performance they do wrt this novel way of measuring generalization is the only way the proposed method to measure generalization and the evaluation can be used to draw conclusions about more general problem settings. thus, although this is a very promising direction on an important problem, the manuscript is not ready yet for publication.", "accepted": 0}
{"paper_id": "iclr_2022_ht61oVsaya", "review_text": "this paper investigated online safe reinforcement learning problem in the constraint mdp setting. by introducing safety agent and task agent, the authors translate the rl problem into a markov game. the ac agrees with all reviewers that there is a lack of theoretical analysis and experimental comparisons with existing benchmarks. it has not reached the bar of iclr papers.", "accepted": 0}
{"paper_id": "iclr_2022_7ADMMyZpeY", "review_text": "in this paper, authors introduce two properties of feature representations, namely local alignment and local congregation, and show how these properties can be predictive of downstream performance. the paper has a heavier focus on providing theoretical statements using these properties but authors also empirically evaluate their suggested method. strong points  the paper is wellwritten and easy to follow.  the proposed concepts local alignment and local congregation are intuitive.  the theoretical statements and their proofs are correct.  the proposed metric shows some advantage against a few baselines.  prior work on feature representations and transferability are discussed. weak points  the connections to prior work on knearest neighbors and linear classifiers are not properly discussed. this is very important because authors assume that the network that outputs the feature representations is trained on a different data and they reduced the analysis to that of a binary linear classifier. hence, all classical learning theory results on binary classifiers apply in this setting. furthermore, knn methods and analysis can be simply applied on the features as well. in light of this and the lack of discussion on this matter, the significance of the theoretical and empirical results are not clear.  the main proposed properties could be improved further. it looks like the defined properties local alignment and local congregation could be improved by merging them into one property about separability of data? the current properties are sensitive to scaling which is undesirable given that classification performance is invariant to scaling of the features. it seems like local congregation is mostly capturing the scale so some normalized version of local alignment might be able to capture the main property of interest.  the theoretical results in their current form are not very significant. one limiting factor on the theoretical results is that since the analysis is done only on the classification layer, it does not say anything about the relationship of the upstream and downstream tasks. but perhaps the most important limitation is that the properties are defined based on the downstream task distribution as opposed to downstream training data. that makes it difficult to measure them in practical settings where we have a limited number of data points. classical results on learning theory avoid this and only use measures that depend on the given training set.  the empirical evaluation could benefit from stronger baselines authors mentioned we therefore consider only baselines that make minimal assumptions about the pretrained feature representation and the target task and hence avoided comparing to many prior methods. however, i think the appropriate approach would be to compare the performance of the proposed method to strong baselines but then explain how they differ in terms of their assumptions, etc. moreover, there are other simple heuristic baselines to consider, eg. knn which is not computationally expensive in the fewshot settings or a classifier that is trained by initializing it to be the sum of feature vectors in the first class assuming binary classification minus sum of feature vectors in the second class and doing a few sgd updates on it. therefore, i believe authors could improve the empirical section significantly by taking these suggestions into account. final decision rationale this is a borderline paper. while the paper has a nice combination of theoretical and empirical contributions, both theoretical and empirical contributions have a lot of room for improvement and a clear path to get there as pointed above. in particular, i believe having either strong theoretical contributions or strong empirical contributions would have been enough for acceptance and i hope authors would take the above suggestions into account and submit the improved version of this work again!", "accepted": 1}
{"paper_id": "iclr_2022_jgAl403zfau", "review_text": "this paper proposes a hardwareaware pruning method which structurally prunes the given deep neural networks to retain their accuracy while satisfying the latency constraints. specifically, the authors formulate the latencyconstrained pruning problem as a combinatorial optimization problem to find the optimal combination of neurons to maximize the sum of the importance scores, and propose an augmented knapsack solver to solve it, as well as a neuron grouping technique to speed up the training. the proposed method is validated for its classification tasks on two devices, namely titan v and jetson tx2, and for object detection performance on titan v, and is shown to achieve superior accuracylatency tradeoff compared to existing pruning methods, including latencyaware ones. the paper received split reviews initially, and the following is the summary of the pros and cons mentioned by the reviewers. pros  the proposed formulation of the latencyconstrained pruning problem as a constrained knapsack problem is novel.  the method achieves competitive performance against existing latencyconstrained pruning methods.  the paper is written well, with clear motivation and descriptions of the proposed method. cons  the idea is not very exciting since posing pruning as a combinatorial optimization problem, or a knapsack problem is not new, and the proposed method only adds in additional latency constraints.  the title hardwareaware is vague and misleading since what the authors do are latencyconstrained pruning.  the experimental validation is only done on two devices, which makes the method less convincing as a hardwareaware method and how it generalizes to other devices e.g. cpu, fpga  use of lookup tables to obtain the latency constraints is not novel, has a limited scalability, and is inefficient.  missing discussion of design choices. during the discussion period, the authors cleared away some of the concerns, which resulted in two of the reviewers increasing their scores. however, one reviewer maintained the negative rating of 5, and the positive reviewers were still concerned with limited novelty. i believe that this is a good paper that proposes a neat solution for latency pruning, which may have some practical impact. however, the novelty of the idea is limited, as pointed out by the reviewers. the use of lookup tables also does not seem to be an efficient solution for adapting to edge devices for which the collection of latency measurements could be slow. the experimental validation on only two devices of the same type gpu also seems insufficient, as how the method generalizes to diverse devices is uncertain. it would be worthwhile to consider using a latency predictor e.g. brpnas dudziak et al. 20, and perform experimental validation on diverse hardware platforms e.g. cpu and fpga. comparing against recently proposed hardwareaware nas methods could be also interesting, as there has been a rapid progress on the topic recently. thus, despite the overall practicality and the quality of the paper, the paper may benefit from another round of revision, since both the method and the experimental validation part could be improved. dudziak et al. 20 brpnas predictionbased nas using gcns, neurips 2020", "accepted": null}
{"paper_id": "iclr_2022_66kgCIYQW3", "review_text": "the authors consider the task of interpretable video classification. first, a set of binary concepts is predicted, and these concept features are then used for classifying a video. the set itself is automatically generated from natural language descriptions, instead of relying on expert annotations. the authors collect two datasets to validate the proposed approach and show that the model can match the performance of a standard video classification model, while being interpretable. the reviewers felt that the paper was well written and that the method and empirical results were clearly outlined. they also appreciated the empirical results whereby interpretability doesnt necessarily come at the expense of accuracy and consider interpretability as a desirable property. the main reason for the borderline results is the heuristic nature of the proposed automatic concept labeling and the empirical evaluation against alternative baselines. in particular, one needs to show that the proposed method generalises to other datasets. secondly, one of the main contributions, namely the automatic concept extraction, still ends up requiring human annotation in the form of narrations, and this cost should be quantified and contextualised. i suggest the authors address these points and resubmit.", "accepted": null}
{"paper_id": "iclr_2022_IEsx-jwFk3g", "review_text": "this paper proposes a graph neural network model to estimate latent dynamics in the human brain using functional magnetic resonance imaging fmri and diffusion weighted imaging dwi. the representation is tested on a classification task. while reviewers acknowledge the importance of this application, various concerns have been raised and partially addressed. the work focuses on graph deep learning and offers limited evidence of its superiority over more traditional ml or non graph based deep learning. besides the methodological novelty is unclearly argued, which is not ideal for the audience of a conference like iclr. for all these reasons, this work cannot be endorsed for publication at iclr 2022.", "accepted": null}
{"paper_id": "iclr_2022_xs-tJn58XKv", "review_text": "the idea of learning unstable features from source tasks to help learn stable features for a target task is interesting and wellmotivated. as the proposed method and its theoretical analysis of learning unstable features from tasks are an incremental extension of an existing work bao et al. 2021, the technical contributions line in applying the idea of stable and unstable features learning to the setting of transfer learning. therefore, the evaluation of this work is focused on the effectiveness of the proposed method in the transfer learning setting. in transfer learning, one major goal is to make use of knowledge extracted from source tasks to help learn a precise target classifier even with a few or no labeled examples of the target task. it would be more convincing if experiments are conducted to show how the performance of the proposed method changes when the size of labeled data of the target task changes. this is to verify whether the exploitation of unstable features can help to learn a stable classifier for the target tasks more efficiently i.e., with fewer labeled examples. in addition, as some baseline methods used for comparison do not need to access any labeled data of the target task like unsupervised domain adaptation or domain generalization approaches, it is not fair to conduct comparison experiments in the setting where there are sufficient labeled examples of the target task since the original designs of such baselines may fail to fully exploit label information in the target task. another concern is whether the proposed method is realistic for realworld transfer learning problems. though in the rebuttal, the authors provided experimental results on a natural environment celeba, the constructed transfer learning problem is more like a toy problem. indeed, there are many transfer learning benchmark datasets that contain multiple domainstasks. it would be more convincing if experiments are conducted on those datasets. by considering the above two concerns, this paper is on the borderline. my recommendation is a weak rejection based on the current form of this paper. note that as some references listed by reviewers rjhj and j8m5 are not really related to the proposed research here, the novelty of the proposed method compared with those references is not taken into consideration to make this recommendation.", "accepted": 0}
{"paper_id": "iclr_2022_FASW5Ed837", "review_text": "the reviewers have the following remain concerns 1. the bounded function value assumption is strong. note that the previous works for sgd and sgdm for other lr schemes do not necessarily need this assumption, hence it may be unfair to compare with existing results and say that this work has improvements for nonmonotonic schemes. the authors also agree that it is not easy to prove and remove this assumption. 2. the novelty is limited, and the contributions are somewhat incremental. the bandwidth step size scheme was already introduced in a previous work with a very similar setting. the convergence rate for the proposed lr scheme is the same as previous works for other schemes or only better by a logarithmic term, which makes the results incremental. 3. some of the claims are not well supported. for example, the reviewers comment that it is not clear how the proposed bandwidth step size can help to escape local minima. although the authors aim to show this empirically, the toy setting is not strong enough to conclude the superior performance of the proposed scheme. we encourage the authors to improve their paper and resubmit to another venue. here are the related suggestions 1. the authors might try to investigate and provide a rigorous proof of how the nonmonotonic step size can help to escape local minima. it also helps to characterize the effectiveness of each cyclic rule cosine triangular or any other and make clear what property cosinelinear rules or bandwidth or nonmonotonicity contributes most in the good performance of a lr scheme. 2. it is better if the assumption on the bounded function value can be removed. in addition, a theoreticalempirical analysis on the generalization performance of the proposed scheme might also be helpful.", "accepted": 0}
{"paper_id": "iclr_2022_GMYWzWztDx5", "review_text": "this submission proposes a few small changes to the preln transformer architecture that enable training with higher learning rates and therefore can result in faster convergence. the changes include the addition of two layer norm operations as well as a learnable head scaling operation in multiheaded attention. the proposed operations add only a small computational overhead and should be simple to implement. experiments are conducted on language modeling and masked language modeling, with improved results demonstrated at various scales and according to various evaluation procedures. the paper also includes a good amount of ablation study as well as some analysis. reviews on the paper were mixed, and a great deal of changes were made to the paper during the rebuttal period. to summarize the concerns and recommendations, reviewers requested  better connection between the proposed changes and the purported issue gradient scale mismatch between earlylate layers  better analysis of why gradient scale mismatch is a major issue and investigation of where it comes from  better comparison to existing techniques that allow for higher learning rate training of transformers  additional experiments on different model types and ideally different codebasesimplementations i think overall this is a solid submission, since it proposes a simple change that is reasonably likely to be helpful or at least not harmful. however, i think that there are enough concerns with the current draft and there were enough changes made during rebuttal that this paper should be resubmitted to a future conference. i would suggest the authors take the final updated form from this round, add additional motivationanalysisexperiments, and resubmit, and i suspect a positive outcome.", "accepted": 1}
{"paper_id": "iclr_2022_ONTz_GFWkFR", "review_text": "this paper proposes a way to train gaussian variational autoencoders that does not require the computation of empirical expectations but instead approximates the decoder network by its taylor series. results on 3 datasets show the competitiveness of the approach. based on the limited novelty of the approach, three out of 4 knowledgeable reviewers recommend rejection and i agree. variational autoencoders are simply doing variational inference in a specific model and, as one of the reviewers has pointed out, these types of approximations have been exploited in the inference world before the popularization of the reparameterization trick for many years. methods, where we replace a term in the joint distribution with a simpler function, are known in the variational inference world as local variational approximations, see, e.g. murphys book machine learning a probabilistic perspective, 2012, sec. 21.8 as a reference. the community has departed from such approaches as using the reparameterization trick is unbiased, more general e.g., not limited to gaussian encoders and allows for highly automated methods no need to do derivations on a casebycase basis. nevertheless, i encourage the authors to thoroughly explore the literature on variational inference with regards to these types of approximations. it may well be the case that, in the future, we revert back to these methods if they perform well in practice with modern architectures. for this, more comprehensive evaluations and comparisons are needed.", "accepted": 1}
{"paper_id": "iclr_2022_2hMEdc35xZ6", "review_text": "the paper proposes a gan based method for synthesizing various types of defects as foreground on different product images background. the method builds upon starganv2, and adds the cyclecontent consistency loss and classification loss between foreground and background. while the paper considers an important problemapplication, the reviewers found it lacking sufficient novelty for publication. the paper will be more suited for publication at an application oriented venue.", "accepted": 0}
{"paper_id": "iclr_2022_d20jtFYzyxe", "review_text": "casting domain generalization as a ratedistortion problem and developing an informationtheoretic approach to solving it looks like an interesting idea. while the proposed method is technically sound, the assumption made in the proposed method is too strong to hold in realworld applications. though in the rebuttal the authors provided additional experiments on two benchmark datasets, reviewers concerns about the strong assumption made in the proposed algorithm still remain. to address this issue, i think besides conducting more extensive experiments, the authors also need to analyze when the assumption does not hold in practice, why the proposed algorithm could still perform well compared with other domain generalization methods. in summary, this is a borderline paper below the acceptance bar of iclr.", "accepted": 0}
{"paper_id": "iclr_2022_vRhkfX8G_H9", "review_text": "this work proposes to train largescale graph neural networks by replacing the moving averages used in the stochastic compositional optimization sco framework with sparse moving averages. this reduces the memory required for sco, allowing their algorithm to scale to larger graphs. the consensus is that the approach is reasonable, but incremental both in the change over scgd and the change in the analysis. more importantly, the reviewers identified several samplingbased methods for scaling up training of gnns that are important baselines for the proposed algorithm; the relative merits of the method against these approaches should be established with further experiments.", "accepted": 0}
{"paper_id": "iclr_2022_Gpp1dfvZYYH", "review_text": "as pointed out by some reviewers, the proposed method basically puts progressive training in the federated context. the theoretical analysis only concerns the centralized or nonfederated setting and thus give no insight or guidelines for progressive training in federated learning. the main advantage of saving communication mainly comes from the simple observation that less parameters are computed and communicated during each round before the full endtoend stage. however, this may cause extra overhead in hyperparameter tuning including number of stage, learning rate schedules and stagewise warmup. despite its potential effectiveness in practice, the current version of the paper falls short of the acceptance bar due to the weakness in novelty and relevant theory for federated learning.", "accepted": 1}
{"paper_id": "iclr_2022_H_qwVb8DQb-", "review_text": "in general, the reviewers recognized the importance of the question and the innovation in the proposed algorithm, but do not seem to be super excited about the overall contribution of the paper. one or two reviewers did not seem to respond authors response after the acs reminder. the ac read the reviews and responses and observed that the main concern appears to be the empirical performance  the improvements are not as strong for larger models or if more computational time is allowed. modern models are indeed typically large, and it would be good to discuss this point more thoroughly. if the works focus is limited resource setting, the paper might want to state that upfront. indeed, one reviewer is still concerned postrebuttal about a clocktime comparison. given these considerations, the ac will recommend reject for the paper but encourage the authors to resubmit to a top venue conference after revising the paper.", "accepted": null}
{"paper_id": "iclr_2022_rqolQhuq6Hs", "review_text": "the paper studies rate at which sgd escapes local minima and provides a potential justification for the flat minima observation. reviewers agree that the paper studies an interesting problem and provides a nice result. but it seems like that paper in its current shape is not ready for publication at iclr. issue is that the papers writing is not up to bar, and requires a fair bit of work. in particular, the paper doesnt define the key quantities formally, doesnt provide all the assumptions in one place and justify why they might be reasonable. finally, it would be great if the final result about escape rate is provided clearly with a self contained theoremlemma that definedescribe most of the key quantities in the rate.", "accepted": 0}
{"paper_id": "iclr_2022_obi9EkyVeED", "review_text": "the paper introduces dropout probabilities which are adaptive to the similarity of model parameters between clients. the reviewers liked the idea, however missed several aspects, such as a convergence analysis or at least discussion, as well as an analysis of additional cost of the adaptive step, and finally several concerns on the strength of the experimental setup and benchmarks. unfortunately consensus among the reviewers is that it remains below the bar even after the discussion phase. we hope the detailed feedback helps to strengthen the paper for a future occasion.", "accepted": 0}
{"paper_id": "iclr_2022_OqlohL9sVO", "review_text": "the paper proposes a new method to combine global and local image features, targeted at image retrieval applications. the main idea is a model branch where both spatial and channel attention are used. the local feature branch undergoes supervision directly and this branchs output is concatenated to the global feature branchs output in order to eventually produce the final image embedding. the reviewers appreciated the care in the evaluation ablative analysis and the promise of the approach compared to existing baselines. the reviewers also expressed concerns about several claims, for instance that the proposed approach is able to learn homography transformations, the quality of the exposition, and missing baselines. the reviewers also pointed out that several parts of the paper were hard to follow and important details were missing. the authors submitted responses to the reviewers comments. after reading the response, updating the reviews, and discussion, the reviewers considered that the concatenation of local features with global ones works does not mean at all that some geometric transformation is learned and the justification provided for omitting baselines suggested by the reviewers were unconvincing. the feedback provided was already fruitful, yet major issues still remain. we encourage the paper to pursue their approach further taking into account the reviewers comments, encouragements, and suggestions. the detailed feedback lays out a clear path to generate a stronger submission to a future venue. reject.", "accepted": 0}
{"paper_id": "iclr_2022_kHkWgqOysk_", "review_text": "all reviewers agreed that this work on ood and pseudolabeling presents interesting and strong results. the authors rebuttal has addressed some of reviewers concerns. based on the current review and discussion, there are still several major concerns towards the expensive computational cost introduced by the clustering method, the lack of discussion around how the proposed work can be incorporated into ssl methods, and the sensitivity towards the selection of k.", "accepted": 0}
{"paper_id": "iclr_2022_Ih7LAeOYIb0", "review_text": "the paper proposes a new architecture named iterative memory network imn to encode long user behavior sequence for recommendations. reviewers appreciate the clarity of the writing as well as practicality and the ol complexity of the proposed architecture, however do raise questions on novelty. different design choices employed in the paper are not well explained. the rebuttal was not able to convince the reviewers to accept the work at this venue, but reviewers do feel the paper could fly in an application oriented venue.", "accepted": null}
{"paper_id": "iclr_2022_wMXYbJB-gX", "review_text": "the authors proposed a twostage algorithm for exploiting label smoothing and provided some analysis based on how label smoothing may have reduced the variance in the stochastic gradient. while the authors provided substantial experiments to justify their work with additional ones during the response stage, none of the reviewers was very excited in the end, for obvious reasons perhaps a the twostage algorithm is a straightforward combination of existing practices first run with label smoothing and then run without label smoothing, without any new, interesting insight from the authors side; b the analysis is a direct consequence of the authors assumptions. basically, if label smoothing reduces variance, sgd would converge faster and vice versa, which is nothing surprising or insightful. the key is to understand when and how any particular way to smooth the label would lead to significant reduction of the variance, which the authors did not provide any guidance or insight other than offering some empirical results. overall, we do not believe this work, in its current form, adds significant value to our understanding of label smoothing.", "accepted": null}
{"paper_id": "iclr_2022_L_sHGieq1D", "review_text": "this paper presents a domain generalization method for semantic segmentation. the model is trained on synthetic data source and is tested on unseen real datasets target. the authors propose a simple data augmentation method, advstyle, generating unconstrained adversarial examples for the training on the source domain. there was no consensus on the method among the reviewers. several issues have been raised. after rebuttal and discussion, no one really changed herhis mind. the motivation of why focus just on driving scenes is still questionable. definitively, it could be interesting to investigate further why it is not straightforward to have gains on other kinds of scenes. finally, we encourage the authors to address the raised concerns regarding the discussion with previous works and the comparisons for future publication.", "accepted": 1}
{"paper_id": "iclr_2022_DXRwVRh4i8g", "review_text": "the authors present a method for creating a curriculum for goalconditioned reinforcement learning. in particular, they propose to use reachability traces to define a sequence of subgoals that aid learning. during the review process, the reviewers mentioned the novelty of the proposed approach and the intuitive explanations provided by the authors. however, the reviewers also pointed out that the experiments could be more thorough, errors in the theoretical justification of the method as well as simplicity of the evaluation environments, among others. some of the reviewers increased their score after the authors rebuttal but it was not enough to advocate for acceptance of the paper. i encourage the authors to incorporate reviewers feedback in the next version of the paper.", "accepted": 0}
{"paper_id": "iclr_2022_TLnReGgZEdW", "review_text": "the paper discusses new rl algorithms for solving large. tsp instances. the algorithm is novel and the problem is important however certain technical questions regarding the soundness of the algorithm were raised. furthermore, it seems that despite much larger computational time, the algorithm provides only very moderate gains over previous baselines. finally, it is not clear how the proposed methods e.g. equivariance can be applied outside of the tsp problem scope. thus the concern is the limited impact of the method on the field. the authors addressed some of the concerns of the reviewers in the rebuttal however it is still not clear 1 how the presented mechanism can be applied for other combinatorial problems beyond tsp and therefore how useful it can be for the machine learning community, 2 how novel the paper is the use of equivariance is as direct as in the regular graph neural network setup. furthermore, the experiments show that the deep learning approach to tsp is still not competitive with standard nonmachine baselines. thus it is not clear whether the proposed algorithm is a right approach to solve this problem, even though it beats other deep learning techniques. the paper is very well written though and the presented method is definitely of value to the research community working on the tsp. therefore it seems that at this point the paper is more suited for one of the mathematical journals on combinatorics and graph theory.", "accepted": 1}
{"paper_id": "iclr_2022_MUpxS9vDbZr", "review_text": "this paper studies whether the bellman error is a good metric to reflect the quality of value function estimation, focusing on finitesample offpolicy data sets. both theoretical analyses and empirical experiments have been provided, showing that the bellman error is often not the right metric to consider. however, while i appreciate the authors theoretical attempts, the current theoretical contributions are not deepsignificant enough. as the reviewers mentioned, the failure of the direct use of brm is not surprising given the insufficiency of data namely, no algorithm can make predictions on completely unseen regions unless further modeling structure is present. the authors might want to further strengthen their theory along this important direction.", "accepted": 1}
{"paper_id": "iclr_2022_IxCAF8IMatf", "review_text": "the paper proposes a framework for distilling deep directed graphical models where the teacher and student models have the same number of latent variables z. the key idea is to reparameterize both models in terms of standardized random variables epsilon with fixed distributions and train the student to match the conditional distributions of the observed variablestargets given the values of the standardized rvs epsilon. the approach aims to avoid error compounding that affects the local distillation approach, where the student is trained to match conditional distributions of the teacher model without the above reparameterization. to deal with discrete latent variables and vanishing gradients the authors augment the target matching loss with the latent distillation loss that matches the local distribution for each z_i given the standardized variables epsilon it depends on. positives the paper tackles an important problem. the idea of using reparameterization for distillation in this way makes a lot of sense for continuous latent variables and could be impactful. the experiments provide some evidence in support of the idea. negatives there are considerable issues with the clarity of writing for example, it is really not clear how and why the method is supposed to work for discrete latent variables. the explanation provided by the authors in their response to the reviewers was helpful but still not clear enough. the fact that the teacher and student models need to have the same number of latent variables and perhaps even the same structure is a big limitation of the method given the claim of its generality, and thus needs to be clearly acknowledged and discussed. for example, the method cannot be used to train a student model with fewer latent variables than the teacher, which seems like a very common use case. the experimental evaluation is extensive but insufficient, in large part due to the evaluation metrics. given that vaes are trained by maximizing the elbo and distilled by minimizing a sum of kls, it makes sense to also evaluate them based on the elbo rather than solely on the fid, is done in the paper. the vrnn experiment would be much more informative if it included a quantitative evaluation e.g. based on elbo. in summary, the paper has considerable potential but needs to be substantially improved before being published.", "accepted": null}
{"paper_id": "iclr_2022_XpmTU4k-5uf", "review_text": "this paper has been reviewed by four experts. their independent evaluations were all below the acceptance threshold citing various issues ranging from disconnection between stated goals of the presented work and the means in which the approach was evaluated, to doubts about the scalability of the proposed approach, to the lack of clarity regarding the actual novelty of the approach given some key missed references, to name a few items of criticism. most reviewers were impressed with the empirical performance achieved in the conducted experiments, and one of the reviewers raised their mark in response to the authors rebuttal. yet, the overall evaluation places this work as it stands now below the threshold for iclr acceptance. i would like to encourage the authors to continue pushing their promising endeavor and systematically incorporating the feedback received here to improve the overall quality of this work.", "accepted": 0}
{"paper_id": "iclr_2022_bmGLlsX_iJl", "review_text": "this paper proposes a data imputation method for mcar and mar data by combining em and normalizing flows. the paper is clearly written. the idea is interesting and they show better performance compared to mcflow and competing methods on ten multivariate uci data, mnist and cfar10 image data. issues regarding limited novelty compared to mcflow was raised. issues regarding the validity of assumption 2 on the dependencies in the latent space and observation space was also raised.", "accepted": 0}
{"paper_id": "iclr_2022_iGffRQ9jQpQ", "review_text": "this paper proposes a new method for the important problem of semisupervised learning. this method relies on an auxiliary task, label observability prediction, to weight the examples according to the confidence in their pseudolabels, so as to avoid the propagation of errors encountered in selftraining. limited experiments show that the proposed method can compete with other methods in terms of performance or training time. on the positive side, all evaluators agree on the potential value of the proposed approach, which is generic in nature. on the negative side, the experimental evaluation, although strengthened during the discussion, is not yet strong enough to have really convinced of the real merits of the method. in particular, comparisons with the state of the art still need to be improved. in addition, the paper would benefit from some rewriting, in particular of the mathematics e.g. the d notation for task b should be avoided as suggested by one reviewer, there is a misplaced partial derivative in equation 6. the authors could also simplify their derivation by using the envelope theorem. i therefore recommend rejection, with an encouragement to strengthen the experimental part, and to improve the derivation of the proposed method.", "accepted": 0}
{"paper_id": "iclr_2022_U9zTUXVdoIr", "review_text": "this paper proposes a more generalized form of certified robustness and attempts to provide new results on applying randomized smoothing to semantic transformations such as different types of blurs or distortions. the main idea is to use an imagetoimage neural network to approximate semantic transformations, and then certify robustness based on bounds on that neural network. the authors provide empirical results on standard benchmark datasets like mnist and cifar showing that their method can achieve improved results on some transformations compared to prior work. the review committee appreciates the authors taking the time to attempt to respond to the concerns of all reviewers, and for updating and improving their work during the rebuttal process. the committee is glad to see that they do provide empirical evidence of improvement to commoncorruption robustness, compared to augmix one of the stateoftheart approaches for standard commoncorruption robustness and tss. however, the reviewers still have concerns about the novelty of the paper. the main novelty is not improvement for resolvable transformations prior works that the authors cite perform about the same or better, but rather, is the ability to handle nonresolvable transformations. the reviewers agree that robustness to nonresolvable transformations is important; however, the reviewers think certified robustness to nonresolvable transformations is not meaningful, because they are only being certified with respect to a neural network that is trained to approximate those nonresolvable transformations. without mturk studies to confirm how good the neural networks nonresolvable transforms are, the reviewers do not find certified robustness here meaningful.", "accepted": 1}
{"paper_id": "iclr_2022_fGEoHDk0C", "review_text": "the paper formulates resnet like classifiers as the evolution of a base classifier through an operator corresponding to a pde, up to a given final time. using a set of assumptions on the desired properties of the flow operator, the authors show that it can be obtained as the solution of a convectiondiffusion equation. this generalizes ideas developed e.g. for neural odes. the authors provide several examples showing that their formulation encompasses regularization methods proposed for deep nns. they further provide robustness guaranties for a classifier defined according to their framework. they introduce an algorithm based on a restricted version of their framework and propose different experiments showing the increased robustness of their model to a family of adversarial attacks compared to baseline resnets. the paper introduces an original idea, providing a very interesting connection between resnets and pdes. this allows the authors to exploit known properties of pdes and opens the way to new theoretical insights on dnns while allowing the development of dnn models with proved properties. as mentioned this generalizes the view of resnets introduced in neural odes. besides, the paper presents weaknesses. first the form will make it accessible only to a very small audience in the ml community. no effort is made in the writing to introduce the required pde concepts that would help a lot understanding and appreciating the contribution. this is a pity since given the current trend on this topic this could be of interest to a large community. then the use cases in the experiments focus solely on robustness properties and one type of attacks. this illustrates only one aspect of the potential of the framework, and this does not provide a strong case in support of the ideas introduced before. the global message carried out by the paper then becomes unclear. overall, the current version could be largely improved and this will certainly lead to a strong contribution.", "accepted": 0}
{"paper_id": "iclr_2022_AT0K-SZ3QGq", "review_text": "dear authors, i have carefully read the reviews, rebuttals and the subsequent discussion. the review scores are mixed 5, 5, 6, 6. let me comment on some of the key issues raised by the reviewers. i will elaborate on some of them with my own insights. 1 you insist that p4 cannot be regarded as a particular case of p3. but this is trivially incorrect. the hard constraint in the reformulation p6 you mention in the discussion can be written as a regularizer indicator function of the constraint set. indeed, let cal c be the set of points ww_1,dots,w_k for which there exists w such that w_k  m_k circ w for all k. then the regularizer defined by cal rw  0 if win cal c and cal rw  infty if otherwise does the job. this is a well defined regularizer. such regularizers are routinely used in optimization to model hard constraints. so, the formulation you consider is a special case of p3. moreover, as pointed out by reviewer zg2f, and acknowledged by the authors, the idea of using sparse masks to model personalization for federated learning is not novel in this work. prior works utilize this idea with other techniques li et al., 2020 vahidian et al., 2021. moreover, several sidebenefits such as low communication cost, cheaper computation, and fewer memory requirements should also be attributed to those original works where sparse masks are used, and the same sidebenefits of sparsity were mentioned. the claim that one of the novel contributions of fedspa is we formulate a clear optimization problem for fedspa is weak, especially in the light of the above comment, and the moral existence of the formulation in prior work, albeit not expressed in a mathematical notation. the fact that previous works did not formulate this properly is a major issue with those works, and not a major contribution of this work. a clear mathematical formulation of what one wants to achieve should be a standard requirement. in any case, i appreciate the clarity nevertheless. 2 the same reviewer states that the key idea of the paper that differs from the above two mentioned papers is how the sparse masks are handled. one of the two ideas proposed is trivial and is equivalent to standard nonpersonalized fl if all masks are the same, the submodes they defined can be considered a global model. the second idea does not seem to have any interestingdistinctive theoretical support. 3 sparsetosparse training in fedspa may be novel, but the claim that the masks continue to evolve towards the optimal masks in the training process is not supported by theory nor experiments. if indeed you can show that the local masks evolve to some meaningful notion of an optimal mask, this would be interesting. 4 i also agree with the other points raised by this reviewer. i have read the author response to these comments. btw language such as you bet is inappropriate. while some of them make sense, they do not reduce the severity of the concerns by a large enough margin. 5 the comment about the weakness of the main theorem is particularly concerning. indeed, the main theorem may be vacuous, and the authors need to do a thorough explanation of the result and its importance on its own and in comparison with existing literature and rates. i do not believe such a comparison could be advantageous to the proposed method though. the expressions are complicated. it seems that for any meaningful mask size, the nonvanishing term will be too large. the theorem is not a valid convergence result as the authors do not show that the right hand side can indeed be provably made arbitrarily small by some choice of the parameters of the method. for instance, it is not guaranteed that distm_k,t, m_k will converge to zero. in this sense, calling this theorem convergence of personalized models is incorrect and misleading. this is a fatal issue, unfortunately. the authors should make it absolutely clear that the result does not prove convergence. 6 assumptions 1, 2, and 4 are very strong. for example, assumption 2 is not provably satisfied for lower bounded nonconvex smooth functions when subsampling minibatching is used to produce the stochastic gradient. assumption 3 is also quite strong it is not satisfied by convex quadratics. assumption 1 is also strong  most recent works on fl do not require any similarity assumptions. in summary, while this direction of research is interesting, the level of contributions in this work is marginal at best. the key theoretical result is misleading in that it does not imply convergence while it is marketed as such. moreover, strong assumptions relative to what is achieved in the latest papers are used to obtain it. because of these concerns, and other concerns raised by the reviewers, i do not have any other choice but to reject the paper. area chair", "accepted": null}
{"paper_id": "iclr_2022_b8mo34uDObn", "review_text": "this paper presents a method for ensembling light finetuning methods and full finetuning methods to achieve better performance both indomain and outofdomain distributions. as authors agree, similar idea has been explored in the computer vision literature. the reviewers like the overall idea of the paper, but they all had some concerns regarding the experiments. the reviewers provide valuable feedback on how to improve the experiments, potentially running the same idea on more datasets and tasks, provide more analyses and discussions on how to understand the results.", "accepted": 0}
{"paper_id": "iclr_2022_NrkAAcMpRoT", "review_text": "this was a somewhat unusual submission in that the authors tried to motivate their paper by pointing to a separate anonymous manuscript. however, the authors didnt seem to want to confirm they would merge the manuscripts when asked about this. it was thought that in fairness the submitted manuscript should be judged on its own. after discussion, it was agreed that the submitted paper on its own, did not generate enough enthusiasm to merit acceptance.", "accepted": 1}
{"paper_id": "iclr_2022_nK7eZEURiJ4", "review_text": "it appears that the reviewers have reached a consensus that the paper is not ready for publication at iclr.", "accepted": 0}
{"paper_id": "iclr_2022_ZumkmSpY9G4", "review_text": "this paper presents an approach for online continual learning where only a single pass over each tasks data is allowed. instead of the oftused softmax classification setting in continual learning, the paper proposes to use the generative setting based on the nearest class mean ncm. the paper claims that it avoids the logits bias problem in the softmax classifier and helps combat catastrophic forgetting. while the reviewers found the basic idea interesting, there were concerns about novelty and lack of clarity regarding the reasons for improved performance. in particular, there are several aspects from existing work that are leveraged in this paper e.g, replay, metric learning loss, combination of generative and discriminative classification, etc but the paper lacks in establishing which of these components affect the performance and in what ways. the authors and reviewers engaged in detailed discussions; however, the reviewers were still unsatisfied and did not change their assessment. based on my own reading of the paper as well as going through the reviews and discussions, i too concur with their assessment. it would be a stronger paper if the paper could shed more light on the above aspects as well as address the other concerns raised by the reviewers. however, in the current shape, it is not ready for publication.", "accepted": null}
{"paper_id": "iclr_2022_BlyXYc4wF2-", "review_text": "the paper addresses safe multiagent reinforcement learning and makes two key contributions. first is a safety concerned multi agent benchmark, which is an extension of mamujoco. second, is the formulation and two solution to safety marl problem. the authors pose safe marl, and marl problem with safety constraints, as a constrained markov game. the safety constrained marl is an important, difficult, and understudied problem. the problem is more difficult that the single agent safe rl because of the nonstationarity in the marl setting, which renders any theoretical guaranties conditioned on the assumptions of the behaviors of other agents. the authors are right to point out the lack of the benchmarks in the space. that said, reflecting on the reviewers feedback and my own reading of the paper, this paper is attempting to do too much benchmark, problem formulation, and two methods, in too little space, and is falling short. for example, the benchmark is an important contribution, but it is barely mentioned in the main text of the paper. if this was fully safety benchmark paper, there is an opportunity to go beyond mamujoco, which feels like a forced multiagent problem, and construct a safety benchmark with energy constraints, cooperative and competitive tasks etc... if this was fully methods paper, there would be an opportunity for more indepth analysis of the results that the reviewers pointed out. in its current form, the paper feels like proposing a benchmark not grounded in a real world problem, and then a method to solve the problem. i would suggest the authors to either  submit the paper to a journal where a space constraint would not be in a way, or  split it into two papers, a more comprehensive benchmark, and methods paper evaluated on more difficult problems. minor  please update the literature. some of the papers have been published, and they are cited as arxiv papers.", "accepted": null}
{"paper_id": "iclr_2022_mL07kYPn3E", "review_text": "thanks for your submission to iclr. this paper presents an extension to prototypical networks based on using hyperspheres to represent the prototypes. strong empirical results are presented using this approach. overall, this is a very borderline paper and could go either way. the idea itself it simple, though the results seem to be fairly strong. i read through the paper myself and tend to think that it could use a bit more work before its ready. some of the issues raised by the reviewersparticularly with respect to experiments and literature revieware worth nailing down. further, i think that the method could be explored in a more principledtheoretical way. for instance, when reading this idea, the first thing that pops into my mind is that representing the prototype with a hypersphere is very similar to representing a distribution e.g., a gaussian using a mean and covariance in this case, a spherical covariance. indeed, if you take the kl divergence between two spherical gaussians, you get something very similar to the expression used in the paper. this is all to say that there may be other more general directions to take this idea, or other interpretations of what is going on. please do keep in mind the comments of the reviewers when preparing a future version of the manuscript.", "accepted": null}
{"paper_id": "iclr_2022_97ru13Fdmbt", "review_text": "this paper proposes a new approach to enforce monotonicity in the context of risk minimization, or to promote it as an inductive bias. this improves upon existing pointwise gradient based methods by expanding the region where monotonicity is enforced. group monotonicity is found valuable as a regularization for convolutional models, and multiple applications were shown where the approach appears effective. the paper is well written, and received detailed discussion. despite the rebuttal, some major concerns remain, such as drop in accuracy, and empirical estimate of the probability that definition 1 would not hold over the distribution in question. overall, revisions are needed to make the paper publishable.", "accepted": null}
{"paper_id": "iclr_2022_Vq_QHT5kcAK", "review_text": "the authors present a new framework to make deep ensembles provide better coverage of the posterior and be less reliant on initialisation. the authors generally did a good job presenting their approach, avoiding dubious claims that deepensembles are nonbayesian, and instead focusing on ways in which deep ensembles can be improved, practically and theoretically. it is worth noting in a revised version, however, that many approximate inference procedures do not have theoretical guarantees. the claim that deep ensembles have arbitrary bad approximation guarantees is vague and appears to single them out in a way that could confuse the reader. regarding priors, it is also worth noting that wilson  izmailov 2020 provide evidence that the prior in weight space induces a prior in function space with useful properties, although the prior can be improved. the authors do a good job of responding to reviewers, and describing limitations. ultimately, however, the general opinion was not swayed to accept. in addition to reviewer concerns, the experimental evaluation could be substantially improved. there are several procedures that build on deep ensembles to capture uncertainty within modes. how does this procedure compare? why are no likelihood evaluations considered? what about accuracy? in its present form, its unclear what practical value the contributions are providing, besides possibly better ood detection, but even that direction is explored in a relatively limited way. it could also be interesting to measure the distance of the predictive distribution to a good proxy for the bayesian model average. overall, there are the raw ingredients of a good paper here, and the authors are encouraged to continue with this work.", "accepted": 0}
{"paper_id": "iclr_2022_kcadk-DShNO", "review_text": "this paper offers flowbased alignment methods for alignment of distributions in a domain adaptation setting. while there are many positive aspects of the submission, the experimental results only weakly support the results. the ac agrees with the critical comments mentioned by reviewer sz2c, and in particular observes that the experimentation is not state of the art with regard to current domain adaptation literature. unfortunately the submission is not acceptable in present form.", "accepted": 0}
{"paper_id": "iclr_2022_7Bc2U-dLJ6N", "review_text": "the reviewers have the following concerns 1. the theoretical results for the proposed method are weak. theorem 4.2 cannot be considered as a convergence result, because the bound depends on some random variables r_t,i. the reviewers agree that a proper analysis would require some knowledge on the lower bound of these variables. although there is some empirical explanation for this, the lower bounded assumption of r_t,i is not theoretically justified. the authors acknowledge that this is the main challenge for the present algorithm. in addition, the analysis requires bounded gradient and bounded function value, which is also strong for nonconvex settings. 2. the empirical performance is not strong. in most experiments, the proposed method is not better than the baseline aegd. the novelty and contribution of sgem over aegd is quite limited, since the idea of adding momentum is not new. the suggestions to improve this paper are as follows 1. since the lower bounded assumption on r_t,i is not standard and hard to verify, the authors might consider analyzing a theoretical guarantee for it. on the other hand, they could verify more experiments with various data sets to have some sense whether this assumption may be true or not. next, please try to relax the strong assumptions as discussed. 2. it is better if the authors can show the performance of sgem for convex settings, and for other deep learning tasks e.g. nlp as suggested by the reviewers. the authors should consider to improve the paper based on the reviewers comments and suggestions and resubmit this paper in the future venues.", "accepted": null}
{"paper_id": "iclr_2022_xaTensJtCP5", "review_text": "this paper proposes guiding principles with which to design objective functions for proposal distributions for mcmc. they design one such objective based on gsm titsias and dellaportas, 2019. the two concerns raised by reviewers that resonated the most with me were  it was not clear that the actual proposed objective was the best way to implement these guiding principles  a weak empirical evaluation that did not consider online tuning and highdim, highly nongaussian targets. after rebuttal, revision, and discussion, reviewers felt that the authors did a reasonable job of addressing the issue of online tuning, but very highly nongaussian targets were not addressed. there was still a sense that the ultimate instantiation of the design principles was a somewhat adhoc loss. ultimately, i think that this work is just below the bar for acceptance and it can be improved by clarifying the choices made in implementing the objective and some more ambitious experiments.", "accepted": 1}
{"paper_id": "iclr_2022_xw04RdwI2kS", "review_text": "summary this paper studies an inverse linear contextual bandits icb problem, where, given a tround realization of a bandit policys actions and observed rewards, the goal is to design an algorithm to estimate the underlying environment parameter, along with the belief trajectory of the bandit policy. a particular emphasis is placed on the belief trajectory being interpretable and capturing changes in the policys knowledge of the world over time. the papers main contributions are i formalizing the inverse contextual bandits problem, ii designing two algorithms for this problem based on two different ways of modelling beliefs of the bandit policy, and iii providing empirical illustrations of how their algorithm can be used to investigate and explain changes in medical decisionmaking over time discussion this paper has received high quality, long and detailed reviews that highlighted some flaws, in particular in the wellposedness of the problem and the clarity of the writing. the authors response was long and detailed as well, and its quality was recognized by the committee. however, the consensus is that this work would require a full pass allowing to include most of the feedback received in the main text rather than in appendices, to discuss related problems in the literature in more depth and perhaps to refocus the exposition on the problem considered. recommendation reject.", "accepted": null}
{"paper_id": "iclr_2022_uy602F8cTrh", "review_text": "the paper describes a new method to improve the generalization of modelbased rl by means of interventional data augmentation. the key idea is to intervene the value of a particular variable e.g., object property in the learned dynamic model for episode simulations. experimental results show that it improves i the generalization ablity in the ood scenarios with respect to the intervened variable, ii sample efficiency in the presence of unbalanced training distribution. strengths  connects data augmentation to counterfactual property generation  clearly written  novel about applying counterfactual data augmentation to dyna, as opposed to standard data augmentation techniques in other areas of machine learning  the paper well demonstrates the benefits of counterfactual data augmentation for modelbased rl weaknesses  a lack of explanation for how the model is supervised to be equivariant to different data augmentations.  empirical results seem to suggest that the proposed data augmentation does not have much of an effect on performance  the claimed connection between the scm and the proposed dynamic model seems vague  the technical contribution seems limited and involves very strong assumptions.  the structural causal model it introduces does not appear to be used by the method at all.  the presentation does not cleanly separate counterfactual reasoning from intervention  he greatest weakness of the method, acknowledged by the authors, is that there is no way to train the model on altered data. thus, the performance of the policy on these altered data hinges on the extent to which the model, trained without such data, happens to make accurate predictions all the reviewers voted for rejection. i recommend the authors to use the reviewrs comments to improve the paper and resubmit to another venue.", "accepted": 0}
{"paper_id": "iclr_2022_vpiOnyOBTzQ", "review_text": "this manuscript tackles an interesting and significant line of research of longterm prediction and outofdistribution generalization in time series models. i strongly believe this problem is an important one to solve. however, in its current form, its novelty is marginal, and the experiments fail to decisively show advantages. it also lacks of systematic improvements and error analysis. further work could make it ready for publication at a next conference.", "accepted": 0}
{"paper_id": "iclr_2022_DzKPXXr-CLK", "review_text": "the authors consider an interesting approach for modeling analogical relations through abelian group networks. while the conceptual contributions in the work, the explicit introduction of abelian relations in particular, were generally appreciated, the reviewers found the numerical results provided in the paper lacking. in addition, several issues regarding the scope of the problems to which the proposed approach applies have been raised. thus, given this, and the exchanges between the reviewers and the authors, in its present form, the paper cannot be recommended for acceptance. the authors are encouraged to incorporate the valuable feedback provided by the knowledgeable reviewers.", "accepted": 0}
{"paper_id": "iclr_2022_2NqIV8dzR7N", "review_text": "in this paper, the stopping condition of bayesian optimization bo is discussed. this problem is very important when bo is applied to the hyperparameter optimization hpo task. all the reviewers agree that the proposed approach based on highprobability confidence bound on the regret is interesting and reasonable. an important issue raised by a reviewer is that many existing bo works discussed how to achieve efficiency and saving budget in bo although they did not explicitly mention the stopping condition. due to the lack of discussion regarding the relationship with these highly related studies, we have to conclude that the paper cannot be accepted in its current form.", "accepted": null}
{"paper_id": "iclr_2022_aWA3-vIQDv", "review_text": "this paper observes the similarity between the universality in renormalization group and the lottery ticket hypothesis and proposes that the iterative magnitude pruning, which is used to find the winning tickets, could be a renormalization group scheme. the authors also provide some evidence on their theory on vision model of resnet families. while it is interesting to try a theoretical explanation of the transferability of lottery ticket used in similar tasks using the theory from statistical physics, the paper does not provide enough experimental results to show how to use such an explanation to improve iterative magnitude pruning or determine the best architecture that can be transferred for different tasks. therefore the work is more like working in the progress report and not ready for publication yet.", "accepted": 0}
{"paper_id": "iclr_2022_qfGcsAGhFbc", "review_text": "the paper proposes an fl framework that optimizes the performance of a subset of clients. reviewers did appreciate the value of several contributions, but unfortunately consensus is that it remains below the bar, even after the discussion phase. concerns remained on correctness issues, motivation of assumptions, and distinction of personalized vs selfish fl, even after the author feedback. we hope the detailed feedback helps to strengthen the paper for a future occasion.", "accepted": 0}
{"paper_id": "iclr_2022_-fORBF5k2ZB", "review_text": "this paper studies how recurrent neural networks, and more specifically grus, store and access information. the authors analyze the solution obtained by gradient descent to the variable delay copy memory task for discrete sequences. they use concepts from dynamical systems, such as slowmanifold, to understand the behavior of the learned model. finally, based on this analysis, the authors propose a synthetic solution to a simplified version of the delay copy memory task. overall, while the scores for the paper are rather positive, i still have concerns about the paper, based on the reviews and discussion. i do not believe that these concerned were well addressed by the authors in their rebuttal. first, i tend to agree that the paper is somewhat lacking novelty and insightful findings reviewers tn5r, aflb, mtoe. for example, i think that tools from dynamical systems are mostly useful to analyze rnns when the input is constant jordan et al., 2019. in the case of the copy task, this corresponds to the delay period, where in practice the hidden state is almost constant. this behavior is easily explained by the value of the update gate, close to 1. i thus agree that other hypotheses than slow manifold should be discussed to explain how grus store and access information, and that the benefits of using dynamical systems is not obvious. moreover, i believe that previous solutions to the copy task eg, from henaff et al. could be extended to the variable setting by adding a gating mechanism to these solutions. in particular, henaff et al. claimed that lstm could solve this task empirically, while the authors claim otherwise. second, after reading the revised version a couple of times, i still find the paper hard to follow mtoe, aflb, tn5r. for example, i think that the concept of slow manifold is not introduced properly, and in particular, how it applies to the learned solution is not clear. more generally, i found the sections regarding how information is stored and accessed a bit confusing. finally, i think that the studied task is simple, and probably does not provide strong insight about the working of recurrent networks. specifically, lstms tend to perform similarly or slightly better than grus on many tasks, while the authors claim that this architecture cannot solve the studied task.", "accepted": 1}
{"paper_id": "iclr_2022_qvUJV2-t_c", "review_text": "this work proposes a method for automatic adaptation of the learning rate via a estimating quadratic approximation of the full batch during training. the method motivated by two observed properties of the loss landscape, first the full batch loss along the gradient direction is well approximated by a quadratic polynomial, and second the optimal full batch step size does not change quickly during training. two primary criticisms raised by reviewers is the weak experimental evidence provided to validate the method and similarities with other approaches for adapting the learning rate. ultimately reviewers remain unconvinced by the rebuttal and maintained their scores. the ac further stresses the difficulty of properly and fairly comparing optimization methods in deep learning. as is consistently shown in the literature, optimizer performance is typically dominated by hyperparameter tuning, this is particularly problematic when submissions tune their own baselines as authors naturally are incentivized to tweak their own methods until the method looks favorable relative to others. comparing directly against prior published results tuned by other researchers would help alleviate reviewer concerns regarding hyperparameter tuning.", "accepted": 0}
{"paper_id": "iclr_2022_di0r7vfKrq5", "review_text": "this paper presents a method to improve search engines; the method is designed based on the bm25 retrieval method and is evaluated on nq open dataset. the reviewers agree that the motivation is interesting and implementation is reasonable, but the authors have only showed the impact of their approach over one retrieval method and one dataset, which is limited and does not show if the method is general enough or not.", "accepted": 0}
{"paper_id": "iclr_2022_tsg-Lf1MYp", "review_text": "this paper offers natural attribute methods for shift detection. several reviewers are positive, but reviewer czxp is the most authoritative in the eyes of the area chair. in particular the ac is concerned that the task that this paper defines is artificial and not useful. naturally occurring shifts are real, happen all the time and meaningfully affect model performance... natural shifts should be defined over distributions not singular instances. the authors create artificial instantiations of natural shifts to illustrate a well known flaw in ood detection algorithms. to demonstrate the usefulness of this approach to natural shifts the authors should should show how this algorithm performs in settings like wilds... where the shift are meaningful and not artificial, in the opinion of the ac. the paper should cite and contrast wilds httpsarxiv.orgabs2012.07421 and mandoline httpproceedings.mlr.pressv139chen21i.html in a future revision.", "accepted": 1}
{"paper_id": "iclr_2022_yql6px0bcT", "review_text": "this paper presents a decentralized version of the cem technique, where an ensemble of cem instances run independently from one another and each performs a local improvement of its own sampling distribution. the paper shows that the proposed technique can alleviate the problem of centralized cem related to converging to a local optimum. the paper includes a theoretical analysis and simulation experiments that show some benefits of the proposed technique over centralized cem. the key criticisms from the reviewers include the straightforward nature of the proposed idea, which limits the technical contribution of the paper, as well as the limited improvements over centralized cem in the simulation experiments. in summary, this is a borderline paper. while the paper is wellwritten and the proposed approach is clearly explained, the lack of strong empirical results that show a pronounced improvement of decentralized cem coupled with the incremental nature of the idea of decentralized cem makes me lean toward a rejection.", "accepted": 0}
{"paper_id": "iclr_2022_qPzR-M6HY8x", "review_text": "the paper proposes a new method for the problem of learning under instancedependent noise idn. the idea is to construct a variational approximation to the ideal training objective, which involves learning a single scalar cx per instance. in turn, each such scalar is treated as an additional parameter to be learned by the network. reviewers generally found the basic idea of the proposal to be interesting and novel, with the response clarifying some initial questions on the design of the network to learn cx. the paper is also wellwritten, and presents experiments on image and text classification benchmarks. some concerns were however raised 1 _limited theoretical justification_. there is limited formal analysis of when the proposed method can work well. 2 _lack of comparison to idn baselines_. the original submission did not include any idn baselines as comparison. the revision included results of the method of zhang et al., 21a, which is onpar or better than the proposed method; it seems that this baseline really ought to have been included in the original submission, but it is appreciated that these have been added. a related concern was the marginal gains over the gce method on the cifar datasets. 3 _sufficiency of learning a single parameter_. the paper learns a single scalar per sample. several reviewers were unsure on the sufficiency of this parameter to capture the underlying noise distribution. for 1, the authors acknowledge theoretical analysis as an important future direction. this is perfectly reasonable, but does then require weighting more any issues with the the conceptual and empirical contributions of the paper. for 2, the response clarified that most of these operate either in the binary setting, or require auxiliary information. this is a valid motivation for the present work; it would however be more compelling to include results in a binary setting, to better understand the strengths and weaknesses compared to existing proposals. the response also clarified the present method does not claim to improve upon stateoftheart performance, but rather proposes a simple method which has additional applications as shown in appendix e. this is a reasonable claim; however, to my taste, there is insufficient discussion of the plc method zhang et al., 21a, and what new conceptual information the present work offers. for 3, the response argued that the present results already demonstrate the efficacy of using a single parameter, and that using multiple parameters can be studied in future work. one reviewer was not convinced of the efficacy being shown in some of the results in appendix e. it could strengthen the work if there is an empirical analysis of when the single parameter assumption starts to break down; e.g., perhaps under increasing levels of ccn noise? overall, the paper has interesting ideas and some nice analyses. at the same time, there was clear scope for improvement in the original submission. this was partially addressed in the revision, but given that several domain experts retain reservations particularly in regards to comparisons against prior idn works, it is encouraged that the authors incorporate the above comments for a future version of the paper.", "accepted": 0}
{"paper_id": "iclr_2022_LBv-JtAmm4P", "review_text": "the reviewers agree that the paper is addressing an interesting problem. however, the authors analyze the effect of heterophily on gnn for node classification. the authors simplify the analysis by removing the nonlinearity in the gnn model and derive some theoretical results. however, the analysis is very specific to the simplified version of gnn, and the link to later proposed solution is also weak. furthermore, a more significant improvement in experiments will also make the paper more convincing.", "accepted": 1}
{"paper_id": "iclr_2022_sWbXSWzHPa", "review_text": "this paper introduces a novel method for learning distributional robust machine learning models when only partial group labels are available to improve performance of learning algorithms on minority groups. pros the paper is well motivated and written. the ideas are interesting. most work on distributional robust optimization dro are in unsupervised settings where group information is not available. they provide an approach for the semisupervised setting through a constraint set. cons the empirical results do not show better performance over unsupervised baselines as pointed out by reviewers. the authors claim one of the benefits of their proposed approach is a onestage approach, in contrast to competing models that require a twostage approach; hence, allowing their approach to reduce compute time. itll be helpful to strengthen this point by showing time comparisons. missing labels in this case due to participants withholding sensitive information is not an mcar case, but the proposed work makes an mcar assumption. itll help to add a discussion and point out such limitations of the approach. summary this paper has novel and interesting ideas, but still has several issues as pointed out by the reviewers before it is ready for publication.", "accepted": 0}
{"paper_id": "iclr_2022_J9_7t9m8xRj", "review_text": "the work proposed multiview learning framework that combines diversity and consistency objectives for semisupervised learning. while reviewers appreciated that simplicity of the proposed method, they raised concerns on the limited contribution on top of the original bayesian cotraining work. although authors provided detailed rebuttals that addressed some of the reviewers concerns, and one reviewer did raise their score, the other reviewers scores remained unchanged. given the work is closely based off the bct work, i would like to see more detailed analyses on the importance of the changes brought in this work, such as changing the base learners and introduction of diversity objectives as pointed out by the authors.", "accepted": 0}
{"paper_id": "iclr_2022_Jt8FYFnyTLR", "review_text": "this paper proposes a metric for the safety and interpretability of supervised learning models based on the maximum deviation from interpretable whitebox models. the safety and interpretability of blackbox models is an important topic, and many reviewers agree that the approach proposed by the authors is interesting. however, the maximum deviation from popular models such as decision trees, generalized linear and additive models have been intensively studied in the context of robust statisticslearning. without explicit discussion on the connections with these existing studies, the novelty of the proposed approach cannot be properly evaluated. we thus have to conclude that the paper cannot be accepted in its current form.", "accepted": 0}
{"paper_id": "iclr_2022_CyKQiiCPBEv", "review_text": "the paper builds fast and highquality smilesbased molecular embeddings by distilling stateoftheart graphbased models teachers. this has the advantage of speeding inference time w.rt to graph based methods. the reviews were split regarding the motivation of the work, in the sense of why not train directly on smiles instead of distilling graph based methods that are in some tasks behind smiles transformer. authors provided clarifications in the rebuttal showing that on knowledge distillation of graph models surpasses smiles only model training. i think given the experimental nature of the paper the main motivation of the paper should be better clarified and supported with more experimentation and downstream tasks.", "accepted": 1}
{"paper_id": "iclr_2022_9gz8qakpyhG", "review_text": "the paper builds on ideas in testtime adaptation and testtime normalization to improve performance under covariate shift. concretely, the paper proposes i alphabn, a method to calibrate batch statistics by mixing source and target statistics and ii testtime adaptation using the core loss which was proposed by jin et al., 2020. the authors compare the the proposed approach to existing approaches on multiple benchmarks. the reviewers found the idea interesting and appreciated the additional ablations. the main concerns were around novelty as the idea is closely related to prior work in testtime adaptation and normalization and hyperparameter selection e.g. how to choose alpha in practice. overall, the reviewers and i felt that the current version falls slightly below the acceptance threshold. i encourage the authors to revise and resubmit to another venue. minor comment about appendix c this didnt affect the score, just a suggestion for future revisions i think it might be interesting to include other alternatives to crossentropy that downweight easy examples, cf. focal loss httpsarxiv.orgabs1708.02002 and httpsarxiv.orgabs2002.09437. im curious to see if core and focal loss consistently outperform cross entropy.", "accepted": null}
{"paper_id": "iclr_2022_JV4tkMi4xg", "review_text": "the paper considers the problem of blackbox optimization and proposes a discrete mbo framework using piecewiselinear neural networks as surrogate models and mixedinteger linear programming. the reviewers generally agree that the paper suggests an interesting approach but they also raised several concerns in their initial reviews. the response from the authors addressed a number of these concerns, for instance regarding scalability and expressivity of the model. however, some of these concerns remained after the discussion period, including doubts about the usefulness for typical applications in discrete blackbox optimization and some concerns about the balance between exploration and exploration. overall the paper falls below the acceptance bar for now but the direction taken by the authors has some potential. i encourage the authors to address the problems discussed in the reviews before resubmitting.", "accepted": 0}
{"paper_id": "iclr_2022_p7LSrQ3AADp", "review_text": "the reviews end up split. the most positive reviewer notes that the paper may be useful broadly to researchers and practitioners. that is the main promise of the work. however as another reviewer points out, the authors fall short of convincingly explaining how the said practitioners and researchers would benefit from the work. and, also as noted in a review, the paper does not quite go deep enough into the discussion of what saliency means for different methods analyzed. contrary to the positive reviewers comment, i do not think that a paper must provide novel work to built upon, but for a paper that doesnt, there is a significant threshold on how convincing it must be regarding the potential impact of the work. i think the paper in its current state, even after the rebuttalrevision, does not meet this threshold.", "accepted": 1}
{"paper_id": "iclr_2022_pzgENfIRBil", "review_text": "this paper presents a numerical approach to solving the multibody schrodinger equation. three reviews give low confidence scores and the one review with high confidence, and high score, is very brief and the reviewer appears to have a weak background in this area. my feeling is that the iclr reviewer pool does not contain reviewers who are really competent to review this paper. there is a large literature in the physics community on this problem and the paper should be reviewed in an appropriate venue. this is especially true for evaluating the empirical results. if the mathematical techniques are relevant to general machine learning, and the authors want to have an impact on machine learning community, then it should be possible to give empirical results on a problem commonly used to evaluate machine learning methods at machine learning venues. whether or not this is important for physics should be judged by physicists. in any case, the reviews are for the most part not enthusiastic.", "accepted": 1}
{"paper_id": "iclr_2022_1O5UK-zoK8g", "review_text": "this submission presents a technique to improve generalization of urban scenes segmentation. based on a pretrained deep net on synthetic data, the approach aims at adapting statistics on real target domain such as cityscapes, bdd or idd datasets using an instanceadaptive batch normalization iabn at test time. results are reported on several synthetic to real scenarios. most of the reviewers were not convinced by the approach and have raised several issues. after rebuttal and discussion, no one really changed herhis mind. the novelty of the proposed method is limited to the use of the existing iabn in this context except the onesample adaptation. although the proposed method is effective on some benchmarks, the extra processing time may be a significant limitation. additional comparisons are necessary. we encourage the authors to consider the reviewers feedbacks for future publication.", "accepted": null}
{"paper_id": "iclr_2022_iaqgio-pOv", "review_text": "this paper presents two novel approaches to provide explanations for the similarity between two samples based on 1 the importance measure of individual features and 2 some of the other pairs of examples used as analogies. the proposed approach to explain similarity prediction is a relatively less explored area, which makes the problem addressed and the proposed method unique. however, reviewers expressed concerns about evaluation methods and there were some concerns about the design choices that were not well motivated. the major issue is, as pointed out by the majority of the reviewers, the evaluation methods. given the paper, reviews, and responses of the authors and the reviewers, it appears that there is certainly room for improvement for more convincing evaluation methodologies to convince a crosssection of machine learning researchers that the proposed approach advances the field. overall, this is a good paper, but appears to be borderline to marginally below the threshold for the acceptance.", "accepted": null}
{"paper_id": "iclr_2022_tlkMbWBEAFb", "review_text": "the paper proposed a new kind of neurons for 3d spherical data classification. all the reviewers agreed that the new kind of neurons makes a good contribution. however, all the reviewers also agreed that the experiments are too weak only at the proofofconcept level and no comparison with the state of the arts. only reviewer fkmy advocated accepting the paper because we need new ideas, and all other reviewers leaned towards rejecting the paper. the ac had exactly the same feeling as the reviewers. particularly, the ac also agreed with reviewer fkmy that we should not look at experimental results only. however, the ac would like to point out that this by no means means that the experiments can be too simple. note that this paper is to propose a new tool to improve classification performance, rather than a new theory to explain or predict something. so some basic requirements on the experiments are necessary. if the authors could provide comparison with the state of the arts and with reasonably good performance, not necessarily exceeding or even on par with the state of the arts namely can be inferior but not too inferior so that others can believe adding engineering tricks could fill in the gap, the ac would consider accepting the paper.", "accepted": 1}
{"paper_id": "iclr_2022_6uu1t8jQ-M", "review_text": "the authors consider the problem of unconditional image generation in the lowdata regime, such as learning from frames of a single video or even from a single image. the main idea is to apply gans with a specific twobranch discriminator architecture such that the content features and layout features are handled independently. secondly, to improve the variability of generated images the authors apply diversity regularization. the authors show that the proposed model is able to, to a certain extent, generate diverse high quality samples. the paper is wellwritten, the authors described their method and the evaluation protocol thoroughly and clearly. the reviewers felt that this submission was borderline, with questionable novelty and significance. in an extensive rebuttal and discussion phase the authors addressed several raised challenges and improved their paper. however, two points remain  technical novelty content and layout separation as well as diversity regularisation previously appeared in many contexts and papers.  motivation and practicality one of the main arguments for the utility of the proposed method is to use it for data augmentation. while it may indeed result in contentbased augmentations, it nevertheless necessitates training of a gan for every single image, which is severely limiting in practice. after reading the manuscript, reviews, and the rebuttals, my view is that the paper is below the acceptance bar and i agree with the points on novelty and significance. in particular, the main application to data augmentation seems to be unexciting and the proposed method impractical. at the same time the proposed method is a combination of already known techniques, albeit in a different setting. i suggest the authors condense the arguments in the extensive rebuttal to improve the points raised above and resubmit.", "accepted": 0}
{"paper_id": "iclr_2022_ZWykq5n4zx", "review_text": "confidence boosting via aggregating multiple run of algorithms has been used before. the main result of the paper relies on a generic confidence boosting trick. the authors for instance cite shalevschwartz et al 2010 theorem 26 in remark 4 of their paper and correctly point out that for deterministic algorithms like erm one can use that for confidence boosting. while that theorem there is proved for excess risk and for deterministic algorithms, the main idea there to me seems like what is used in the authors paper as well. the basic idea property a holds in expectation, hence use markov inequality to get a low grade probability version of it in each of the k pieces now probability that at least one of the pieces is good is high since each piece is independent of the other finally aggregate with simple concentration with union bound. in shalevschwartz et al 2010 this is done with property being excess risk, here it is done with generalization error. oh and i should add, the fact that the algorithm is randomized does not affect this line of reasoning as long as we use fresh randomness for each of the k blocks. now the missing piece covered is that onaverage stability implies generalization in expectation. but isnt this already known to be true in the stability literature? to me it seems like the main technical contribution of the paper is not as novel. further, as one of the reviewers points out, the main goal should be to prove high probability guarantee for the algorithm popularly used like sgd not the confidence boosted version of it. none the less, it seems like the application of the result to sgd seems interesting and somewhat new. i am reluctant to propose an accept here.", "accepted": 1}
{"paper_id": "iclr_2022_a_nR4BPPJF1", "review_text": "this paper aims to explain the pretraining effectiveness of masked language model, based on the concept of diversity of classes. they empirically study how a diversity regularizer, based on this theory can improve model performance, as an empirical support. before rebuttal, reviewers consistently found the empirical study rather preliminary, while authors, through rebuttals, argue the theoretical study should be highlighted as their main contribution, and expressed concerns that the lack of empirical rigor should not be a ground to reject. we agree with these concerns, but rebuttals and discussions failed to convince reviewers that assumptions and evaluations are proper for connecting the proposed theory to potential impacts in pretrained language model scenarios. revising to make this connection clearer would address the reviewer disagreements in the future.", "accepted": 1}
{"paper_id": "iclr_2022_cKTBRHIVjy9", "review_text": "this paper develops a new mechanism submix that provides nexttoken prediction under a variation of the differential privacy constraint. there is disagreement among the reviewers when assessing the quality of this work. even though the study of private predictions in large language models is new, the reviewers raised several issues in the proposed approach. first, the formulation of partitionlevel dp created confusion about the privacy guarantees provided by the mechanism. given the similarity to pate, it might be useful to articulate if there is any difference between the privacy guarantee in this paper and the one of pate. second, the authors might want to further clarify the reason for having two subparts, which has also created some confusion. even after reading the authors response and the updated revision, the ac still could not understand the relevant privacy argument. in summary, the paper may require further clarification and revision before it is ready for publication.", "accepted": 1}
{"paper_id": "iclr_2022_cWlMII1LwTZ", "review_text": "this work considers the problem of how to predict on sensitive user points while preserving their privacy. it proposes a fairly straightforward way to create a local randomizer that optimizes loss for a given model subject to preserving ldp. the work also gives theoretical analysis of the randomizer for least squares linear regression. the problem formulation is different from the standard ldp framework where privacy of training data points needs to be preserved. the submission does not motivate this setting and i dont see a good motivation for this problem either. more importantly, it does not sufficiently emphasize that the problem is entirely different from prior work. indeed all reviewers were confused about various aspects of comparison with previous work. therefore, in my opinion, the submission is not sufficiently well motivated and clearly presented to be accepted.", "accepted": 0}
{"paper_id": "iclr_2022_biyvmQe5jM", "review_text": "the authors provide an investigation into tuning learning rate schedules. the problem is certainly of great practical importance. after discussion, the reviewers felt the main idea of the paper is worth pursuing, but could use significant refinement. one reviewer suggests  ...better treatment of the background material, clearer identification on when the weight norm behaviour happens beside norm possibly looking also for counterexamples!, rethinking section 6, and a more convincing set of experiments for showing convincing evidence about e.g. 5.2. regarding this last point, i want to clarify that in my review i mentioned 1 not for the grid search, but rather for the timecontrolled experiments. if you go with random search for selecting the hyperparameters of the learning rate adaptation methods. i personally think that a recipe to make the comparison fair enough is to choose a prior distribution e.g. uniformlog uniform that covers reasonable values e.g. as used for different datasets with mean equalclose to the known wellperforming optimal value. other reviewers were generally of a similar opinion. the authors are encouraged to continue with the work, taking reviewer comments into account for updated versions.", "accepted": 0}
{"paper_id": "iclr_2022_bOcUqfdH3S8", "review_text": "the paper studies an important problem of quantifying uncertainty as measure by calibration of predictions made by an ml algorithm in the presence of distribution drift. however, all reviewers point out a slew of concerns that went unrebutted by the authors. the reviewers concurred that the paper deserved to be rejected at the current stage, and i concur. i recommend that the authors take the critical and constructive feedback into account to improve the paper and perhaps resubmit to a different venue in 2022.", "accepted": null}
{"paper_id": "iclr_2022_-0Cjhnl-dhK", "review_text": "the reviewers highlight that several of significant claims of the paper are not backed up by experiments, and the experiments themselves lack sufficient detail, therefore, at this stage, i recommend rejection. i suggest the authors address the questions and comments they have received before considering whether they might resubmit or not.", "accepted": 0}
{"paper_id": "iclr_2022_2aC0_RxkBL_", "review_text": "this paper investigates the role of representation learning when the distribution over the feature space has a long tail. the main motivation is to determine how much of the overall learning, in this case, is bottlenecked specifically by representation learning. the main findings are that vanilla learning gives brittle longtailed representations, harming overall performance. the paper suggests a form of data augmentation to remedy this. reviewers acknowledge that this investigation is worthwhile. however, many concerns were raised as to whether experiments support the drawn conclusions. a more principled approach to the data augmentation methodology is also needed. the authors address some of these, providing further experiments, but these were not enough to sway reviewers. since results are fundamentally empirical in nature, this shortcoming indicates that the paper is not ready to share with the community just yet. stronger experiments with clearer evidence are needed to fully support the thesis of the work.", "accepted": 1}
{"paper_id": "iclr_2022_H6mR1eaBP1l", "review_text": "this paper presents an approach for using prior knowledge to constrain transitions for consecutive time steps and aims to replace conditional random fields for sequence tagging tasks in sequence labeling. however, the paper seems incomplete with no experimental results and analysis to validate the proposed ideas.", "accepted": null}
{"paper_id": "iclr_2022_SjGRJ4vSZlP", "review_text": "the paper studies an interesting problem, but as pointed out by reviewers, the presentation of the problem statement and contributions need to be improved.", "accepted": 0}
{"paper_id": "iclr_2022_nKZvpGRdJlG", "review_text": "the paper aims at developing mechanisms for adversarial attack and defense towards combinatorial optimization solvers, where the solver is treated as a blackbox function and the original problems underlying graph structure is attacked under a given budget. while the reviewers found the problem novel and interesting, they are not convinced by the problem formulation and the proposed solutions, as well as the experimental setup. some of the points that the reviewers brought up during the discussion include i the attack to the tsp does not follow the main papers attack principle of adding and deleting edges, ii, in general, it has not been explained why all these modification are really relaxations, iii the notations are very confusing, and iv while authors response on loosening the constraints makes sense, but the experiments i.e., the tsp problem setting in this work are not consistent with such clarification. addressing the above points will significantly improve the manuscript.", "accepted": 0}
{"paper_id": "iclr_2022_2jYxq9_TkpG", "review_text": "this paper presents the use of simulated annealing sa for pruning and optimizing the architecture of a neural network. after reviewing the paper and taking into consideration of the reviewing process, here are my comments  the contribution of the paper and the novelty is limited and not well presented  the related work is very sparse. it requires a major improvement.  the main concern is about the simplistic experiments and the lack of comparison between the results of the proposal and the sota methods.  conclusions are not well supported by the results. from the above, the paper does not fulfill the standards of the iclr.", "accepted": 0}
{"paper_id": "iclr_2022_K47zHehHcRc", "review_text": "the paper introduces the notion of interventional consistency of a representation learned using autoencoders, which is claimed to be a desirable property for disentanglement. the reviewers agree that the contributions are novel and relevant, but they also found the paper hard to follow due to a lack of clarity and motivation. further, they considered the underlying assumptions very strong and possibly hard to find practical instances where they may hold e.g., the assumption that statistical dependencies in the prior are preserved by the response map. the reviewers also noted that some realworld examples showing the interventional consistency would be helpful. after all, the paper contains interesting ideas and we would like to encourage the authors to pursue this line of work. still, the paper in its current form is not ready for publication. we encourage the authors to address the reviewers comments explicitly in a future version of the manuscript.", "accepted": 0}
{"paper_id": "iclr_2022_v-27phh2c8O", "review_text": "this paper proposes to use evolutionary methods to learn auxiliary loss functions, demonstrating superior performance vs. typical auxiliary losses previously proposed in the rl literature. demonstrating that it is possible to learn auxiliary losses by evolution, both for pixel and state representations, that help train significantly faster even on new environments is definitely a meaningful contribution, as acknowledge by the majority of reviewers. although many of the original reviews concerns were addressed by the authors during the discussion period, two major ones were only partially answered, both related to the limited empirical evaluation of the proposed approach which is crucial for such a contribution that aims to demonstrate an improvement over existing related techniques 1. the limited set of environments used for evaluation and in particular the lack of partially observable environments 2. the fact that the baseline being compared to was curl, which the paper describes as the stateoftheart pixelbased rl algorithm, while reviewers mentioned drq and rad as two more recent and better algorithms that were known well ahead of the iclr submission deadline note that the more recent drqv2 is now even better. since the data augmentation techniques used by these algorithms help shape the internal representation, like auxiliary losses do, it would have been important to validate that the proposed technique could be useful when plugged on top of such baselines. the authors did try their best to address these major concerns during the rebuttal period, but the discussion between reviewers and myself came to the conclusion that this wasnt quite convincing enough yet. i encourage the authors to investigate these points in more depth in a future version of this work so as to make the empirical validation stronger nb the links provided in the last comment by authors on nov. 30th didnt work, but this wasnt the main factor in the decision.", "accepted": 0}
{"paper_id": "iclr_2022_5SgoJKayTvs", "review_text": "this paper aims at improving aaes with an intervention loss. while the topic is important, the reviewers agree that  the paper has poor clarity,  the related work is not adequately put into perspective,  there are concerns with technical correctness,  experimental evidence is lacking, as the authors have not addressed any of these concerns, the paper can not be accepted in its current form.", "accepted": 0}
{"paper_id": "iclr_2022_T-uEidE-Xpv", "review_text": "description the paper applies ideas from contrastive representation learning to train binary neural networks. namely, the algorithm promotes binary representations to be similar to the fullprecision representations while at the same time it promotes binary representations to be dissimilar from fullprecision representations corresponding to other input images. this is enforced for activations in all layers by the added contrastive loss 9.  decision the main weakness of the paper pointed by reviewers were 1 overlap of the large part of derivation with the prior work 25 tian et al. contrastive representation distillation, iclr 2020; and 2 the meaning of the derivation when applied in the setting of the paper to binary and full precision weights and its soundedness. the authors proposed their arguments for 1. the reviewers board considered these arguments and did not agree see below. point 2 was not addressed by authors no paper revision, justifications, proofs corresponding to the missing supplementary. it was discussed further and was found critical see below, such that it is a clear reason for rejection regardless of 1. overall, the idea is interesting and the method appears to be helpful experimentally, however the paper needs a major revision that would address the two points.  details  overlap with crd reviewers were in a consensus on this issue, disagreeing with authors. since the whole derivation chain of the contrastive loss already exists in the crd work 25, it is redundant to repeat this derivation if not raising ethical concerns. instead an original work should review or just refer to the existing derivation and only discuss the new context and e.g. change the critic function hat h.  meaning of the derivation the reviewers have questioned the soundness of the initial criterion of mi between binary and full precision activations, as it reduces to just the entropy of binary activations. in particular, it seems very different in meaning to the contrastive loss the paper optimizes in the end. here is additional feedback from the discussion. 1. maximizing the entropy of binary activations with respect to the data distribution makes some sense. if a single binary activation was considered, its entropy is maximized when it is in the state 1 exactly for 50 of the data. which makes it discriminative of the input. a similar centering can be achieved by batch normalization put in front of the activation  if the preactivation distribution was symmetric, then bn would achieve the max entropy for the sign of preactivation. such network design is not uncommon. maximizing the entropy of the full vector of binary activations appears more difficult. however we can also understand it as the mutual information between the input image and the layer of binary activations. thus the criterion is to retain as much information about the input as possible. this makes sense as a regularization often neural networks are regularized by adding data reconstruction capabilities  loss, and is aligned well with goals such as reusing the features for other tasks as in sec .3.5 but contradicts to some other principles proposed in the literature, e.g. the information bottleneck that the maximum information about the target rather than the input should be preserved. amongst methods that study the direction of maximizing the entropy in binary networks, reviewers mention irnet and regularizing activation distribution for training binarized deep networks. the architecture with bn before activation is used in the latter work and some more recent works, e.g. boolnet. 2. it is not clear whether optimizing the contrastive loss retains the same meaning as maximizing mi. the derivation from crd paper used here applies several lower bounding steps. maybe the strongest one is that the critic is chosen to be of a specific function rather than a universal approximator. however there is no obvious gap. in fact knowing that binary activations are just a sign mapping of full precision ones, should allow one to estimate pij ai_b, aj_f in a simple way. 3. in the estimator h in 8 the authors make a mistake applying their and crd theory incorrectly h should be the probability of a conditional bernoulli variable estimating pij ai_b, aj_f. it should not depend on aj_f for other values of j than the given one. however in the denominator in 8 it does. therefore this estimator, and as a result the specific nce loss proposed, appear unjustified. if the critic from crd eq. 19 is adopted, it is not clear whether it makes sense for a pair of binary and full precision descriptors note that for ij the scalar product between the two is just a_f_1. it seems that the design of a meaningful critic is a serious gap the authors should address. observing that the initial objective, the mi criterion, was in fact independent of fullprecision states as it is the entropy of binary states, one can propose that an appropriate critique should use binary states only, such as  ha_bi,a_bj  sigmalefta_bi, a_bjright  c .  when fixing hat h the result in 10 that the maximum likelihood estimator for pij  a_bi, a_fj with a generic neural network can approximate this distribution arbitrary well becomes irrelevant. when the paper speaks of randomness, e.g. binary and full precision activations as random variables, considering ij as a random variable, it is needed to specify the source of randomness or the distribution, i.e. to add for a network input drawn from the data distribution in the first case and under i and j picked at random uniformly in the batch in the second. theoretically, the paper would become more convincing, if the the entropy of binary activations was measured by independent tools from the literature after training with and without nce loss and it was shown that indeed the method achieves an improvement in this objective, reconfirming that the principle and the derivation were sound. an ablation study on other modifications such as weight decay may be helpful to convince researchers that the main source of improvements in experiments is the new contrastive loss. note that not all reviewers were convinced by current experimental results due to lack of descriptions  code to fully reproduce and or lack of such ablation studies.", "accepted": 0}
{"paper_id": "iclr_2022_eYyvftCgtD", "review_text": "the authors propose modifications to the transformer architecture in bert by using grouped ffn and an additional convolution module. the paper doesnt have all the results and comparison that should be done for a model that has seen similar architecture modification in the previous papers. while it is not necessary to show improvements on multiple hardware systems, it is important to see comparisons to more, stronger baselines and metrics on the full downstream glue eval rather than just squad to establish improvements. a reject.", "accepted": 0}
{"paper_id": "iclr_2022__faKHAwA8O", "review_text": "authors present an approach to consolidate multiple teachers into a single student model that can be adapted to new tasks. the method involves using a proxy dataset to facilitate distillation to prevent having to replay images from the teacher datasets. a multitask multihead objective is utilized, agnostic to the loss function, in which two are studied. downstream task performance is used as the performance measure. pros  the problem of how to best leverage multiple teachers for a downstream task is important and interesting.  presents a method to generate distilled students that can be finetuned to tasks that demonstrates performance gains over baselines imagenet alone or task specific teacher.  easy to follow and implement.  analysis across multiple datasets. cons  multiple reviewers expressed concerns about current level of novelty  contribution. in some sense, it is natural to expect that combinations of taskrelated and generalist distillation would improve performance.  main results demonstrate improvements in performance when teacher and tasks are related to one another. but authors do not address how to select taskspecific teachers for distillation. related tasks and their matching to the target task are assumed to be known. authors cited related prior works that attempt to do this matching, but do not apply it to their study for a full solution.  authors do not study variations of generalist teachers. how does changing the generalist teacher impact performance?  some reviewers expressed concern presentation is not clear. in particular, the style of figures may not be appropriate to best convey results and analyses of this type of work. comparing different approaches is difficult looking at thin lines. tables are perhaps better suited to convey these results.  multiple reviewers expressed concerns fullfinetuning results are not convincing fig 4, though fewshot results look more convincing authors and reviewers had interaction, but reviewers maintained their recommendation of weak reject. all reviews unanimous in their decisions. authors are encouraged to take into consideration all the comments and submit to another venue.", "accepted": 0}
{"paper_id": "iclr_2022_bHqI0DvSIId", "review_text": "this work presents the neural simulated annealing nsa approach as a heuristic for general combinatorial optimization problems. after revising the paper and reading the comments from the reviewers, here are the general comments  in general, the paper is clear enough. the contributions are stated in a proper way.  the novelty is rather limited, but the key idea of using neural networks in sa, and training it with rl, has merit.  this approach has merit but the novelty is very limited.  the nsa improves the vanilla sa, but the benchmark reveals that nsa is not enough competitive with other stateoftheart methods.  the benchmark does not reveal enough information about the nsa against the sota methods.  the work needs technical improvements and validation is required before accepting the work.", "accepted": 1}
{"paper_id": "iclr_2022_CpgtwW8GBxe", "review_text": "this paper investigates a semisupervised label refining approach to searching for similar voices for voicedubbing. the apporach is based on generating refined labels using a clustering algorithm on the initial labels. therefore, better voice characteristics can be extracted and used to select a new voice in the target language that closely matches the voice characteristics of the source language. experiments are carried out on masseffect as the main dataset and skyrim as the second dataset and results show that the proposed approach slightly outperforms state of the art. while the topic under investigation is interesting and has its value to the applications such as voice casting, there are strong concerns raised by the reviewers. reviewers find the paper difficult to follow. some important pieces of information are either missing or only vaguely explained e.g. nonexpert initial labels, clear interpretation of pvectors, etc., which greatly hinders a deep understanding of the work. some technical details such as network architecture and its training should be elaborated. this paper needs some good improvement in order to get accepted. no rebuttal is provided by the authors so all these concerns still stand.", "accepted": 0}
{"paper_id": "iclr_2022_dmq_-R2LhQk", "review_text": "this paper studies the following hypothesis that gradientbased explanations are more meaningful the more they are aligned with the tangent space of the data manifold. the reviews are negative overall. the general feeling is that the paper reads like a set of subjective observations about the meaningfulness of explanation and relationship with data manifold  tangential theory. there isnt a coherent story.", "accepted": 0}
{"paper_id": "iclr_2022_6p8D4V_Wmyp", "review_text": "this paper proposes a new dataset, called rainnet, obtained from gridded precipitation data, for training precipitation downscaling methods, as well as a new neural networkbased architecture for that task, which estimates the underlying dynamics of the local weather system, and new metrics for evaluating precipitation downscaling methods. reviewers praised the large, novel and useful dataset d3tq, szbd, ggkx and novel metrics for evaluating statistical downscaling methods d3tq, along with evaluation on 14 baselines szbd, ggkx. there were however many issues highlighted by the reviewers. first, reviewer d3tq raised concerns about the paper being resubmitted after rejection from neurips pdf?idvvzzjiqb51l, with minimal changes pdf?id6p8d4v_wmyp, and noticed that the authors did not follow up on most reviewer recommendations. d3tq noticed however that in the iclr resubmission, the cross validation results were presented to provide a more robust comparison between models, and that the discussion of metrics in section 4 was much more thorough than in the previous version. other themes in the negative reviews included concerns about missing standard errors in the crossvalidation results d3tq, 5pvg or measures of uncertainty in the upscaling ggkx, lack of information about hyperparameter tuning d3tq, inadequate literature review about statistical downscaling d3tq, lack of information about the dataset 5pvg, missing discussion about applications ggkx and insufficient proofreading d3tq, 5pvg. i will not take into consideration the criticism from szbd who dont feel that iclr is the right venue for this work as i do not find such opinions to be much helpful. the authors did not provide a rebuttal to the initial reviews and there was no discussion about this paper among the reviewers. given the issues raised by the reviewers and the scores of 3, 3, 5 and 6, i believe that this paper does not meet the acceptance bar in its current form. sincerely, ac", "accepted": 0}
{"paper_id": "iclr_2022_FOfKpDnp2P", "review_text": "the paper proposes to improve generated images via a postprocessing update procedure guided by gradients from a robust classifier. after the author response and discussion, all reviewers agree that the paper is below the acceptance threshold. reviewer concerns include limited technical novelty and missing experimental comparison to relevant baselines.", "accepted": 0}
{"paper_id": "iclr_2022_N2nJzgb_ldR", "review_text": "all reviewers are in agreement to reject this paper. the main objection is that the tasks chosen are small scale and that the mixed results are not strong enough. the authors did not attempt to raise substantial issues to be discussed.", "accepted": 0}
{"paper_id": "iclr_2022_QEBHPRodWYE", "review_text": "all reviewers agree that this is a reasonable contribution but that it is also extremely limited in scope. the authors suggest in one of their response that their technique could apply to any data mixing method with batched ksum structure. such a larger level of generality might make the paper more interesting, but at the moment it is an extremely niche result.", "accepted": null}
{"paper_id": "iclr_2022_5fmBRf5rrC", "review_text": "this paper proposes to address the problem of domain adaption using knotherosenblatt transport withe the method denoted as krda . the main idea is to perform density estimation of the different distributions with mixture of gaussians and then estimate a an explicit mapping between the distribution using knotherosenblatt. experiments show that the proposed method works well on toy and real life datasets. the paper had low score during the reviews 3,3,3,3. while the reviewers appreciated the idea, they felt that the originality of the method is not well justified compared to a number of existing uda approaches using ot. also the reviewers noted several important references missing and that should also be compared during the numerical experiments. a discussion about the limits of the method in high dimension would also be very interesting. the authors did not provide a reply to the reviewers comments so their opinion stayed t same during the discussion. the paper is then rejected and the ac strongly suggests that the authors take into account the numerous comments from the reviewers before resubmitting ton a new venue.", "accepted": 0}
{"paper_id": "iclr_2022_q9zIvzRaU94", "review_text": "this paper extends lowe et al. 2020 to discover causal relations from nonstationary time series by assuming conditionally stationarity of the times series. based on the assumption, a deep learning method based on vae is proposed to learn the causal relations from data. the paper is wellmotivated and wellorganized. however, there are some concerns from the reviewers. 1 the presentation needs significant improvement, e.g., clarification of the notations. 2 the identifiability of the causal graph is not given. it is unclear under what conditions the proposed method can discover the true causal graph. 3 the capacity of the discrete states might not be able to handle complex real situations. 4 the experiments are limited to synthetic and low complexity cases. this further weakens the significance of the proposed method given that there are also no theoretical guarantees of the proposed method. 5 discussions about some important relevant works are missing. overall, the paper studies an interesting problem. however, given the above concerns, the novelty and significance of the paper will degenerate. both theoretical and empirical analysis of the proposed method need further improvement. addressing the concerns needs a significant amount of work. thus, i do not recommend acceptance of this paper.", "accepted": 0}
{"paper_id": "iclr_2022_KeBPcg5E3X", "review_text": "the paper presents modifying latent optimization for representation disentanglement using contrastive learning, resulting in improved performance on disentanglement benchmarks. despite the empirical success, the proposed algorithm has many moving parts and loss functions. most reviewers agree that given the incremental and complex nature of the proposed technique, the empirical results are not sufficient for acceptance at iclr, especially since the results do not present additional insights into the inner workings of the method. i encourage the authors to try to simplify the technique, or provide a convincing evidence that such complexity is necessary. ps i didnt find much discussion of how the hyperparameters are chosen temperature, lambda terms, etc.. a discussion of recent selfsupervised disentanglement methods e.g., httpsarxiv.orgabs2102.08850 and httpsarxiv.orgabs2007.00810 can be helpful.", "accepted": 1}
{"paper_id": "iclr_2022_t3BFUDHwEJU", "review_text": "overall, the reviewers were insufficiently enthused by this paper. there was no rebuttal, and the authors did not engage or answer questions raised. i concur with the reviewers, and encourage the authors to carefully consider the provided feedback.", "accepted": 0}
{"paper_id": "iclr_2022_3Qh8ezpsca", "review_text": "most reviewers came to the conclusion, that this work lacks novelty and theoretical depth. further severe concerns about the validity of some statements and about the experimental setup have been raised. the rebuttal was not perceived as being fully convincing, and nobody wanted to champion this paper. i share most of these points of criticism. although there is certainly some potential in this work, i think it is not ready for publication and would at least need a major revision.", "accepted": 0}
{"paper_id": "iclr_2022_IY4IsjvUhZ", "review_text": "the paper analyses the loss landscape induced by auc loss. reviewers found critical issues with the paper, and the authors have not provided feedback. as such i have to recommend rejecting the paper. i thank the authors for submitting the paper to the iclr conference. i hope the reviews will be helpful in improving the paper.", "accepted": 0}
{"paper_id": "iclr_2022_dYUdt59fJ0e", "review_text": "this paper presents yformer to perform long sequence time series forecasting based on a yshaped encoderdecoder architecture. inspired by the unet architecture, the key idea of this paper is to improve the prediction resolution by employing skip connection and to stabilize the encoder and decoder by reconstructing the recent past. the experiment results on two datasets named ett and ecl partially showed the effectiveness of the proposed method. reviewers have common concerns about the overall technical novelty, presentation quality, and experiment details. the authors only provided a rebuttal to one reviewer and most concerns from the other three reviewers were not addressed in the rebuttal and discussion phase. the final scores were unanimously below the acceptance bar. ac read the paper and agreed that, while the paper has some merit such as an effective yformer model for the particular problem setup, the reviewers concerns are reasonable and need to be addressed in a more convincing way. the weaknesses are quite obvious and will be questioned again by the next set of reviewers, so the authors are required to substantially revise their work before resubmitting.", "accepted": 0}
{"paper_id": "iclr_2022_UXwlFxVWks", "review_text": "this paper explores the representations that are learned by the same network, on the same data, but with different objectivestasks rl, supervised, unsupervised. though the reviewers were positive about some aspects of the paper, the reviews were generally low 3,3,3,6 and indicated rejection. the principle recurring theme in the reviews as to why this was a rejection was the lack of clear motivationsimplications. the authors decided not to submit an updated version of the paper. as such, this was a reject decision.", "accepted": 0}
{"paper_id": "iclr_2022_CC-BbehJKTe", "review_text": "this paper explores the hypothesis that bloat can be prevented in genetic programming by identifying winning subtrees from simplified solutions, and use these to seed new gp runs. this idea is connected with the lottery ticket hypothesis in deep learning. reviewer are unanimous that the paper as it stands is not ready to publish. one big issue is that the empirical results are not particularly good. another is that the conceptual foundations of the paper, in particular the parallell to the lottery ticket hypothesis, might be flawed. nevertheless, there is much interesting research to do in this direction.", "accepted": 0}
{"paper_id": "iclr_2022_wIK1fWFXvU9", "review_text": "all reviewers agree that the proposed idea looks interesting but the paper is seriously lacking in the definition of its scope there is no quantitative result, experiments are quite limited, and there is not enough discussion of the limitations. with more work this could become a very interesting paper.", "accepted": 0}
{"paper_id": "iclr_2022_vnENCLwVBET", "review_text": "the authors propose a referenceless metric for evaluating nlg systems by training a discriminator which distinguishes between humangenerated and machinegenerated text. the main concerns raised by the reviewers were i lack of clarity in certain portions of the paper ii lack of demonstration of the universal applicability of the proposed metric only evaluated for poetry generation iii lack of clear guidelines on how to use the proposed metric in a reproducible manner iv lack of details about what exactly does the proposed metric capture and look for in the generated text. the authors did not respond to the specific queries of the reviewer and agreed that more work is needed on their part.", "accepted": 0}
{"paper_id": "iclr_2022_KFUWHgRYEDF", "review_text": "the submission considers a method involving adversarial training to speed up the finetuning of large pretrained transformer language models. reviewers consider it to be a borderline paper. many suggestions are made by the reviewers which will help improve the presentation and substance and make it more useful for the community.", "accepted": null}
{"paper_id": "iclr_2022_VABfTTrrOv", "review_text": "the reviews received for this paper raise several critical concerns to which the authors have not provided a response. thus, in its present form, the paper is not ready for publication.", "accepted": 0}
{"paper_id": "iclr_2022_OqHtVOo-zy", "review_text": "this paper received a majority voting of rejection. in the internal discussion, one reviewer updated hisher score from 1 to 3 according to the author response. i have read all the materials of this paper including manuscript, appendix, comments and response. based on collected information from all reviewers and my personal judgement, i can make the recommendation on this paper, rejection. here are the comments that i summarized, which include my opinion and evidence. interesting idea every reviewer including me agree that the idea of modelling bayes label transition is novel and interesting. the motivation lacks of supportive evidence the second motivation that the feasible solution space of the bayes label transition matrix is much smaller than that of the clean label transition matrix is not well supported. the authors should theoretically or empirically demonstrate this point. the current description on uncertainty is not strong enough. moreover, if so, the benefits are not illustrated. the feasible solution space, even with a small coverage area is continuous with infinite solutions. a new concept the authors tried to sell the concept of a new transition matrix, but failed. i believe it might result from the organization and presentation. the authors spent too much pages introducing others work. at least, a formal definition of the new concept should be given. in the current version, definition 1 is from cheng et al., 2020 on distilled examples. title literally from title, i guess dnn is a key component or a selling point of this paper. actually no. we expect the authors could provide the insights on what benefits are using dnn over other techniques and how to apply dnn to estimate the transition matrix. if this is not a selling point, this word might be removed from the title. algorithm 1 i am a little surprised that the only algorithm listed in this paper is label noise generation. instead the proposed algorithm of this paper is expected. experimental evaluation the experimental results look much better than other baselines. it is a little confusing that some best results are bold, some not. presentation although i did not notice obvious grammar errors, some sentences are very long 3 lines. they made difficulties to follow the idea. i have to read these sentences several times. in my eyes, this is the biggest one! presentation means how to sell the idea to audience not only reviewers, but also future readers in an easy understood way. the current version spent much space introducing others work; on the contrary, the original or key part is not well illustrated. although this paper has a novel idea and good experimental support, other issues listed above demonstrate the current version is not ready for a toptier conference. no objection from reviewers was raised to again this recommendation.", "accepted": 1}
{"paper_id": "iclr_2022_jJWK09skiNl", "review_text": "all five reviewers unanimously agree that the paper needs to be rejected. one of the main concerns is the lack of technical noveltyoriginality. the reviewers also point out lacking citation and comparison to prior work, and missing experiments. the authors have not provided any rebuttal.this paper describes an approach for zero shot detection of seen and unseen objects in scenarios. all five reviewers unanimously agree that the paper needs to be rejected. one of the main concerns is the lack of technical noveltyoriginality. the reviewers also point out lacking citation and comparison to prior work, and underwhelming experiments. the authors have not provided any rebuttal. we recommend rejecting the paper.", "accepted": 0}
{"paper_id": "iclr_2022_lKrchawH4sB", "review_text": "all reviewers recommended reject. no responses from the authors.", "accepted": null}
{"paper_id": "iclr_2022_zou-Ry64vqx", "review_text": "the paper proposes fedmorph to address the communication and computation heterogeneity problem in federated learning. the proposed fedmorph extracts submodels from the global model and dispatch these to the clients to perform local training. then, the morphed subnetworks get aggregated into the global model via distillation. the paper reports two to three orders of magnitudes savings in communication bandwidth using the proposed method. however, as agreed by all reviewers, the paper has some critical problems as listed below that prevent it being accepted at this point. 1. the idea of training smaller networks to workaround heterogeneity is not novel, though the authors proposed a formulation that optimizes the subnetwork together with a distillation loss when updating server model parameters. authors should include in the related work and compare against other distillationrelated fl work in terms of 1 communication costs savings, 2 easing the overfit problem, 3 reducing the compute and memory footprints of performing local training. 2. optimizing the distillation loss relies on using a validation dataset on the server, and the quality of distillation relies heavily on whether the distribution the validation dataset is close to that of the decentralized training set. this seems to be a rather strong requirement in federated learning where the data is hard to obtain and the distribution may evolve over time. therefore it makes me question whether the distillation is a realistic proposal in practice. 3. the test dataset is used as the distillation dataset which is a major experimentation flaw that the pixels from the test set are leaked into the training algorithm. 4. it may be unrealistic to assume that there exists a representative validation dataset for the global model in fl. the proposed methods error in theorem 2 depends on the distance between the distillation and the local training datasets, which can be arbitrarily large in practice.", "accepted": 0}
{"paper_id": "iclr_2022_K9KiBYAthi9", "review_text": "this paper is proposed to improve base cnn models by dual multiscale attention module. to achieve a better feature representational ability, authors consider the multiscale mechanism from both channeldimension and spatialdimension. the proposed method has been verified on several benchmarks, including imagenet and ms coco. however, all reviewers consider rejecting this paper because this work lacks novelty, the results are suspicious, and the writing is poor. no responses are submitted by authors to address the reviewers concerns.", "accepted": 0}
{"paper_id": "iclr_2022_ZnUHvSyjstv", "review_text": "all reviewers agree that the paper is below the acceptance threshold and the authors did not respond to the reviews. in summary, this is a clear reject", "accepted": 0}
{"paper_id": "iclr_2022_K1m0oSiGasn", "review_text": "four reviewers have reviewed this submission. three of them recommended to reject the paper and one was marginally above the acceptance threshold. the authors have not responded to the criticisms or questions of reviewers. among many concerns were the issues with the use of lean and single target object images, lack of discussions on related models such as adaptive bilinear pooling and multidomain pooling, lack of evaluations on datasets such as largescale inaturalist. given the above criticisms and the lack of authors response, this submission falls below the acceptance bar.", "accepted": 0}
{"paper_id": "nips_2021_gwP8pc1OgN_", "review_text": "the paper revisits the problem of recovering positions of points given the similarity information. it proposes to do first, a spectral embedding and second, a nor linear embedding to obtain the positions. as reviewers note, the interesting contributions are in connecting the geometry between spectral approximations x and true embeddings z. overall, the theory and experiments are interesting. a minor comment, currently, the paper assumes that a is fully known and the theory goes through. what happens a is partially known akin to recovering positions from partial similarity information. what kinds of consistency results are required for the approximations to succeed?", "accepted": null}
{"paper_id": "nips_2021_az0BBDjDvwD", "review_text": "all reviewers are enthusiastic about the papers findings and gave it high scores. the use of onebranch instead of two for blind super resolution is interesting, and one reviewer was particularly interested in the analytical aspect of the paper.", "accepted": 1}
{"paper_id": "nips_2021_Yw7ZNeDVpBS", "review_text": "this article introduces a novel bayesian method for nonparametric regression on manifolds, referred to as bayesian additive regression spanning trees bast. the basic idea is to build a sparse graph mathcalg on the observed covariate data points, and use a bartlike ensemble of treebased models on mathcalg. the graph mathcalg captures the lowerdimensional geometry of the manifold on which the covariates live, and the treebased models yield piecewiseconstant functions on partitions of the data. a gibbs sampler algorithm is provided for posterior inference. in particular, a wellchosen prior on spanning trees facilitates exact sampling from the full conditional on trees for each treebased model component in the ensemble, using a previously known result from the literature. experiments are performed on simulated and real data. generally speaking, the reviewers and i found the paper to be interesting and novel. the paper is wellwritten and the method appears to provide a valuable contribution to this area of research. the method provides a compelling improvement in performance relative to competing methods. in my view, the main limitations are 1 the computation time is quite costly compared to competitors. for instance, on the ushape example, bast takes 651.49 seconds while bart takes only 15.83 seconds see authors reply to reviewer tbpc. the authors mention that a more computationally efficient implementation of bast is under investigation. 2 performance is likely to degrade significantly as the dimensionality of the data grows; see point 5 by reviewer tbpc and the authors reply. 3 the method for outofsample predictions seems ad hoc and could potentially be improved. the method does not provide a coherent i.e., projective model for all points in the space, since it is defined only on the observed points due to reliance on the graph mathcalg. thus, the model does not lead to a natural technique for making outofsample predictions, which is presumably why an ad hoc method was used. the only score below the acceptance threshold was from reviewer r75x, with a score of 5. although r75x has not responded further, i found the authors reply to satisfactorily address the main criticisms and questions.", "accepted": 1}
{"paper_id": "nips_2021_z1F9G4VnGZ-", "review_text": "this paper presents a new method for scene text detection and recognition based on the integration of individual local responses and their centripedal shifts. the paper has received 4 expert reviews, which were quite positive, with in particular one reviewer championing the paper. while the reviewers and the ac agreed, that the paper had weaknesses in presentation and writing, there was a general agreement that the paper has merits, sufficient novelty, and convincing results. the ac concurs and proposes acceptance.", "accepted": 1}
{"paper_id": "nips_2021_KXRTmcv3dQ8", "review_text": "the paper proposes a new 1bit mean estimation algorithm. while the idea of using random rotation matrices have been explored in this problem, the paper proposes a new elegant algorithm that has better performance both in theory and experiments. the paper is well written and i recommend acceptance. i am curious to see how this method compares empirically to the variable length encoding presented in distributed mean estimation with limited communication paper. i encourage authors to add this comparison and incorporate other reviewer comments in the final version.", "accepted": null}
{"paper_id": "nips_2021_LT5QcAeuM15", "review_text": "the paper focuses on a blackbox search approach to planning problems. the paper provides a theoretical analysis of an existing method in this space  lamcts  and suggests a modification of that method grounded in their analysis, resulting in a new approach called plalam. this analysis was valued by all reviewers and considered technically sound. and of course, planning problems are a relevant problem to the machine learning community. although the authors did compare their method to various method on a variety of planning problems, initially, it was unclear how the performance of the proposed plalam modification compares to the original lamcts. this was brought up in review and was addressed by the authors in their reply. other, minor, concerns of the reviewers were also satisfactorily addressed by the authors. the paper was considered to be well written.", "accepted": null}
{"paper_id": "nips_2021_RcjW7p7z8aJ", "review_text": "the majority of reviewers were positive about this paper, commending the connection it makes between generalization and training dynamics. nonetheless, several technical concerns were raised, in particular  the paper equates complexity with small lipschitz constant. obviously small lipschitz constant is sufficient to ensure generalization, but as far as the committee is aware of it is not necessary. identifying the two will potentially mislead the uninformed readership.  the learned parameter to which the analysis mostly applies is the bias in the first layer. it is perhaps a bit trivial to associate lipschitz constant with the first layer bias, as the gradient with respect to this bias and with respect to the input are equal. reviewers felt that this might be an indication of the analysis being somewhat fragile and not representing overarching phenomena. despite the above, the paper was deemed interesting and significant enough to warrant acceptance. i encourage the authors to be more transparent with regards to the above points and clearly highlight the limitations of their framework in the text.", "accepted": 1}
{"paper_id": "nips_2021_OP6ihHjllEc", "review_text": "three expert reviewers were initially positive about the paper, whereas a fourth reviewer was borderline negative. after rebuttal the positive reviewers became more positive, especially with additional results on realworld data. the more negative reviewer had requested these as well but did not interact in the reviewing process further. overall this paper seems provides a positive step in trying to tackle the very difficult problem of spatiotemporal classagnostic object reconstruction from video so i am recommending acceptance.", "accepted": 1}
{"paper_id": "nips_2021_ZB8Du-E1KUz", "review_text": "this paper develops topicnet, a generative model that can learn hierarchical topics by incorporating a predefined semantic graph. the main contribution of the paper seems to be on the application side, and all reviewers agreed the paper is above the acceptance threshold. i would like to encourage the authors to incorporate the reviewers feedback in their revised version. in particular  incorporate the new results from the response to reviewer azkr.  add the metrics of intertopic dissimilarity and topic specificity.  add an experiment to showcase that the method can benefit from prior knowledge in terms of both interpretability and performance on downstream tasks.  add a more detailed figure to illustrate the gaussian sawetm and topicnet models.  clarify the other points raised in the reviews.", "accepted": null}
{"paper_id": "nips_2021_OThHxQUDzkp", "review_text": "adversarial robustness is an important problem of neural network models. this paper proposed a new regularization based on hilbertschmidt independence criterion hsic to improve adversarial robustness. the regularization consists two hsic terms one aims to reduce the nonlinear dependence between input and features, while the other enhances the dependence between features and outputs. such a regularization is shown to enhance adversarial robustness in both natural training and adversarial training. the reviewers unanimously accept this paper after discussions. there are several minor points raised by reviewers that the authors promise to revise accordingly. in the preparation of final version, the authors are expected to incorporate these points suggested by the reviewers.", "accepted": null}
{"paper_id": "nips_2021_F7LYy9FnK2x", "review_text": "generation of discrete objects from a conditional density a decoder with a continuous latent is of interest in many applications. this work proposes to use importance sampling to estimate the uncertainty of the decoder, and use this quantity to avoid latent space regions with high uncertainty when generating highdimensional discrete data. the key contribution of the work is an importancesamplingbased estimate to measure decoder uncertainty. this is used to avoid latent codes that correspond to invalid objects. the paper is found solid and insightful by the reviewers. during the rebuttal, the authors successfully answered several concerns, and provided a detailed performance comparison with prior methods for molecular generation as well as for measuring performance on additional metric to illustrate the performance of the proposed method. overall, there is a consensus for acceptance and i also agree with this decision.", "accepted": null}
{"paper_id": "nips_2021_yAIYc7YjGbd", "review_text": "the majority reviewers are in favor of accepting this paper. the reviewers in general liked the connection made between the soft clustering ensemble problem and discrete wasserstein barycenter. the hardness result and convergence analysis rounded out the paper. however, there was concern about the lack of direct practical implications of the result due to the exponential dependence on the number of centers.", "accepted": null}
{"paper_id": "nips_2021_aohkNJxjYJX", "review_text": "the lstr model is original and well explained and explored in the paper. it was well received by all reviewers, and the rebuttal has addressed most of the concerns raised. one outstanding issue is that the model is not applied to several of the current datasets and tasks of interest e.g. ava, epickitchens and this will somewhat limit interest in it. the authors are encouraged to address this in the final version of the paper.", "accepted": 1}
{"paper_id": "nips_2021_Am_qvhPRQTq", "review_text": "the paper studies uniform stability of nesterov accelerated gradient descent for smooth convex optimization and proves that, unlike in the quadratic case and contrary to a conjecture by chen et al., 2018, the error can accumulate exponentially fast as opposed to quadratically fast, which happens for quadratics. the crux of the approach is in introducing clever constructions of onedimensional adversarial examples. overall, the exposition of the paper is clear and the paper appears technically sound as far as it was possible to verify by reviewers. the paper adds a solid contribution to understanding of stability of acceleratedmomentum methods.", "accepted": 1}
{"paper_id": "nips_2021_bDHBNVtB9XA", "review_text": "this work proposes an ipbased solution to learning predictive clinical checklists. overall, the paper was well received, but reviewers make a number of recommendations to help improve the paper that should be incorporated prior to presentation including error barsconfidence intervals including an ablation of the submodularity heuristic and the path algorithm compare to the strongest baselines possible thorough baseline hyperparameter turning framing of fairness results i encourage authors to carefully consider reviewer feedback when working on their revision.", "accepted": 1}
{"paper_id": "nips_2021_1Rxp-demAH0", "review_text": "this paper considers rl in averagereward setting. in particular, it provides sample complexity guarantees for 1 tdlambda with linear function approximation for policy evaluation; 2 qlearning in the tabular setting for policy optimization. reviewers believe there is a clear disconnection of the two parts of the paper since algorithmsettingproof techniques are all different. furthermore, reviewers are concerned with the assumptions and technical contribution of the second part. although reviewers agree the first part does have some interesting technical contribution, but these contribution of the first part alone may not be sufficient for neurips.", "accepted": null}
{"paper_id": "nips_2021_BGS3o8SpjI3", "review_text": "the paper addresses an important wellknown problem qvalue overestimation, but in a more context which is more complex than the traditional setup multiagent rl. in this context the authors claim that the problem is less studied, which the reviewers generally dont contest and i personally agree as well. this paper studies this problem in this context a clear way, and also propose an algorithm to mitigate it. two of the reviewers are voting to accept the paper, whereas the third is recommending a reject. i still recommend acceptance, because thats the majority vote but also because i am not convinced that the third reviewer qgzd has a full understanding of the paper. the authors have addressed the criticisms of qgzd and i tend to agree with them. unfortunately qgzd hasnt answered the rebuttal.", "accepted": 0}
{"paper_id": "nips_2021_OUH25e12YyH", "review_text": "the reviewers found that the proposed work proposes a sufficiently novel and interesting analysis and approach to solve an important problem, together with compelling experiments. the rebuttal provides clear responses to the comments and questions of the reviewers. the authors are strongly encouraged to take into account the comments of the reviewers and the elements they themselves contributed to this discussion when preparing the final version of the manuscript.", "accepted": null}
{"paper_id": "nips_2021_-Z7FuZGUzv", "review_text": "we thank the authors for this submission. overall, the paper is about a topologybased ownership verification mechanism that can prevent lotteryticket theft under various verification schemes and attacks. the paper wellmotivates the approach. the authors have provided extensive responses to the concerns raised and the ac  reviewers really thank them for their effort. overall, the new results obtained during the rebuttal definitely improve the quality of the paper. we all believe that the inclusion of these results during the rebuttal period is something that does not heavily change the message of this paper. there was discussion and consensus that this work is interesting. having in mind issuesconcerns raised by the reviewers, the main points of reviewers during further discussion were that this paper deserves publication, given the promised fixes by the authors during the discussion period.", "accepted": 1}
{"paper_id": "nips_2021_j4oYd8SGop", "review_text": "to be updated  this is one, where i think the score underrates the work and should probably be accepted once reviewers update their scores based on the author response. i just emailed all of the reviewers who have yet to indicate they have read the author response.", "accepted": null}
{"paper_id": "nips_2021_C1mPUP7uKNp", "review_text": "the authors tackle an important and salient problem inference on microcontrollers for dl, where memory is very constrained. the proposed scheme, patch by patch inference, works well, the reviewers were happy with the paper and the method itself, therefore i recommend acceptance.", "accepted": null}
{"paper_id": "nips_2021_a5-37ER8qTI", "review_text": "the paper investigates the instancebased interpretation method based on influence functions in the variational autoencoder framework. the paper is wellorganized and rather easy to follow. the reviewers tend to agree on the positive aspects of the paper, such as  even though some ideas were introduced elsewhere, the application to vaes is novel and interesting.  the paper is wellpositioned in the literature review. the main disadvantage of the paper is the experimental part  the paper does not compare their experimental results with other existing data cleaning methods.  there is little experimental evidence for illustrating the benefits and potential applications of using vaetracin in training and test samples. overall, all reviews have a tendency towards the acceptance with one reviewer being positive about the acceptance. therefore, i believe that the paper could be accepted.", "accepted": null}
{"paper_id": "nips_2021_FeFIzwifdoL", "review_text": "we thank the authors for rebuttal and the reviewers for engaging in discussions. we reached a consensus for weak acceptance of the paper. however, the method is straightforward given pearl and her, and the reviewers and i agree that the current tasks are not challenging enough and do not convincingly demonstrate the value of this approach, and highly encourage the authors to add additional validations.", "accepted": null}
{"paper_id": "nips_2021_14-dXLRn4fE", "review_text": "this paper introduces an architecture of using all the outputs from the internal classifiers in an earlyexit network. it is claimed the proposed method can achieve both more accurate results and faster prediction. initially this paper receives 3 negative and 1 positive scores. the main problem of negative reviews concerns the novelty of the proposed method. the main trick of adding direct connections between ics is very similar to what has been proposed in previous work of improved techniques for training adaptive deep network by zhang et al., although the setting is claimed to be different. after discussion, the positive reviewer realizes heshe has missed this reference, and decreases the original score. a more clear discussion of the differences between the existing technique and the proposed one should be conducted in the revision. i personally also think testing the proposed method on larger scale datasets such as imagenet would be beneficial. i suggest the authors revise the paper according the the detailed reviews, which i believe would make it much stronger for future submissions.", "accepted": 0}
{"paper_id": "nips_2021_EAdJEN8xKUl", "review_text": "the theoretical contribution for multisource domain adaptation without source data is nice. though some components in the proposed method are borrowed from existing techniques, the overall design of the proposed method is based on the insights of the theoretical analysis. as 1 is the first work if i am not wrong, which only provides theoretical analysis, the authors should add a section to discuss the difference in terms of the theoretical findings between the proposed method and 1 in a revision. overall, i recommend acceptance for this work.", "accepted": null}
{"paper_id": "nips_2021_Jhp38rtUTV", "review_text": "this paper proposes a ucbbased algorithm for multinomial logit bandits. its initial scores were 6, 6, 5, and 5; and they did not change during the discussion. the reviewers liked the rebuttal of the authors and agreed that this paper would a good contribution to neurips. the main factor in this decision was the importance of the application. on the other hand, the paper builds heavily on faury et al. 2020 and contains no experiments. in my opinion, this paper is borderline plus and i support its acceptance.", "accepted": null}
{"paper_id": "nips_2021_iQICgKcrGpE", "review_text": "this paper studies a variety of theoretical questions regarding the bayes optimal classifier and associated robustness guarantees. this is an important topic, adding to the growing discussion around theoretical tools for robustnessrelated questions. the reviewers found the theoretical results to be substantial, using novel arguments in linear algebra and measure theory. the reviewers also believe that the results are interesting, wellmotivated, and technically sound. therefore, i recommend accepting this paper. in the reviews, there were two main concerns that the authors should address for the final version of the paper 1 certain relevant references were missing and should be discussed, and 2 there was a strong suggestion to improve the readability of the supplemental results and proofs in the paper. i agree that both of these should be addressed, and that writing a good theory paper can be challenging but is ultimately a good service to the community. specifically, for 1, i think the authors can also be clearer in the paper about the relationship with prior work, focusing more on the theoretical comparison between the prior results in some of the references and the present results e.g., a proper technical overview may be a good way to address both points and set up the exposition for the appendix and proofs.", "accepted": 1}
{"paper_id": "nips_2021_kbzx0uNZdS", "review_text": "dear authors, following the discussion period, there is still some disagreement about this paper, hence i took a look at the paper. after reading the paper, and taking into account the full discussion, i consider the paper is worth publication, assuming the authors incorporate all the clarification points in the final version which i believe can be done with little effort. hence i recommend acceptation.", "accepted": null}
{"paper_id": "nips_2021_ebQXflQre5a", "review_text": "the paper proposes a bilevel optimization framework to design a training loss for longtail and groupfair learning. the reviews attracted a a lot of backandforth discussion with the authors, and i appreciate the authors for providing very detailed responses and additional experimental results. while there were concerns raised about the proposed approach being prone to overfitting to the validation sample, i think the authors have satisfactorily explained how they take precautions to avoid it. i think reviewer mspxs concern about there being limited novelty does have some merit, but given that this is one of the first few works to successfully engineer an elegant losstuning procedure for longtail learning and that the experimental results are significant, i would recommend an accept. having said this, i strongly urge the authors to use the feedback provided to improve their manuscript, and in particular include the following promised additions to the final version  a detailed discussion on the risk of overfitting and the precautions taken to avoid it do report the validation errors, perhaps in the appendix  comparison to other bayesian optimization approaches  comparison to the additional baselines mentioned by reviewer gfga finally, here are some additional references on prior work on learning loss functions for specialized tasks, which may be of some relevance httpsarxiv.orgpdf1905.10108.pdf httpsarxiv.orgpdf1803.09050.pdf", "accepted": 0}
{"paper_id": "nips_2021_wFuWSdCD7BN", "review_text": "the authors present the first constant approximation factor on the optimal risk for kmedian with no substitution under a random arrival order model. the problem studied in the paper is an interesting problem and the authors present a simple and elegant algorithm that outperforms previous work. the only concern with the paper is that the problem may not be of interest for a wide audience. nevertheless, after the discussion phase, all the reviewers agree that it should be accepted.", "accepted": 1}
{"paper_id": "nips_2021_TgDTMyA9Nk", "review_text": "reviewers unilaterally supported the paper  the approach is simple and the introduced subsidy idea aligns well with platform markets realities, the characterization of the problem is reasonably complete, exposition is good, and the results are clean. the most critical reviewer, soa5, brings up solid points about regret and expositionmotivation around the subsidy, and in future versions of this work it would beneficial for the authors to address these concerns directly.", "accepted": 1}
{"paper_id": "nips_2021_z36cUrI0jKJ", "review_text": "reviewers agree that the paper is wellmotivated, wellwritten and the the proposed method, motivated by theoretical analysis, shows convincing experimental results on hyperparameter tuning of modelbased rl, a hard yet important problem to address. on the other hand, minor concerns remain mainly about whether the theory is connected well with the practice, whether another outside rl loop over modelbased rl is too heavy in practice and requires high sample complexity. authors have address many of the concerns raised by the reviewers and i am happy to accept this paper.", "accepted": 1}
{"paper_id": "nips_2021_YDGJ5YExiw6", "review_text": "this paper proposes a new exploration algorithm specifically for multiagent rl with factorized and centralized value function critic. the idea is to use individual agents value prediction errors as intrinsic rewards based on the intuition that this captures the influence from other agents thanks to the factorizedcentralized aspect. in addition, the paper proposes an episodic memory that exploits past good experiences to further improve sampleefficiency. improving exploration in the context of multiagent rl is also an important problem. all of the reviewers appreciated that the proposed idea is insightful and wellmotivated, and the results are also strong. there is still a remaining concern regarding the effectiveness of the episodic memory especially in stochastic environments. the authors added a new result showing that the proposed method is still effective to some degree of stochasticity during the rebuttal period, and the reviewers are generally satisfied with this result. therefore, i recommend accepting this paper and suggest the authors to include this for the cameraready version.", "accepted": null}
{"paper_id": "nips_2021_Gi6SHsbxkgY", "review_text": "the submission discusses the properties of indistribution id, i.e. same task distribution and outofdistribution ood, i.e. subject to a task distribution shift metalearning evaluation. it contends that current fewshot classification benchmarks reflect ood performance, and that approaches which perform well in the ood setting may not necessarily perform well in the id setting. finally, it highlights model selection pitfalls and issues with the consistency of metalearning performance comparisons. reviewers found that the submission raises interesting questions, as evidenced by their engagement in the discussions. they agree that the discussion surrounding id and ood evaluation for fewshot classification is insightful and of value to the research community, especially since a lot of existing theoretical work makes an id assumption. other claimed contributions are less appealing to some reviewers  the model selection results appear inconclusive.  the research community is already moving towards larger and more diverse benchmarks, which lessens the impact of the observation that small test sets are unreliable.  the 6 common id datasets and 4 common ood datasets and the number of approaches investigated remains a small sample size to support the claims made in the paper according to some reviewers. reviewers weigh these strengths and weaknesses differently, and resultingly their opinion is split on acceptance. i read the paper to form an independent opinion and think that it raises interesting questions, even if imperfectly. in any case, i dont believe any reviewer is _indifferent_ to it, which to me is a good sign that its publication would have an impact on the fewshot learning community. i therefore recommend acceptance.", "accepted": 1}
{"paper_id": "nips_2021_73FeFxePGc", "review_text": "inspired by the diversity produced by evolution, this work presents a method of learning a latent space of policies that is conditioned by an rl algorithm like ppo. they show that their model can learn a diverse set of policies, and since the latent space is not too high dimensional, it is easy to adapt to changes in the environment. they demonstrate the model in a gridworld farm environment and a twoplayer soccer environment i also downloaded the zip file from the supplementary materials, and i found the web page useful to understand some results from the animations.. the strengths of the paper best summarized by reviewer pf7s 1. the proposed method integrates the goals of quality diversity into deep rl by simulating an entire population of agents via a generative model of policies. 2. the authors evaluated this method using three different experiments and showed that this method was able to learn a more multimodal and effective policy space than any of the other baselines. there were issues with the writing and clarity, raised by reviewer q4dy. the authors have responded to the questions, and have pledged to clarify issues with the paper discussed in the thread. reviewer xpri, who wrote a critical, but fair and balanced review, also highlighted a list of important points in their review related to not only improving presentation of the results, but also filling in key missing experiments, and improving statistical confidence in the results. the authors performed several additional studies, and after a long and detailed dialog between reviewer and author this is where open review really shines, an understanding is established and the reviewer is satisfied, improving their score to 6 even to 7 if additional seeds can be run on remaining experiments. in summary, this paper presents a simple and promising idea for creating diverse adaptable agent populations in rl; a work that would be of interest to the neurips community. i believe the review process has helped strengthen the paper to a state that we are happy to publish the work at neurips. for these reasons im recommending acceptance of the work as a poster.", "accepted": null}
{"paper_id": "nips_2021_1XwPDFrJObw", "review_text": "this paper presents adaptive sample complexity bounds for offline rl. while the technique is not entirely novel, the results are new and interesting in the literature and thus the ac recommends acceptance. the authors are encouraged to incorporate comments from the reviewers.", "accepted": 1}
{"paper_id": "nips_2021_jV5m8NAWb0E", "review_text": "this work deepens our understanding of pacbayes bounds. while the study is mostly exploratory, and perhaps unconventional for a neurips paper, the reviewers believe that there is clear value to the community, as do i. the connections studied in this work are interesting and have pedagogical value, and it is quite likely that interesting discussions will come out of this work. while the open questionsproblems are not yet resolved, the understanding gained may lead us closer to their being resolved. in more detail, theorem 4 is quite interesting and one reviewer mentioned that the proof checks out. especially from the discussion phase, all reviewers support acceptance of this paper. this paper will make for an interesting contribution to the neurips proceedings, and i hope serve as a great reference for future works as well.", "accepted": 1}
{"paper_id": "nips_2021_u14Kuxl8fN", "review_text": "this paper presents a model for multineuronal spike trains called the mglm. the observation in time bin t is a binary vector of length n, where n is the number of neurons, and it is modeled as a categorical random variable that can take on one of 2n values. the categorical probabilities are modeled via a glm, with past spiking and external inputs as covariates. the authors then allow for timevarying parameters and propose an ompbased fitting procedure. the reviewers were generally favorable of this paper. however, as reviewer kgmn pointed out, the core idea of a multinomial glm to capture dependencies in instantaneous counts is one that has been explored previously ba et al, 2014, so the main technical advance here is in the timevarying parameters and the omp fitting procedure. id also note that many have recognized the limitations of conditional independence assumptions in standard glms for neural data; indeed this is a main motivation for latent variable models like the poisson lds see macke et al, neurips 2011, e.g. and the recurrent linear model pachitariu, neurips 2011. im also skeptical of the exponential complexity in model parameters in the mglm, and the timevarying aspect seems to exacerbate the problem. despite my reservations, i will go along with the reviewers and recommend acceptance.", "accepted": null}
{"paper_id": "nips_2021_8ygF02Zm51q", "review_text": "the papers strength and appeal is in proposing a simple modification to the error feedback mechanism that has both theoretical and empirical benefits. the idea and mechanism proposed in the paper could have significant implications in important problems include distributed optimization and in optimization under various constraints communication restricted, memory restricted, privacy restricted, etc. the proposed ef21 mechanism is also nicely and clearly motivated and presented. for these reason, i think the paper can make for an interesting oral presentation. the acs and myself were disappointed by the the authors choosing to restrict the scope of the paper to presenting the core idea only, without much applications and extensions. it is much more useful to the community and makes for a much stronger paper to include these in the paper, so as to justify importance of the core idea. while the paper is certainly acceptable without them, in order to be a milestone paper and a foundational reference, these further applications and extensions are appropriate. this is certainly much preferred to splitting the contribution into multiple papers each with 1 mpu minimum publishable unit worth of content. in a sense the recommendation for an oral presentation is on credit, and based on the expectation that this will urge the authors to make the paper more comprehensive and complete. i should also say that i found the comment regarding reviewer scores at a workshop entirely inappropriate. while i appreciate the authors frustration with scores they disagree with, the author response is meant to point out factual issues with the reviews and address reviewer questions, and the authors also have the option of commenting on review quality. but the paper should speak for its own merit, and including recommendation letters for the paper, let alone vague and anonymous scores without justification, is not part of the reviewing process. furthermore, the criteria in a broad conference such as neurips and in a technical workshop are different. in particular, for a broad conference, it would be appropriate to include a better discussion and examples of applications and extensions so as to help show the significance of the results to the reviewers, as representatives of the general audience.", "accepted": null}
{"paper_id": "nips_2021_KCd-3Pz8VjM", "review_text": "all reviewers have praised the importance of selecting outlier detectors in an automatic way and recognized the proposed metalearning solution as a novel contribution. important points were raised during the reviewing and discussion period. they concern  the influence of the distribution similarity between train and test to make the metalearning succeed,  the effect of some hyperparameters involved and  some rewriting of the presentation to ease reading notation  english grammar typos and to better motivate the use of a collaborative filtering approach. the paper is accepted subject to the above points are addressed in the cameraready.", "accepted": null}
{"paper_id": "nips_2021_e0nZIFEpmYh", "review_text": "the paper initially received divergent recommendations, but after the detailed author response all reviewers lean to accept the paper. i recommend accept under the expectation that authors will revise the paper according to the detailed author response, addressing the reviewers concerns. this includes, but is not limited to the following points 1 clarifications  delineations, including to prior work. 2 additionally results and ablations tables provided in author response 3 details on model size in flops some can go to supplement if they dont fit in the page limit", "accepted": null}
{"paper_id": "nips_2021_IBdEfhLveS", "review_text": "this paper introduces an interesting problem, where the metaagent is required to quickly learn unseen tasks given some offline data for training tasks. in addition, the paper also formally introduces the mdp ambiguity problem induced by the new problem setup. although the proposed solution offpolicy version of varibad  reward relabelling is a little bit straightforward, it is a reasonable solution to the problem considered in this paper, and the empirical results are also solid. during the postrebuttal discussion period, the majority of the reviewers agreed that the formal description of the mdp ambiguity problem and the bayesian rl perspective is interesting enough to be presented at neurips. therefore, i recommend accepting this paper.", "accepted": null}
{"paper_id": "nips_2021_L5vbEVIePyb", "review_text": "this paper presents an intuitive approach to updating multiple options together while learning in a hierarchical deep reinforcement learning setting. the approach seems principled, and performs well empirically. all reviewers have advocated for acceptance, with the expectation that the requested clarifications in the paper are made.", "accepted": 1}
{"paper_id": "nips_2021_LAwuz_L9U9j", "review_text": "this paper introduces a new type of variational autoencoders that can handle streams of highfrequency eventbased spatiotemporal data t, x, y, p in order to decode dense images from a small set of n events, then combines a pretrained evae with traditional reinforcement learning ppo to learn visuomotor policies to control and fly while avoiding obstacles a quadcopter in the airsim simulator. reviewers have praised the idea, the application of rl to eventbased data streams, the writing of the paper and the extent of experiment and comparison to noneventbased vae  rl. reviewers had some questions about equations, about specifics of some ablations, about rendering time for eventbased simulation, about showing the simulator, etc., that were all satisfactorily answered by the authors. after careful consideration of the paper and given review scores of 6, 6, 7, 7, mean 6.5, it appears that the paper should be accepted. the authors are invited to carry out all the changes they promised to the reviewers, including opensourcing their code.", "accepted": null}
{"paper_id": "nips_2021_GlEWs-V9boR", "review_text": "this submission introduces a timely and important contribution that integrates sdf representations with neural volume rendering. all reviewers are positive, and three of the four reviewers strongly recommend acceptance. the ac agrees. the authors should try to address the reviewers concerns in the cameraready version. this includes adding the ablation studies from the rebuttal period, incorporating the discussion as l3kt suggested, among others.", "accepted": 1}
{"paper_id": "nips_2021_4CRpaV4pYp", "review_text": "the reviewers appreciated the paper and agree it provides an interesting new problem and a useful new method. the authors are expected to address the points raised by reviewers in a final version, including as they outlined in their response. most crucially, the authors should address the issues raised by the ethics reviewers the additional discussion outlined by the authors in their response was deemed an appropriate way to do this and the authors need to implement this.", "accepted": null}
{"paper_id": "nips_2021_LU687itn08w", "review_text": "reviewers are positive, and a consensus is reached for acceptance spotlight through rebuttal  internal discussions. while the method is very simple, the message is clear and the authors have done excellent job on concise and thorough writing and experimentation. as offline rl  d4rl benchmark are becoming mainstream, such work can likely guide the community to explore more impactful research directions.", "accepted": 1}
{"paper_id": "nips_2021_mjyMGFL8N2", "review_text": "this paper provides a new theoretical framework for contrastive learning. in particular, the authors find a new type of assumption, connecting spectral graph theory with selfsupervised contrastive learning, which is much more realistic than what is common in prior statistical theory for contrastive learning. all reviewers agree on the significance of theoretical contribution provided by the paper. one of major concerns was on the connectivity assumption of the augmentation graph raised by reviewer 1aqu. as the authors did in their rebuttal, it is quite useful to provide some concrete visual examples in the final draft, to avoid some misunderstanding. ac thinks it is also useful to provide some realworld or synthetic scenarios when the assumption breaks, possibly with supporting experiments to see how much the assumption is crucial. in overall, ac thinks that the paper is very well written and could be a pioneering work, not only for theoretical purposes, but also for some better algorithmic solutions in the future.", "accepted": 1}
{"paper_id": "nips_2021_Oeb2LbHAfJ4", "review_text": "the paper contributes a generative model for cad sketches. in particular, the work proposes a language to address the heterogeneity in sketch primitives and constraints, which allows a cad sketch to be tokenized such that it can be handled using sequence models such as transformers followed by a constraint optimizer. overall, the reviewers are positive about the paper. the reviewers think the work is well motivated and the paper is clearly written. while several reviewers think the experiments can be strengthened with more comparison, the reviewers generally agree the work is novel and makes reasonable modeling choices, which are valuable to the field. the reviewers raised a set of questions for clarification, which were mostly addressed by the authors in the rebuttal. the authors should further address these points in the revision.", "accepted": null}
{"paper_id": "nips_2021_NXGnwTLlWiR", "review_text": "this paper introduces a novel causal baseline to reduce the variance in policy gradient methods. all reviewers like the paper and i recommend acceptance.", "accepted": null}
{"paper_id": "nips_2021_rJwDMui8DI", "review_text": "this paper presents a largescale  systematic study fitting many modern deep nets to mouse calcium data from the allen institute. this paper received two accepts, one borderline accept and one borderline reject and was discussed on the forum. there was a bit of a discussion regarding the actual contributions of this paper. there was some agreement between the reviewers that the authors oversold the claim that they introduced a novel regression methods and the authors agreed to remove or significantly downplay this claim.there was also some discussion about the lack of clear hypotheses being tested or significant results contributing to our understanding of the visual cortex. the study is an impressive feat from the engineering perspective but at the end of the day from the dozens of models and results presented the fit are relatively low correlation coefficients r0.2 which means r20.04. this is to be expected given how noisy calcium data are and given the small number of stimuli available 100. however, given these scores it is possible that we are essentially fitting noise and indeed the models are all performing very similarly so there is no really salient results. with that said, the positive reviewers also emphasized that the present study seem to be at least partially addressing a controversy in the field because a previous study had found that random models did about as well as trained ones. here at least the authors report the accuracy of trained models that significantly outperform untrained ones. the code is also made freely available and positive reviewers see that as a launching point for additional work on mouse as opposed to primate cortex. all in all, this really is a borderline submission. given the issues highlighted above, this paper could be accepted if space permits.", "accepted": 1}
{"paper_id": "nips_2021_UQsbDkuGM0N", "review_text": "the main concerns by the reviewers were adequately addressed during the rebuttal and subsequent discussion between the authors and reviewers. the reviewers unanimously vote for accepting this paper. the metareviewer sincerely thanks the authors and reviewers for engaging into fruitful discussions.", "accepted": 1}
{"paper_id": "nips_2021_Ai73e_POVd", "review_text": "this was a very borderline paper. the original submission appears to be technically sound and novel, but reading the paper without difficulty requires a fair amount of prior expertise in pa. it is dense to the point that it might impact the significance of the work in the general neurips community. however, the reviewers greatly appreciated the enthusiastic interaction with the authors during the rebuttal, and have generally agreed to accept the paper under the assumption that the authors make significant improvements to clarity in the camera ready. as just one example, the reviewers strongly recommend including a highlevel introduction of pa in the final submission. please read through all of the reviewer feedback carefully and follow through on promised adjustments.", "accepted": null}
{"paper_id": "nips_2021_EIfV-XAggKo", "review_text": "the paper worked on contrastive learning on classimbalanced data and proposed modelaware kcenter for the purpose. it makes use of additional unlabeled dataset which is bigger than the targeted unlabeled dataset for contrastive learning in terms of the number of instances and the number of classes. according to three principles, namely tailness, proximity, and diversity, certain unlabeled data are sampled from the pool dataset to rebalance the targeted dataset. the writing is clear, the motivation is strong, the idea is novel, and the results are significant. thus, it should be accepted for publication. it is quite interesting that the proposed method can satisfy the tailness and the proximity at the same time as shown in figure 1, because the former sounds like accepting longtail classes and the latter sounds like rejecting longtail instances. since contrastive learning should be regarded as pretraining, it may not be very natural if the label space for the downstream tasks is fixed at the time of contrastive learning it is unsupervised and there is no label at all. moreover, since the targeted dataset is smaller, is it possible that a class exist in both datasets but no data has been drawn from this class in the targeted dataset and then all the data of this class in the bigger pool dataset become outofdistribution data by the proposed method i.e, some data should be accepted but will be rejected since this class is missing in the targeted dataset? perhaps i have some misunderstanding because i didnt carefully go through the full paper by myself, but i believe clarifying my questions not concerns, just questions are very helpful and can maximize the impact of your work. btw, the following paper should be related to your work, which can sample indistribution data and reject outofdistribution data though it considered to enlarge but not rebalanceenrich the targeted dataset i.e., quantity vs quality yixing xu, yunhe wang, hanting chen, kai han, chunjing xu, dacheng tao, and chang xu. positiveunlabeled compression on the cloud. neurips 2019.", "accepted": null}
{"paper_id": "nips_2021_P84bifNCpFQ", "review_text": "the reviewers are unanimously positive about the paper. relevant question, good analysis and welljustified remedy.", "accepted": null}
{"paper_id": "nips_2021_GPwmbxtG9Ow", "review_text": "the reviewers were unanimous in their support for this paper, with a clear theoretical contribution to our understanding of an important algorithm. most reviewers also felt the paper was wellwritten, and we thus feel it will be broadly interesting to the neurips community.", "accepted": 1}
{"paper_id": "nips_2021_CCvpHGFOzC3", "review_text": "the paper contributes to the literature of sparse variational gaussian processes applied to an online setting. the authors introduce the novel idea of treating inducing variables and inducing inputs at time t1 as pseudodata to be used at time t, leading to recursive expressions for the posterior distribution. the paper appears technically solid. the most critical reviewer does not oppose acceptance. comments regarding clarity and typos highlighted by the reviewers can be addressed in the cameraready version.", "accepted": null}
{"paper_id": "nips_2021_zEuLFJCRk4X", "review_text": "this paper studies the training dynamics of neural network by considering a stochastic differential equation model based on the local elasticity assumption of features of neural networks. the proposed model is justified by numerical experiments and sheds new light on the training dynamics. overall the referees all find this paper interesting and novel, and hence the metareviewer would recommend acceptance of the paper as a poster.", "accepted": null}
{"paper_id": "nips_2021_GOnkx08Gm6", "review_text": "in this submission, the authors study a very interesting problem, i.e., how to measure from the producer or seller side in a twosided marketplace. this studied task can have great application potential. due to this, here, i recommend to accept this submission. however, i do have the same major concern with reviewer nelm, that is, whether the strong assumptions the authors made can be guaranteed. this can hurt the impact of this submission. further, this submission also can be improved based on the comments from all the reviewers and the discussion between reviewers and authors. hope they find these useful, and make this submission a better one.", "accepted": 1}
{"paper_id": "nips_2021_bYi_2708mKK", "review_text": "thanks for the strong submission. the reviewers were unanimous that the paper provided a valuable contribution, and disagreed only on whether it was submitted to the right track. our job is to provide platform for good work, and since there was no disagreement on the quality of the work, we decided to accept.", "accepted": 1}
{"paper_id": "nips_2021_MMZ4djXrwbu", "review_text": "this paper develops a functional neural network for image restorationreconstruction tasks. the authors develop an adaptive restoration for different parameter levels using a single model essentially by linearly mixing the two networks. this is somewhat akin to doing metalearning for restoration. the authors provide various experiments for a variety of restorations tasks showing improvements compared to existing approaches. the reviewers mostly thought the papers approach was interesting and commended the clarity of the paper. however, they also raised concerns about novelty, differentiation of task and network parameters, baselines. there was a lively discussion in the rebuttal period and both the authors and reviewers engaged in the discussion. as a result most reviewers while somewhat lukewarm lean towards acceptance. i share this opinion. the paper has some interesting ideas but can improve in other aspects. i recommend acceptance but ask the authors to follow the recommendations of the reviewers to improve the final peresentation.", "accepted": null}
{"paper_id": "nips_2021_-DyvEp1VsmT", "review_text": "this paper provides an irl algorithm for control processes with continuous states, discrete controls, and unknown transition dynamics with formal sampletime complexity guarantees. the authors addressed many of the initial concerns raised by the reviewers. the primary remaining ones are 1 clarity and organization of the workparticularly where the assumptions are introduced; 2 the standardness and practicality of the assumption of a known and strictly optimal expert policy rather than trajectory samples. the first issue seems possible to resolve in the papers revision. for the second issue, the reviewers in discussion tended to view the paper as making an important theoretical contribution that could lead to extensions with more realisticrelaxed assumptions or serve as a bridge toward continuous state and control settings. thus, there is still significant merit to the paper even with these concerns remaining. given all of this, i recommend weak acceptance for the paper, but consider it to be the most borderline of the papers i am recommending for acceptance and i am not opposed to papers with stronger proponents being prioritized above it as space limits may require. as a minor comment to the authors i agree with reviewer prjn that the paper title could potentially be misleading and suggest revision to ... continuous state ...", "accepted": null}
{"paper_id": "nips_2021_8RnRLP4SHe0", "review_text": "this paper studies the problem of auditing whether algorithmic filtering implementations respect a given regulation from a theoretical perspective. the authors propose a simple hypothesis test that regulators could use to determine compliance given blackbox access to a filtering algorithm. they show that their algorithm has desirable asymptotic properties. finally, the authors show that under their framework platforms are incentivized to provide diverse content to their users. the topic is very timely and the presented theorymethodology is a strong contribution. initially, some of the reviewers had concerns regarding the presentation of the results. the authors reply as well as especially the followup explanationdescription by one of the reviewers, who enthusiastically championed the paper, cleared up these concerns to some extent. however, it is very important that the authors incorporate the reviewers suggestions in the final version of the paper, including the running example provided by one of the reviewers, so that a wider audience can appreciate the contribution.", "accepted": 1}
{"paper_id": "nips_2021_JhCcUMFEq7", "review_text": "this paper addresses semisupervised 3d viewpoint estimation by lifting 2d cnn features into objectcentric feature cuboids, which are rotated and rendered to generate features maps of alternative views, while also updates in a contrastive manner exploiting labelled and pseudolabelled examples in an iterative manner. all reviewers agree on the novelty of the approach pursued in the paper. the rebuttal submitted by the authors clarified many concerns of the reviewers regarding handling of occlusions and selfocclusions, scalability of the method with respect to the number of annotated examples, evaluation metric and thresholds used.", "accepted": 1}
{"paper_id": "nips_2021_Z2LtauFNu2r", "review_text": "authors propose to use recursively shared filter basis for compact cnns. i agree with most reviewers that the idea of shared filter basis in cnn is not new and similar systems have been discussed before. however, ideas such as imposing orthogonality constraint on the shared basis seem interesting, and the effectiveness is well explained. extensive experiments have been conducted to show improvement. 3 out of 4 reviewers are inclined to accept the paper.", "accepted": null}
{"paper_id": "nips_2021_iCoK73Q9TW2", "review_text": "the reviewers unanimously support acceptance of the paper.", "accepted": null}
{"paper_id": "nips_2021_AhlzUugOFIo", "review_text": "through a productive discussion with the authors, the reviewers came to a borderlineaccept consensus. the proposed directed spectrum measure is simple in fact, the authors say it was an introduced as an intermediate quantity in a 1982 paper by geweke, but appears to have some nice properties. while some reviewers praised the clarity of this paper, reviewer 3jsz and i found the paper rather dense and the unnecessary subscripts didnt help. i hope the authors will work to improve this aspect of the paper before publication.", "accepted": null}
{"paper_id": "nips_2021_v_4XcXsAZUn", "review_text": "this paper tackles the adversarial robustness of largescale gnns, which is practically important but has been overlooked. the authors provide a novel observation that the conventional crossentropy and carliniwagner loss guides the attacker to attack nodes that are already misclassified when attacking large gnns, which results in the waste of the computing budget. then, the authors show that under certain assumptions, a budgetaware loss can achieve the optimal solution, and propose a realization of such surrogate loss, masked cross entropy mce, which only considers correctly classified samples, and thus is more efficient. the authors further propose scalable attacks and a scalable defense mechanism which modifies the aggregation function in the message passing framework. the experimental validation of the proposed attacks and the defenses show that they are indeed effective, and efficient in terms of computation and memory cost. the following are the pros and cons of the paper mentioned in the initial reviews. pros  the tackled problem of ensuring robustness of a largescale gnns against adversarial attacks, is an important yet unexplored problem.  the study of the drawbacks of the crossentropy loss when attacking gnns is both novel and interesting.  the proposed surrogate loss, masked cross entropy, is both novel and effective for attacking largescale gnns, and has a potential to be further explored for other adversarial defense problems.  the proposed attack and the defense methods are efficient and scalable, which makes the model practical.  the paper is wellwritten and is accompanied with wellstructured source code. cons  the proposed attack and the defense methods are heuristic and lack theoretical justification.  the assumption of whitebox attack for largescale gnns is impractical.  the assumption on the expected budget required to ensure the optimality of the solution with the budgetaware loss is unrealistic.  the effectiveness of the proposed attacks are shown on a smallscale graph.  lack of discussion and experimental comparison against relevant baselines, such as li et al. 20, which also considers defense against adversarial attacks on large graphs.  missing memory and time complexities of the different types of attacks. while the initial reviews were a bit split, during the discussion period, the reviewers and the authors actively engaged in very thorough, constructive discussions. this cleared away many of the concerns from the reviewers, and the reviewers unanimously agreed to accept the paper at the end. the authors convinced the reviewers that the assumption of whitebox attack is reasonable in the defenders point of view, and acknowledged the reviewers arguments that the blackbox attack could be more practical when considering the tradeoff between clean and robust accuracy. the authors provided a more detailed explanation of the assumption, asymptotic time and memory complexities, and detailed discussion and experimental comparison against li et al. 20. the lack of strict theoretical guarantees for the proposed attacks and the defenses are not critical, as their effectiveness have been verified with extensive empirical analysis. in sum, this is a strong paper that tackles a novel and practical problem, analyzes the problem with an existing approach, and proposes an effective solution backed up with both theoretical and empirical analysis, which makes it a clear accept. i praise the authors and the reviewers for their constructive and active discussions, and advise the authors to incorporate them into the revision, as well as the new results they provided in the responses.", "accepted": null}
{"paper_id": "nips_2021_MSr3u_FCRW", "review_text": "the paper proposed a novel approach that combines meta learning and reinforcement learning for designing fewshot acquisition functions for bayesian optimization. all reviewers find the problem setup interesting and appreciate the novelty and applicability of the proposed algorithm. after a few rounds of interaction during the discussion phase, the reviewers are convinced about the empirical significance of the proposed work. when preparing a revision, the authors are strongly encouraged to take into account the reviews and accommodate the changes reflected in the author discussionsin particular, to further strengthen the empirical analysis, incorporate new references, clarification of the technical challenge, as well as elaborate on details of the fsaf algorithm and its application scope.", "accepted": null}
{"paper_id": "nips_2021_7HQiArc-sKf", "review_text": "majority of the reviewers are excited about the novel applications of ml techniques to social choice and were impressed by the good performance of the solution proposed in this paper. the paper is wellwritten and the work is solid. some concerns were raised about the significance and motivation of the actual technical problem solved in this paper i.e., learning social welfare maximizers and technical depth in ml. the response was generally effective and clarified some points raised by the reviewers. after the discussions, reviewers opinions did not change much except the score of one reviewer was raised from 6 to 7. the overall sentiment remained positive. after all, the novelty and potential to stimulate future work and discussions outweigh the cons, which is the main reason behind the recommendation.", "accepted": null}
{"paper_id": "nips_2021_PesaDDyvSk", "review_text": "this paper argues, in rl applications specifically focusing on sac algorithm typical deeper and wider architectures dont provide big gains that we see in supervised learning. then paper shows that spectral normalization could be used to improve the performance of deep architectures. the paper focuses on the continuous control problems. the authors did a very good job during the rebuttal period and they managed to address the most concerns raised by the reviewers. specifically, the authors have addressed the limited scope of just discussing on sac by adding ddpg results too.. the paper could be improved by having the idea tested on a wider class of algorithms and with more smoothness controlling techniques, which the authors have identified as the limitations in the work. the results provided in this paper would still be valuable for the neurips community. i would recommend the authors to cite and discuss about the concurrent work suggested by the reviewer fnpt 1. 1 spectral normalisation for deep reinforcement learning an optimisation perspective florin gogianu, tudor berariu, mihaela rosca, claudia clopath, lucian busoniu, razvan pascanu, icml 2021", "accepted": null}
{"paper_id": "nips_2021_Dti5bw14YZF", "review_text": "this paper seeks to remedy the lowresource drug discovery problem by transferring the knowledge from previous assays, namely invivo experiments, by different laboratories and against various target proteins. the authors propose a functional rationalized metalearning algorithm frml for such knowledge transfer. the approach appears well motivated and the empirical results appear advance the stateoftheart. the reviewers provided detailed reviews providing a long list of pros and a rather short list of cons of the approachmanuscript. they are all coming to the same conclusion that this is a good paper for neurips. i follow the recommendation of the reviewers to accept the manuscript. from the scores, it may not be in the spotlight or oral presentation range 7, however, i think it could be bumped up if one aims to increase healthrelated topics in the oral presentation program.", "accepted": null}
{"paper_id": "nips_2021_f_eOQN87eXc", "review_text": "the authors propose a method for distilling an ensemble to a single model by training on inputs which are perturbed to make ensemble member outputs disagree. most reviewers 1b8g, 7xwm, z2qy seemed to agree that the method was sensible, elegant, and clearlypresented, and that the gradientmatching intuition was clever and wellsupported. reviewer p6sf raised significant concerns about correctness of the authors transferability assumption, but after rebuttal and discussion these were mostly addressed. p6sf also raised concerns about clarity, but these too were partially addressed and not in my opinion sufficient to justify rejection. another common concern 1b8g, 7xwm, z2qy was the lack of largescale experiments, but the authors address this with tinyimagenet experiments in rebuttal to the satisfaction of multiple reviewers 1b8g, z2qy. therefore i recommend acceptance.", "accepted": null}
{"paper_id": "nips_2021_NGPmH3vbAA_", "review_text": "all the reviewers agree that this submission made a significant contribution to the community. the combination of moe and spatial dynamic computation is interesting and smart. the experimental study of this paper is valuable to the community. although combining moe to transformer has been studied in the nlp community and thus the originality of this submission is kind of limited, the submission still makes valuable contributions as mentioned by the reviewers. ac has read the submission, reviews and discussion and agrees with the reviewers on their recommendation.", "accepted": 1}
{"paper_id": "nips_2021_b83ibRX55T", "review_text": "the paper proposes a truncated unrolling type method for solving a bilevel optimization with nonconvex lowerlevel. it contains two interesting ideas, namely, initialization auxiliary and pessimistic trajectory truncation. the proposed method admits a convergence guarantee, meanwhile, the computation cost is reduced. the techniques and analytic framework are novel, and enhance the understanding of gradientbased bilevel optimization methods. in summary, this work will possibly inspire some new bilevel optimization algorithms and technical analysis.", "accepted": null}
{"paper_id": "nips_2021__wPmKqEMxss", "review_text": "this paper introduces a novel spatially adaptive progressive encoding layer and demonstrates that it performs well on a variety of tasks. the reviewers found to be interesting and relevant for the neurips community, and i recommend acceptance.", "accepted": 1}
{"paper_id": "nips_2021_nFdJSm9dy83", "review_text": "during the discussion phase, the paper has received intensive discussion between the authors and the reviewers about the merits and the concerns. two reviewers have a strong favor of accepting this paper given that the paper presents a new analysis of adam method for constrained nonconvex optimization and also an improved variant of adam by using recursive variance reduction methods with an optimal complexity. the major concerns are from the used convergence measure that is different from the standard convergence measure, and the authors argument about its implication for the convergence of the standard measure in terms of proximal gradient norm. the ac agrees that the presented results are interesting and the analysis of the adamstyle methods for constrained nonconvex optimization is novel. the concern about the inconsistency between the presented convergence measure and the standard convergence measure is understandable given that the paper considers the constrained nonconvex optimization. however, the authors should weaken their argument about its implication for the convergence of the gradient norm or add more evidence such as empirical results presented in the rebuttal for further supporting their claim. the ac believes the concern should be addressable, and hence recommends an acceptance.", "accepted": 1}
{"paper_id": "nips_2021_F-H4oe3MXXI", "review_text": "the paper proposes an nf with simultaneous manifold and densityonmanifold learning. inflation noise is added to make px smooth and the reconstruction regularizer makes the first d variables independent from noise and hence extracts information of clean data. the proposed method is wellmotivated and shows good performance. we expect that the authors revise the paper according to the discussion made with reviewers.", "accepted": null}
{"paper_id": "nips_2021_IBHP61avv0R", "review_text": "this paper uses a hypothesis testing based approach to provide a statistical assessment of corruption robustness, with a scalable mcbased computational approach. the authors have made an impressive effort to address the concerns of the reviewers. though not all the concerns are fully addressed, the emerging consensus is that the additional comments, clarifications, and also experiments, have satisfactorily addressed many of the concerns raised.", "accepted": null}
{"paper_id": "nips_2021_m7XHyicfGTq", "review_text": "as mentioned by multiple reviewers, the ideas in this work and especially analysis of errorsfailure modes were found to be interesting and novel. however, the reviewers raised significant legitimate empirical and experimental deficiencies that were surprising. this includes lack of standard experimental settings e.g. varying amount of labels from 110 and comparisons to recent state of art that were published before the neurips deadline unbiased teacher, instantteaching, and humble teacher. such adherence to standard practices and fair comparisons to related work are necessary to move the field forward and should have been included initially. during the rebuttal the authors provided these results and comparisons and addressed the empirical concerns. the authors should add these results, in addition to descriptions of how the proposed method differs from ideas presented in those papers. unfortunately, this did not leave much room for discussion of other aspects of the work. for example, there is a question of novelty mentioned by 123z and furthermore there are several uncertaintybased pseudolabeling methods in the context of classification or even segmentation, e.g. a, b that have come out that are not even discussed. the authors should clearly address this and discuss how their methods differ besides utilizing a similar idea in a new task, object detection. finally, several reviewers pointed out several specific writing improvements that should be addressed. these should be included in the final version. overall, the paper did provide interesting analysis and addressed the experimental concerns, and so can be accepted. however, all of the above must be addressed. a in defense of pseudolabeling an uncertaintyaware pseudolabel selection framework for semisupervised learning, iclr 2021. b rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation, ijcv 2021.", "accepted": null}
{"paper_id": "nips_2021_Ur2B8gSfZm3", "review_text": "the paper proposes a method for unsupervised domain adaptation uda by constructing virtual mirror samples of source domain in target domain and vice versa and then aligning the corresponding mirror pairs across domains for matching feature distributions. the method is novel reviewer zzru raised a concern on limited novelty by pointing out the connection with cycada but i think the proposed idea is sufficiently different and shows strong empirical performance. the method has some heuristic components as pointed out by reviewers eg, reliance on kmeans to get clusters, the method to construct mirrors however the authors have pointed out connections to optimal transport theory in their response. i suggest the authors make this more prominent in the revised version of the paper. overall, the paper is above the acceptance threshold in my view.", "accepted": null}
{"paper_id": "nips_2021_MLT9wFYMlJ9", "review_text": "the submission discusses the benefits of spectral normalization for gan training. initial reviewer assessment was mostly positive while one reviewer was a bit more concerned. after a discussion with the authors the reviewer appreciated the contributions while remaining unconvinced that controlling the variance via lecun initialization is the key for sns success. ac thinks this paper provides interesting insights that could spur future research.", "accepted": 1}
{"paper_id": "nips_2021_LKntyz8tp1S", "review_text": "overall, reviewers are mostly positive about this paper especially after author response. the reviewers agree that the paper studies a new and relevant problem, and the concentration bound result is interesting. according to reviewers comments and discussions, it would be nicer if authors can improve the presentation to highlight the challenges and improvement over prior work.", "accepted": null}
{"paper_id": "nips_2021_XXxoCgHsiRv", "review_text": "this paper observes that classifiers with a certain degree of robustness lead to more transferable targeted adversarial examples. the manuscript shows how features learned by robust classifiers transfer better to other classifiers. this provides some further insights building on httpsarxiv.orgabs2102.05110 into transferability, a phenomenon which remains poorly understood in the adversarial ml community. thus, there is merit to the work proposed here. i encourage the authors to take into account discussions from the reviews while preparing the camera ready of their manuscript, and to opensource their code. in particular, the authors acknowledged that their discussion of universality is hard to follow, if not a bit circular, so i recommend following suggestions made in the author response to rewrite section 4. finally, the authors conducted their experiments using a single optimizer see line 71, i would encourage them to confirm their findings on additional optimizers to obtain a more complete result.", "accepted": null}
{"paper_id": "nips_2021_SwfsoPuGYku", "review_text": "the paper originally got 2 marginally above the acceptance thresholds and 2 marginally below the acceptance threshold, all with relatively high confidences. the major challenges include a number of comparisons to existing works being lacking both in the theory and experiments, relatively weak experiments, missing some important details, etc. the authors did heavy rebuttals, including presenting extra experiments, and they seemed to take effect. reviewer xt3g raised hisher score from 5 to 7. reviewer ivrz also raised to 7. considering that the extra experiments and clarifications can be easily incorporated in the revision, the ac deemed that the paper is acceptable, thus recommended acceptance.", "accepted": null}
{"paper_id": "nips_2021_d2CejHDZJh", "review_text": "following reviewers discussion, i recommend accepting the paper under the condition that the authors will explain that its main contributions are applications for the projective clustering problem in probabilitystatistics, and cite the relevant existing solutions from different fields coresets in computational geometry, subspace clustering in db, dictionary learning in signal processing. it should also be clarified that the suggested solution is given as a simple and possibly inefficient solver of projective clustering for the special case where no noise exists the points lie on the subspace. in this case, the paper can serve as an interesting bridge between the cs, ml and statistics community.", "accepted": null}
{"paper_id": "nips_2021_r-oRRT-ElX", "review_text": "this work provides theoretical and empirical evidence that oversmoothing does not necessarily happen in practice it is shown that a deep gcn is expressive as long as properly trained, as well as that it can converge to a globally optimal solution. the paper also discusses the generalization capability of gcns. the reviewers and ac agree that these contributions are valuable to the gnn community and nontrivial. the paper contained some small bugs, but these should be easily fixable in the cameraready version.", "accepted": null}
{"paper_id": "nips_2021_5af9FHClUZu", "review_text": "this paper poses a new method for projecting vectors onto the kcapped simplex. this method reformulates the quadratic program into a form that can harness newton method, a secondorder iterative optimization algorithm, to solve the projection goal. the key novelty is using a secondorder method to iteratively solve for a minimum, as opposed to sortingbased methods with a cubic complexity. out of the 4 reviewers, 3 support accepting the manuscript. detailed technical comments are provided in the reviews and the authors responded to these reviews in detail and appropriately. one reviewer suggests to reject the paper based on the fact that the manuscript has typos and an opinion that some elements of the manuscript would be better moved to the supplement. in essence, i think the low score by that reviewer is not well justified and in my own assessment of this manuscript i have excluded it. whats remaining are three reviewers that support the paper quite strongly overall scores 7,7,6. i therefore recommend to accept this manuscript.", "accepted": 0}
{"paper_id": "nips_2021_2JwLvfKR8AI", "review_text": "the paper introduces a method to train neural networks in a way that is more biologically plausible than backpropagation. the key idea is to derive an update that depends on a global error signal. work in this direction had already been identified for neural network with a scalar output, however the authors generalize the setup with vector outputs, but restricting the architectures to a very particular class of neural networks they introduce called vnn effectively operating on vectors with vectorized weight sharing. they show the resulting update matches the sign of the gradient. reviewers were in agreement the idea was interesting and elegant. doubts remained regarding biological plausibility, with reviewers thinking the authors arguments in that direction were somewhat handwavy. nevertheless, we hope this work can inspire further work in that field that could be even better substantiated. there were some concerns regarding the extensiveness of the literature review that should be addressed for the final version.", "accepted": 1}
{"paper_id": "nips_2021_FLHl3Wg4sv", "review_text": "this paper develops an approach to mitigate catastrophic forgetting in continual learning using neuron calibration techniques, including model weight and feature calibration, with rehearsal over the data. the resulting approach has impressive performance on taskincremental learning, especially compared to the other baselines. the paper is welldeveloped and clear with compelling results. there are still a few remaining concerns that the authors are encouraged to address. most notably, adding classincremental learning results and comparisons with related work would have provided more insights into the strengths and weakness of the proposed approach. it would also help improve the strength of the results and broaden the paper. the authors are also encouraged to see if they can provide a deeper explanation of the results in figure 2, helping to explain why the their method performs so well in comparison to offline learning. the authors responses during the discussions included a number of clarifications and detail that should be added to the paper, including a promised clarification on what exactly is meant by the taskagnostic aspects of the approach.", "accepted": null}
{"paper_id": "nips_2021_uDeDDoFOEpj", "review_text": "the submission discusses how to unify methods for semantic, instance and panoptic segmentation. for this the authors propose to use a group of learnable kernels and a corresponding update strategy. all reviewers appreciated the contributions of this paper and recommend acceptanceleaning to accept after having read each others reviews and the rebuttal, which addressed some concerns about missingincomplete ablations. however, all reviewers also pointed out that presentation and writing are not satisfactory and lack clarity. ac read the paper and concurs. e.g., with the constructed m_0, k_0, and f, the kernel update head can start produce groupaware kernels k_s, s  1, 2, ..., s iteratively by s times and obtain the refined mask prediction m_s. randomly picked sentence. sentences like this are hard to parse and may prevent others from replicating the results. this being said, the authors promised to release code and models.", "accepted": 1}
{"paper_id": "nips_2021_70fOkZPtGqT", "review_text": "overall, the paper contains interesting technical contributions on generalization bounds for dro problems. however, the technical exposition makes it difficult to access the results in the paper and has left much to be desired. in particular, the motivations and technical development need to be substantially improved.", "accepted": 1}
{"paper_id": "nips_2021_B0rmtp9q6-_", "review_text": "the paper presents a new approach to select kernel functions via transformers. the proposed idea is interesting, and is a novel contribution to the field of gaussian process and kernel selection with significantly reduced computational overhead during test time. experimental results can be further strengthened by adding in more ablation studies. moreover, organization and presentation can be improved.", "accepted": null}
{"paper_id": "nips_2021_1J21t9pd1AU", "review_text": "the paper is interested in defining an interactive curriculum for learning from demonstrations. like in weinshall et al., 2018, the idea is that the most useful demonstrations are the easiest ones for the teacheroracle and the most difficult ones for the current learner where the difficulty of a demonstration w.r.t. a policy is computed as its probability w.r.t. this policy. the lesson is backed by a theoretical analysis in the linear case, considering maxentirl and crossentbc algorithms. three approaches are experimentally compared cur selecting the demonstration less difficult wrt the expert and most difficult wrt the current learner, curl where the difficulty wrt the expert is replaced with a heuristic measure and curt that mostly considers the difficulty wrt the expert. the results suggest that the most important is to take into account the difficulty w.r.t. the current learner. the area chair did not take into account the most negative review nb this reviewer was eventually convinced by the good and thorough job done by the authors w.r.t. all reviewers, to explain the issues and to promise to revise their paper w.r.t. all points and suggestions raised by the reviewers. however, the area chair considered that the novelty of the paper w.r.t. kamalaruban et al.s paper ijcai 2019, cited needed to be discussed  a main difference lies in considering maxentirl and crossentbc whereas kamalaruban et al. considered mceirl;  the structure of the proof section 4.3 in kamalaruban et al; appendix c in the submitted paper is similar ;  the experimental setting the car driving problem is the same;  this paper argues that it improves on kamalaruban et al as it does not require the knowledge of the optimal policy, as it replaces the difficulty w.r.t. the expert policy with a heuristic function. the quality of this heuristic function however does not seem to be very important point 3 in response to reviewer ramd; unless i missed it, the authors did not comment further on it and did not consider any lesion of this, to study its impact. the significance of the contributions in the submitted paper w.r.t. kamalaruban et als k, in the following has been extensively discussed, concluding that  k presents an omniscient algorithm, aimed to yield the optimal theta policy parameter using the gradient g_t  k also presents an approximation thereof, based on an estimation of theta_t, though with no guarantees and specifically based on the maxentirl  mceirl freamework. overall, the paper is considering a hot topic and presenting interesting results; the many additional complements explanations, discussion and results in the answers to the reviews let us hope that the revised version will be way better than the submitted one. it is mandatory that in the final version, the authors clarify the novelty issue, and compare their results with the blackbox teaching baseline section 5 in k.", "accepted": null}
{"paper_id": "nips_2021__IvXbsw3Zvu", "review_text": "two reviewers recommend acceptance and, after the discussion period, the other two reviewers dont oppose. i read the paper and i agree that although individual parts of the model and inference are known, it is their combination that brings significance to the work, especially by allowing its application to large scale data in neuroscience. i recommend accept.", "accepted": null}
{"paper_id": "nips_2021_9Oolof9tfnD", "review_text": "compositional generalization is an important problem in machine learning. the problem setup is well motivated and the paper is well written. the proposed method is simple and effective. it combines several existing techniques with innovations. all reviewers agree the acceptance.", "accepted": null}
{"paper_id": "nips_2021_8tgchc2XhD", "review_text": "the paper describes an effective method for classincremental semantic segmentation. the different components integrated to achieve this goal have limited novelty per se, but are combined nicely and result in a system that works well, particularly under the nondisjoint setting. authors are encouraged to include a discussion on limitations in the final version of the paper.", "accepted": null}
{"paper_id": "nips_2021_T1f0YKPP_K", "review_text": "the paper proposes a new model for compositional reasoning with extensive evaluation on synthetic and real image datasets. additionally, the paper contributes a new challenging ref test set based on clevr which is manually verified. given the interesting results and technical contributions the paper provides, i recommend to accept this paper under the expectation that the authors will revise the paper as promised in the author response, including 1 fix notational issues  references, consistent notation 2 additional analysis using different types of kernel and sizes 3 adding results on modularizing the guidance kernels several reviewers raised the concern how modular this design is and how good the grounding is. i encourage the authors to include more of the discussion they provide in the author response to the final version of the paper. i think it would be very convincing to see a quantitative analysis on this, on the precision and maybe recall for completeness as well of the attention, i.e. a quantify the analysis of figure 4, ideally comparing to baselines and prior work. i believe this should be reasonable easy given the annotations provided with clevr, and significantly increase the insight of this work and thus increase its impact.", "accepted": null}
{"paper_id": "nips_2021_ivXd1iOKx9M", "review_text": "the reviewers appreciated the novel idea presented in the paper to use gradientbased search in a continuous relaxation of contextfree grammar rules for synthesis of program architectures. the additional experiments with comparison to enumerative approaches and ablation experiments, as well as more information about empirical savings in search space and relationship with near were also greatly appreciated. there was still a concern about improving the writing and presentation in the paper, which hopefully the authors can improve in the final version taking feedback from the detailed reviews. it would also be great to incorporate the new experiments and discussions from the responses in the final version.", "accepted": null}
{"paper_id": "nips_2021_U9NNzquYEHC", "review_text": "this paper was uneasy to review. the ac asked 10 ppl if they could review it, but still could only find 2 reviewers. with the help of sac, we found a 3rd reviewer. unfortunately, some reviewer didnt fully understand the technicality of the paper, and the last reviewer who knows the subject better did not appreciate the significance of the papers results. based on all reviewers feedback, we can do nothing but reject this paper. the fact that we had a very hard time finding goodquality reviewers for this paper hints that the papers topic is far from mainstream machine learning which isnt necessarily an issue, but the paper is also not presented in a way to appeal to the machine learning community which is a real issue. we recommend that the reviewers either submit the paper to venues more focused on differential equations, or substantially revise the paper to suit a machine learning conference.", "accepted": 0}
{"paper_id": "nips_2021_owQmPJ9q9u", "review_text": "this paper considers deep learning models for applications where rules are important e.g. physics. the idea is to construct a rule encoder to incorporate userspecified rules. specifically, this integration is done by perturbing the observations and enforce some userspecified rules to regulate between the models outputs on the original  perturbed inputs. reviewers conclude that the proposed approach is novel and the machine learning tasks considered in the paper are important. initially they have concerns on missing comparisons with other approaches. this is partly addressed by further experimental results provided by the author feedback. personally i think the paper can be an interesting presentation to the neurips community. i find this paper also connected to existing works related to constrained optimisationcontrol and adversarial training these works also consider regularising networks to satisfy some constraints andor make sure the network returns similar results on perturbed inputs. it would be useful to discuss them in the camera ready.", "accepted": 1}
{"paper_id": "nips_2021_YxxzNLfXBz", "review_text": "this paper presents a datasharing strategy called conservative data sharing cds to tackle the distributional shift of the offline rl algorithm in the multitask setting. all the reviewers evaluated positively about the work, and this would make a solid constribution to neurips. i hope that reviewer comments help the authors for future work.", "accepted": 1}
{"paper_id": "nips_2021__wdgJCH-Jf", "review_text": "the paper provides an interesting contribution of theoretical nature, that essentially shows that one can work with a single encoder but vary the decoder to achieve different ratedistortionperception tradesoff in image compression. the work is likely to stimulate interesting discussion at neurips, and provides useful insights towards future research in lossy compression.", "accepted": 1}
{"paper_id": "nips_2021_t4485RO6O8P", "review_text": "all reviewers are positive about this paper. the idea of using cam to localize the most salient object in an image and use that to reduce background and contextual bias in scenecentric as opposed to objectcentered datasets for selfsupervised learning is interesting and seems to indeed alleviate the issues. i recommend to accept the paper as a poster.", "accepted": null}
{"paper_id": "nips_2021_Esd7tGH3Spl", "review_text": "this paper goes deep into the issues of oneshot supernet based nas methods as well as recent zerocost nas methods. the authors have done an excellent job analyzing empirically the various issues and suggesting improvements to oneshot estimators like deisomorphic sampling and the fact that zerocost ones consistently overestimate the performance of larger networks and the short proof of it and so on. during the rich discussion phase one concern that consistently came up was that the paper was packing a lot of dense information see concerns of reviewer su6e and at times it was hard to read because of that. but the reviewers and authors have excellent suggestions on improving the readability. please incorporate those aspects into the manuscript. reviewer q7zt has suggested a number of improvements including citations to other relevant work and even concurrent work that should be included in the next version. also the new results using the original implementation relu_logdet and the changed conclusion with respect to number of params. overall this paper presents valuable insights to the nas community!", "accepted": 1}
{"paper_id": "nips_2021_9B0JMeySlZM", "review_text": "the paper considers a relevant problem, but reviewers have identified some sloppiness regarding causal claims the discussion of the mediators case on p. 4 struck several reviewers as confusing and possibly confused. the formula given between lines 119 and 120 is sensible with respect to the moderators case, but hardly makes sense with respect to the mediators case. the authors should either explain how to read or understand the formula in the mediators case so that it evidently expresses a meaningful effect differential, or use a different, clearly applicable formula for the mediators case, or at least restraint from suggesting that the said formula is also applicable to the mediators case. unfortunately, the authors replied to this issue in a very nonexplicit way although its a crucial point from the causal perspective. we decided to accept this paper despite this problem. it should be emphasised, however, that we considered it serious enough to discuss rejection since a paper on causality should be careful about causal claims. we therefore ask the authors to consider the issue carefully in the revised version.", "accepted": 1}
{"paper_id": "nips_2021_96uH8HeGb9G", "review_text": "this is an interesting paper, and the reviewers find much to recommend it. moreover, the rebuttal and discussion postreview seems to have addressed many, if not all, the concerns of the reviewers. the issue of the attenuation of the attack was discussed. perhaps the portion of the appendix where this is empirically addressed can be highlighted in the paper.", "accepted": null}
{"paper_id": "nips_2021_WL7pr00_fnJ", "review_text": "this paper investigates the impact of hyperparameters on lottery ticket performance and uses these analyses to provide guidance as to how to best select these hyperparameters. all 3 reviewers praised the paper for its comprehensive and compelling experiments as well as its clarity. there was some concern regarding the stringent definitions used in this work, but clarifications in the discussion were sufficient to resolve these concerns so long as the authors add additional discussion to the manuscript. given the rigor of this work and its relevance to the lottery ticket community, i recommend this paper is accepted.", "accepted": 1}
{"paper_id": "nips_2021_P9_gOq5w7Eb", "review_text": "this paper considers a setting in which one may have different traintest distributions, but the predictor can choose to abstain at a cost of alpha. all reviewers believed that the results were interesting, nontrivial, and clearlystated, and all four recommended acceptance. what criticisms there were were adequately addressed in the author response. please be sure to make any changes that have been promised to the reviewers, and seriously consider attempting to address any other concerns that they might have raised, in particular utdks request for concrete examples.", "accepted": 1}
{"paper_id": "nips_2021_yMf3SLah5-y", "review_text": "the paper studies the uniform upe problem in the offline rl setting with both upper and lower bound. most of the reviewers believe the paper is wellwritten. there is one major concern about the epsilon range. the current results give the bound for eps1sqrts. the paper would be much stronger if results about eps1sqrts is also provided.", "accepted": 0}
{"paper_id": "nips_2021_xdk17QJpf5q", "review_text": "this paper looks at an interesting variant of reinforcementonline learning, where users arrive with different preferences over several criteria. the problem is therefore linked to multiobjective optimization. preferences could have been modelled and tackled differently than by linear aggregation, but one must start somewhere. the content of the paper, as well as its writing, justify its acceptance.", "accepted": null}
{"paper_id": "nips_2021_FyaSaEbNm1W", "review_text": "after the discussion, all reviewers recommend accept 7. for example, one reviewers emphasized the submission made a nontrivial contribution upon existing works li et al., bai et al., in the sense that it formally shows the convergence for binaryconnect for nonconvex problems.. i find this alone sufficient novel. also, the unification and extension of previously suggested algorithms are quite interesting, and the empirical results are not bad for a theoretical paper. the only concern raising from the reviews seems to be that the paper is dense, and i hope the authors can improve improve readability, if possible.", "accepted": null}
{"paper_id": "nips_2021_6p2jG0FJ5j", "review_text": "the reviewers all agree that the idea of using a dense set of inducing points to share information across tasks in the fewshot setting is novel and interesting. the discussions yielded a number of clarifications as well as new experiments on fewshot regression and uncertainty quantification. please be sure to add these to the final draft.", "accepted": null}
{"paper_id": "nips_2021_HFvPfNDTShj", "review_text": "this paper considers the fundamental problem of estimating entropy of discrete distributions and analyzes the error of the empirical estimator for this problem. if one relies on l1 continuity a dependence on the domain size d is unavoidable there will be a log d term. the authors define a new notion of continuity for entropy that provides nice bounds on the estimation error of empirical distributions. the paper is worth publishing.", "accepted": null}
{"paper_id": "nips_2021_xmJsuh8xlq", "review_text": "the paper presents an interesting blend of vae and flow based approach for tts. the reviewers raised several points about the comparisons  including that some of the baselines are possibly not as good as the original work, since original implementations were not released and third party implementations had to be used for comparison. the authors address a lot of these concerns in the discussions and added analyses and clarification that i hope will make it to the final submission as they strengthen the presentation significantly. thanks, to the reviewers for their constructive suggestions.", "accepted": null}
{"paper_id": "nips_2021_VvGIT6AOGsx", "review_text": "the paper proposed a conditional video synthesis model based on the vision transformer architecture and the vqvae architecture all the reviewers considered the paper above the bar. the rebuttal successfully answered several questions raised by the reviewers, with two reviewers upgraded the score to more positive ratings. overall, the reviewers consider the paper a welcomed extension of the transformer plus vqvae paradigm for video synthesis. the quantitative results were convincing. the metareviewer agrees with the assessment and would like to recommend its acceptance.", "accepted": null}
{"paper_id": "nips_2021_kAFq29tuVw0", "review_text": "this paper addresses the problem of speeding up the computation of accurate similarity between two graphs, which is one of the key problems in the highly active field of graph learning. in order to mediate between timeconsumingbutdetailed comparisons and fastbutinsufficient comparisons, this paper proposes a distillation approach. the use of distillation in this context of graph comparison is new and makes much sense, and the tailed experiments also supports the effectiveness of the proposed method although the property for very large graph is still open to investigation. overall, this paper makes a solid technical contribution to this field and will be well received by the neurips community.", "accepted": null}
{"paper_id": "nips_2021_9FREJhzo1q", "review_text": "novel, technically sound contribution to the field of computational neuroscience that proposes a combined neural model for inference and decision making. particularly, the analysis linking the neural activity of the proposed model to experimental observations was deemed valuable by reviewers. accept.", "accepted": 1}
{"paper_id": "nips_2021_tu5Wg41hWl_", "review_text": "all reviewers rate this work as interesting and unanimously recommend acceptance of the paper but still see room for improvement.", "accepted": null}
{"paper_id": "nips_2021_jSz59N8NvUP", "review_text": "this paper proposes retrieve to address the computational cost issue in previous semisupervised learning algorithms. it is wellwritten and wellmotivated. the proposed idea is incremental but technically sound. the claims are well supported by theoretical analyses and extensive experimental results.", "accepted": null}
{"paper_id": "nips_2021_utt-q6jW5_w", "review_text": "this paper considers the problem of biologically plausible learning rules. for this, it looks at a local learning rule that uses reinforce to learn local updates for individual neurons. the main contribution is a novel variance reduction scheme for reinforce. while there were some fundamental question raised on biologically plausible learning rules which is an active area of research, the reviewers found this a significant contribution and the paper interesting. as such, i would recommend acceptance of this paper.", "accepted": null}
{"paper_id": "nips_2021_nVwJse40s1", "review_text": "this paper investigates the problem of selfsupervised learning in the context of graph representation learning. it proposes to adopt the technique of adversarial training to automatically augment a training set, and then devises the corresponding adaptation scheme to make adversarial training viable. the authors conducted thorough and insightful experiments also supplementing some experimental results in the rebuttal on several benchmark datasets. although adversarial training was originally proposed as a defensive algorithm aiming at increasing the robustness of a certain learning model, it has also been recognized as a variant of data augmentation or hard example mining in many other works. moreover, employing the philosophy of adversarial training to help generalization has also been discussed in other fields like nlp and style transformation. thus, it is reasonable to see that the same theory can be verified in the domain of graph learning. of course, verifying this theory in a new domain like graph learning requires huge efforts and insightful designs, which holds as the main contribution of this work. the proposed method is relatively simple but very effective on the evaluated tasks. in the rebuttal phase, the authors adequately answered the questions from the reviewers, including addressing the issues of writing clarity and evaluations in extra experimental settings. eventually, the four reviewers reached the consensus of accepting the paper. therefore, the ac recommends acceptance as poster regarding this submission.", "accepted": null}
{"paper_id": "nips_2021_-nLW4nhdkO", "review_text": "the paper proposes a new method for ood detection using deep generative models based on bayesian hypothesis testing, that they refer to as the locally most powerful bayesian test lmpbt. overall, the reviewers found the paper wellmotivated and the experiments support the key claims. during the discussion phase, reviewers osss and 6jox increased their score and recommended acceptance. reviewers msuz and huvy leaned towards acceptance but raised some concerns in the initial review; after reading the author rebuttal, i think that the authors satisfactorily address most of these concerns. i recommend acceptance and encourage the authors to incorporate the reviewer feedback in the final version. additional comment while the paper shows that they outperform some existing methods such as ic, lr, llr, i believe there are stronger published results e.g. dose httpsarxiv.orgabs2006.09273 that should probably be mentioned.", "accepted": null}
{"paper_id": "nips_2021_wHoIjrT6MMb", "review_text": "the authors provide the first demonstration that compressed sensing with generative priors can be a competitive approach for mri reconstruction relative to endtoend, l1, and untrained neural network methods. the authors additionally demonstrate that the method is more robust than baselines in the context of certain distribution shifts between training and inversion. the paper also provides novel theoretical guarantees about distributional robustness of posterior sampling approach. there was some concern from one of the reviewers about consistency with the literature about the performance of baseline methods, likely due to training being on rss images with finetuning on mvue images. the authors should provide commentary about this issue, as discussed with the reviewers during the rebuttal. the authors should add additional comments that the paper should not be used for medical purposes without subsequent study by medical professionals, as per the ethics review.", "accepted": 0}
{"paper_id": "nips_2021_apK65PUH0l9", "review_text": "the paper proposed a novel approach that leverages ensembles and selftraining for unsupervised accuracy estimation and error detection. all reviewers find the problem setup interesting and the paper well written, with theoretical justification although relying on strong assumptions and reasonable empirical support. there are some useful suggestions during the discussion phase, in particular for improving the experimental results. after a few rounds of interaction during the discussion phase, the committee reached a consensus on the technical contributions reviewers agree upon the technical novelty as using ensembles and selftraining for improving model disagreement  a subtle but different approach to the existing work of proxyrisk, which warrants the novelty of this work; it is worth mentioning that reviewers also note that the use of domain invariant representations, a check model, and disagreement are in a similar spirit to prior work. an initial disagreement among the committee was on the scope of the experiments whether a modified version of proxyrisk with an ensemble is necessary to be included. although the modification suggested in the discussion phase by reviewer 6edi deviates from the proxyrisk algorithm proposed in the original work, the committee agree that this could be viewed as an additional ablation of the role of ensembling vs selftraining, which would have made the work stronger. the authors are encouraged to take into consideration of such feedback when preparing a revision.", "accepted": null}
{"paper_id": "nips_2021_HbViCqfbd7", "review_text": "recently, neural networks nn have had great success in approximating solutions of partial differential equations pdes. one key observation is that the nn approach to pdes do not seem to suffer from the curseofdimensionality. hence, it is important to understand the strengths and limitations of this approach. the authors take on a particular class of pdes i.e., linear elliptic pdes with dirichlet boundary conditions and provide a theoretical characterization of how many parameters are needed to approximate their solutions within a desired accuracy. they identify small networks suffice where the number of parameters depend polynomially on the dimension and linearly on the number of parameters required to express the pde. the theoretical analysis is nontrivial and the paper provides a great path moving forward in the pde research vein. while the initial scores of the work were below the threshold, the rebuttal was effective in clarifying the concerns of the reviewers, in particular mooting a counterexample proposed by a reviewer, confusions on the applications of the gradient operators due to the final activations, and the way in which the nns are constructed by authors i.e., growing network in each iteration. as a result, the scores improved uniformly and i thank both the authors and the reviewers for their efforts.", "accepted": 1}
{"paper_id": "nips_2021_USq7LP5pnDH", "review_text": "the paper gives an online algorithm for learning convex piecewise functions under certain assumptions regarding dispersion of the points of discontinuity. this follows recent work on algorithm metadesign and online learning.", "accepted": null}
{"paper_id": "nips_2021_sNKpWhzEDWS", "review_text": "this paper analyzes the inverse decision theory task of recovering the loss function of a decision maker making observational decisions under uncertainty. the papers surprising insight is that uncertainty of the decision maker can enable better loss function recovery. this is supported with sample complexity bounds. there were some initial concerns among the reviewers about clarity in distinguishing clear vs. uncertain decisions and the positioning of the work among some other papers, but the authors waswere able to alleviate those concerns and the reviewers are satisfied with the intended direction of the final version of the paper. the reviewers discussed the lack of experiments in the paper, concluding that experiments demonstrating the analyzed benefitsparticularly with suboptimal decision makerswould enhance the paper, but that the paper was a significant contribution without additional experiments and worthy of acceptance.", "accepted": null}
{"paper_id": "nips_2021_ByPR_hOE_EY", "review_text": "metareview in the context of image classification, the authors propose a method for constructing challenge sets of natural outofcontext examples from boundingbox annotations. they apply the method to coco to construct a suite of challenge test sets, and evaluate their new benchmark task against other algorithms. all reviewers agreed on technical soundness. most reviewers h5rh, lywb, wxhx agreed that a benchmark of natural outofcontext examples is a valuable contribution. reviewer 25hs questions the motivation given that other natural challenge sets exist, but in their rebuttal the authors differentiate by focusing on context shifts specifically. reviewers 25hs and h5rh argue that the method is too specific to coco e.g. due to reliance on bounding boxes, but after rebuttal, h5rh doesnt think this detracts significantly and i tend to agree the methodology seems secondary to the result, i.e. a potentiallyuseful new benchmark. on balance, i recommend acceptance.", "accepted": 1}
{"paper_id": "nips_2021_70eD741FHyI", "review_text": "the authors propose a method for simultaneous semantic and instance segmentation which can be optimized endtoend. in particular, the authors make use of predictions obtained by solving the asymmetric multiwaycut problem, coupled with a surrogate loss whose gradient is estimated using a perturbationbased approach. promising empirical results were presented on coco and cityscapes datasets. reviewers appreciated the clarity of exposition and felt that the proposed approach was a rather elegant solution, notwithstanding the fact that the empirical results were not stateoftheart. the rebuttals did a great job addressing most concerns, but scalability and practicality remain one of the key drawbacks. nevertheless, the reviewers felt that the contribution is significant enough to merit acceptance. i agree and i would like to see the authors include a detailed limitations section to highlight the remaining issues.", "accepted": null}
{"paper_id": "nips_2021_WBVbl8POq8v", "review_text": "this paper combines thompson sampling and doubly robust estimators for the linear contextual bandits. we have had many discussions and comments. the authors successfully answered all the questions raised during the rebuttal. all the reviewers agree that this is a nice paper with a solid contribution.", "accepted": null}
{"paper_id": "nips_2021_9IJLHPuLpvZ", "review_text": "all reviewers thought this paper made useful contributions around strengthening the theoretical foundations of rl through an analysis of an ac algorithm with neural networks. ac is a very popular approach in rl and this will likely be of interest to the large body of researchers interested in rl. the authors are encouraged to address the reviewers feedback in their camera ready paper.", "accepted": null}
{"paper_id": "nips_2021_MXmmuhJYPdU", "review_text": "this work presents a method for single image superresolution based with emphasis on challenging situations such as textured and edge pixels with high uncertainty. the proposed uncertaintydriven loss improves results over other common losses mse and l1. all reviewers liked the paper. they asked some questions such as connection to other works, computational complexity at training and inference time, the reason why linear scaling would be a natural option, and the meaning of uncertainty in the proposed scheme. the authors provided a strong rebuttal and all reviewers at the end lean toward accepting the paper. i concur with them and recommend accept. please make sure to include items suggested by reviewers in the final revision e.g. comparison with other types of the weighted loss function.", "accepted": null}
{"paper_id": "nips_2021_0IqTX6FcZWv", "review_text": "this paper has reached a consensus in that the authors are offering several useful items to the study of embeddings, including a new method that balances between the flavors of two popular classes of existing embedding methods. this is a relatively practical area with many different proposed methods, so the authors approachto bridge known gaps and provide useful theoretical and empirical insightsis welcome. the reviewers have very little variance between them; post clarifications by authors almost all the scores are identical. i also largely agree with the papers strengths. im positive about the contributions offered here. perhaps the main argument against acceptance, as also noted by several reviewers, is that the paper is a bit of a grabbag in a sense, in that theres some theoretical analysis of existing methods, the introduction of a new method, along with a large empirical study not necessarily focused on just evaluating the proposed method. this is not a major problem, since all of these parts are useful, provide interesting insights that are likely to be helpful to practitioners, and the authors writing quality in weaving together these concepts is pretty good. the additional discussion period helped clarify a number of things here as well. i think the authors made a wise choice to study the basics and paint a fairly complete picture e.g., the focus on representation quality rather than generalization, attempting to control all the factors involved in quality in the empirical results. this is a more wellgrounded and scientific approach than is typically found in papers in this area.", "accepted": null}
{"paper_id": "nips_2021_w0ZNeU5S-l", "review_text": "in this paper, the authors propose an approach for smoothing algorithms. while existing works were typically specialised for specific tasks, this works attempts to be more general by smoothing lowlevel operations such as ifelse branches, while loops, and array indexing. the authors successfully demonstrate their approach on four diverse algorithms  bubble sort, shortest path, rendering and levenshtein. the paper received an average score of 6.25, which is slightly above acceptance and one of the reviewers is ready to back up the paper. while the paper would benefit from more polishing, it is overall well written and we would therefore like to recommend acceptance, assuming that the the authors will comply to the following requests, to further improve the paper  while the authors did a pretty good with citing the relevant literature, one reviewer mentioned additional references on stick breaking processes and array indexing. it is important to add these references.  the authors should discuss more the pros and cons of montecarlo approaches vs. the proposed approach. montecarlo approaches, such as the one of berthet et al, can run the original program unmodified, which is a key benefit.  limitations most importantly, the authors should add a section on limitations of the approach. indeed, while the approach is general at first glance, it has restrictions. first, the authors should discuss computational cost in more depth. because of the local averaging, structures like trees would take exponential time to evaluate since they need to be fully expanded. second, it is not clear for what class of programs the proposed approach is gonna work. all implemented applications bubble sort, shortest path, rendering and levenshtein seem to essentially have a fixed computational graph. algorithms like dijkstra or quick sort wouldnt work with the proposed approach because these algorithms have inherently boolean decision steps, and making these decisions probabilistic would break the algorithm. the authors should try to delineate what program characteristics are needed for the proposed approach to work. this will help people who want to use the proposed framework know the operations they can and cant do. lastly, when a program includes smoothed while loops, its not clear if the program is even guaranteed to halt. some further clarifications are needed.", "accepted": null}
{"paper_id": "nips_2021_syu7m80S_CA", "review_text": "the paper proposes an equivariant version of the stein variational gradient descent algorithm for sampling from densities with symmetries. using this sampling algorithm, the authors developed equivariant versions of energybased models for learning invariant densities. the reviewers all agree that this is an interesting paper with nice contributions. i recommend acceptance as poster.", "accepted": null}
{"paper_id": "nips_2021_Fa-w-10s7YQ", "review_text": "the paper presents a sparse steerable convolution to accelerate steerable convolution stconv via sparse tensors, while still preserving the se3equivariance. the authors also show application of the proposed method to 3d semantic analysis including the tasks of instancelevel 6d pose estimation, categorylevel 6d pose and size estimation, and categorylevel 6d pose tracking and they achieve superior performance on these datasets. all reviewers agree on the soundness of the proposed method and its efficiency over steerable convolutions or dense convolutions.", "accepted": null}
{"paper_id": "nips_2021_pzmwfDLoANS", "review_text": "the paper proposes a theoretical analysis on different normalization schemes. it demonstrates that normalizing schemes which dont rely on batchstatistics suffer from channel collapse or lack or expressivity. it then proposes a novel normalization scheme to address those issues. although the practicality of the algorithm could be improved, proxynorm is able to match bn while not using the batch statistics. additionally, the analysis presented in the paper is novel and reviewers agreed that it would be of value to the community.", "accepted": null}
{"paper_id": "nips_2021_3BI2dazLpN", "review_text": "all reviewers appreciated the novel approach for learning to execute symbolic programs using both program syntax and semantics. adding additional experimental evaluation and comparisons to ipagnn and symbolic planner for minecraft was also greatly appreciated. it would be great to incorporate the feedback, and the experimental and architectural clarifications from the reviews and author responses in the final version.", "accepted": null}
{"paper_id": "nips_2021_76tTYokjtG", "review_text": "the paper develops a new, normalizing constant estimation method which is unbiased, and leverages this to achieve improved sampling methods for the same family of distributions. from a technical perspective the insights are definitely above the bar. the main concern that the reviewers and i shared is the presentation of the paper and the lack of intuition provided for some of the more technical aspects of it. while i understand that space is limited, i encourage the authors to expand upon the exposition in section 2 in particular. additionally, the experimental evaluation was lacking, however, in the discussions, the authors provided additional experiments and context which i think will improve the paper substantially.", "accepted": null}
{"paper_id": "nips_2021_hJOLFJIJ_zy", "review_text": "reviewers agreed that this is a solid paper that deserves acceptance. authors are highly encouraged to address the key comments reported by reviewers as well as to implement all the improvements as indicated by authors in the rebuttal in the final cameraready version.", "accepted": 1}
{"paper_id": "nips_2021_wJXWzCsGlZw", "review_text": "the paper presents an algorithm for solving a set of problems in the realizable interactive learning framework via a new sampling based algorithm. all the reviewers liked the theoretical results in the paper. there was considerable and productive discussion among the authors and the reviewers and in the end all the reviewers agreed that the results in the paper warrant publication but the authors must do a better job in the camera ready version of discussing the pros and cons of their bounds and how they related to prior work.", "accepted": 1}
{"paper_id": "nips_2021_an8FSGbuCw", "review_text": "reviewers were in agreement the proposed framework provided a novel and elegant model for learning in neurons with physical delays, with clear writing and presentation, and thorough experimental results.", "accepted": 1}
{"paper_id": "nips_2021_x4oe1W8Hpl3", "review_text": "the authors introduce action hypergraph and an associated task activity parsing, as well as a new dataset moma for complex activity recognition. all of the reviewers are positive, and find the representation has certain novelty and the dataset will be valuable to the community. the ac concurs that this paper is acceptable, and recommends a poster.", "accepted": null}
{"paper_id": "nips_2021_Rav_oC35ToB", "review_text": "the paper presents a faster algorithm for iterative machine teaching, which repeatedly traverses the training set to find samples for the learner. the improvement is obtained by replacing linear scan over the data set with a sampling approach based on localitysensitive hashing. empirical evaluation shows substantial speedups, up to 23 orders of magnitude, and similar energy savings.", "accepted": 1}
{"paper_id": "nips_2021_pvCLqcsLJ1N", "review_text": "the paper considers the problem of machine unlearning after a model is trained on a dataset, there is a request to delete a point from the dataset. the goal is to design an efficient method to update the trained model s.t. is nearly indistinguishable from what we would have obtained had we trained on the dataset without the point. one baseline from prior work is to use dp. the authors give a method that can efficiently unlearn more points that dptraining in the context of in convex learning problems. the reviewers agree that this is an interesting paper with solid contribution, and all of them support acceptance.", "accepted": null}
{"paper_id": "nips_2021_f9mSLa07Ncc", "review_text": "there has been ampled discussion between reviewers and authors. the authors are encourage to leverage the elements in this exchange to improve the paper. in particular, a key conlcusion of the discussion between reviewers is that given the key role of a mixture oracle in this work, all reviewers agree that it would be really important to add a paragraph describing existing identifiability results for mixture oracles, as well as a corollary that gives an exact set of assumptions under which the algorithm is consistent.", "accepted": null}
{"paper_id": "nips_2021_pmWeMLm411_", "review_text": "the reviewers have discussed the paper and have not come to an agreement. reasons to accept the proposed approach is noted to be simple and yet effective, and it can improve the performance of various existing object detectors on different datasets significantly. most of the issues raised in the reviews were properly addressed in the author responses including extra experimental results. reasons to reject some of the reviewers were not satisfied with the provided responses and decided not to support the paper. i think that the overall pros of the paper outweigh the cons and would like to recommend accepting the paper.", "accepted": null}
{"paper_id": "nips_2021_OLyhLK2eQP", "review_text": "this paper proposes a method for 3d scene graph prediction from point clouds in which a graph autoencoder model learns prototypical representations for object categories using scene graph annotations, that are used as prior knowledge during scene graph inference from a point cloud input. reviewers acknowledge the novelty of the autoencoding model proposed for learning categorical priors. reviewers point out that the paper is not clear regarding the input of the autoencoding graph model. they further point out that vague words like common sense are not well defined and are used arbitrarily in the paper. authors are strongly encouraged to clarify the paper writing and corresponding figures, following reviewers comments.", "accepted": null}
{"paper_id": "nips_2021_0qnPBmvJSaf", "review_text": "this paper generated a good amount of discussion between the authors and reviewers, which helped resolve some issues in the original reviews. certain issues remain, but the paper does seem to be making a reasonable and novel contribution in light of those issues. the reviewers have worked hard to suggest improvements and the authors have already indicated the inclusion of new experiments and adjustments to the text. it is expected that the authors will follow through on these promises. finally, i would like to point out that the authors have not properly characterized the related paper pointed out by one of the reviewers. jesse hostetler, alan fern, thomas dietterich. samplebased tree search with fixed and adaptive state abstractions, jair 2017 the authors indicate in their response that the paper assumes more than a samplebased model, but that is not accurate. it does not assume access to the entire set of next states. also note that the algorithm in the experiments of that paper is based on forwardsearch sparse sampling, which is a trajectorysampling algorithm in the spirit of mctsuct. thus a modification to mcts is not as distance as the authors may think.", "accepted": null}
{"paper_id": "nips_2021_Mfi0LZmFB5a", "review_text": "the paper considers the problem of offpolicy evaluation and shows that it is possible to interpolate between the importance sampling estimator, which is unbiased but high variance, and correcting state distributions, which is low variance but could be very biased since the distributions need to be estimated. the authors showed that the optimal estimator with lowest mse is indeed taking an intermediate values in the spectrum to mix these two estimators. the proposed estimator is derived in a principled fashion, the results are insightful, and the paper is very well written. generally this papers contribution are nontrivial yet interesting, and it can definitely be a valuable addition in the vast literature of offpolicy evaluation in rl. some potential drawback of this paper includes simplistic experiments that are aimed only for proofofconcept, and the restriction to to nstep estimators. during the discussion phase, the reviewer also recommends doing another round of improvement before publications to study the confounding effect of these two estimators. please try to do so to improve the already good quality of this work. in general i believe the merits of this work surpass the improvements required. so i also recommend acceptance.", "accepted": 1}
{"paper_id": "nips_2021_HbaQ4FEh-6", "review_text": "despite some suggestions for additional results that could round out the paper, and presentation suggestions for clarifying the papers conceptual contributions, i would like to recommend it for acceptance. in particular  as an algorithmic paper, providing a new suite of algorithms and complementing them with matching lower bounds tells a complete story. experiments would be nice, but given the other results i do not believe they are necessary.  concerning reviewer pyyhs comment about inexact projections, the authors proposed modifications should indeed be implemented carefully. ensuring that the approximate projections do not even in the worstcase over the projection algorithm significantly increase sensitivity affects the privacy guarantee of the final algorithm. so this is an important effect to be accounted for in detail.", "accepted": null}
{"paper_id": "nips_2021_p99rWde9fVJ", "review_text": "this paper studies hyperparameter optimization in fl, which is often neglected but a crucial part of fl. the paper provides interesting ideas to tackle this challenging problem. after the discussion phase, the reviewers are all in favor of accepting the paper. i recommend acceptance. i suggest the authors revise the paper to address the reviewers concerns.", "accepted": null}
{"paper_id": "nips_2021_U7SBcmRf65", "review_text": "the reviewers feel that this paper introduces an interesting and potentially useful approach to learning in changing environments. they consider the approach sensible. various reviewers originally had concerns both about the computational cost and about the experimental validation especially details of hyperparameter optimization and choices of tasks and baselines. after some discussion, the reviewers feel like the authors have addressed their main concerns, and all of them recommend acceptance. i concur.", "accepted": null}
{"paper_id": "nips_2021_-JJy-Hw8TFB", "review_text": "this submission received 4 positive final ratings 7, 7, 8, 6. the reviewers mostly appreciated novelty while noting high similarity with lasr, clear presentation and strong empirical performance. the remaining questions and concerns seemed to be addressed in the rebuttal, as acknowledged by the reviewers. the final recommendation is therefore to accept as a spotlight.", "accepted": 1}
{"paper_id": "nips_2021_uFORMPcA_b", "review_text": "overall there is not enough support from reviewers for me to recommend acceptance. reviewers agreed on some real strengths in the paper, including 1 that is wellwritten and wellorganized, and 2 that it tackles an important problem. on 1  reviewer g6bm i really enjoyed reading this paper! the paper is extremely wellwritten and wellorganized  reviewer cjd3 the authors introduced their automl library in a sorted, logical way on 2  reviewer g6bm the paper considers a very important problem and provides a very satisfactory solution both in technical and practical point of view  reviewer d3tc automl is an important application, and if successful, can greatly reduce data scientists effort  reviewer h7jw automl is a useful tool ... and efficient automl system is an interesting research topic but the weight of opinion was that the paper 1 doesnt present a clear and significant scientific contribution, 2 shows limited empirical validation, and 3 doesnt concern a software package with large enough practical impact on the neurips community. on 1  reviewer cjd3 furthermore, i dont see a clear path for scientific future work building on top of lale and im not convinced that the new operators can express substantially more than the formats of already existing automl tools ... if the expressiveness of the format would be larger than prior work, i would have expected that we need also specialized optimizers for this ... i have the impression that lale could be a convenient package for new automl users, but this alone does not justify a neurips paper  reviewer h7jw be more clear about the scientific contribution from the very beginning of the paper, e.g., a more expressive formalization of the automl system.  reviewer d3tc the highlevel scientific contribution is missing. the paper provides too many low level details without describing what are the main challenges to implement combinators. and it is not clear why the proposed method is contributing to automl on 2  reviewer cjd3 the user studies only include 9 participants, making statements not very meaningful. additionally, the survey might even be biased towards lale  reviewer h7jw organize the presentation of the empirical study following the general scientific principles; and to recruit more participants if possible so that one could draw some statistically significant conclusions. on 3  reviewer cjd3 overall, im not fully convinced that there will be many users of lale at the end of the day. and lale is not even close to the level of pytorch. it has 225 stars on github and was forked 52 times. if that would be the level of impact we expect from a neurips software paper, we will have thousands of papers of those each year. reviewer g6bm disagreed on 1 and 3, and felt the paper does clear the bar, but offered this followup commentary i suggest the authors address those concerns in the next version of the paper to make the paper stronger, e.g., by highlighting the first point more and providing concretequantitative evidence on how much useful the lale library is and will be for the neurips community. the other reviewers offered several suggestions for the authors to improve the paper, in addition to those quoted above  reviewer cjd3 i liked section 3 regarding the gradual automation which is a real problem for new automl practitioners and i believe that there is a lot of untouched potential here. unfortunately, this is not really the main focus of the paper.  reviewer cjd3 maybe jmlr mloss would be a better fit for this paper overall, we werent able to justify acceptance based either on the significance of the scientific contribution or on the practical impact of the software described here within the neurips community. for that reason i recommend rejecting the paper, but i hope that the reviewers feedback is helpful to the authors in improving it.", "accepted": 1}
{"paper_id": "nips_2021_R6U4-Qkcg21", "review_text": "this paper proposes a discriminatorconstrained optimal transport network dotn for unsupervised speech enhancement. the authors apply joint distribution optimal transport with a discriminator under wasserstein gan to generate enhanced speech signals. the novelty of the work is the combination of joint distribution ot and wgan for both domain adaptation and unsupervised learning simultaneously in the application of speech enhancement, which is a regression setting. the authors conduct experiments on voice bank and timit datasets under various conditions and show good performance under the pesq and stoi metrics. the paper is well written. the work is theoretically solid and the performance under pesq and stoi is decent. however, there are a few standing concerns. first of all, the authors should make it clear that both joint distribution optimal transport and wgan are existing techniques by directly citing the previous work when mathematically formulating the problem in section 3.2 even though they are mentioned in the related work. second, there has been a significant concern regarding the quality of the generated speech by dotn in the demo where distortions are quite noticeable perceptually. it is not clear whether this is due to the unsupervised setting. as suggested by one of the reviewers, the authors upload the comparative performance between metricgan and the proposed dotn in terms of pesq and stoi. it shows that dotn has slightly better stoi and is quite a bit lower in pesq since sota of metricgan is optimized under pesq. this appears to clear up the concern to some degree. all reviewers consider the work interesting and the authors are responsive in the discussion period. i would recommend acceptance. but the authors should include the additional results in the rebuttal and discussion in the revised version. furthermore, it would be greatly helpful to include subjective evaluation results in the revision to make the work more convincing.", "accepted": 1}
{"paper_id": "nips_2021_YSzTMntO1KY", "review_text": "this paper presents a method for learning disentanglement of scenes as seen by an agent into scene related content and object related content. it is based on transformers and a latent representation separated into timevarying and timeinvariant slots. the paper received 4 expert reviews, and in the discussion phase very quickly a consensus emerged on acceptance. some weaknesses were raised, in particular with experiments, positioning and mid and longterm objectives of this line work, and the advantages or disadvantages of key design choices missing ablations. the authors provided a thorough and detail review, which answered most questions related to framing the method, and lead to increased ratings. all reviewers agreed that this paper is of interest to the community and recommend acceptance. the ac concurs.", "accepted": 1}
{"paper_id": "nips_2021_VNYKJfYvoCq", "review_text": "the reviewers agree that this generally a good paper although not entirely without minor flaws. please take the reviewers comments in consideration when preparing a revision. the answers provided by the authors were given due consideration.", "accepted": null}
{"paper_id": "nips_2021_jTEGbvLjgp", "review_text": "congratulations, the paper is accepted to neurips 2021! please reformulate your contributions in light of reviewers qxvv comments. please incorporate other corrections and additions from the rebuttalreviews.", "accepted": null}
{"paper_id": "nips_2021_R-ZAZ-K1ILb", "review_text": "there is consensus on acceptance among reviewers. there are a few suggestions and discussions that can be helpful to improve the manuscript, in particular to appeal the potential audience at neurips 2021.", "accepted": null}
{"paper_id": "nips_2021_85BzB3WP-qj", "review_text": "this paper presents a novel probabilistic alignment of stimulus driven neural activity for decoding stimulus identity. reviewers agree that this is an important contribution that can contribute to advancing neuroscientific understanding. at the same time, reviewers also agree on the assumptions that may limit its applicability outside the olfactory system. please make sure to add the discussions through the rebuttal in the final version.", "accepted": 1}
{"paper_id": "nips_2021_8kk8a_zvWua", "review_text": "all reviewers agree that this is an important result filling in the gap of theoretically understanding backdoor attacks.", "accepted": 1}
{"paper_id": "nips_2021_IUqgofswxo", "review_text": "reviewers believe that the improved regret bound in this paper makes a significant contribution and that the paper should be considered for acceptance on the basis of that contribution alone. however, the one sticking point during reviewer discussion concerned the distributional rl connections in the paper. more precisely, a long nested discussion starting with nuces initial review concerns about the mostly undeveloped link to distributional rl also reflected in other reviews and continued towards the end of the discussion chain with reviewer tvfs. the authors presented a detailed mathematical presentation in their final nested comment that convinced reviewer tvfs of the connection being claimed. while ultimately the reviewer concerns were addressed with this final author response, it is critically important that this mathematical presentation and discussion be included on revision, perhaps in an appendix. furthermore, while reviewer tvfs is satisfied with the author response, reviewer nuce would still prefer that a revised paper focus on the regret bound contributions and focus on the distributional rl discussion as a a side remark that opens to future interesting research directions. the authors are asked to carefully consider all of these comments as they prepare their final version.", "accepted": null}
{"paper_id": "nips_2021_Kb26p7chwhf", "review_text": "the work is a well written and well executed empirical study of phenomena associated with large cohort sizes when training practical crossdevice fl models. all reviewers ultimately recommended to accept this paper for publication, and i concur. moreover, i enjoyed the rich discussion between the reviewers and the authors. i have read the paper in detail myself, and have enjoyed the experience. however, since no theoretical explanation is offered, and because the scope of the experiments is necessarily limited, i am not fully convinced about the robustness of the conclusions. because of this, i consider this paper to be a borderline accept.  a few caveats  i believe that some of these observations may have natural theoretical explanations from existingknown theory granted, under some constraining assumptions on the models and losses trained, which the authors are not exploring.  given the experimental setup, whatever the results would be, something would certainly be observed, and a commentary on that something could always be made. would that mean that a pattern or a phenomenon was discovered? maybe, but not necessarily. many more experiments across a vast array of model types and sizes would be needed for a more convincing argument to be made. it is very hard to say whether the presented observations are robust enough  would they be observed for other models and datasets? what would happen, for example, if you experimented with simple linear models that lead to convex optimization problems? what happens if local steps are removed and one uses compressed communication instead? having said that, the observations, however fragile, are useful for further theoretical and empirical studies, and when replicated by other researchers in other settings, may serve as an inspiration to design practical mitigation strategies, such as those outlined in the work.", "accepted": null}
{"paper_id": "nips_2021_VJ7u6SbqorK", "review_text": "this paper proposes the memoryeconomic sparse training mest framework for efficient sparse training on edge devices. the author proposes an elastic mutation em and a soft memory bound algorithm for sparse training. their training methods can keep the entire training process sparse and avoiding involve dense forwardbackward computation. this paper also investigates the impact of different sparsity schemes on sparse training performance in accuracy and speed acceleration. besides, the method also introduces dataefficient training in sparse training scenarios. the rebuttal has resolved most reviewers concerns. all reviewers are quite possible on this paper.", "accepted": 1}
{"paper_id": "nips_2021_lGoKo9WS2A_", "review_text": "the paper proposed a transformerbased generator architecture for gan and showed it achieved comparable image synthesis results with the stylegan2 baseline. it initially received a mixed rating, with two reviewers rated it above the bar and two below the bar. the rebuttal addressed some of the concerns. both of the reviewers that were originally leaning toward a rejection recommendation upgraded the score. overall, the metareviewer agreed that the proposed method has its merit. while, given the success of transformers for various vision tasks, it was kind of expected that one could achieve comparable results to cnns with a transformerbased architecture for a gan generator, it is still nice to see such a design being revealed and shown comparable performance. while it is unfortunate that the discriminator is still based on cnns and the additional capability achieved by the architecture is underwhelming, the metareviewer thought the paper still presents a good first step. the authors are encouraged to incorporate the reviewer feedback in the final version.", "accepted": null}
{"paper_id": "nips_2021__0kknjKJ6BH", "review_text": "a solid paper proposing a new method combining endtoend supervised training with learned regularization in a variational setting. the method adversarially trains the weights of a deep neural network used as a prior for regularization while also learning an endtoend, unrolled deep network. reviewers liked the manuscript and were satisfied by the long rebuttals and detailed conversation.", "accepted": null}
{"paper_id": "nips_2021_nwWLJsTJfv", "review_text": "this interesting paper explores a bilevel approach to solving combinatorial optimization problems by using a learning algorithm to edit the graph until a nonlearningbased heuristic can return the solution. the exposition should be improved, experiments should show ablation studies to understand the effect of network structure, and of course, it would be much better for code to be made available; but the effort taken by the authors in the review period increases my confidence that the cameraready version will address these issues.", "accepted": 0}
{"paper_id": "nips_2021_MgsSQPOYNx1", "review_text": "all reviewers have praised the quality of the paper both substance and form and acknowledged their appreciation of the responses made by the authors to their comments. this is a clear accept, with an expectation from the authors to do their best to incorporate as much as the insightful comments they provided in the feedback into the main text.", "accepted": 1}
{"paper_id": "nips_2021_9rphbXqgmqM", "review_text": "this paper studied an interesting yet challenging problem in machine teaching and provided an intuitive teaching algorithm for synthesizing labels for teaching with theoretical analysis and justifications. all reviewers agree that the problem studied in this paper is practically relevant, with rigorous theoretical analysis and justifications. empirical results on several large data sets seem promising. meanwhile, there were common concerns in the positioning of the work, in particular in the lack of discussion of connections to related work, and some reviews concern the clarity of the presentation e.g. missing some details in experimental results such as computation time, and the significance of the theoretical contribution e.g. whether the averagecase bound is significant for practical scenarios. the authors provided effective feedback during the discussion phase, which helped clarify many of the above concerns. should these concerns be addressed in the revision, it would make a solid paper. the authors are encouraged to take into account the feedback from the discussion phase to further improve the discussion of the proposed algorithm.", "accepted": 1}
{"paper_id": "nips_2021_78GFU9e56Dq", "review_text": "the paper presents a direct way to do instance segmentation in detr instead of producing segmentation masks with an fpn, it regresses segmentation masks from compressed representations as queries. this allows training detr for segmentation endtoend while the original detr and followups have segmentation trained as a second step to detection. this paper is roughly the result of the combination of dfdetr with mask encoding dct, which is criticized by reviewer 94kv and f92m. i believe there is nonetheless originality in making it work so well that the approach reaches very strong numbers in segmentation on coco 39.7 apseg with a resnet50 backbone and 45.9 with a swinl. reviewer f92m constructively points out that the gap between apbox and apseg is bigger for this approach than for others, which could mean that it requires a stronger detector. another interpretation can be that training the dfdetr model with this additional segmentation loss endtoend boosts the apbox. overall, i believe the paper presents a significant contribution, and the authors answered some of the concerns of the reviewers. this is suitable for publication at neurips.", "accepted": 1}
{"paper_id": "nips_2021_uZJJFpFl60W", "review_text": "reviewers all liked the paper, which seems to provide an interesting new take on analysis of adaptive online learning.", "accepted": null}
{"paper_id": "nips_2021_L8-54wkift", "review_text": "the paper presents a significant contribution to distributed optimization over timevarying networks. in particular it establishes a lowerbound on complexity by identifying a specific pattern of timevarying networks, and proposes two schemes one, adom, requiring access to dual gradients, and another one avoiding the need to access such dual gradients that are proven to match this lower bound. this thus gives a rather complete treatment of the problem of distributed convex optimization for timevarying networks as those considered in the paper. while the papers analysis is made for specific assumptions eg on the type of time varying networks and the lower bound is of a worstcase nature, the authors clarified these points, and proposed to develop discussions of these and other points in the final version which should put the paper in a correct perspective.", "accepted": 1}
{"paper_id": "nips_2021_RwASmRpLp-", "review_text": "overview the paper presents a novel and simple method to measure quality of speech. they compare the input against a small set of examples. the comparison is performed used measures such as snr and sisdr which do not require matching references. this is a big advantage for this sort of measurement. the experimental results bolster the claims in the paper. reviews the reviewers were consistent in appreciating the novelty and simplicity of the proposed approach. the few minor nits and reservations that reviewers shared were adequately addressed by the authors in their response. i would urge the authors to incorporate them into the revised version of the paper.", "accepted": 1}
{"paper_id": "nips_2021_J4gRj6d5Qm", "review_text": "this paper investigates the problem of longterm forecasting for time series models. it proposes a transformerbased architecture with an attention mechanism based on autocorrelation and combines it with a periodicitybased time series decomposition approach. the problem studied is relevant and important. the proposed architecture is novel although the basic concepts it builds on are well known in time series analysis. the proposed approach is technically sound and the empirical evaluation of the approach provides adequate evidence that it has the potential to provide improved performance on the long term time series forecasting problem relative to existing approaches. following the author response and discussion, most initial reviewer concerns were addressed and the consensus is that the paper should be accepted. one important point for the authors to clarify are the limitations of the approach with respect to time series with different structural properties e.g, complex seasonality, quasiperiodicity, absence of periodicity, etc..", "accepted": null}
{"paper_id": "nips_2021_8PA2nX9v_r2", "review_text": "all reviewers agree this is a solid paper with a good novel contribution, great experimental results, and detailed analysisablation. the authors did a good job addressing the reviewers concerns in their responses too. clear accept.", "accepted": 1}
{"paper_id": "nips_2021_l7-DBWawSZH", "review_text": "the discussion with the authors helped clear all the reviewer concerns. the scores were all updated to clear accept.", "accepted": 1}
{"paper_id": "nips_2021_PqkKlKQuGZw", "review_text": "this paper analyzes two extensions of the recently studied stochastic model based optimization methods i minibatch variant, ii momentum variant. the authors focus on a class of nonsmooth nonconvex functions which become convex after sufficiently strong tikhonov regularization. the analysis approach is based on leveraging a recent result by davis and drusvyatskiy characterizing the degree of stationarity of a point via the gradient of its moreau envelope, and on utilizing tools from algorithmic stability literature. the authors recover several existing and several new convergence results. the reviewers assessed the paper favorably. there was a reasonable discussion between the reviewers and the authors. i have read the paper myself and agree that the work is of sufficient quality to be published in neurips. i have noticed a number of grammatical and stylistic issues and would recommend the authors to carefully proofread the paper.", "accepted": 1}
{"paper_id": "nips_2021_WDLf8cTq_V8", "review_text": "this submission had a range of scores across reviewers, with votes both for reject and accept. however, there is actually a strong consensus on the strength and weaknesses of the paper. conceptually, its a fairly straightforward idea, but one that may be worth exploring in light of the performance of other doubleoracle methods. however, all reviewers agreed that it is not a particularly novel direction. the numerical strength of the approach is not very positive the authors show good results on games that the authors designed in order to get good performance from their proposed algorithms. on the existing games that were tried, the proposed algorithms had fairly weak performance. at the same time, the reviewers all felt that the authors did a good job experimentally they tried a good number of small and mediumsized games and show that xdonxdo performs poorly in certain settings and quite well in other somewhat contrived settings. thus one could argue that the paper is a pretty good reference point for the method perhaps the approach is not that useful overall, and this paper lays out that performance in a reasonable way. the authors are strongly encouraged to include the additional experiments from the rebuttals.", "accepted": null}
{"paper_id": "nips_2021_bV89lw5OF8x", "review_text": "the paper shows that popular heuristics in deep learning, e.g. dropout, in the context of linear regression amount to regularizing the empirical objective with subquadratic penalties which promote sparsity. the paper introduces interesting tools that may be interesting outside the scope of the paper. overall, a good paper.", "accepted": 1}
{"paper_id": "nips_2021_kFJoj7zuDVi", "review_text": "the authors give upper and lower bounds on ood generalization error in terms of an expansion function. they propose a model selection technique based on these, and validate with experiments. most reviewers moma, zmen, wfmp agreed that the paper addresses an important problem, namely formalizing notions of ood generalization. a major concern from reviewer moma, also wfmp somewhat is that the bounds given arent particularly insightful, e.g. towards understanding methods which make ood generalization possible through structural assumptions e.g. icp, irm. reviewer moma also felt that the paper overclaims throughout, e.g. in claiming a complete characterization of ood generalization. i agree with both of these criticisms, the authors rebuttal notwithstanding, and moreover i think theyre important enough to prevent acceptance at this time.", "accepted": 0}
{"paper_id": "nips_2021_ICBPhB079dQ", "review_text": "the paper presents a new framework for multilabel learning that does not require annotators to precisely assign class labels for each instance and thus is costeffective. the theory part supports the algorithm design and the experiments show good results. the authors responses addressed the questions of the reviewers.", "accepted": 1}
{"paper_id": "nips_2021_0BHU7WvZ29", "review_text": "to solve the image denoising problem, this paper proposed to tune a single gain parameter for each channel on a single test image. the tuning is done via minimizing an unsupervised loss. compared to nonfinetuning methods, tuning can provide better adaptation to outofdistribution images. compared to other tuningmethods, this approach is designed to avoid the overfitting issue of finetuning all hyperparameters. the idea is simple and natural, and this work has presented many experiments to show that it outperforms existing methods. after rebuttal, all reviewers think that the rebuttal has addressed their most concerns, and agree with the acceptance though some are on the borderline. thus i recommend acceptance. note that one reviewer requested to add experimental comparison with conditioned models such as dvdnet. the goal of the reviewer is to understand the difference with the works that study image restoration by providing info about the degradation to the restoration neuralnet. it will be good to see which method is better.", "accepted": null}
{"paper_id": "nips_2021_qZpOqPbwhy", "review_text": "thanks to the authors for their engaging submission on an important topic. there was a lot of positive response in the reviews to this work, and all reviewers were positive about the underlying idea. however, there are areas in which this submission could be strengthened. a major reviewer concern is understanding the strengths and weaknesses of the proposed method. an explicitly stated motivating factor is that the parametric assumptions of standard mixed effects models are too restrictive, and that the flexibility afforded by learning the reimannian manifold will allow models to better fit the data and form better trajectories. and intuitively this makes sense. but the empirical evidence of this is lacking. as reviewer w1hq points out, a standard linear mixed effects model would make for an informative baseline to empirically justify these claims. as the abstract introduces this method as an improvement upon and alternative to a standard linear mixed effects model, having a direct empirical comparison would help identify what parts of the proposed method are most performant, and in what situations the proposed method may be inappropriate. further, a simulation study where aspects of the data generating process are known and manipulated could enhance such a direct comparison to a standard mixed effects model.", "accepted": null}
{"paper_id": "nips_2021_BFYlnDtJSqW", "review_text": "this paper proposes an optimized algorithm to compute a sequence of forward  backward  offload  prefetch operations on activations that optimizes training throughput of linearized dnns under memory constraints. the rebuttal solves the reviewers concerns, and the reviewers unanimously agrees to accept the paper.", "accepted": null}
{"paper_id": "nips_2021_d3k38LTDCyO", "review_text": "the reviewers found this paper somewhat unsurprising, but potentially highimpact and useful to the community. they also noted that papers like this one can be judged harshly, subjecting the authors to a potentially infinite number of experimental comparison. in this case, the consensus was that the revisionsresponses from the authors did a good job addressing the concerns of the reviewers. we recommend adopting the suggestions arising from the discussion phase, particularly around the choice of title.", "accepted": null}
{"paper_id": "nips_2021_Z2ZWIvNeVUl", "review_text": "this paper analyzes the stability properties of _deep markov models_ dmms, which are nonlinear dynamical systems that are parameterized by deep neural networks basically, replacing the transitionemission functions of an hmm with neural nets. the form of stability used in this paper is _stochastic stability_, which essentially means that the first and second moments of a dynamical process asymptotically converge. note that this is different from the notion of _algorithmic stability_ used in learning theory. the paper provides sufficient conditions under which a dmm is stochastically stable, and then analyzes how various architectural considerations activation, weights, depth, etc. affect stability. a numerical study rounds out the picture. the reviews were unanimously positive, with 7s accept across the board. the theory is sound; the work feels complete; and the paper is well written and easy to follow, with minor typos. none of the reviewers were all that confident in their assessments perhaps this is a niche subject for the neurips audience, but all agreed that its definitely a solid paper. in my opinion, the main weakness echoed by reviewer fyk8 is that not much discussion is devoted to motivating stability. why is stochastic stability important in applications of dynamical systems? the introduction ignores this question and starts from the assumption that stability is important and, therefore, worth studying. as far as i can tell, the only discussion of why stability is important comes up at the end, in the broader impact statement, which only says, stability is the major concern of many safetycritical systems. ... the methods presented in this paper have the potential for practical impact in many realworld applications such as unmanned autonomous vehicles, robotics, or process control applications. ok, but why? what would happen if an _unstable_ system were deployed in an autonomous vehicle? how specifically does stochastic stability ensure safety in an autonomous vehicle? i can kind of fill in the blanks myself, but id like the paper to be more explicit about how this highly theoretical study has practical impact. one other thing ill note is that the term stability, in the context of ml theory, typically implies something different from stochastic stability which comes from control theory. readers familiar with _algorithmic stability_ see bousquet  elisseeff, jmlr 2002 may be confused or misled by the titleabstract. i recommend adding something in the abstract and intro that a briefly describes stochastic stability and b makes it clear that it is not the same as algorithmic stability.", "accepted": null}
{"paper_id": "nips_2021_oZg-aOyHL-h", "review_text": "a solid progress in a well studied line of research", "accepted": 1}
{"paper_id": "nips_2021_xVLzpMOexqo", "review_text": "the reviewers discussed this paper and came to a consensus that it is a useful contribution and should be accepted. there were concerns about the fact that this work is primarily in the tabular setting and novelty relative to existing work. the restriction to the tabular setting is quite limited, in that convergence behavior is known to be quite different for biased policy gradient methods in the tabular setting namely that convergence is still guaranteed, as opposed to under function approximation where there are known counterexamples. an issue raised by a reviewer is in the omission of previous work for offpolicy policy gradients, that highlight some of these issues, and that consider a different behavior distribution in the objective somewhat similarly to what is done here, though it is less general since it is restricted to the distribution under a fixed behavior. this connection should be discussed, and the issue of potential divergence more clearly highlighted. the authors do acknowledge this in the paper, refering to the work on biased actorcritic, but it is somewhat buried. this paper would be improved with a much more explicit discussion about this, and if or how the authors think these results might extend to function approximation. as mentioned, despite some of these issues, the insights provided, particularly about convergence rates and relationships to the state weightings, are useful.", "accepted": 1}
{"paper_id": "nips_2021_ZDMqRGSksHs", "review_text": "while the review scores are a bit divergent one reviewer scored 4 while others gave greater than or equal to 6, the reviews are overall positive, and some of the major concerns regarding the organization of theoretical guarantee and the advantage of cal a_opt are properly addressed during the discussion period. re. the access to the oracle i believe this assumption is not very strong in light of practical scenarios, although it would definitely be better if the algorithm does not depend on it. also, the authors explained in the rebuttal how to react to such challenging scenarios properly. overall i believe this paper is worth being published, given that the theoretical result is further elaborated in the main text as well as the advantage of cal a_opt is highlighted with sufficient experimental supports, as the authors promised.", "accepted": 1}
{"paper_id": "nips_2021_Ggikq6Tdxch", "review_text": "the main concerns raised by the reviewers were 1 comparison to multitask, singletask, taskspecific experts and cnnbased multitask models. 2 more detailed discussions on the privacy issues 3 a discussioncomparison of communication costs. overall, the reviewers found that the authors response did a good job in addressing all these concerns. indeed, the new resultsdiscussion significantly strengthens the paper and should be included in a revised version.", "accepted": null}
{"paper_id": "nips_2021_zdmF437BCB", "review_text": "the authors study unsupervised domain adaptation. they point out that if the data supports overlap, the encoder might need domainspecific information to recover an invariant representation which elicits a highaccuracy classifier. they propose a method which models this domainspecific information as a latent variable while encouraging it to have minimal influence on a decoder. reviewers agreed that the motivating analysis necessity of domaindependent encoders was informative. most reviewers dkiw, 8khw, 219a thought the experiments were promising reviewer d96z remarked that officehome results were missing, but the authors address this in the rebuttal. reviewers also tended to agree that the method itself was overcomplicated and this might hurt its usefulness in practice 319a, d96z, but i feel this shouldnt by itself prevent acceptance.", "accepted": 1}
{"paper_id": "nips_2021_tjwQaOI9tdy", "review_text": "this paper received review scores with a high variance 5,6,6,9. the paper was actively discussed, both with the authors and in private. reviewer ug8s score 9 championed the paper for its simplicity and strong results. yet, the review text and the arguments given e.g., i see no real reason for this work for not being accepted. do not sound like a score of 9 at neurips, more like a 6 or 7. negative points in the reviews and the discussion were the lack of originality the paper heuristically combines existing methods rl in order to sample the initial population of evolutionary methods, to solve symbolic regression problems. a brief note about the neurips checklist the authors responses to the questions are very casual e.g. a simple yes would be preferable over it would be a bad idea not to, as if these points are obviously satisfied; however, the paper did not attach the code and as the code repo merely gave www.anonymous.submission. that does not count as the code being available; it should be available for reviewers to check in an anonymized repo or attached as supplementary material. overall, in view of the strong results and the simplicity of the method, i am leaning towards acceptance as a poster.", "accepted": 1}
{"paper_id": "nips_2021_ngdcA1tlDvj", "review_text": "four knowledgeable reviewers praised the idea of combining forward chaining with differentiable and probabilistic programming to tackle tasks such as vqa where the combination of symbolic and subsymbolic systems clearly pays off. during the rebuttal, authors provided enough context and details to better cast the proposed scallop into the literature and to better evaluate the experiments. the paper is accepted, as it can spawn further discussion in the neurosymbolic community about such an important research direction. however, acceptance is conditional on the authors including all the promised and discussed details in the cameraready. one additional point to clarify in the cameraready is how scallop differs from modern implementation of deep problog where essentially forward reasoning is used.", "accepted": null}
{"paper_id": "nips_2021_i8kfkuiCJCI", "review_text": "the paper discusses a method for unsupervised learning of correspondences across video frames. different from prior work like crw 13 the proposed method offers additional flexibility e.g., an anchor can be matched to various points. to establish the use of this additional flexibility the reviewers asked for additional baselines which the authors provided in the rebuttal. in a discussion one reviewer remained concerned about the novelty and its use the authors admit that the improvement compared to 13 is slight. moreover, the authors also state that testing on kinetics or trackingnet would be useful. in particular, we recognise that training on kinetics400 would enable a direct comparison to crw 13. up until writing of this metareview the authors have not provided this result. ac thinks a careful comparison to very related work is desirable and shouldnt be omitted. further, as suggested by the reviewer that remained more concerned, ac concurs that the writing and organization of this paper should be improved significantly and ac can understand why the reviewer remained concerned. ac strongly encourages the authors to improve the paper by adding additional experimental evidence and by assessing organization.", "accepted": 0}
{"paper_id": "nips_2021_YTkQQrqSyE1", "review_text": "all reviewers are in unanimous agreement for acceptance. please incorporate the reviewers feedback for the cameraready version. congratulations on nice work!", "accepted": 1}
{"paper_id": "nips_2021_ykN3tbJ0qmX", "review_text": "this paper tightens the standard variational bound optimized in bayesian deep learning by drawing on collapsed variational inference. the authors consider the prior parameters as latent variables and derive a hierarchical variational inference procedure in which the toplevel latent variables are marginalized out. the authors showed strong empirical performance of their method compared to a variety of baselines. the paper provides a nontrivial methodological contribution to the bayesian deep learning community. its mathematical derivations are not simple but generally wellexplained the authors sometimes dont clearly distinguish between latent variables and variational parameters at times. the main point of criticism was suboptimal structuring and writing. i strongly encourage the authors to make their paper more accessible by following the detailed advice that the reviewers provided. overall, this is a very good paper.", "accepted": 1}
{"paper_id": "nips_2021_Ri2G086_3v", "review_text": "the reviewers liked both the theoretical guarantees and the empirical performance achieved by the algorithm proposed in this paper, which improves over stateoftheart algorithms for submodular maximization. for the final version, please include the additional comparison to fmz19 that was discussed in the response.", "accepted": 1}
{"paper_id": "nips_2021_K4Su8BIivap", "review_text": "i agree with the reviewers that this paper deserves to be accepted; it has a novel foundational contribution and is well written. the paper presents a new approach that iteratively generates a barrier certificate, an environment model and a policy, in a way that ensures that there are no safety violations even during training. the algorithm requires an initial safe but possibly suboptimal policy, and optimizes it while ensuring that there will not be any safety violations during training. as pointed out by one of the reviewers, the main downside of the paper is that the experiments are not very ambitious; the approach is evaluated on only a few problems with a very lowdimensional state space. however, the task of training with no safety violations is already challenging even for these relatively simple environments. there are also a few places where the explanations could be a little clearer, for example, the way algorithm 2 is presented, it is not entirely clear where the initial barrier certificate comes from. but overall, this is a very strong paper with a solid contribution to the state of the art in safe reinforcement learning.", "accepted": null}
{"paper_id": "nips_2021_1oR_gQGp3Rm", "review_text": "the paper considers the problem of finding second order stationary points via only functional evaluations. the paper considers specifically random search based methods and demonstrates a random search method that establishes convergence to sosps within odeps2 function evaluations. the analysis and results are solid and the algorithmic contribution is strong. the main criticism towards the paper is that of novelty of results compared to existing results on zeroth order optimization convergence to sosps which have been shown to achieve the same rate. the relative novelty here is that the paper focuses specifically on random search method as opposed to approximating gradient type of methods that exist in literature. there are benefits of random search methods as elucidated by the authors. overall the paper is right on borderline. i am recommending accept based on the reviewers unanimous agreement with its contributions. i strongly suggest the authors to do a very clear comparison with flokas et al stating their result and the comparison of their result with it.", "accepted": null}
{"paper_id": "nips_2021_omDF-uQ_OZ", "review_text": "the reviewers came to consensus that the theoretical strength overtakes the concerns such as the assumption on the existence of gcw and the heaviness of the technical materials. i agree with these opinions and please sincerely address the concerns raised by the reviewers in the final version such as the relation with the previous work pointed out by npr2. in particular, though i agree that the dense notation of this paper might be somewhat unavoidable, i expect that the authors make the best effort to improve the readability with more intuitions for them so that the paper becomes a good starting point for multidueling setting.", "accepted": null}
{"paper_id": "nips_2021_lN2Uqm-ScC", "review_text": "the paper studies the connection between the generalization ability of sgld and the covariance structure of its noise term. the reviewers initially had a lot of concerns about the lack of clarity of the paper and some details in the derivation of the proofs. most of these concerns were addressed by the authors in the rebuttal and some reviewers raised their scores as a result. overall, the contribution made in the paper is seen as interesting and worthy of acceptance. however, many reviewers, and myself included, think that the writing in some sections of the paper and the clarity are poor. i strongly advise the authors to follow the suggestions made by the reviewers to improve the manuscript for the cameraready version.", "accepted": null}
{"paper_id": "nips_2021_nVofoXjTmA_", "review_text": "the authors present a simple algorithm for converting a vision transformer vit trained for classification into a detection model. the approach is based on dropping the the cls tokens in vit and appends learnable det tokens, followed by matching these to the groundtruth during training. empirical results are not sota, but are competitive. the reviewers appreciate the simplicity and the relatively strong performance of the resulting model. during the discussion the reviewers agreed that the rebuttal clarified the remaining points. i will hence recommend acceptance. i urge the authors to include suggested improvements in the revised version.", "accepted": 1}
{"paper_id": "nips_2021_KCsNBfdYI7E", "review_text": "this paper proposes a data augmentation scheme that synthesizes image background so that models can achieve better generalization by learning from one synthetic image and generalizing to real natural images. all reviewers recommended to accept. accept.", "accepted": null}
{"paper_id": "nips_2021_zL1szwVKdwc", "review_text": "this paper investigates whether lottery tickets can transfer across architectures of different depths, following up on prior work which showed transfer across datasets for the same architecture. reviewers all found the clarity and originality to be strong and the topic is of significant interest. there were some concerns regarding the soundness of the claims presented, but these were resolved through additional experiments performed during the rebuttal period. i would strongly encourage the authors to include these experiments in the final version of the paper. overall, i think this paper presents a valuable contribution and i recommend acceptance.", "accepted": 1}
{"paper_id": "nips_2021_N6ubGJ2lQf", "review_text": "while the review team recognizes the importance of the problem and like the direction that the paper took, they also felt that the paper is not yet ready for publication. the main concerns are  model is not well motivated. in some problems like oim the feedback assumed here is inconsistent with what is observed in practice nodes vs edges.  the regret bounds dont directly reflect the performance of the algorithm  lower bound is too stylized and may not say much about the actual hardness of the problem while the reviewers appreciate the author responses on those issues, they were not sufficient to significantly change the sentiment.", "accepted": 0}
{"paper_id": "nips_2021_Ee7IOrpLwT", "review_text": "throughout the discussion the reviewers agreed unanimously that the paper provides an interesting theoretical contribution and should be accepted to neurips. we urge the authors to revise the paper according to the comments provided by the reviewers in the rebuttal. thanks for submitting your work to neurips!", "accepted": 1}
{"paper_id": "nips_2021_iEEAPq3TUEZ", "review_text": "the paper proposed a method to unify any number of multimodal control signal inputs for conditional image synthesis. the key thing was a transformer model that could convert a variable number of multimodal control inputs to the discrete latent space of a vqgan. all the reviewers rated the paper above the bar, with one reviewer upgraded the score from 6 to 7 after the rebuttal. the metareviewer agreed with the assessment and concluded the paper is above the bar. please include the reviewer feedback in the updated manuscript in the final version.", "accepted": null}
{"paper_id": "nips_2021_lVBu4PqM9HU", "review_text": "this paper received mixed ratings, with one reviewer recommending acceptance and three rejection. the paper was thoroughly discussed by the reviewers but the authors feedback did not convince the negative reviewers to recommend acceptance. in particular, the reviewers main concerns include the motivation behind the proposed sampling strategy and its theoretical justification, the lack of comparison with some baselines although the authors provided some of these comparisons in their feedback, the reviewers remained unconvinced, and the significance of the improvements obtained in some experiments. based on this, we therefore believe that this paper is not ready for acceptance to neurips but encourage the authors to revise it based on the feedback and resubmit to a future venue.", "accepted": 0}
{"paper_id": "nips_2021_fyL9HD-kImm", "review_text": "this paper develops the tools necessary to construct gaugeequivariant transformers  and to apply selfattention to manifolds or meshes with a focus on 2d smooth and orientable manifolds. the presented methods can be applied to a wide variety of problems. this paper is clearly written and technically sound. the experiments clearly show the strength of the proposed method e.g. in comparison to meshcnns for shape classification. experimental results tables 1, 2 also support the idea that directly incorporating equivariances and invariances into the models architecture is more efficient than using dataaugmentation.", "accepted": 1}
{"paper_id": "nips_2021_bGXIX-CVzrq", "review_text": "the paper is on imputing missing variables when they are missing not at random mnar. all reviewers agree that the paper makes a valuable contribution on a topic that is important but often overlooked. this is a clear accept. in the cameraready version, please incorporate the reviewers feedback. moreover, during discussion, reviewers brought up the idea of illustrating assumptions a1a3 on a simple example, which i would indeed recommend to do e.g in the appendix.", "accepted": 1}
{"paper_id": "nips_2021_Ghk0AJ8XtVx", "review_text": "the paper propose a multihop reasoning system called baleen based on the idea of iterative retrieval. it includes three main ideas 1 condensed retrieval which summarizes the documents at each hop; focused late interaction which ranks the topk scores and only includes those for later computation; latent hop ordering which learns to order the passages. the method is evaluated on two datasets, hotpotqa and hover. both experiments demonstrate the proposed method achieves significant better performance than baselines. all reviewers agree on the quality of the work. the performance is solid. the paper is clear and easy to follow. the authors may want to address the questions raised by reviewers. in addition, the reference part is rather nonstandard. please cite the correct source, i.e. the official publication should be cited if it is published, instead of the arxiv version.", "accepted": 1}
{"paper_id": "nips_2021_U5Af9S_RcI0", "review_text": "this paper continues a line of investigation initiated by chen, valiant, and valiant in neurips20. that work proposed a new model of statistical estimation for worstcase data that is randomly collected and gave a polynomial time algorithm minimizes the expected error. the current work provides a significantly faster algorithm for this problem, matching the guarantees of the previous work, when the data is bounded in ell_inftynorm. moreover, additional new results are obtained in the current work, e.g., for the setting that the data is bounded in ell_2norm. at the technical level, the proposed algorithms rely on some version of online gradient descent, as opposed to blackbox convex optimization in the previous work. with the exception of one reviewer who was unconvinced about the model itself, i.e., the prior neurips20 work, the reviewers ranked this paper above the acceptance threshold.", "accepted": 0}
{"paper_id": "nips_2021_wEOlVzVhMW_", "review_text": "a summary from one of the reviews this paper shows that one needs to be cautious when rescaling your data prior to using a causal structure learning algorithms. the authors introduce the concept of varsortability that measures the agreement in how much the marginal variance tends to increase along the causal order. they show that standardization of your data can hurt performance in identifying the dag or its equivalence class which can be explained by varsortability. the authors claim that this concept of varsortability also explains why even after standardization certain continuous structure learning algorithm perform well. the authors focuses on additive noise models and perform an extensive benchmark. while the initial reviewer opinions were split, the eventual consensus on this paper is that it brings a valuable message of caution regarding developing and benchmarking causal inference methods with simulated data.", "accepted": null}
{"paper_id": "nips_2021_zOngaSKrElL", "review_text": "the paper provides a novel approach to train a bug detector by cotraining a bug injection procedure together with the bug detector. this is an interesting idea, and while the resulting bug detector has a high number of false positives, it was able to find new bugs in pypi packages. 19 of 1000 reported bugs turned out to be real bugs. the bug injection procedure is based on transformations that are meant to introduce bugs in the code at a known location; the transformations are handcrafted, but the model learns where to apply them to make the bugs hard to find. the system also relies on semantics preserving transformations to introduce additional variety to the set of nonbuggy programs. one limitation of the approach is that the bug introducing transformations may not actually be introducing bugs in all cases for example, if they swap two variables that happen to be aliases of each other, and the semantics preserving transformation may not be semantics preserving as one of the reviewers pointed out. overall, this is a strong paper; it presents a novel and interesting idea, and while there was a desire from some reviewers for additional baselines, the evaluation is still fairly convincing. one of the reviewers surfaced a paper that i think is relevant and should be cited generating adversarial computer programs using optimized obfuscations, but that paper is quite different does not detract from the novelty of this work.", "accepted": null}
{"paper_id": "nips_2021_uY-XMIbyXec", "review_text": "this paper studies the influence of biases in training data that are due to distributional shifts on the use of gnns for semisupervised learning. the key idea is to adopt the linearized gnn models and central moment discrepancy cmd between biased training and i.i.d samples for regularization. this paper proposes a new method to tackle this bias issue, i.e., the shiftrobust gnn srgnn. srgnn is shown to perform better than standard gnns in the presence of distributionalrelated bias. however, there exists some limitations as follows. 1 the practicality does there really exist distributional shift in the real world graph dataset, such as cora, citeseer, pubmed, ogbarxiv datasets? meanwhile, all of the experiments in this paper were conducted on postintervention datasets, which may not occur in real life. 2 the relationship uneven labeling refers that there is no random or uniform labeling the training node on the graph. distributional shift means that there exits gap between training and test dataset distribution. is there any relationship between the label shift and the distributional shift? why does the deviation of the label lead to the deviation of the training set? 3 the shift how to quantitatively change the difference between the distribution of the training set and the test set. what is the specific value of the distribution difference in the datasets in table 1 and table 2? does increasing alpha mean increasing the distribution gap? if your answer is yes, please give some theoretical or experimental proofs. if not, please add experiments similar to figure 1, that is, when the distribution difference changes from small to large, your proposed method can alleviate the negative correlation between performance and distribution difference to a certain extent. this paper is a boardline case according to the average rating. while the reviewers had some concerns on the significance, the authors did a particularly good job in their rebuttal. thus, all of us have agreed to marginally accept this paper for publication! please include the additional experimental results in the next version.", "accepted": null}
{"paper_id": "nips_2021_y8y6GJUL01H", "review_text": "this paper considers online optimization problems in hadamard manifolds  i.e., simply connected, complete riemannian manifolds with everywhere nonpositive sectional curvature. the authors consider several different settings  fullgradient versus gradientfree algorithms against both convex or stronglyconvex objectives  and they provide bounds that mirror the corresponding bounds for the euclidean case. the reviews speak for themselves and the concerns raised during the review phase were addressed by the authors. as a result, there were no reservations about making an accept recommendation. in preparing the cameraready version of their paper, the authors should make sure to include all the comments made by the reviewers. in addition, i would have the following recommendationsquestions  an aspect which has regrettably been overlooked in the paper is what the hadamard requirement really means. by the cartanhadamard theorem, every ndimensional hadamard manifold is diffeomorphic to mathbbrn, so the topology of these manifolds is trivial. this is a crucial limitation, because barycentric constructions cannot be defined otherwise at the very least, not easily, and it is not possible to use the theory of the paper to solve optimization problems defined on, say, a matrix group  like the set of invertible matrices mathrmgln, or the set of orthogonal matrices mathrmon. the paper doesnt make any such allusions but, at the same time, its important to state clearly  and early  that hadamard manifolds are topologically and diffeomorphically trivial.  the section on riemannian manifolds should be expanded, and the authors should be careful with the notation they employ i am strongly in favor of representing vector fields as differential operators which they do, sometimes explicitly other times implicitly, but this should be carefully explained, and some elements of the appendix should be transferred to the main text. in this regard, the authors might find helpful lees textbooks on smooth and riemannian manifolds  the notation is already quite close, so this would essentially be a matter of fixing a few glitches, as in the definition of the hessian.  i would also be curious to see a comparison between the authors work and recent riemannian approaches to online optimization like the 2019 iclr paper by b\u00e9cigneul and ganea riemannian adaptive optimization methods and the 2020 iclr paper by antonakopoulos et al. online and stochastic optimization beyond lipschitz continuity a riemannian approach. a more detailed presentation of previous works on the topic  like bonnabels 2013 paper  would also help with the positioning of this work. to be clear, the settings and results are quite different and there is no issue of an overlap, but explaining these differences would be helpful to the reader", "accepted": null}
{"paper_id": "nips_2021__Rtm4rYnIIL", "review_text": "i appreciate the authors for detailed rebuttal and additional experimental results, and the reviewers for engaging in detailed feedback especially reviewers axqz and eb6p for active discussions. the novelty and impact were better communicated after the discussions. i like that the paper studies a generic problem of ilfo from modelbased perspectives, discusses its fundamental differences from il theoretically, and proposes better strategic exploration through models as a practical solution. one concern is some missing referencesbaselines for modelbased gailgaifolike algorithms, e.g. mail baram et al. 2016, but moblie ablation study without optimism appears to cover similar cases.", "accepted": 0}
{"paper_id": "nips_2021_kaIcRYq-NpG", "review_text": "the reviewers are overall positive. the presented approach is mathematically sound and shows that grid cells can emerge from an optimization framework with minimal assumptions. the main criticism raised is whether the approach is useful beyond already existing models. the authors should clarify potential applications more explicitly in the revision, as well as include the additional experiments they cited in the rebuttal.", "accepted": null}
{"paper_id": "nips_2021_sAaymAJB_OW", "review_text": "one of the reviews lacks thoroughness  im happy to disregard this. the work proposes meta learning through neural processes and contrastive learning. there is some disagreement about the level of contribution here, with several reviewers pointing out that the fclr method also contributes contrastive learning, and another reviewer pointing out that this is not so incremental because of extensions in several directions. one reviewer raises a strong technical objection, that the stochastic process perspective taken in the paper seems only tenuously related to the actual training and prediction schemes. the authors refute  using a deterministic embedding of context data that is sampled from a stochastic process is not invalid. the authors agree to amend the work to improve clarity, and the reviewer conceded that clarifying this point in the manuscript would make for a strong contribution. one reviewer raised issues with the experiments  that they do illuminate the method or support the proposed ideas. yet im inclined to agree with another reviewer who found the detailed analysis, descriptions, and hyperparameters in the supplementary material. i also approve of the experiment to show limitations of the method in learning the strongly multimodal case. overall, i think the authors have enough feedback from the reviewers to make some tweaks to some of the explanations in the paper, making this a contribution to the neurips community.", "accepted": 0}
{"paper_id": "nips_2021_P3268DYnsXh", "review_text": "in this paper, the authors present a simple and efficient convnorm method that can fully exploit the convolutional structure in the fourier domain. the paper is clearly written and well motivated. extensive experiments empirically demonstrate the effectiveness of such normalization. as all reviewers achieve consensus that the paper is well written and can be accepted, i vote for acceptance. the authors are expected to make a thorough revision by considering the the reviews.", "accepted": null}
{"paper_id": "nips_2021_dUEpGV2mhf", "review_text": "the reviewers all agreed that this work is promising. the authors also provided important clarifications in the response, including increasing the number of runs and clarifying that hyperparameter selection was actually done with an approach that they developed for this paper suitable for the offline setting. this improvements undoubtedly make the paper stronger. however, the original omission of the strategy to select hyperparameters offline is a large omission, as it is a critical part of the algorithm. this algorithm should be discussed and contrasted to other approaches. the theory was not extensively discussed unfortunately, and relies heavily on the cql theory. the cql theory already has some constants that make for relatively loose bounds. here, there are additional issues with the introduction of beta. the first main result relies on having a potentially very large beta, especially if you look at the proof and see the terms that betasmall positive constant is overcoming. the second result, on policy improvement, relies again a potentially large beta and on a term being positive that is not guaranteed to be positive there is simply a discussion in the paper on why it reasonably could be positive in settings of interest. theory for offline rl is hard, and so progress should be acknowledged. but, at the same time, given the complexity of these results, it is important to more clearly explain the limitations and what this theory can truly guarantee. as a note, the concern about perenvironment hyperparameter tuning was resolved in the discussion. it is reasonable to pick hyperparameters per environment, since you have an automated algorithm to do so. this is very different from optimizing tuning hyperparameters with sweeps, which is not an algorithm to be used in practice but rather an approach to evaluate methods. as it stands, this work is borderline, and would highly benefit from another round to incorporate these changes and be rereviewed. some benefit of the doubt can be given that these changes may be done for the final paper, and so if there is space in the program, this paper could be accepted.", "accepted": 1}
{"paper_id": "nips_2021_enKhMfthDFS", "review_text": "i recommend to accept this paper in this paper, the authors proposed a novel method to address an important yet under explored problem outofdistribution detection for multilabel classification models. this paper is wellwritten, addressing important problem, proposing simplebuteffective method. all the reviewers are inclined to accept this paper. i would suggest the authors to add additional experimental results done in the rebuttal phase and take the reviewers suggestions into the cameraready version.", "accepted": null}
{"paper_id": "nips_2021_0lz4QxW2tDf", "review_text": "the reviewers all agreed that this submission should be rejected. the reviewers find that the method lacks sufficient novelty and significance, and that the experimental results are not strong enough. in particular, the results show that the proposed method does not provide a significant improvement when used as a training objective, and the experiments are not comprehensive enough for the task of evaluation. as a very minor side note, i also noticed that the paper claims that the emd requires the point sets to have the same number of points, which certainly isnt true. the emd is just the wasserstein distance, which can be evaluated exactly between any two discrete distributions regardless of the finite number of points in the support.", "accepted": null}
{"paper_id": "nips_2021_uXc42E9ZPFs", "review_text": "while evaluating this paper, the reviewers had an extensive discussion about the relative strengths, and in particular what counts as novelty. while the reviewers did not come to unanimous consensus here, i am swayed by the majority of reviewers who cited the strong empirical rigor in analyzing the behavior of the proposed models, in addition to the careful combination of many small improvements that lead to strong results on an important application area in predicting the functional effects of protein mutations. indeed, my own view is that the term novelty is often overly constrained to mean a brand new modeling technique when instead it should be interpreted to mean something important that adds to the fields overall understanding and knowledge. indeed, the recent ml literature is filled with important papers that usefully shine new light on previously known modeling techniques or methods. given the strong reviews from the majority of reviewers, and the acknowledgement of the strong empirical results and careful explication of the rigorous evaluations, i am happy to recommend acceptance of this paper. i do expect that the authors will use the extensive reviewer feedback to further revise and strengthen the paper for its final form.", "accepted": 1}
{"paper_id": "nips_2021_VvUldGZ3izR", "review_text": "the paper describes an approach to training an agent to follow instructions for tasks composed of multiple lowlevel subtasks. the method uses reward shaping, whereby the agent receives supplementary rewards when subtasks are achieved. the shaped rewards are determined using two classifiers, a terminal classifier that classifies the completion of a lowlevel subtask, and a relevance classifier that assesses the relationship between lowlevel tasks and the success of the highlevel task. the latter is learned online without the need to break highlevel instructions into their lowerlevel components. experimental results on the the babyai task and a grid world task demonstrate gains in sample efficiency relative to recent baselines. the reviewers agree that the paper considers an important and challenging domain. the problem of understanding natural language instructions has long been the focus of ai and robotics research, and has recently received renewed attention within the broader machine learning community. the idea of exploiting the hierarchical nature of the tasks as a means of encouraging exploration through reward shaping is reasonable and well motivated. the reviewers point out that the approach relies on several strong assumptions e.g., involving the existence of terminal states and access to lowlevel instructions. two of the reviewers also raise concerns about the baselines and the amount of information that they are provided with compared to ella. the author response helped to resolve some of these concerns, but the authors are encouraged to ensure that they are addressed in any subsequent version of the paper.", "accepted": null}
{"paper_id": "nips_2021_gXdOOeqRN8g", "review_text": "this work establishes computational hardness for the problem of learning periodic functions with a small amount of adversarial additive noise added to each example. this problem was recently considered in a work by song et al., 2017 who established hardness in the statistical query model. the current work provides a reduction from the computationally hard problem of continuous learning with errors clwe  a continuous analogue of lwe that was introduced in a recent work where its computational hardness was established. a second contribution of the paper is an efficient algebraic algorithm solving the underlying learning problem when the additive noise is very small. the reviewers uniformly agreed that this is an interesting contribution that should appear in neurips.", "accepted": 1}
{"paper_id": "nips_2021_gISH-80g05u", "review_text": "update the revision of this paper has been reviewed and the paper has been accepted. however, the following additional minor changes are required for the cameraready  acknowledge in the main text the issue that the new model has the same challenges as pulse i.e., input images with darker skin are now stylized as lighter skinned faces in the output.  update the broader impacts section to mention the fact that face data is biometric data and thus needs to be sourced and distributed more carefully, with attention to potential privacy, consent and copyright issues.  it came to the attention of the program chairs and ethics review chairs very late in the review process that this paper deals with face generation, a sensitive application, but was not flagged for ethics review. an emergency ethics review was obtained and the program chairs and ethics review chairs then discussed the paper in detail. based on this discussion, we have decided to conditionally accept the paper, given the ethical concerns related to face generation more broadly. in order for this paper to be fully accepted, authors need to address the following concerns  provide examples beyond lightskinned faces and other potentially biased results or demonstrate some mitigation and reflection on any biased outcomes of the model. include an analysis or disaggregated evaluation acknowledging any limitations in how the model handles faces of differing demographics.  communicate any face data distribution restrictions or limitations to model distribution in light of privacy or malicious use concerns.  acknowledge any potential harmful applications or depictions that could arise from the use of this technology in an expanded broader impacts and ethical considerations section. we appreciate the cooperation of the authors in this process, and we hope that these adjustments and further reflection will improve the overall quality of the work. we hope authors can commit to these improvements as a requirement for inclusion at this conference. the original metareview from the ac follows.  this paper proposes a model for synthesizing faces with diverse styles. the model can generate arbitrary stylized faces and their natural photo versions at the same time. in general, many reviewers find the idea interesting and the results encouraging. there are concerns regarding the technical novelty and comparisons against swapping autoencoder and toonify. the rebuttal addressed most of the concerns and clarified the difference between the proposed work vs. toonify and swapping autoencoder. the ac agreed with the reviewers consensus and recommended accepting the paper.", "accepted": null}
{"paper_id": "nips_2021_fOaks7LY5R", "review_text": "this paper analyzes a setting in which multiple users, each with their own dataset, wish to collaborate so as to learn better personalized model, while maintaining the privacy of their data. in the framework analyzed, all users share a common, lowdimensional embedding model, the output of which is used as the input for each personalized model. the paper presents two algorithms, based on two dp mechanisms, and proves that they are epsilon, deltadp while also providing bounds on the excess population risk w.r.t. nonprivate learning  i.e., the price paid for private learning. this is a very interesting and timely topic at the intersection of federated learning, ondevice personalization and differential privacy. the theoretical results are compelling and clearly presented. the writing is ok, although i noticed a few typos in the intro  mostly missing articles. the reviews ended up being unanimously positive. one reviewer was originally against acceptance, but was later convinced to change their score by the authors response. i am recommending this paper for a spotlight because i believe that dp personalization is an important emerging topic that has not yet received much attention in the community. however, i recognize that the paper did not receive very high scores, so i am comfortable with the sacpcs downgrading it to a poster if need be. note that the submitted manuscript does not contain experimental results, but the authors provided some during discussion at the request of a reviewer. i encourage the authors to include these in the paper. experiments may strengthen the case for a spotlight  or even a full oral. i also encourage the authors to give the paper a close read for grammar, syntax and clarity, as there were multiple complaints about the papers clarity, and i myself think the writing could be polished.", "accepted": null}
{"paper_id": "nips_2021_yRTebElmilN", "review_text": "two reviewers recommend rejection and two reviewers recommend acceptance. after reading the reviews, the rebuttal, the internal discussion among reviewers and after my own reading of the paper, i believe this work provides a genuine contribution to ica for multiple views and therefore i recommend accept. the method provides identifiable components under certain restrictions which makes it robust, and the empirical section shows this advantage of the method and its usefulness in downstream tasks, e.g. reconstructions of leftout subjects in fmri data. two reviewers that recommend rejection have mainly asked for comparisons against other baselines. however, i believe that the authors have included suitable baselines when testing their model. i recommend the authors follow the recommendations of the reviewers regarding the presentation and fixing any typos.", "accepted": 0}
{"paper_id": "nips_2021_kLWGdQYsmC5", "review_text": "this is a very empirical paper that documents an interesting and surprising fact fixedseed identical training runs fit runs can have surprisingly different fairness characteristics. i think this is an important fact that deserves to be known widely in the community and will likely be used to justify a lot of follow up work. the paper also includes a systematic literature survey to argue that this is both a little known fact and one that can affect the validity of some existing results. that said, while the specific focus on fairness is novel, and i dont disagree with the authors that more people need to be aware of the variance that can be introduced by various sources of implementationlevel nondeterminism, i think what the paper claims as implications of these results are mostly things that people should know to do already. for example, the suggestion that people should use statistical tests to check the validity of any proposed improvements is a good one, but it is not one that follows from this research. given that training algorithms are fundamentally stochastic, applying statistical tests to confirm claimed improvements should be a requirement even if implementationlevel nondeterminism were not an issue.", "accepted": null}
{"paper_id": "nips_2021_Yu8Q6341U7W", "review_text": "this paper proposes a biologically plausible learning algorithm that implements unsupervised contrastive learning. the algorithm can learn hierarchical representations. a particular innovation is the use of saccades to generate contrastive samples. reviewers all agree that the contribution is novel and significant, the manuscript is wellwritten, and the computational experiments are convincing.", "accepted": 1}
{"paper_id": "nips_2022_xnI37HyfoP", "review_text": "all reviewers found the paper clearly interesting, and agree that it makes a valuable and novel contribution in the field of tensor learning, and the consensus to accept the paper is thus without ambiguity. however, all reviewers, who were all from the start fairly positive about the paper, and who made a number of constructive comments and suggestions that could improve the paper, were quite disappointed by the responses of the authors which seem to suggest that the latter were only prepared to make minimal changes to address the concerns of the reviewers. this explains why the ratings of the paper are not higher... we obviously understand that it would not have been possible to address some of the concerns of the reviewers during the rebuttal period, but the authors are now strongly encouraged to take into account the comments of the reviewers when preparing the final version of this manuscript. the authors are in particular encouraged to take into account the questions and comments about related work and to the extend possible to include more detailed discussions of the related work and the connections with this work. also, they are encouraged to clarify the technical parts of the manuscript about which the reviewers asked clarification questions.", "accepted": null}
{"paper_id": "nips_2022_82N_rasrUT_", "review_text": "i thank the authors for their submission and active participation in the discussions. this paper studies the problem of devising an rl planner that produces bahviour consistent with human observer preferences. reviewers remarked that the paper studies a timely problem vqj9,gaqq,5gpj, containing clear writing vqj9,pbhq and useful visualizations vqj9, and provides insightful human evaluations vqj9,gaqq,5gpj, and that the method is sound and elegant pbhq,tx5x. during acreviewer discussion, reviewers tx5x, gaqq and vqj9 agree that the author response has addressed the main concerns. i am slightly discounting the negative score by reviewer pbhq as i dont find their suggestion to include more experiments and real world data very concrete or actionable. thus, i overall see support for accepting the paper and am therefore recommending acceptance while encouraging the authors to further improve their paper based on the reviewer feedback.", "accepted": 0}
{"paper_id": "nips_2022_WBv9Z6qpA8x", "review_text": "the papers recognizes an important open question in gnns expressivenesscomplexity tradeoff. current more expressive gnns kwl equivalent gnns are not practical for even small values of k, and several works have found great empirical success in graph learning tasks without highly expressive models. this paper puts forth a more finegrained ruler, which can more gradually increase expressiveness to investigate the expressivenesscomplexity tradeoff. the paper addresses an important problem of going beyond 1wl, however, in such a manner such that expressivity is improved in a progressive way while runtime is still manageable. experimental results on graph classification and substructure identification as well as regression zinc12k are promising. however, the committee has concerns on the presentation of the paper. the authors are suggested to make the paper more accessible by providing more highlevel descriptions and interpretations of the results.", "accepted": 0}
{"paper_id": "nips_2022_F_9w7Wl78IH", "review_text": "this paper proposes to evaluate deep rl algorithms on a family of mdps, rather than single point mdps. this seems a reasonable way to reduce performance variance, though i found the paper to be quite lacking in terms of depth, realism, and real impact. in particular, the construction of the family of mdps seems like a hard problem in general, which does not get a deep treatment in this paper. for instance, for many point mdps that might actually be of realworld interested it might be quite easy to construct a family of mdps in a seemingly reasonable way, but for which change the conclusions in such a way that they do not generalise to problems of actual interest anymore. furthermore, as also raised by reviewers, if the goal is to evaluate generality of an algorithm, it would seem much better to evaluate on a benchmark of carefullychosen e.g., actuallyinteresting diverse point mdps than on a family of similar mdps constructed around a single point. finally, many point mdps might contain substantial randomness themselves, and the distinction and similarlities between sampling multiple random task seeds on a single point mdp versus sampling from a family of mdps is not sufficiently discussed. despite these limitations, i think it is good for the community to engage into discussions on how best to evaluate its methods. i will therefore take the recommendation of the reviewers, and accept this paper. i hope the comments on limitations above will resonate with the authors, or might inspire reasonable pushback about parts on which im perhaps being naive, given the authors will likely have thought about these issues a great deal.", "accepted": null}
{"paper_id": "nips_2022_WcxJooGBCc", "review_text": "the reviewers and the area chair judged the paper as technically sound and found that the problem treated is an interesting variant in the spirit of but different from the wellknown grouplasso. the range of applicability of the approach was considered promising. while the narrative of the proof leading to a convex problem was judged standard and by itself perhaps less interesting, overall the contribution was judged as valuable to the community and we recommend acceptance of the paper.", "accepted": 0}
{"paper_id": "nips_2022_9sKZ60VtRmi", "review_text": "the paper proposes a novel technique to extract symmetry inductive biases from data that can be applied to any neural network architecture. please include more discussions with the related work and possible experimental comparisons in the updated version.", "accepted": null}
{"paper_id": "nips_2022_cxZEBQFDoFK", "review_text": "all 4 knowledgeable reviewers recommended acceptance of the paper 2x accept, 1x weak accept, 1x borderline accept, appreciating the the importance of the studied problem, the first principles approach and the obtained theoretical and empirical results. i mainly agree and recommend acceptance of the paper. still, i ask the authors to carefully consider the reviewers comments when preparing the final version of the paper and in particular improve the presentation in line with the suggestions. also, some of the raised points on limitations should be included in a revised discussion.", "accepted": null}
{"paper_id": "nips_2022_hjqTeP05OMB", "review_text": "the paper studies a regret minimization model of bidding in repeated first price auctions, when a noisy hint about the highest competing bid is available. the question is whether the availability of such a hint can significantly reduce the best achievable regret. the authors give almost matching lower and upper bounds on the regret in several cases e.g., hint is provided as a pointinterval estimate, or if the distribution of the highest competing bid has a small finite support. they also present experimental evaluation of the algorithms. the reviewers agree that this is a practically relevant and theoretically interesting model, and the results are nontrivial and interesting. the proofs are technically involved, although they do not introduce particularly new techniques. while the main contribution of the paper is theoretical, the experimental results nicely complement the theory. overall, this is a nice paper and can be accepted to neurips.", "accepted": null}
{"paper_id": "nips_2022_8ViFz-5Mnnv", "review_text": "after author response and the discussion the paper received 1x borderline reject, 1x borderline accept, 3x weak accept note that one reviewer mentioned the score increase only in the discussion. the main strength are  overall novel framework for zeroshot segmentation  strong performance  the authors revised the paper and addressed manymost of the reviewers concernssuggestions in the author response. i recommend acceptance, with the expectation  the authors provide the additional revisions as promised  if possible address the comment of reviewer 1qtt what if remove eq. 3? it seems pc_new is already good enough from figure 2.", "accepted": 0}
{"paper_id": "nips_2022_uOdTKkg2FtP", "review_text": "the authors propose an improvement to offbelief learning, offteam learning, which closes the gap between belief models trained on fixed policies, and evaluation on learned policies for zsc coordination problems. all reviewers have voted to weakborderline accept  since i see no conceptual issues with the proposed framework and the evaluation seems sound, i will also vote to accept. the major area of constructive criticism, is that the work seems to be somewhat incremental with respect to hu et al. icml 21.", "accepted": null}
{"paper_id": "nips_2022_lKULHf7oFDo", "review_text": "this paper introduces core stability as a fairness notion for federated learning, which is motivated by social choice theory. the reviewers all agreed that the paper provides a novel contribution to studying fairness in federated learning. the authors have also addressed reviewers questions during the discussion period.", "accepted": null}
{"paper_id": "nips_2022_TATzsweWfof", "review_text": "this paper has three main contributions 1 a generalization bound for learning in adversarial learning frameworks such as gans based on rademacher complexity, 2 a proof that local sgda with constant stepsize does not converge for these problems in federated settings and therefore does not achieve linear convergence, and 3 a new method which circumvents these issues. overall, the consensus was that result 1 was both somewhat underwhelming and also seemed somewhat disjointed from the paper. however, results 2 and 3 are compelling, and will be of interest to the federated learning community. this is especially true after the authors removed some of the technical assumptions that they required in the first version of the paper. the updated version of the paper could still use some cleaning up andor reorganizing, however, i think that overall the paper is above the bar for acceptance.", "accepted": 0}
{"paper_id": "nips_2022_0VhrZPJXcTU", "review_text": "this paper clearly documents a wellexecuted exploration of an imitation learning  neural diving approach to improving node selection in bb solvers using a gnn approach. i believe this paper is a useful contribution that pushes forward the important project of integrating modern ml techniques to improve integer programming. the reviewers were less sanguine than the metareviewer. i will explain why. the authors note, and i concur, that node selection strategies in modern bb solvers have been heavily researched for decades, so beating their performance on the majority of these difficult tasks is quite an achievement! this point confused the first reviewer. the second reviewer was disappointed that the authors used an antiquated gnn architecture, or compared to antiquated baselines, but did not substantiate which modern architectures or comparisons would have been better. in the absence of constructive criticism, i construe these as a misapplication of the standards of deep learning very fast progress in beating benchmarks to integer programming a mature field where progress is slower. while several gnn approaches are available for variable selection in bb solvers, node selection is an important and independent challenge. i am not aware of any previous work on deep learning for node selection nor are the authors, which explains why the authors chose to compare their gnnbased approach to other strategies they created and the default strategy in scip rather than the nonexistent deeplearning sota for the problem. this point confused the third reviewer.", "accepted": 0}
{"paper_id": "nips_2022_1l5hEEK_j13", "review_text": "this paper considers the problem of parameter estimation using smoothed observations indeed, after smoothing although the asymptotic variance may increase we also have finite sample guarantees for the mle. although it is not clear how useful this methodology would be for highdimensional applications, i am impressed by this neat observation and the solid writing.", "accepted": null}
{"paper_id": "nips_2022_NqDXfe2oC_1", "review_text": "the paper proposes the notion of performative power to measure a firms ability to affect its users. this notion is insightful in performative prediction, and the authors have demonstrated it in multiple concrete settings. overall, all reviewers are very positive about the paper. we recommend including the paper in the program and believe it could open up potential future research directions.", "accepted": 1}
{"paper_id": "nips_2022_mux7gn3g_3", "review_text": "reviewer qvwu summarizes the paper well the paper presents a study comparing popular metalearning approaches like reptile, pearl and rl2 with standard multitask pretraining  finetuning on 3 vision based benchmarks, namely procgen, rlbench and atari. on all three benchmarks, they test the generalization ability of the approaches on a completely novel task rather than variations of existing tasks from the distribution. they show that on all the tasks multitask pretraining with finetuning performs equally or better than the metarl counterparts proposing multitask pretraining  finetuning as a simple yet strong baseline for such tasks. the other reviewers voted to reject the paper. their main concerns were  evaluation of visiononly benchmarks  issues in evaluation setup i believe the authors have sufficiently addressed these concerns, but unfortunately, the reviewers did not respond to the authors. in particular prior work already shows finetuning is competitive to metalearning algorithms on statebased tasks. the main contribution of this work is in showing that these findings hold true in visionbased settings too and particularly in the scenario where tasks are different. this is a useful addition to the growing body of work comparing vanilla finetuning against metalearning. i therefore recommend the paper be accepted.", "accepted": 0}
{"paper_id": "nips_2022_3y80RPgHL7s", "review_text": "after the rebuttal, the majority of the reviewers were convinced that the paper is novel and interesting and it should be accepted. in preparing the cameraready, i suggest to the authors to take into account the reviewers comments to better explain the used assumptions to the readers and avoid potential misunderstandings.", "accepted": 0}
{"paper_id": "nips_2022_V22VeIZ9QU", "review_text": "the topic of this paper is interesting, and i wanted to get the basic idea, so i looked at the paper myself. had i been a reviewer i would have recommended rejection. however, as the existing reviews are positive, and the authors will not have a chance to respond to my objections, i will recommend acceptance in spite of the low confidence scores of the reviewers where the high scores have confidence 2. for the sake of the authors i will list my complaints. my fundamental complaint is about clarity. the central problem is the notation xp. in the main text this is only defined as a stochastic program. but what is it allowed to be a distribution on? what is the allowed dimension of the parameter vector? p? if xp is an arbitrary stochastic program what is exp? if xp defines a distribution on a finite abstract set then clearly exp cannot mean the expectation of a random value. the first sentence of appendix b.2 need to appear before any appearance of the notation exp in the main text. even if that is done, the authors are assuming a fixed embedding of each random value of x in a euclidean space. but in the vast majority of modern applications that embedding must be learned. for example, in a vae model of grammar induction one must learn an embedding of the grammar nonterminal symbols. the formal set up would be greatly clarified by explicitly introducing a loss function f as part of the given data and write, for example e_x sim xp fx. the main body of the text needs more intuition about the meaning of w and y in theorem 2.2 and some kind of sketch of a proof. the paper would be significantly simpler if limited it to the case where xp is entirely discrete. this is the interesting case and there is then no need for xp. the technically hard issue is w and y in theorem 2.2. if a mixed stochasticcontinuous case is needed for the general case then it could come later where it is well motivated. an explicit form for w and y should appear in the body of the paper rather than a naked claim that they exist. any implementation must construct w and y so the proof needs to be constructive. examples are nice, but they are no substitute for the proof of the general case. some sketch of that proof needs to appear in the body of the text  most importantly a computable solution for w and y. as it stands it appears that the authors are trying to hide the artificial nature of their technical setup  the use of apriori fixed embeddings of abstract tokens.", "accepted": null}
{"paper_id": "nips_2022_yJV9zp5OKAY", "review_text": "this paper presents a new augmentation method though its similar to adversarial training that significantly improves various benchmarks, methods, and tasks. besides, the authors provide an informationtheoretic ground for the proposed method. ac appreciate the technical contribution to the community. after rebuttal, the authors addressed most of the concerns. ac recommends accept. ac also would like to suggest the authors comprehensively compare your method with adversarial training in revision.", "accepted": 1}
{"paper_id": "nips_2022_9xVWIHFSyfl", "review_text": "this paper formulates the problem of learning how to stimulate a visual neuroprosthesis as a hybrid autoencoder. while the decoder can be taken as a known and fixed model that describes how stimuli produce percepts, the encoder needs to be learned. once learned the encoder maps target percepts into stimuli that can be passed into the device decoder. motivation and formulation of the problem is especially clear  strong. the paper is well written and the reviewers and i appreciated the nice solution strategy for a potentially impactful application area. there were some concerns about how generally applicable the approach is. however, the results presented likely do advance the state of the art in this setting. given my own reading of the paper and the consistently positive reviewer scores, im very comfortable endorsing this paper for acceptance.", "accepted": null}
{"paper_id": "nips_2022_mzze3bubjk", "review_text": "the paper establishes new bridges between two fundamental notions for the study of average case hardness of hypothesis testing problems lowdegree functions and the franzparisi free energy, and also connect them to mcmc hardness. the paper has been very well received by the reviewers, who have done a great job. i agree with the reviewers on the quality of the paper. i also think that despite neurips not being the most obvious avenue to submit such a paper, there indeed is an active subcommunity of neurips readers very much enthusiastic about such results, connections with statistical physics, etc. all issues raised seems to have been properly answered by the authors, so this is a clear accept for a highquality paper full of rich notions and new fundamental connections.", "accepted": 1}
{"paper_id": "nips_2022_dYhB_alLyCO", "review_text": "addressing heterogeneity in differentially private aggregation and estimation is an practically important topic that shows up in many real world settings. this paper makes the first step in closing the gap between existing sophisticated algorithms that work under homogeneous setting and the practical scenarios with heterogeneous users. the reviewers agree that this is an important paper that opens up several exciting research directions.", "accepted": null}
{"paper_id": "nips_2022_oUigTwc7Cw5", "review_text": "this paper received 1 accept, 2 strong accepts and 1 reject. all reviewers agree that the proposed model is elegant and that the technical work is impressive even the negative reviewer. the main criticism of the negative reviewer is that the main take away is not clear. the authors submitted a revised version of the manuscript. sadly, the reviewer did not read the rebuttal andor engage in a discussion postrebuttal. the ac considers that the main criticism of this reviewer was addressed. in light of this, the ac recommends the paper to be accepted.", "accepted": 1}
{"paper_id": "nips_2022_lWq3KDEIXIE", "review_text": "this submission did not reach a full agreement among pc members. i will not repeat the arguments here as they can be read in the reviews please see e.g. review wy8e, which according to the authors captures well their intention. the main open criticism is the lack of a real application where the type of assumption used in the paper is present the other important one being the comparison with other approaches, which seems to have been justified by the authors to a good extent. the concern was originally about the existence of those applications, but later this was resolved it remained as a concern that the real application is not shown in this work. i consider that this is small when weighted against the positive comments in all reviews.", "accepted": 0}
{"paper_id": "nips_2022_8xccCiF9JQ6", "review_text": "the reviewers generally agreed that the paper has novel and solid contributions. moreover, they were satisfied with the authors responses. please incorporate the reviewers suggestions in your revision.", "accepted": null}
{"paper_id": "nips_2022_FR--mkQu0dw", "review_text": "the paper considers dp convex optimization in highdimension, providing bounds independent of the model size on the empirical and population risk extending prior work that assumes that gradients belong to a fixed lowrank space. all the reviewers agree that this is an important problem and the results are interesting, and support accepting this paper.", "accepted": 1}
{"paper_id": "nips_2022_51f5sPXJD_E", "review_text": "the paper presents a new cryoem reconstruction algorithm for data with multiple structural states for the same protein. in contrast to previous approaches which use a coordinatebased implicit representation of reconstructed density, this paper reconstructs an explicit volumetric grid and the optimization is formulated in the fourier domain. good accuracy and faster convergence is demonstrated, in comparison to prior methods. while all reviewers agreed that the method constituted an important contribution to the field, they also pointed to shortcomings in the evaluation of the method. in response, the authors added analysis of the convergence based on fscbased resolution estimates, and offered to add a comparison to the 3dva which uses a similar representation to the proposed method. based on the reviews and author responses, i recommend acceptance of this paper. i urge the authors to follow through on writing a more expansive comparison to 3dva, which will be included in the cameraready version of the manuscript.", "accepted": 0}
{"paper_id": "nips_2022_e8PVEkSa4Fq", "review_text": "this paper proposes a technique for training prompts for openvocabulary vision models e.g. clip at test time, i.e. without any labeled data. the model is trained to minimize the entropy of the average prediction of many augmented views of a test image. this improves performance to varying degrees without requiring any additional labeling. the method and approach are interesting and some useful analysis is provided. reviewers agreed that the paper should be accepted. beyond the suggested changes made by reviewers, id recommend some additional references to better situate the paper with respect to past work on consistency regularization, entropy minimization, and transductive learning.", "accepted": null}
{"paper_id": "nips_2022_PQFr7FbGbO", "review_text": "the paper makes an observation that average pooling in unets implicitly learn a haar wavelet basis representation and build a theory for hierarchical vaes hvaes on top of it. the proposed interpretation of hvaes lead to modification to hvaes that reduce the number of parameters and improve stability. i think the haar wavelet basis representation is somewhat obvious but the analysis of hvaes look nontrivial. i would recommend accepting this paper with the following suggestions i suggest the authors to focus on the hvae part and improve the presentation. more specifically, the definition of unet architectures used in hvaes should be in the main text. also make more connection between hvaes and diffusion models. for example, time information is explicitly handled in diffusion models, whereas this paper suggest that it is handled implicitly in hvaes. the parameter sharing is common for diffusion models. at the end of the day, the proposed improvement to hvaes seem to make hvaes closer to diffusion models.", "accepted": 1}
{"paper_id": "nips_2022_4L2zYEJ9d_", "review_text": "this paper proposes a different approach to uncertainty quantification in regression and classification problems. it extends denoising diffusion probabilistic models in combination with a pretrained conditional mean model to provide a conditional generative model. the proposed approach is evaluated on uci regression tasks and cifar10 classification. additional results on cifar100 and imagenet were presented during the rebuttal period and additional analyses. the paper received some relatively weak support among reviewers. all the reviewers agree the paper proposes an interesting and novel formulation of uncertaintyquantified regression and classification models. the main issues raised by the reviewers were that of i conducting experiments on larger datasets, ii more analysis in the classification case, and iii the effect of conditioning on x in the diffusion processes. i believe the authors have provided additional significant empirical evidence of the benefits of their approach and some of the reviewers updated their scores accordingly. given this, the potential impact of the paper, the novelty nature of the contribution, and despite the seemingly weak scores, i believe there are no significant concerns remaining about this paper and it is worth presenting in its current form to the neurips community.", "accepted": null}
{"paper_id": "nips_2022_lArVAWWpY3", "review_text": "all the reviewers are positive about the paper, they found that it is well written and provides very interesting theoretical as well as practical contributions which are relevant for the machine learning practice.", "accepted": null}
{"paper_id": "nips_2022_5HaIds3ux5O", "review_text": "this paper proposes quantized reward konditioning quark, an algorithm for unlearning language model misalignments. this is an important research direction given the importance of developing betteraligned large language models, and the paper does a good job at presenting why it matters and how it is related to prior work. the reviewers think that the paper is well written and clear, and that the approach is novel, sound and interesting. after the rebuttal, all reviewers vote to accept.", "accepted": 1}
{"paper_id": "nips_2022_dix1iktX7Qt", "review_text": "in multifidelity monte carlo, a sequence of estimators of increasing cost and quality in the sense of approximating some, typically intractable, limiting model are available. the authors define a target distribution on an extended space including both fidelity and model parameters. then, a markov chain which targets a distribution over this extended space can recover the marginal of interest. all reviewers were positive about the work and there seems to be a consensus that the paper is wellwritten and clear, the method is applicable in a broad setting and demonstrates an advantage in tested benchmarks albeit perhaps low dimensional, reviewer ga3z raised a valid concern about the mixing time considering the nature of the unbiased estimation methods based on couplings described in section 3. these methods achieve unbiased estimates at the expense of estimation variance and expected running time time to coupling. the reviewer mentions that this algorithm can still suffer from the same problems that annealing based methods can suffer in terms of mixing. such limitations should be made explicit in the paper. after the rebuttal, i feel that most concerns have been addressed and the paper can be a valuable contribution to neurips, hence i suggest acceptance.", "accepted": null}
{"paper_id": "nips_2022_RczPtvlaXPH", "review_text": "this paper introduces an neural network based equilibrium solver which utilizes a special equivariant neural network architecture to approximately predict nes, ces, and cces of normalform games. experiments show the effectiveness of the proposed methods across multiple dataset. all reviewers support the acceptance of this paper. while i agree on the merit of this paper worth acceptance, id also recommend authors to revise a bit in the final version regarding to the theoretical complexity of finding equilibrium. 1 in line 18 solving for an equilibrium of a game can be computationally complex 9, 8, in fact, the cited intractable results only apply to finding nash in multiplayer generalsum games. finding cecce can be always done by lp, which is tractable, and can be guaranteed to finish in polynomial time; 2 this paper emphasize that prior methods may take an nondeterministic time to converge, while this method proposed in this paper gives determinism. however, it appears to be the methods proposed in this paper is not provided with guarantees to converge in certain time thus without determinism either. its better if the authors can clarify or modify corresponding arguments.", "accepted": 1}
{"paper_id": "nips_2022_Vhd-jh9B8Hc", "review_text": "this is an interesting and solid paper in which the authors address the identification of a ranking in the setting of duelling bandits under the assumption that the underlying preference probabilities are weakly stochastic transitive. they introduce a novel algorithm which solves the problem in a deltapac manner and has an instancewise sample complexity guarantee. all reviewers agree that this is a significant contribution. questions and open points could be clarified in the discussion phase.", "accepted": null}
{"paper_id": "nips_2022_IRSyuxfYNb", "review_text": "all of the authors agree that the work meets the neurips standards, with the two lowestscoring reviewers upping their recommendation from 4 to 5 on rebuttal. the work is described as a fundamental and important problem and timely, a good contribution. reviewer 6ntb summarises the technical contribution the paper generalizes latent state inference from hsmms to continuoustime chains in latent space. in this case, the posterior is not simply proportional to the usual forward and backward probabilities. instead, the transition random variables currents are markov. the authors take the limit of step size approaching 0 for these currents its clear to me that the the work has been communicated really well, since all of the reviewers were able to grasp the paper and there was very little misunderstanding in the discussions. there were some recommendations from the reviewers  please ensure these are fixed in the cameraready version.", "accepted": null}
{"paper_id": "nips_2022_5OWV-sZvMl", "review_text": "the focus of the submission is supervised learning with functional inputs and outputs. particularly, the authors consider the encoderapproximatordecoder architecture 23 to tackle this task. after discussing the limitations of linear decoders in this scheme meant in l2 and uniform sense; the latter is elaborated in proposition 1, the authors present the nonlinear nomad architecture under the assumption of operator learning manifold hypothesis 12 which captures a lowdimensional output space condition. they demonstrate the efficiency of the approach compared to the deeponet method relying on linear decoders, fourier neural operators and loca i when using stochastic gradient descent on the empirical loss 1, ii on the learning problem of the antiderivative operator and learning the solution of two partial differential equations. functional data analysis is a fundamental area of machine learning with a large number of applications. the authors present a new method in this context which can be of definite interest to the community as it was assessed by the reviewers.", "accepted": 1}
{"paper_id": "nips_2022_j0J9upqN5va", "review_text": "the paper proposes a method that allows single model uncertainty estimation by training a model with a random data augmentation. the proposed approach is simple and scalable. it is comparable to or better than deep ensemble in terms of nll, ece, and brier score. the application to sequential optimization tasks presented in the paper looks interesting. all reviewers support accepting this paper. while there could be more theoretical support, i think this paper would be of wide interest to the neurips community. if possible, accept as spotlight.", "accepted": null}
{"paper_id": "nips_2022_AODVskSug8", "review_text": "the paper provides a theoretical analysis of sparsely activated neural networks. they introduce lsh local sensitive hashing as a new routing function for theoretical analysis and proved a few results on representation power and inference time. one reviewer pointed out that the theoretical results are expected and do not provide much interesting insight, which i agree with. nevertheless, this is one of the early papers that study sparsely activated networks and may serve as a starting point. i recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_XmK56zbGeCp", "review_text": "technical review and decision this paper proposes four sets of rewards such that an rl maximizing the sum of those rewards combined with the environment reward to mimic doctors behavior and increase the trust in the automatic differential diagnosis systems. the paper is wellwritten and the exchange between the reviewers and the authors have been constructive. there are multiple questions and the reviewers are convinced by the response. the authors should include the clarifications in the cameraready version of the paper. while the methodological contributions are limited to a reward design, this paper qualifies as a good application paper. ethics review the ethical reviewers have identified that the authors need to elaborate more on the doctor consultation process and make it more transparent. i strongly suggest including a discussion of ethical concerns, as discussed in the ethical reviews.", "accepted": 0}
{"paper_id": "nips_2022_yjWir-w3gki", "review_text": "this paper studies various forms of discounting in continuous time, with a deep learning solution. one of the motivations for doing so is to broaden the range of inverse rl algorithms. overall, the reviewers appreciated the perspective taken in this paper and the proposed application of the idea in an irl context. there was some discussion on whether actual human data experiments were necessary; i note that the experimental results are currently fairly preliminary. the use of the term reinforcement learning is also somewhat misleading given that this is closer to more traditional or work, including the assumption that the mdp parameters are known. however, there was general agreement that this paper plays a useful role in bridging different fields and makes a good contribution. the authors are encouraged to give a more complete discussion of how this work relates to other techniques such as preference elicitation.", "accepted": null}
{"paper_id": "nips_2022_FlWdTyUznCc", "review_text": "the paper studies multiplicative filter networks, which are coordinate neural networks in which each layer applies a multiplicative hadamard product filter and a sinusoidal nonlinearity. the paper shows how introducing residual connections and initializing appropriately can lead to networks where the frequency content of the image separates over layers. this leads to a learned version of classical coarsetofine reconstruction methods, which the paper terms residual multiplicative filter networks. the paper illustrates its proposals with experiments on image approximation and on cryoem reconstruction. reviewers found that the paper presents a simple idea, which can be easily adopted whenever a coarsetofine reconstruction is desired, and as such is likely to see followup work. the main questions concerned the necessity of a coarsetofine approach in applications where one ultimately seeks a reconstruction at just a single scale, and the cryoem experiments, which show good performance compared to a baseline, when the coordinate network model is integrated into a larger system. overall, the reviewers found that paper presents a natural modification to mfns which improves both their interpretability and applicability in inverse problems in imaging.", "accepted": null}
{"paper_id": "nips_2022_JJCnsgk4OIS", "review_text": "this paper proposes pet, an approach to classifying rows in tabular data using retrieval methods and hypergraph neural networks to make predictions. the key ideas are  use information retrieval techniques to find similar rows to each row that needs to be labeled.  connect the similar rows in a hypergraph structure.  learn a representation over the hypergraph structure with graph neural networks. experiments show that pet can singificantly outperform multiple state of the art methods on two tasks. ablations also validate the design and each component of pet. during the review process, the authors added additional experiments addressing many of the reviewers open concerns. the reviewers agreed that the paper is very well written, presents a significantly useful method, and that while pet builds on pieces that have been developed separately, it combines them in an interesting way. reviewers also felt that the work is likely to be of interest to the wider graph neural network community and has the potential to influence future work.", "accepted": null}
{"paper_id": "nips_2022_thgItcQrJ4y", "review_text": "while there is a rather large gap between the reviewers scores, all the reviewers agreed that the paper is novel and the contributions are significant, especially given the little knowledge about the eos phenomenon. while one of the reviewers raised important concerns about the appropriateness of the assumptions, i do believe that in a field without a rich enough literature, initial theoretical results with potentially strong assumptions are still valuable. hence i am recommending an acceptance for the paper. please implement all the changes that have been requested by the reviewers. on the other hand please avoid using gender pronouns like he when addressing the reviewers.", "accepted": 1}
{"paper_id": "nips_2022_Jz-kcwIJqB", "review_text": "this paper studies an interesting problem, and overall the reviewers agreed the exposition and validation are sufficient. we encourage the authors to consider the issues raised by the reviewers and further improve the work in the final version.", "accepted": null}
{"paper_id": "nips_2022_v6NNlubbSQ", "review_text": "decision accept this paper propose a method for computing the epistemicaleatoric uncertainties using a kernelbased estimator. empirical evaluations on ood detection  classification with rejection demonstrate the benefits of the proposed approach. reviewers commended that the proposed method is flexible, easy to be adapted to any neural network architecture. however, in initial reviews, concerns were raised about a clarity, b uncertainty evaluations in experiments e.g., calibration, epistemic vs aleatoric uncertainty. in author feedback, authors provided more explanations and some additional experimental results, which addressed many of the reviewers concerns. still the question regarding calibration metrics is there. after reviewerac discussions, it is concluded that we can accept this paper to recommend its contribution in ood detection research domains. still id suggest in revision the paper will benefit from an edit to improve clarity, and id encourage the authors to consider including calibration metrics. as a side note, for the classification with rejection task, id suggest the authors to include a baseline regarding selective classification, e.g., the method of httpsarxiv.orgabs1705.08500.", "accepted": null}
{"paper_id": "nips_2022_9ZWgrozGP0", "review_text": "the paper considers the task of ood detection facing adversarial manipulations. it shows two existing defenses can be broken. it then proposes a method called prood to construct a classifier with both provability and high clean accuracy, and prove that it avoids asymptotic overconfidence. the paper studies an important topic and has made significant contributions welldesigned method with guarantees, thorough experiments with strong performance. the reviewers have some concerns that are addressed by the reviewers 1. interpretation of the experimental results. prood doesnt always outperformance in all metrics. in the response, the authors clarify that among the methods with high accuracy, prood enjoys strong outdistribution robustness and also with certificates. making this explicit can improve the presentation. 2. presentation of the experimental results. the authors have incorporated some suggestions and the presentation is improved. overall, the work is a good contribution, and acceptance is recommended.", "accepted": 0}
{"paper_id": "nips_2022_crRhj1Y2wv", "review_text": "the authors propose a new method for quantifying variable importance and thus, for variable selection in a totally nonparametric setting. they use a bayesian setting gp and quantify the variable importance based on the l2norm of the corresponding partial derivative. the method comes with strong theoretical guarantees rate of contraction, bvm theorem and numerical experiments. the reviewers initially agreed that the method proposed was nice and that the theoretical results were interesting enough to justify publication. however, some of them also pointed out various problems weaknesses in the experiments reviewers aeut, 2fry, problems in some figures aeut, missing references that would require further discussion adbr, absence of a nonasymptotic ie, d not fixed analysis smu5, adbr, assumptions to be clarified adbr. the authors addressed all these points, fixing the minor mistakes, adding new experiments and nonasymptotic results to the paper on a personal note, i will add that i appreciate the summary of the changes provided by the authors, that make it easier to track the modifications in the paper. the reviewers agreed the new version of the paper is much better, and ready for publication in its current state.", "accepted": null}
{"paper_id": "nips_2022_J7zY9j75GoG", "review_text": "i thank the authors for their submission and active participation in the discussions. the paper presents a method for interpretable deep learning. all reviewers unanimously agree that this is a solid paper worthy of acceptance. in particular, reviewers noted that the paper tackles an important problem snnk, and presents a novel jg58,xfpf, interesting snnk,mgx1 and intuitive 31jg framing of interpretability as alignment problem. furthermore, the writing is clear jg58,xfpf and the empirical validation extensive xfpf. thus, i am recommending acceptance of the paper and encourage the authors to further improve their paper based on the reviewer feedback.", "accepted": null}
{"paper_id": "nips_2022_1beC9_dmOQ0", "review_text": "the paper presents a novel architecture named jump selfattention to capture the highorder statistics in transformers. specifically, the model builds an gcn layer on top of the attention layer, based on the attention scores. the reviews are generally positive. the major concerns are around the significance of the improvement comparing to the original softmax. the authors may want to improve this part more in the final version.", "accepted": null}
{"paper_id": "nips_2022_yZgxl3bgumu", "review_text": "the paper proposes a method for modeling soft tissue deformations using a graph neural network trained to approximate the solution of the more classic finite element method. the proposed method is significantly faster than the classic method and with sufficient accuracy, enabling the prospect of using the method to predict tissue deformation in imageguided neurosurgery. all reviewers appreciated the novelty and utility of the proposed method. the biggest weakness in the study had to do with the limited validation performed. this limitation was clearly addressed by the authors both in the limitations section and in their response to the reviewers. despite the limitation, the reviewers unanimously recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_BlF6CWzWKT7", "review_text": "the authors offer a methodologically novel and very interesting approach to an important problem of cate or conditional late with binary instrumenttreatment. the paper should be published at a good ml venue. unfortunately, i need to recommend rejection primarily because the key proofs supporting their main theorems were missing from the original supplementary material and were only submitted after the rebuttal. given that these were crucial elements supporting their main results and that they were submitted post the original submission deadline it seems unfair even if it was most probably an honest mistake of the authors. another issue that came up in the discussion phase that seems crucial to revise before a resubmission is that the authors main estimation rate result relies on a main theorem of the unpublished work of kennedy 2020, which has since been revised and the new theorem in kennedy 2020, is technically very different and requires different assumptions. hence the authors need to revise their main estimation rate result accordingly. however, i acknowledge that this is not the main contribution of this work and a simple invocation of past work, but rather the main contribution is to formulate a loss for cate using the idea of want and tchetgentchetgen. so this wouldnt most probably be a reason for rejection.", "accepted": null}
{"paper_id": "nips_2022_NIJFp_n4MXt", "review_text": "the reviewers appreciated the novelty factor of the contrastive metalearning algorithm proposed in the paper, the theoretical analysis establishing a formal connection with equilibrium propagation, and the appealing features of the resulting metalearning procedure, which include memory and computation efficiency, as well as the fact that the algorithm affords a biologicallyplausible implementation that only requires locally available information for parameter updates. to concretely showcase these properties the paper demonstrates two instantiations of the proposed algorithm that are mechanistically realized through synaptic consolidation and topdown neuronal modulation, respectively. finally, the paper validates the algorithm on standard fewshot learning benchmarks. the main weaknesses of the paper identified by the reviewers are the empirical evaluation, which would benefit from a more extensive comparison between methods and more experiments on more challenging metalearning datasets, and the discussion on the candidate neurobiological substrate for a brain implementation of contrastive metalearning, which would benefit from a more detailed and systematic description. these limitations however do not substantially detract from the overall quality, relevance and interest of the paper, which reviewers unanimously recommend for acceptance.", "accepted": 1}
{"paper_id": "nips_2022_mWaYC6CZf5", "review_text": "this paper highlights that contemporary sparse mixtureofexperts smoe models suffer from representation collapse in the gating mechanism. the paper then proposes a simple fix reducing the dimensionality of the gating representations and using cosine similarity. the paper shows qualitatively that the new representation suffers less from collapse and that model trained with the new algorithm exhibit small but consistent improvements when evaluated on crosslingual language understanding and machine translation. all of the reviewers had a concern that the method is tested only in the top1 setting vs. the most common practice of using top2 or more experts. the paper added experiments on top of gshard that alleviates this concern. another concern was limited scope of evaluation. the authors added mt results as well as inlanguage results, which showed the same trend of small, but consistent improvements. as a result of the discussion, two of the three reviewers were happy of how their concerns were addressed and increased their score to recommend that the paper is accepted. the third reviewer has not been active in discussion.", "accepted": 0}
{"paper_id": "nips_2022_FjqBs4XKe87", "review_text": "the paper proposes a method for distilling prompts into the parameters of a model. reviewers liked that the method can improve the efficiency of inference by avoiding having to attend over prompts, and the evaluation on the personachat dataset is a good use case for this approach. however, several important concerns were raised. as pointed out by reviewer y47p, similar ideas have been explored in previous work, and claims of novelty need to be toned down. as acknowledged in the author response, most of the experiments are on tasks with short inputs, so gain little benefit from the approach. the difficulty in finding suitable tasks where the approach has a clear benefit might suggest the method has limited applicability. the additional experiment on msc is a nice addition in the author response, although results here are a bit underwhelming. overall, this is borderline, leaning reject.", "accepted": null}
{"paper_id": "nips_2022_d9usspxbWmk", "review_text": "in this paper, the authors exploit imitation learning with a twostage gnn to learn the reduction rule for oda to accelerate the solver and reduce the unnecessary computation. the authors evaluate the performances of the proposed method and demonstrate the advantages. in sum, this paper consider an interesting application of machine learning for optimization and provide a promising solution. all reviewers provide relatively positive feedback of this submission. please consider the reviewers suggestions to improve the submission  justify the mdp modeling with concrete definition of the state and action for the reduction rule for oda.  specify the data set construction and justify the generalization ability in the imitation learning.  provide comprehensive comparison, especially with pmoco.", "accepted": null}
{"paper_id": "nips_2022_zyrBT58h_J", "review_text": "the authors address the issue of inconsistencies between modeled and training data for auto bidding rl policies and demonstrate the efficacy of their approach both analytically and experimentally.", "accepted": null}
{"paper_id": "nips_2022_DpKaP-PY8bK", "review_text": "the paper studies identifiability of ica for two families of nonlinear functions conformal maps and orthogonal coordinate transforms. for conformal maps, they prove identifiability for d  2, improving an old 99 result for d2 due to hyvarinen and pajunen. for orthogonal coord. transforms, they prove a weaker notion of local identifiability. there was quite a lot of discussion on the various strengths and weakness of the paper. 1 experiments though the paper had very little experiments, the reviewers agreed that since the paper is primarily a theory paper, an extensive experimental section is not necessary. 2 theory the reviewers found both the proofs of theorems 2 and 3 quite interesting, involving new ideas. theyre heavy on tools from complex analysis, which is not surprising giving that conformal maps are natural through the lens of complex analysis; but they found the connections to pdes interesting and potentially useful in the future. there were some potential worries about correctness, but no definite error was identified. 3 how strong of an assumption is conformality in practice the reviewers agree this is probably quite restrictive as an assumption, but idenfiability of nonlinear ica is always going to require some strong conditions, and were still very far from understanding when its possible when no auxiliary variables are involved. the paper shrinks the gap bw theory and practice even if the theory has very strong assumptions.", "accepted": 0}
{"paper_id": "nips_2022_5XtsqM57-Zb", "review_text": "the reviewers came to consensus that this paper makes a good progress on online learning with stochastic feedback graphs. i agree with these opinions and please polish the manuscript by addressing the raised minor concerns such as the presentation issues in the final version.", "accepted": null}
{"paper_id": "nips_2022_q-snd9xOG3b", "review_text": "this paper provides conditions for the existence of logconcave multivariate distributions to satisfy fdifferential privacy constraints. the results of the paper have the potential to be broadly applicable.", "accepted": null}
{"paper_id": "nips_2022_G8BExMno316", "review_text": "the paper looks at a method for inference in latent force models that uses the adjoint method to help out with the inference. three of the reviewers describe the work as easy to read, sound, relevant and useful. some of the reviewers lament the lack of more experiments  i would have loved to see the results on the spatial experiment described in the discussion. the authors did add one experiment as part of the rebuttal  i urge them to add this to the manuscript. one reviewer complained about the mathiness of the presentation. they gave an overall score that i dont feel reflects the overall quality of the paper, and im inclined to discard it. yet, i urge the authors to consider whether all of the terms used are maximising the accessibility and therefore impact of the paper. if banach spaces are essential to the work, please explain why. if they are a small technical necessity, but unimportant to understand the main ideas, i suggest you relegate them to a formal proof in the appendix.", "accepted": 0}
{"paper_id": "nips_2022_YgmiL2Ur01P", "review_text": "the paper is interesting and the reviewers on average recommend weakish acceptance. i agree with that asessment.", "accepted": 0}
{"paper_id": "nips_2022_4iEoOIQ7nL", "review_text": "it was agreed among the reviewers and ac that the paper should be accepted. hope the authors will address the remaining comments from the reviews in preparing the final version of the paper.", "accepted": 1}
{"paper_id": "nips_2022_HaZuqj0Gvp2", "review_text": "this work proposes to use gnns in acting as good simulators of fluid dynamics. it is a solid example of using neural networks as differentiable simulators that can then be used for solving for design of components for fluid manipulation tasks. overall, this work has been well received and should inspire related work in inverse design. the authors are encouraged to take the detailed reviews into account for cameraready and especially make sure to make high quality reproducible code available to the community. there has also been discussion about the naming of the paper to reflect proper scope and that should be considered as well.", "accepted": null}
{"paper_id": "nips_2022_nxl-IjnDCRo", "review_text": "the paper analyzes diffusionbased deep generative models ddgms. the paper postulates that a ddgms can be divided into two parts, a denoiser and a generator. after the rebuttal and discussion period, three out of four reviewers supported acceptance of the paper. the reviewers almz, qkfo, and 4jy9 all find the interpretation of a diffusionbased deep generative model as a decomposition of a generator and denoising part interesting. those reviewers also note that this interpretation is useful as parts of the diffusion steps can potentially be replaced with a vae, which would make the synthesis more efficient. the reviewers almz and qkfo also note that one comparison is perhaps slightly unfair in that the number of parameters of the two models considered was different; the authors cleared this up with new simulations showing that matching the number of parameters as should be done for a better comparison does not substantially change the conclusions of the experiment. finally, reviewer s9ub finds the approach and observations not to be novel and states that similar observations have been made in a recent cvpr paper by benny and wolf. while both the paper by benny and wolf and the paper under review study denoising diffusion models, the approaches taken by the two papers are substantially different, and both provide value to the community. i also find the interpretation of the ddgms as a denoiser and generator to be interesting and useful and therefore recommend acceptance of the paper.", "accepted": 0}
{"paper_id": "nips_2022_PpP9TiUZLoF", "review_text": "this is an interesting paper on bandit with nontrivial structure and algorithms. all the reviews are positive, as is my own opinion. i quite happily suggest acceptance for this paper.", "accepted": 1}
{"paper_id": "nips_2022_61UwgeIotn", "review_text": "strengths this paper introduces an interesting new problem setting metarl for preferencebased adaptation that is of practical relevance, along with a sensible new approach that makes progress on addressing the problem. weaknesses after the author discussion period, there are two remaining concerns  additional experiments, particularly human experiments and more complex tasks.  further discussion  clarification  support for when preferencebased feedback is preferable to other forms of supervision, e.g., sparse rewards. overall, the reviewers and ac agree that this paper makes a worthy contribution to neurips, despite the weaknesses. nonetheless, we expect that human experiments in particular would help with both of the weaknesses and increase the impact of the paper, so we especially encourage the authors to work on such experiments before the camera ready version.", "accepted": null}
{"paper_id": "nips_2022_RJemsN3V_kt", "review_text": "this paper identifies a fairness problem in graph contrastive learning gcl, i.e., gcn often performs bad for lowdegree nodes. the key to solve this problem is the observation that gcl can offer more fair representation for both low and high degree nodes. authors also support their claims with theoretical analysis. all reviewers appreciate the contributions made by this submission. it is suggested that to simplify notations and make the theorems are selfcontained in the final version.", "accepted": 1}
{"paper_id": "nips_2022_NM3AbzX-dq", "review_text": "all reviewers lean to accept this paper and this is a clear acceptance.", "accepted": 1}
{"paper_id": "nips_2022_aUoCgjJfmY9", "review_text": "this paper proposes a method named item mixture imix for recommendation systems. the proposed method is based on mixup techniques and can enhance the generalization ability with theoretical guarantees. some concerns regarding the relevance with respect to recommendation systems and the clarity of the paper have been initially raised, but have been addressed during the rebuttal. now that the reviewers are uniformly positive, i recommend acceptance and encourage the authors to make necessary modifications to incorporate the additional contents of the rebuttal into the main paper.", "accepted": null}
{"paper_id": "nips_2022_ufRSbXtgbOo", "review_text": "this paper received a mixed set of reviews. after reading the paper, the reviews, subsequent authorreviewer discussion, and discussing with the reviewers, i recommend that this paper be accepted. the paper studies a multiagent version of the performative prediction problem. the paper contains original and interesting contributions, and we expect the community to appreciate this work. in particular, the theoretical contribution was appreciated by several reviewers. although concerns were raised during the review process, my impression is that these were satisfactorily addressed through the rebuttals. while preparing the camera ready, we strongly encourage the authors to address the main concerns that came up in reviews. these include  strengthening the motivation for the particular multiagent formulation studied in this paper  clarifying the relationship between performative prediction and prior work on distributed optimization", "accepted": 0}
{"paper_id": "nips_2022_0RMDK39mGg", "review_text": "this paper proposes a new algorithm for distributed bilevel optimization with solid theoretical results and empirical validation in multiagent rl examples. during the rebuttaldiscussion period, authors were able to provide additional experiments and addressed most of reviewers concerns. all are in favor for acceptance and appreciate the combination of theory and strong experiments.", "accepted": 1}
{"paper_id": "nips_2022_uOii2cEN2w_", "review_text": "this paper provides a theoretical analysis of the regret for the bore framework and addresses existing issues with a bore method that provides uncertainty quantification in the estimation of the classifier. an analysis of bore is also presented alongside an exploration of the batch bayesian optimization setting. reviewers appreciated the theoretical analysis provided in the paper as well as the explanation of the theoretical results and the assumptions needed for those results to hold. several reviewers expressed the main weaknesses were the empirical analyses in the paper, including the results focusing on the batch case. however, after the rebuttal and revision, several reviewers felt like the additional results were convincing enough to recommend acceptance for this work. please use the extra page to finish addressing the remaining comments of the reviewers. some additional points to keep in mind include 1 the related work section could be expanded to include more literature as well as the context in which the current work relates to the work cited; 2 the figures could be improved to be easier to read, e.g., by increasing font sizes.", "accepted": 0}
{"paper_id": "nips_2022_4XP0ZuQKXmV", "review_text": "the paper makes a significant contribution towards the analysis of asynchronous sgd, which hinges on a new delayadaptive step sizes and a new virtual iteratebased analysis. the authors also cover a rather wide range of assumptions, including lipschitz convex thm 1, smooth lipschitz nonconvex thm 2, smooth convex thm 3.1, smooth strongly convex thm 3.2, and smooth nonconvex thm 3.3 functions. all the reviewers understood this contribution, and its novelty. the authors also did a good job reviewing the related literature, and clearly contextualising their work.", "accepted": 1}
{"paper_id": "nips_2022_jjJgLNrCQB", "review_text": "the papers studied that chromatic correlation clustering problem, the authors introduces a new 2.5approximation lpbased algorithm improving upon existing stateoftheart. the result is not particularly novel but the paper contains a new algorithm for a wellstudied problem and prove that ideas inspired from their theory are impactful in practice so it would be a nice contribution to the neurips program. the committee suggestion is to accept the. paper as a poster.", "accepted": 1}
{"paper_id": "nips_2022_Pu-QtT0h2E", "review_text": "reviewers are in agreement that the paper addresses an important task modelling dynamic scenes and is well presented and written. reviewer 7cq7 considers that the 2stage capture is a limitation, and it is correct that the most general solution, towards which the field strives to move, is a small number of moving or static cameras capturing a complex dynamic scene. however, as noted by other reviewers, this configuration is not impractical, and study of this case will likely contribute to the field overall.", "accepted": 0}
{"paper_id": "nips_2022_XlIUm7Obm6", "review_text": "this submission studies the effect of data encoding from quantum machine learning models. all reviewers agreed on the significance of this work and recommended acceptance.", "accepted": null}
{"paper_id": "nips_2022_9v1_6m0ZKC", "review_text": "the reviewers were all supportive of this paper and commended the authors efforts to clarify their questions with follow up responses and additional experiments. this area chair agrees and recommends acceptance. the authors are encouraged to take the reviewer comments on board in the final camera ready paper. this includes the promise to discuss the limitations raised by the reviewers e.g. z7v1 and cfmm. the additional results with a different backbone and nongt poses can be included in the supplementary material if there is no space in the main paper. minor comments suggested text change in abstract this comes in handy in   this can be valuable in", "accepted": null}
{"paper_id": "nips_2022_EWyhkNNKsd", "review_text": "although the reviewers gave a wide range of ratings to this paper, they all agreed that it is wellwritten and presents two novel, sound algorithms that perform well in practice. the main concern was that the algorithms merely combine existing techniques, which is true. however, given that the resulting algorithms are elegant, wellmotivated, and highly performant, and that achieving the combinations is not trivial, i believe this paper makes a significant contribution to an active field of research and deserves acceptance. the authors should take the reviewers comments into account as they prepare their final revision. in particular, i would encourage them to more explicitly describe their reasons for presenting two different algorithms cardinality constrainedunconstrained. currently, it feels like the two parts of the paper are somewhat disconnected.", "accepted": 0}
{"paper_id": "nips_2022_kGQz0lt6Zu6", "review_text": "from the reviewers comments and my own reading of the paper, the idea of bringing the notion of coreset from computational geometry to bear on the wdro problem is novel and has the potential of further development. the authors should carefully address the reviewers comments in the revision.", "accepted": null}
{"paper_id": "nips_2022_OXourTLd9UO", "review_text": "this paper proposes spherization layer by first transforming the preactivations into angles, then transforming the angles into cartesian coordinates on a sphere, and finally training weight parameters without bias. the proposed spherization layer is geometrically meaningful, and is generally applicable. it is demonstrated on a range of experiments. reviewer v1re, who gave a rating 5, pointed out two related references and asked for more explanation about motivation. the authors explained the difference between their paper and the two references, and explained the motivation. all other reviewers are positive about this paper.", "accepted": null}
{"paper_id": "nips_2022_L9EXtg7h6XE", "review_text": "the paper introduces a new problem statement for rl, i.e., how to identify irreversible states in rl that require help from a human operator. the problem statement and the algorithm are well motivated and the the reviewers also appreciated the reported experiments. the authors addressed the few concerns ablations, comparisons well in the rebuttal. i follow the reviewers and recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_6pC5OtP7eBx", "review_text": "i must say that i am impressed with authors diligence in the rebuttal, adding new experiments and revising the paper. the paper had many issues in the original submission, although it was still wellwritten. the reviewers are more convinced about the paper after the rebuttal. i would recommend a weak acceptance with some unaddressed concerns. to improve the expressivity power of gnn, in this paper the authors developed a mechanism geodesic pooling for improving learnt graph representations, by using an independent second layer based on gnn. they presented good results on edge level and graph level tasks. the reviewers believe the method is novel and worth the acceptance if the many issues raised could be resolved. more details gnn are prone to map different nodes in a graph to the same embedding vectors in the embedding space even if they are far away from each other in the graph. specifically, if the local structure of two nodes are similar, the convolution layer maps both of them to the same embedding vector. the paper proposes to include geodesic information to help to distinguish nodes with similar local structures. however, for a graph classification task, it requires computing the shortest path between two many pairs of nodes which could be computationally expensive. in addition, the paper does not cite and compare to other important baseline. for instance, a simple and effective solution is to simply provide geometrical embedding vectors as initial node attributes to the gnn. some works that can be cited and compared against graph neural networks with learnable structural and positional representations the necessity of geometrical representation for deep graph analysis node proximity is all you need unified structural and positional node and graph embedding inductive graph embeddings through locality encodings", "accepted": null}
{"paper_id": "nips_2022_5JQqvQ1ujSv", "review_text": "the reviewers appreciated the authors response with additional experiments and clarifications. given the feedback from the reviewers and the discussion, i would like to recommend this paper for acceptance and congratulate the authors on a strong submission. i encourage the authors to address the reviewers comments for the final version of the paper.", "accepted": null}
{"paper_id": "nips_2022_wUctlvhsNWg", "review_text": "the authors carefully designed the algorithm and presented their results. the reviewers commented on the analysis of running costs, discussion on related works, and experimental comparisons. the authors, as far as i can see, have addressed these appropriately. the reviewers did not respond to the updates, but the scores should be increased, and i have included them in my decision.", "accepted": 0}
{"paper_id": "nips_2022_lmmKGi7zXn", "review_text": "this paper proposes an infinitewidth autoencoder for recommendation, which trained using the ntg framework by the kernelized ridge regression algorithm. the approach struggle with the large size of the data. to this end, the authors propose a method for data set summarization, called distillcf, for synthesizing tiny highfidelity data summaries. the paper received a mixed evaluation from the reviewers. the strengths of the paper mentioned by the reviewers were  a simple model with only one hyperparameter and a closedform solution  a thoughprovoking and novel framework for recommendation  good performance in the experiments, relatively wide experimental study on the other hand, the identified weaknesses were  the author did not verify the general effectiveness of distillcf beyond coupling with infinite ae, so it is not clear where is the actual gain  technical issues with the experiments  some issues with the readability", "accepted": 0}
{"paper_id": "nips_2022__Lz540aYDPi", "review_text": "this paper motivates and investigates a novel problem in the context of ab testing  specifically, it tries to estimate the fraction of negatively affected individuals beyond average treatment effects. the paper is wellwritten and does a good job of presenting the technical contributions with sufficient rigour as well as discussing their limitations.", "accepted": null}
{"paper_id": "nips_2022_RYTGIZxY5rJ", "review_text": "this paper proposes to solve fgsm catastrophic overfitting by combining different algorithmic methods i.e., masking pattern to the train data, smooth activations, vits, constraints on the first layer convolutional weights. the reviewers have considered the problem studied very relevant but were not convinced by the empirical evaluation, finding that the paper is missing an exhaustive evaluation and for epsilon larger than 8. in addition, they would have appreciated some understandings on the different tricks considered. we encourage the authors to revise their paper, taking into consideration the reviewers feedback and to submit the revised work to a forthcoming conference.", "accepted": 0}
{"paper_id": "nips_2022_Kx1VCs1treH", "review_text": "this paper develops a multigpu differentiable simulation of a programmable microscope, wherein a large convolutional kernel used in the first layer is implemented in the fourier domain. the work outperforms sota approaches for lensless photography, and it allows the endtoend design of a snapshot 3d microscope which beats state of the art systems in simulation. overall the work provides a compelling demonstration of the power of recent technical advances in ml to lead to demonstrable improvements in simulation of computational imaging methods.", "accepted": null}
{"paper_id": "nips_2022_OQs0pLKGGpS", "review_text": "the review process for this manuscript is complex. the reviewers are not in consensus. most of them have engaged considerably with the original submission as well as the significant updates that the authors have made to the manuscript post submission. in my opinion, new full covariance rank results are what make the paper interesting and these were presented after the original submission. normally, i would find this not to be fair as the reviewers are not obligated to read such a big revision to a submitted article. but at least two reviewers have engaged with the revision considerably and i feel like the paper is stronger than what the current scores imply. the last holdout reviewer maintains a few outstanding lowconfidence concerns about the paperi do not think these should hold back the manuscript from being presented and discussed at the conference. i am voting to accept this paper in spite of its low score, but recommend that the authors correct their behavior. such a large revision to a manuscript puts an enormous tax on the review process; this is basically a journal level edit to the submission and normally this would require a second round of review.", "accepted": 0}
{"paper_id": "nips_2022_fbUybomIuE", "review_text": "overall the paper analyzes the key factors or indicators behind the successful identification of winning tickets in lottery ticket hypothesis lth. reviews the paper received four reviews. strong accept absolutely confident, accept absolutely confident, accept confident and reject less confident. it seems that there are at least three reviewer that will champion the paper for publication. the reviewers found the paper is clear and has a clean presentation. the findings are interesting, as well as the pacbayesian perspective. the authors have provided extensive answers to reviewers comments, answering most of them successfully. main issues raised by reviewers  the criterion for finding the winning ticket is unclear  the connection between imp for winning tickets and the pacbayesian model is not strong  the presentation can be improved. however, the authors have tackled most of the concerns raised. after rebuttal the authors have provide extensive clarifications with many additional experimental results, addressing several of the issues raised by the reviewers some of them acknowledged this effort and they have raised their scores. overall, the engaged reviewers seem happy with the changes and propose acceptance. confidence of reviews overall, the reviewers are fairly confident. we will put more weight to the reviews that got engaged in the rebuttal discussion period.", "accepted": 1}
{"paper_id": "nips_2022_4JYq_Kw4zw", "review_text": "all reviewers agree on the merit of the work.", "accepted": null}
{"paper_id": "nips_2022_QPg5TTAdizy", "review_text": "reviewers all find this paper presenting both good theoretical findings and empirical results for an important problem feature attribution robustness. the approaches authors used in connecting the relationship between kendalls rank correlation and cosine similarity, as well as the geometric perspectives, are well received. the presentations are well written, with minor places for quick improvements. reviewers have raised various weakness points but we agreed most of them are minor, do not affect the contribution of this paper, andor can be fixed without much effort. overall we recommend acceptance and would like to encourage the authors to further improve this paper presentation following reviewers suggestions in the next version.", "accepted": null}
{"paper_id": "nips_2022_nzuuao_V-B_", "review_text": "several analyses provided in the paper are not novel and known in the literature. the blackbox setting is not properly motivated and impractical. the angular lipschitz constant is a good contribution, but it seems to be only a small part of the paper. for these reasons, the reviewers are not convinced that the contribution of this paper is significant enough.", "accepted": 0}
{"paper_id": "nips_2022_X5eFS09r9hm", "review_text": "reviewers expressed overwhelmingly positive opinions about the simple, easily implementable, and at the same time innovative procedure proposed in the paper for obtaining gradientbased classcontrastive explanations. appreciation also transpired for the significance of this work in clarifying some technical points in the gradientbased xai literature and the potential for future work that the paper opens up. one of the main criticisms raised in the reviews, the lack of comparisons against other contrastive explanation methods, has been addressed satisfactorily with additional experiments and discussions in the rebuttals. the most important remaining criticism was a doubt on the merits of one of the key technical points in the paper regarding whether gradientbased attributions should be computed according to the softmax outputs or logits. reviewers pointed out that computing attributions with respect to softmax outputs instead of logits is already common practice in the field. reviewers expressed strongly the opinion that it would be appropriate to characterizing and clarify this situation, as it could be potentially misleading and indeed counterproductive even for the paper to denote attribution methods with respect to logits as standard, while its instead the case that some implementation of gradientbased attribution methods already attribute with respect to the softmax output albeit inconsistently. in conclusion, the reviewing panel voted for accepting the paper, under the condition that the cameraready version of the paper explicitly clarify the distinction between the two approaches and discuss the implication of choosing one of the other, without however referring to attribution with respect to logits as standard, but merely pointing out that until now the distinction has been vague and implementations inconsistent. from this technical standpoint, reviewers ask that the contribution of the paper should then be explicitly characterized as clarifying the distinction between logits and softmax attributions, rather than as the proposal of a new procedure in opposition to an already established standard. this is already perceived as a strong contribution to the community, as phrasing it specifically as indicated would help elucidate the state of affairs in the literature and make the community aware of this outstanding blindspot.", "accepted": 0}
{"paper_id": "nips_2022__ekGcr07Dsp", "review_text": "this paper addresses the dgtta problem setting, drawing inspiration from the recent adaptive risk minimisation arm. it goes beyond arm to introduce a mixture of experts and distills from those mixture of experts during adaptation. reviewers agree the moe idea makes sense, and the distillationbased adaptation is interesting and novel enough, and they appreciated the good results and evaluation. concerns included various writing clarity issues and evaluation on other datasets, but these questions were generally resolved in the rebuttal. given that concerns were resolved and all reviewers were positive, i recommend accept.", "accepted": null}
{"paper_id": "nips_2022_Gf5DxrgD2cT", "review_text": "this paper considers minimizing the regret while learning a nearoptimal policy in an episodic constrained mdp with linear function approximation. it proposes and analyzes a ucbbased algorithm proving sublinear regret. the reviewers found the paper wellmotivated and technically sound, and unanimously recommend acceptance. please incorporate the reviewers feedback in the final version of the paper. in order to strengthen the final paper, it would be helpful to  incorporate toy experiments and empirically validate some of the papers claims  include a discussion about the tightness of the upper bound", "accepted": null}
{"paper_id": "nips_2022_2ndfW2bw4mi", "review_text": "the paper addresses an issue of existing selfattention module that is mainly designed for data on euclidean domain; for those on noneuclidean domains, e.g., those on riemannian manifold, the paper proposes a geodesic selfattention counterpart. experiments on tasks of 3d classification and segmentation show the efficacy. all reviewers acknowledge the problem importance and contributions made in the paper, although a few concerns are raised, including additional ablation studies and comparisons with other methods using geodesic metrics. in the rebuttal, the authors clearly respond and address the reviewers concerns. acceptance is recommended. congratulations!", "accepted": 0}
{"paper_id": "nips_2022_Tean8bBjlbB", "review_text": "this is a challenging manuscript to make a final recommendation on accept or reject as there is a clear consensus amongst the reviewers that the manuscript is borderline between accept and reject. the main concern is the incremental nature of the results, extending the prior results of reference a liu, c., zhu, l.,  belkin, m. 2022. loss landscapes and optimization in overparameterized nonlinear systems and neural networks. applied and computational harmonic analysis, 59, 85116. from fully connected networks to the more general setting of dags. the authors and reviewers point out that the dag setting requires substantial adaptation of the technique used to prove the results in a which is the reason i have selected accept over reject. that said, the architecture the reviewers are most interested in is cnns which were pointed out dont fall within the definition of the dag and have been removed from the manuscript. inclusion of shared weights, cnns, would be a great addition to the manuscript which would make the decision for acceptance clearer and would also make the manuscript more compelling for readers who otherwise are unsure the benefit of the dag setting.", "accepted": 0}
{"paper_id": "nips_2022_xatjGRWLRO", "review_text": "all three reviewers recommend acceptance after rebuttal, although they were not very confident. i also read the paper and agree it is well written, has good results and addresses a gap in the largescale point cloud learning literature. however i also see that the paper selectively builds upon ideas from other domains, e.g. vision 19,3 while ignoring closest related work that also does selfsupervised learning in images from large scenes, using also local or localglobal contrastive learning. papers that are highly related include  contrastive learning of global and local features for medical image segmentation with limited annotations. chaitanya et al.  efficient visual pretraining with contrastive detection, henaff et al.  pointlevel region contrast for object detection pretraining. bai et al.  segcontrast 3d point cloud feature representation learning through selfsupervised segment discrimination. nunes et al. i strongly suggest explaining the differences to these papers. in an ideal world the first one should be a baseline to really show if theres something about the fit of the proposed method and point cloud data.", "accepted": null}
{"paper_id": "nips_2022_dsxuTEf01d5", "review_text": "this is a clear accept. congratulations!", "accepted": null}
{"paper_id": "nips_2022_Qi4vSM7sqZq", "review_text": "the paper studies the instabilities of neural networks. most of the reviewers recommend acceptance. i also think the paper seems interesting.", "accepted": null}
{"paper_id": "nips_2022_mhp4wLwiAI-", "review_text": "the paper proposes to promote the training of deep gcns from the gradient flow perspective by introducing topologyaware isometric initialization and dirichlet energy guided rewiring. the intuition of looking into healthy gradient flow is innovative and the observations are insightful. the experiments generally perform good, which demonstrate the effectiveness of proposed method. the reviewers have raised several concerns about the technical and experiments details, such as the use of dirichlet energy and the discussion of trainability issue. the authors provide a nice rebuttal, and the discussions should be included in the revision to better present the research work.", "accepted": 1}
{"paper_id": "nips_2022_3yO3MiSOkH4", "review_text": "the paper attempts to improve few shot learning over graphs. in this regards, the authors propose a multistage approach where first relevant nodes is identified and desirable edge weights is learnt for each fewshot node classification task, so that the input graph structure is tailored for each task individually. this proposed method is based on insights from theoretical analysis. experiments are carried to show the proposed method is more effective over some baseline methods. we thank the authors and reviewers for actively engaging in discussion and taking steps towards improving the paper including for providing additional experiments. some concerns about theoretical analysis remain on interaction of influence between nodes and layers and would be nice to discuss in the final version. other minor fixes  line 23 in appendix typo fix partial latex symbol  eq 6 below line 38 first equality should be inequality", "accepted": null}
{"paper_id": "nips_2022_AYII8AkvD1e", "review_text": "this paper uses diffusion maps to measure curvature from point cloud data and includes some theoretical analysis as well as preliminary experiments demonstrating the value of the curvature measure. the paper benefited from detailed discussion among a number of experts that gave it thorough consideration. while the reviewers did not totally converge to a unanimous accept decision, the ac views their detailedthoughtful discussion as a positive sign that the work will spark discussion and interest at the neurips conference. other than limited experimental evaluation, it seems the main negative aspects of the work mostly raised by reviewer 4sqf are debatable in terms of whether they truly invalidate the research paper. overall, the ac recommends accepting this paper, especially since openreview will show the thoughtful discussion between the reviewers and authors.", "accepted": 0}
{"paper_id": "nips_2022_JkEz1fqN3hX", "review_text": "all reviewers were in favor of acceptance, and after reading the paper myself i am in agreement. the empirical results were good and the experimental work quite comprehensive. the method is well explained and the writing is clear and easy to read. the only real detraction i saw, distinct from things already mentioned by reviewers, was that there are some statements that feel overly strong given the presented results e.g. l161, l179. i was curious about sensitivity to hyperparameters, specifically the settings around how long each phase lasts, etc, along the lines of what was done in the ppg paper. that said, i would generally downplay the importance of this as the proposed method is using the same hyperparameters as for ppg and appears to have undergone minimal hyperparameter tuning. in all i think this makes a clear contribution to the field and should be accepted.", "accepted": null}
{"paper_id": "nips_2022_suHUJr7dV5n", "review_text": "the paper introduces a hessianfree bilevel optimizer called partial zerothorderlike bilevel optimizer pzobo. pzobo uses a zerothorderlike jacobian estimator, which provides accurate hypergradient estimates and a computationally effective way of solving bilevel optimization problems. the paper presents a thorough theoretical analysis and experimental validation on various bilevel problems. strengths 1  the authors address a relevant problem and provide an efficient and accurate solution. 2  convergence guarantees are provided for the proposed approach. 3  experimental results and comparisons with baselines over various bilevel problems. 4  pzobo generally outperforms baseline methods, especially in highdimensional problems. 5  the main idea, to use zeroth approximations of the jacobian of the solution mapping for the lower level problem with respect to the hyperparameters appears to be original and significant. weaknesses 1  one limitation is that there are many assumptions to apply. the assumptions are clearly stated which help alleviate this issue but the authors should address the limitations imposed by the assumptions in the numerical experiments because not made clear that the assumptions needed to apply the theorems are met. decision overall, all the reviewers vote for acceptance. this is a strong paper with very few limitations and with a significant contribution to the field. because of this, i recommend acceptance. i encourage the authors to follow the reviewers comments in order to improve the paper for the cameraready version.", "accepted": null}
{"paper_id": "nips_2022_TIQfmR7IF6H", "review_text": "this paper had very mixed prerebuttal scores, fairly detailed reviews, and significant authorreviewer interaction. following all of this, the reviewers are now generally positive, with several of the initial concerns being resolved. one of the scores remains below the threshold, due to certain claims and statements being too vague, and insufficient distinction between fixed confidence and fixed budget. however, another reviewer responded by noting that the distinction is generally clear from existing works e.g., a. since this remaining concern does not appear to be a dealbreaker, i believe that acceptance is the correct decision. however, the authors should very carefully modify the paper according to the reviewer feedback, and be extra careful of unclear statements such as those pointed out by reviewer bbc9. a emilie kaufmann, olivier capp\u00e9, and aur\u00e9lien garivier. on the complexity of bestarm identification in multiarmed bandit models. jmlr, 2016.", "accepted": 0}
{"paper_id": "nips_2022_u6OfmaGIya1", "review_text": "this paper received reviews and ratings that are leaning positively, the reviewer discussion and ethics review highlighted weaknesses that make this paper quite borderline. strengths 1. the goal and task of the paper are quite well motivated, and they are geared towards positive societal impact refining generated text to be more aligned with human values e.g., gearing text towards moral actions. 2. the text editing approach of the paper using adversarial imitation learning is both novel and intuitive according to the reviewer. 3. the authors give a convincing justification for their textedit paradigm, as prior attributecontrol generation methods e.g., pplm tend to struggle when the context is polluted. experimental results appear to support their claim. 4. the chainofedit paradigm of the paper showing, e.g., how the text morphs into a more moral one eases error diagnosis and enables interactive correction. weaknesses 1. the questions asked to amt workers in the human evaluation do not seem to be well formulated i.e., to what extent does the edited response improve the original response in terms of alignment with human values?. first, the term human value is very generic, even the specific human value gets defined later e.g., deontology. the amt question form reads more like one that would be given to an expert rater, and would need to be either written in plain english or given to trained judges. second, the judges need to identify improvement in alignment with human values could be a source of confusion, as the revised response could align well but no better than the original response in which case the improvement is nonexistent. 2. the ethics reviewers made comments that have bearing on the technical merits of the paper, as they both pointed out that the authors modeling assumption that human values are static and consistent across contexts is probably too simplistic. as ethics reviewer udnx, views and norms of the world are varied, and may depend on complex contexts e.g., one may need a lot of background information about a given situation to know which action or statement is more moral. what seems concerning in the paper is that the context seems to be always reduced to one sentence, and it seems doubtful this provides enough information to make value judgments in many realworld situations. in sum, the paper makes valuable contributions, but there may be biases in the results weakness 1 and the practical utility may be somewhat limited weakness 2. we would recommend that the authors address the amtrelated concern and discuss more extensively their humanvalue assumptions considering the ethics reviews. regarding ethical concerns we also highly recommend that the authors follow the suggestions proposed by the ethics reviewers, e.g., include more information about the representativeness of the participants.", "accepted": null}
{"paper_id": "nips_2022_uxWr9vEdsBh", "review_text": "in this paper, the authors formulate batch active learning as a constrained optimization problem, and develop a primaldual approach to select a diverse set of unlabeled samples. the idea of using constrained optimization for active learning is novel and interesting, and the experimental results are also promising.", "accepted": null}
{"paper_id": "nips_2022_IILJ0KWZMy9", "review_text": "this paper proposes a dynamic low rank training scheme dlrt to optimize the neural network weight matrix imposing low rank constraint and hence yields better computational and memory efficiency. the obtained solution becomes low rank and thus it can realize factorization of the weights by its nature. the optimization procedure can be equipped with an adaptive rank selection scheme. the proposed method is mainly justified by numerical experiments. the optimization method is basically derived from the gradient flow along the low rank matrix manifold. this paper gives a rather theoretically solid optimization scheme. the presentation is overall good. it is well organized and contents required to understand the contribution are appropriately presented. there are still some weakness. first, the paper would benefit from adding some more related topics and giving more discussions about connection to them. second, it would enhance the paper if the additional numerical experiments on larger data and models imagenetresnet50 as presented in the rebuttal phase would be included. in summary, although there are some weakness, this paper gives a novel and solid methodology to obtain a low rank weight matrices. so, i recommend acceptance. on the other hand, i strongly recommend the authors to address the issues pointed out by the reviewers in the final version.", "accepted": 0}
{"paper_id": "nips_2022_PPlAVQDeL6", "review_text": "this paper proposes implicit flow network to estimate flow over network edges under certain physical constraints conversation law and constitutive relationship such as ohms law. most reviewers agreed that the proposed idea of augmenting ifns with physical constraints is novel and interesting, and experimental validation is sufficiently convincing.", "accepted": null}
{"paper_id": "nips_2022_WaKGmSI2-8g", "review_text": "the paper furthers the understanding of designing codebooks in the context of using error correcting codes for multiclass problems. as opposed to continuous relaxation, the state of the art, it advocates a graph colouring approach which finally yields an alternative to codebooks. to the best of understanding the methodology will be hard to apply for large number of classes. though the experimental results only show marginal improvement, the methods does improve upon the state of the art.", "accepted": 1}
{"paper_id": "nips_2022_XSNfXG9HBAu", "review_text": "the initial reviews were divergent. during the rebuttal and discussion phase, however, many of the raised concerns are addressed propertly, leading slightly towards accept. while some of the issues are not checked yet for whether to address them, i believe the authors response answer them adequately. hence i recommend the acceptance of this paper.", "accepted": 0}
{"paper_id": "nips_2022_Jd70afzIvJ4", "review_text": "reviewers agree that the problem of factored action spaces in rl is important and that this paper makes novel contributions to this setting. the reviewers were satisfied with the postrebuttal discusion and have converged on an accept recommendation. on revision, the reviewers request that the authors revise the paper according to the clarifications that occurred during postrebuttal discussion. also, for context, its important to note that the concept of factored action spaces goes back a long way in the factored mdp literature and i would request the authors to acknowledge this in their related work discussion as they prepare their final revision. to the best of my knowledge, the first mention of factored action spaces is in a 1996 multiagent mdp paper craig boutilier. planning, learning and coordination in multiagent decision processes. 1996 httpswww.cs.toronto.educeblypaperstark96.pdf somewhat more recently, the following paper presented a sequential hindsight method for compositional mdps that is an upper bound approximation for weakly coupled mdps. i mention this specific paper since it discusses theoretical results relating to factored action mdp approximations and also presents a simple approximate decomposition methodology that i have found hard to beat empirically aswin raghavan, saket joshi, alan fern, prasad tadepallia, roni khardon. planning in factored action spaces with symbolic dynamic programming. 2012 httpsojs.aaai.orgindex.phpaaaiarticleview8364", "accepted": null}
{"paper_id": "nips_2022_-uezmSLXVoE", "review_text": "this paper contains a fresh and mathematically interesting theoretical analysis of a fundamental problem.", "accepted": 1}
{"paper_id": "nips_2022_MbBTrAvee-N", "review_text": "this paper proposes a decisiontheoretic view of hedging within the framework of probabilistic graphical models augmented with a reward. after reading each others reviews and the authors feedback, the reviewers have solved most of their concerns and agree that the paper deserves publication. however, the authors need to seriously consider the reviewers suggestions for making their paper clearer in the cameraready version.", "accepted": null}
{"paper_id": "nips_2022_493VFz-ZvDD", "review_text": "the paper provides methods to improve the task of sparse training. the reviews agree that the idea is well motivated, novel and that the paper brings insights to sparse training that would be of interest to the community. the experiments seem quite extensive and show that these methods allow to improve the pareto curve of training process flops vs obtained model quality. one of the reviews raised several issues about the paper, questioning the soundness of the experiments and method. after reviewing the discussion, the major issues seem to be not fundamental flaws but unclear details in the paper. since these are clarified in the discussion, i view these as minor issues that can be fixed towards a camera ready version, and i urge the authors to carefully go over the reviews and fix the paper to be more clear. concluding, the consensus around novelty and overall positive feedback that remained positive through the discussion phase, lead me to believe the advantages of the paper outweigh its flaws.", "accepted": 0}
{"paper_id": "nips_2022_NSWNgQgoF71", "review_text": "the paper develops a methodology for computing the lipschitz constant of relu neural networks when the input perturbations are measured in the l_infinity norm. the method is based on computing tight upper bounds on the clarke jacobian. the basic idea is to apply interval bound propagation techniques to the backward computational graph, which yields an upper bound on the norm of the clarke jacobian of the network. experimental results show superiority over sota in terms of scalability, runtime, and the computed bound. the reviewers had a number of concerns most of which were addressed during the discussion phase. i recommend that the authors revise the paper and the experiments according to the reviewers comments as well as their own responses. the paper was also discussed among the reviewers. one main point of discussion was the novelty of the work compared to prior art, e.g. lyu et al and the fact that bounding the norm of clarke jacobian seems to be only beneficial for l_infinity perturbations. however, some reviewers argued and i agree that the paper improves over sota methods quite notably in terms of efficiency and tightness, and the method scales to much larger models compared to prior works scale is actually an important challenge in this topic. as a result, i would vote for accepting the paper. as a matter of taste, i dont think i agree with this sentence in the abstract and similar sentences in other parts of the paper existing methods for computing lipschitz constants either are computationally inefficient or produce loose upper bounds. we do have good methods that provide nontrivial upper bounds on the lipschitz constant of nns. while i agree that the scalability of those methods are still to be improved, we can not really call them inefficient. hence, i believe that this sentence and similar sentences in the paper could be better rephrased.", "accepted": 0}
{"paper_id": "nips_2022_ii9X4vtZGTZ", "review_text": "decision accept this paper proposes a task agnostic method to evaluate the representation quality of neural network, by looking at the eigenspectrum decay power factor for the feature covariance. theoretical results and empirical results show the method can be used as a cheap alternative for understanding neural networks and performing model selection. reviewers commended the approach being simple yet novel, and they are happy with the overall clarity of the manuscript. the main criticisms were 1. whether experimental results generalize to larger dataset and network architectures. 2. the interpretation of the method useful tool for model selection or for understanding neural networks only? in author feedback the above questions were partially addressed. note here the lack of big data experiments is not a major factor of my decision. the more important point is the authors clarification on the eigenspectrum decay being necessary condition for good performance, which makes the argument for model selection weaker in a theoretical sense, although the empirical results seems to be ok. i would suggest the authors to make a clear discussion on this as promised in author feedback. as a side note, my brief read of the paper seems to tell me that the theory part considers pretraining, and i would say pretraining is a broader concept than ssl. this means the theory is not specific to ssl, and i hope the authors can clarify this point. also i suggest the authors to discuss he and ozay icml 2022 in light of this papers results. httpsproceedings.mlr.pressv162he22c.html", "accepted": null}
{"paper_id": "nips_2022_Ir8b8lG_Vc", "review_text": "the paper makes interesting progress on issues related to multiagent reinforcement learning providing fast convergence guarantees as well as a unified framework. this is definitely a hot topic of research and it would make for a nice neurips contribution.", "accepted": null}
{"paper_id": "nips_2022_BsSP7pZGFQO", "review_text": "this work proposes a modelbased metalearning method to forecast physical dynamics. the proposed approach is able to generalize across heterogeneous domains as demonstrated in convincing sets of experiments. the reviewers found the work to be well motivated, clear and selfcontained. authors justified the proposed model architecture and the ablation studies conducted showed the importance of the network components. the authors also provided an adequate description of the data and the evaluation strategy, as well as theoretical guarantees on the generalization error in several settings.", "accepted": 1}
{"paper_id": "nips_2022_TERVhuQVTe", "review_text": "all reviewers are in agreement that this is an interesting theoretical and empirical contribution and a useful tool in better understanding neural networks.", "accepted": null}
{"paper_id": "nips_2022_hPVXHzzK0z", "review_text": "paper was reviewed by five reviewers and received 3 x borderline accept, 1 x borderline reject and 1 x weak accept. generally, the reviewers thought that the paper was interesting and had merits. raised issues revolved around 1 lack of clarity in certain parts of exposition; 2 evaluations and ablations that could have been made stronger, and 3 the role of pnp detr, which isnt a contribution, towards improved performance. additional reservations dealt with 4 claims that learning in the noncombinatorial predicate space is easier than the entity pair space, and 5 fairness of comparisons with respect to model capacity and other factors. authors have provided a compelling rebuttal and this has alleviated many of the reviewer concerns at lest to an extent. post rebuttal, znfu remains concerned that generating more predictions may be what is causing improved performance and points out that certain ablations are still missing and can improve the paper. at the same time, znfu, while remains at borderline reject, acknowledges that rebuttal has resolved some of the issues in the original review. ac has carefully considered the reviews, the rebuttal, and the paper itself. this appears to be a rather borderline case, however, considering that the overall sentiment of reviewers is positive and that rebuttal has convincingly addressed an important fraction of concerns raised by znfu and others, even if not all, it is acs decision that acceptance of the paper is warranted. authors are very strongly encouraged to add the ablations, as well as make corrections, suggest by reviewers, for the camera ready.", "accepted": 0}
{"paper_id": "nips_2022_ztcfHweENtU", "review_text": "the paper grounds several fairness notions used in machine learning in principles of distributive justice. the stated motivation is to understand the normative choices behind each and to combat the shortcoming of some of these notions. the main concerns of the reviews were that this grounding is very limited in terms of its scope and there is little actionable insight that follows. furthermore, many of the connections have already been acknowledged in the literature, e.g. 11, 29. philosophical underpinnings of the sciences are very important, as they can help advance both the questions we ask and the answers we offer. the effort of this paper is thus appreciated. however, as it falls somewhat short of advancing either the philosophy or the science, it may be of limited significance to the community. to garner better appreciation of their work, the authors are advised to elaborate on how their grounding could guide the field e.g., how could one make algorithmic fairness choices in light of this perspective? have there been instances where the wrong choice was made algorithmically relative to the stated intent normatively? are there limitations to this perspective, perhaps in terms of assumptions that should be challenged? etc.", "accepted": 0}
{"paper_id": "nips_2022_R5yl-ySZR0U", "review_text": "a new transformer method for image generation is discussed. reviewers appreciated the results but raised concerns regarding exposition, some questionable ablations, limited novelty and relation to prior work maskgit. the rebuttal was able to address some concerns. in a discussion reviewers generally kept their rating but raised concerns regarding novelty and ablations again. ac thinks the paper just barely made the cut and strongly encourages authors to further improve the ablations in the camera ready version to further strengthen the paper. ac also recommended senior acs to look at this decision and possibly revise.", "accepted": 0}
{"paper_id": "nips_2022_JyTT03dqCFD", "review_text": "getting a reasonable estimation of joint predictions is crucial for many uncertainty estimation tasks. the paper proposed a set of benchmarks for predicting joint probabilities of the outputs over a few input examples. the proposed synthetic tasks are easy to deploy and test on most bayesian methods, including bayesian neural networks. uncertainty estimation is one of the fundamental challenges for modern machine learning algorithms. many downstream application areas in reinforcement learning, active learning, and safety require a model to assess its prediction confidence. yet, unlike the standard classification tasks, there is a lack of benchmark datasets to evaluate the performance of uncertainty methods. the strength of this paper is 1 develop a suite of benchmarks, although synthetic and toyish, to allow a quantitative study of the joint prediction of the existing machine learning methods. the proposed benchmark allows researchers to study uncertainty estimation without invoking any downstream application in rl or active learning. 2 the work bridge the gap between the benchmarks on marginal predictions, such as riquelme et al. deep bayesian bandits showdown and heavy machinery of exploration tasks in rl. the weakness of the current submission is a lack of clarity in the current writing, as pointed out by one of the reviewers. many experimental hyperparameters are omitted from the main text, which would help the readers understand the benchmark details and design choices. also, there is a glaring limitation of the benchmarks simplicity and whether the generative model choice could generalize to highdimensional problems. given the scarcity of other benchmarks in the uncertainty estimation tasks, the strength outweighs the weakness of this paper.", "accepted": null}
{"paper_id": "nips_2022_F2mhzjHkQP", "review_text": "the paper attains the sde approximations for two optimization algorithms rmsprop and adam. the authors have addressed the concerns raised by the reviewers during the rebuttal period. all the reviewers agreed that the paper should be accepted at neurips 2022. please incorporate the reviewers suggestions in their detailed reviews and revise the final version of the paper properly.", "accepted": 1}
{"paper_id": "nips_2022_DTsCy9Lyj5-", "review_text": "there is general consensus among the reviewers that this paper is a valuable contribution to the bilevel optimization literature.  the value function formulation is still relatively unexplored in bilevel optimization althought not completely new. having a new paper developing this direction will be a nice addition to the literature.  the paper seems well written and sound.  the experiments, though they dont really assess the scalability of the approach, are illustrative and diverse. we therefore recommend acceptance. to the authors please take into account the reviewer comments in the cameraready paper. please be careful of not overselling the contributions with superlatives like powerful method.", "accepted": 0}
{"paper_id": "nips_2022_2-CflpDkezH", "review_text": "this paper proposes and examines a notion of correlated equilibrium for constrained stochastic games, that is, stochastic games where the players seek to optimize their payoffs modulo guaranteeing a certain target. the reviewers initial concerns were addressed satisfactorily by the authors during the rebuttal phase, leading to a unanimous accept recommendation from the reviewers. after my own reading of the paper, i concur with this assessment the paper treats an interesting and timely topic, and the results are both interesting and technically challenging. on a personal note, i would urge the authors to explain in more detail the notion of a constrained markov game, as the terminology is not quite standard in game theory where constraints typically have a different meaning than in mdps; however, other than that, the reviews speak for themselves and i am also happy to recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_U6vBmFL9SxP", "review_text": "this paper gives a new method to perform nonlinear sufficient dimension reduction by utilizing a stochastic neural network. the derivation of the proposed method is justified by some theoretical background, and a convergence rate analysis is given for the derived algorithm. the practical performance is evaluated by some numerical experiments on real datasets. although there are some related work, the proposed model is new. the theoretical justification of the proposed method is solid. the paper is overall clearly written. the numerical experiments properly shows effectiveness of the method while they are rather small. in summary, this paper presents a novel method with nice theoretical and numerical justifications. i recommend acceptance.", "accepted": 0}
{"paper_id": "nips_2022_401LFvBGIb", "review_text": "we had quite a bit of discussion on this paper. i read the paper and agree with some of the discussions that the paper in its current form might not attract interest in the community due to the following reasons  while it is highly interdisciplinary and potentially of high impact, the authors did not manage to connect all topics to tell a concise story on the use of equilibriumpoint control theory in making a new recurrent neural network model. here are some final comments from our discussions with the reviewers  while the content looks technically sound, we havent seen from the authors the expected revision to improve the paper. we were expecting a better introduction to the problem and further discussion to convey the contribution.  we think that it would be in the interest of the authors to extend the paper with clarifications and more background information such that a larger audience will be able to learn something from their work.  we believe that this issue is fixable as there is ample space to add additional backgroundintroduction to explain the problemrelevance. however, the authors did not manage to convince the reviewers how they are going to address this during the discussion period.  the authors could elaborate much more in detail on all topics involved to build a better understanding first and convey their contribution and impact better.  also, there are recent advances in statespace models and their use as expressive representation learning algorithms within the community which are disregarded in the paper. in particular, the stability condition, memorization, the efficiency of computing transition matrices which are very relevant to this paper, and more properties are discussed the last few years. here is an example 1 gu et al. efficiently modeling long sequences with structured state spaces, iclr 2022 httpsarxiv.orgabs2111.00396 based on these points, i vote for the rejection of this paper.", "accepted": 0}
{"paper_id": "nips_2022_9i7Sf1aRYq", "review_text": "most reviews are positive and think that the paper solves an interesting and nontrivial problem. one reviewer points out some concerns on the motivating example, and it seems to be addressed in the author response. i have a different concern that in real world, the set of eligible bidders in each auction differs a lot, and maybe the authors can add some discussion on how this affects the result.", "accepted": 0}
{"paper_id": "nips_2022_FhuM-kk8Pbk", "review_text": "the paper presents a novel approach for interpreting a neural networks decision on audio input. the motivation of the research is clear, and all the reviewers have agreed on the importance of the addressed task and the originality of the proposed solution. also, the authors have adequately responded to the reviewers concerns during the rebuttal, including some additional experiments such as a comparison with attribution interpretation or a modified baseline. therefore, i gladly recommend this paper be accepted in neurips 2022.  strengths  the paper addresses one of the underexplored yet important tasks of interpretability of neural networks for the audio domain. as visualization is helpful for interpreting neural networks in the vision domain, sonification is a reasonable and intuitive way to analyze how the neural network works on audio. this paper presents a promising solution for this purpose.  the architecture of the proposed model is original and shows high novelty. the experiment for the evaluation was welldesigned, showing meaningful results.  weaknesses  many reviewers suggested that it would be interesting to see how the proposed model works on more complex data, such as music audio. the authors have explained why they focused on sound event detection as an initial goal. the reviewers suggestion would remain a challenging but valuable goal for future research.", "accepted": null}
{"paper_id": "nips_2022_sof8l4cki9", "review_text": "this paper continues a line of works studying sgd via a similar sde, this time employing a variety of tools not used before, for instance timerescaling and normalization layers. the reviewers on one hand were concerned at times that this was incremental, but also uncovered a variety of interesting contributions. as such, i feel this paper is a clear accept, and am excited to see it in the conference. that said, the discussions between reviewers and authors were quite detailed and i feel the authors could greatly strengthen their presentation by carefully adjusting for them, making their intended message more clear for future readers.", "accepted": 1}
{"paper_id": "nips_2022_rrYWOpf_Vnf", "review_text": "this paper studies the statistical performance of deep ritz methods drm and physics informed neural networks pinn for solving an inverse problem with respect to elliptic equations. as a key example, consider the problem of estimating the solution u to poissons equation delta u  f given noisy observations of f. when the solution is a function in an rkhs, and using a sobolev norm as the objective function, they show that gradient descent achieves implicit acceleration. they use this theory to show that both drm and pinn achieve statistical optimality, but the number of epochs needed for drm is larger. this is a nice contribution to the growing area of deep pde solvers, and provides some illuminating theoretical insights. there was some debate between a reviewer and the authors about whether the results follow from existing bounds for kernel regression, but the authors convincingly argued that the norm of a1 in general need not be bounded and so those results cannot be applied in a blackbox manner. additionally there are other differences and complications.", "accepted": 1}
{"paper_id": "nips_2022_B4OTsjq63T5", "review_text": "all reviewers agree that the paper proposes an interesting approach to bayesian inference incorporating coresets with hamiltonian flows. although some reviewers have some technical concerns at their first reviews, basically those have been resolved by the authors responses. thus, although there are some points that should be modified from the current form, i think we can expect the authors modify the paper in the cameraready by reflecting the discussion. based on these, i recommend acceptance for this paper.", "accepted": 1}
{"paper_id": "nips_2022_qTCiw1frE_l", "review_text": "the paper identifies a hitherto unknown phenomenon in valuebased reinforcement learning called policy churn. all the reviewers agree that this is a very interesting phenomenon, and that the paper studies the phenomenon comprehensively. all reviewers also agreed that this paper is likely to inspire many followup works. understanding the causes and harnessing policy churn can potentially significantly impact the deep rl stateoftheart.", "accepted": 1}
{"paper_id": "nips_2022_AlkMMzUX95", "review_text": "this work introduces a novel layer in order to extract information better at a fine grained level. the basic idea is to introduce a spatial mixture of expert which will localized computations to a specific part of the image. given the novelty the method, i recommend accepting this paper. however, please add the modifications proposed to reviewer vuz5.", "accepted": 0}
{"paper_id": "nips_2022_3vpvnMVOUKE", "review_text": "the contribution of this submission is strong  an analysis of convergence of nonlinear mcmc methods. most reviewers agree that the submission is theoretically interesting and of interest to the neurips community. thus i recommend acceptance. however, i note and share with some reviewers the following concern the empirical results on cifar10 do not show the benefit of the proposed method. the result of the linear baseline ula is very far from sota for resnet and cifar10, yet ula outperforms the nonlinear version in terms of accuracy and time efficiency. it would be great if there is a realistic modeldataset between the simple 2d toy experiment and perhaps too ambitious resnetcifar10 that can be included to show the benefits if any of nonlinear mcmc. some reviewers also raised a concern about the organisationflow of the paper which i hope the authors will fix in the cameraready version.", "accepted": null}
{"paper_id": "nips_2022_2B2xIJ299rx", "review_text": "this paper studies lowcurvature training of neural networks. authors first propose a normalized curvature metric that is invariant to the scaling of the gradient and provide an upper bound for it in terms of the curvatures and slopes of different layers. then they move on to the main contribution which is introducing an alternative activation and batchnormalization components which effectively limits the curvature. finally, they show that when trained with these alternative components, one can achieve the same accuracy while having more stable gradients and more robust networks without significant training time increase. in my opinion, the fact that the robustness to input perturbations and the stable gradients come with simply substituting the previous components with the new one is the biggest advantage. on top of that, in terms of the training time, the proposed method is superior to other robust baseline approaches. while im recommending acceptance, i strongly suggest authors to make the following improvements for the cameraready to increase the impact of their work significantly 1 more data  more models current experiments are very limited in terms of architectures and datasets. i suggest adding imagenet results and two other small datasets say svhn and fashion mnist. also, i suggest repeating the experiments on some noneresnet architectures as well. 2 authors have argued that the stable gradients are useful for interpretably. while this might have been established separately, it is still interesting to have at least one experiment to demonstrate this. 3 even though this method is faster than other robust baselines, it is still 1.6x slower than standard training which is a significant limitation for adaptation of these components. i dont see an inherent reason for this slowness. would be great if the implementation can be further optimized in terms of the running time. 4 one thing that i think is missing at the moment is a clear bottom line written somewhere regarding what we can conclude from the experiments all this information is in the paper, but i think it would be better if it was highlighted somehow as main conclusions or sth of that form.", "accepted": 0}
{"paper_id": "nips_2022_nP6e73uxd1", "review_text": "sampling from logconcave distributions is a well studied problem and there are many existing algorithms that can sample from a distribution close to the true distribution up to a small total variation distance. the paper gives a new reduction that can use these algorithms as a subroutine to get samples from a distribution close to the true distribution in infinity distance i.e. the densities are close everywhere. this problem commonly arises in differentially private optimization. the reduction is simple and can be implemented easily. all reviewers agree that the paper is a significant contribution to the literature, it is well written, and the algorithm has potential to be useful in practice.", "accepted": 1}
{"paper_id": "nips_2022_BqnMaAvTNVq", "review_text": "the authors have largely convinced the reviewers and definitely myself of the merits of the paper after extensive and detailed rebuttal and discussion. i am happy to recommend acceptance.", "accepted": 0}
{"paper_id": "nips_2022_P7TayMSBhnV", "review_text": "the paper authors a new generalization analysis of sgd with mc sampling by using algorithmic stability. the reviewers agreed that the technical contribution is novel and interesting. though initially two reviewers were concerned about the potential applications for sgd with mc sampling, the authors have updated their paper pointing out several applications that fits the type of mc sampling assumed in their proof.", "accepted": 0}
{"paper_id": "nips_2022_oMhmv3hLOF2", "review_text": "all reviewers agree that the paper should be accepted, despite some flaws that can be addressed in future work", "accepted": null}
{"paper_id": "nips_2022_muvlhVKvd4", "review_text": "the authors provide a blanket convergence analysis for several stochastic optimization methods. the techniques are interesting and will be useful. the authors may want to be a bit more careful on the details on some of their convergence results when they make comparisons. for instance, the main difference between 3 and 26 is in the noise assumptions in 26, which allow to use more aggressive stepsize policies. otherwise, the difference in assumptions that the paper alludes to is reflected in the fact that 26 is getting a stronger convergence result to a component of critical points, whereas 3 leaves open the possibility that the process escapes to infinity the assumptions in 26 rule out this behavior. the authors also miss the recent work, which provide a tighter, general characterization y.p. hsieh, p. mertikopoulos, and v. cevher. the limits of minmax optimization algorithms convergence to spurious noncritical sets. in icml 21 proceedings of the 38th international conference on machine learning, 2021.", "accepted": null}
{"paper_id": "nips_2022_vriLTB2-O0G", "review_text": "this paper studied the problem of batch multiobjective bayesian optimization bo. it considers a novel perspective to solve problems with an infinite size pareto optimal set by finding a pareto manifold of solutions. the acquisition strategy uses chebyshev scalarization. the key idea is to learn a mapping from preferences i.e., scalarization parameters to the pareto optimal solution and use it to guide the acquisition strategy and bo process to approximate the pareto set. experimental results demonstrate the effectiveness of the proposed approach. all reviewers agreed about the novel perspective from which the multiobjective bo problem was studied, but also raised some concerns and questions. the authors gave satisfactory responses to most of the review comments and revised the paper to improve it. two reviewers strongly supported accepting the paper and two of them gave borderline accept. authors satisfactorily addressed the main concern of one of them i.e., test problems are too simple. some of the comments from reviewer mk65 needs further work, which is acknowledged by the authors. the overall approach is novel, advances scalarization based multiobjective bo, produced good results, and has the potential to generate good interest in the bo community. therefore, i recommend acceptance.", "accepted": 1}
{"paper_id": "nips_2022_ldxUm0mmhl8", "review_text": "this paper proposed a simple yet sensible method to answer counterfactual questions for temporal point processes. specifically, the authors focus on the counterfactual question of whether a historical event would have happened if the corresponding intensity had been changed. reviewers agree that the idea is clearly presented and theoretically plausible, with interesting and important epidemiological applications. please also pay attention to the reviewers concerns about the paper title and the presentation of some specific parts of the paper.", "accepted": 0}
{"paper_id": "nips_2022_oiztwzmM9l", "review_text": "the authors introduced two asymptotically noregret neural thompson sampling algorithms. they derived regret upper bounds and showed that they are asymptotically noregret under certain conditions. they verified their empirical effectiveness with automl and reinforcement learning experiments. all reviewers liked this paper. please note however, that it is somewhat surprising that in some cases that standard gpucb and gpts competitors performed very badly. one of the reviewers reproduced the lunarlander experiments in botorch and achieved much better performance for these competitor methods.", "accepted": null}
{"paper_id": "nips_2022_QudXypzItbt", "review_text": "there was disagreement among the reviewers about whether this paper should be accepted. but taking the long view about how this paper might be perceived in 10 years, and reading the paper in detail myself, i lean towards acceptance being the right decision for this paper. my reasoning  the problem being solved is completely novel as far as i am aware, and is wellmotivated by a realistic realworld scenario in a large field of research.  the proposed approach is a highlyoriginal variation on bo that, despite not being justified by theory at all, seems like a plausible direction to explore to get faster algorithms. in light of the above, the potential impact of the paper might be large we may discover other applications that have the same cost setup, we may encourage ml researchers to take on chemistrymotivated problems, we may encourage theory researchers to analyze the method or more likely to come up with methods that are justified, or we may see empiricists use similar ideas in other settings. that being said, the reviewers point out some completelyvalid criticisms. so i expect to see quite a few updates in the final version of the paper. please comb through the reviews carefully particularly the two critical reviews and update the paper based on the reviewers comments, including and beyond what is already written in the author response. i could imagine many readers having similar issues e.g., tone down any indication that the algorithm is theoretically justified from a regret perspective, and some of the comments represent ways the paper could be complete in its exploration of the topic.", "accepted": 0}
{"paper_id": "nips_2022_2uAaGwlP_V", "review_text": "the paper is very wellwritten, provides a very useful recipe for training diffusion models faster and combines that with extensive experiments. good work!", "accepted": 1}
{"paper_id": "nips_2022_TEmAR013vK", "review_text": "we thank the reviewers for their helpful feedback. here, we briefly summarize our contributions and present evaluations of dash on a largescale dataset imagenet1k and a new backbone convnextt. the results show that our claims about dashs efficiency and effectiveness extend beyond the tasks and backbones in the original submission, directly addressing concerns raised by the reviewers. in this work, we first identified a crucial problem in todays ml research while many models are being developed in popular domains like cv and nlp, these models only work for the specific tasks they are trained on and often cannot generalize to distinct fields. thus, selecting the right models for arbitrary tasks in understudied domains remains difficult. a key to solving this issue is to automatically design networks that capture input features based on a tasks needswhile some tasks require modeling neighboring interactions, others require modeling longrange dependencies. to achieve such taskspecific feature extraction, we present dash, which customizes the convolutional operations in a model by searching for the optimal kernel sizes and dilation rates in a large and diverse search space. dash represents a new framework for model development in diverse domains with the following benefits  efficiency dash explores the large kernel space efficiently via a novel combination of weightmixing, fourier convolution, and kronecker dilation, whereas existing nas methods cannot do so.  effectiveness dash finds optimal kernels for a wide set of problems. on more than 6 evaluated tasks, the searched models beat all nas baselines and handdesigned expert models.  generalizability dash uses an aggregated conv operator to extract features at different granularities. its ability to generate largekernel networks allows it to tackle new tasks like pde solving and protein folding. existing nas and generalpurpose methods all have failure modes due to their limited sets of feature extractors considered. besides these novelties, dash has practical significance as it can be applied to any model with a conv layer, such as vision transformer 1, coatnet 2, to enhance model performance at a low cost.  imagenet1k results all reviewers mentioned testing dash on larger image datasets and more backbones. though our motivation is not to compete in the crowded vision domain but to provide a general solution to lessstudied domains, we agree that showing backward compatibility is crucial. to complement existing evaluations, we tested dash on imagenet1k with two backbones of distinct scales. results show that dash generalizes to tasks with large input shape 3x224x224, dataset size 1.2m, and number of classes 1000. it improves the accuracy of the original models and searches efficiently regardless of the backbone used. we used wide resnet 164 to be consistent with the automl workflow in the paper and convnextt a largescale cnn that has onpar performance with sota transformers as the backbones and performed experiments on 4 nvidia v100 gpus. to demonstrate dashs efficiency, we first present the perepoch search time for three baselines over the search space k3,5,7,9,11, d1,3,7. a subset of 4096 images is used. search time secsepochwrnconvnext   params3m28m dash151.380.5 mixedweights705.4300.1 mixedresults330.6149.6 dashs efficiency holds for both backbones. though convnext has more parameters, it is searched faster than wrn as it has fewer conv layers and applies downsampling to the input. then, we report dashs runtime vs. the traintime of the vanilla backbone. we let dash search for 10 epochs with subsampling ratio 0.2. retraining takes 50 and 100 epochs for wrn and convnext, respectively. total time hrswrnconvnext  dash search2413 dash retrain5248 backbone train1641 lastly, we report the top1 accuracy of the searched vs. original models to show dash generalizes to large vision input. note that due to the limited response time, we trained convnext for 100 epochs and a single trial. in the revision, we will include results with full training 300 epochs as in 3 and more trials. accwrnconvnext  vanilla backbone60.176.3 dash searched model64.578.4 in general, dash improves backbone performance by adopting taskspecific kernels. we observe that it tends to pick larger kernels for later layers, possibly to extract higherlevel features. e.g., a set of kernels k, d it selects for wrn is 3,3rarr;7,1rarr;7,3rarr;11,3rarr;7,3rarr;11,1rarr;7,3rarr;11,1rarr;7,1rarr;11,7rarr;7,3rarr;11,3rarr;11,1. _references_ 1 dosovitskiy, alexey et al. transformers for image recognition at scale. iclr 2021. 2 dai, zihang et al. coatnet marrying convolution and attention for all data sizes. neurips 2021. 3 liu, zhuang et al. a convnet for the 2020s. cvpr 2022. this paper gives a significantly improved neural architecture discovery method specialized for convolutional networks. the idea is to train a large supernetwork and cache common computation and select the best performing subnetwork. the paper also presents several useful ideas to accelerate to training and exploration of convolutional operators at a large scale. this is based on the observation that the overhead of using fourier transformation is significantly amortized if the transformation is used for exploring convolutions for a wider range of hyperparameters including patch size and dilation factor. in addition the work suggests exploration on multiple tasks at the same tasks for more robust results. originality high although essential the ideas in this paper are based on several earlier works e.g. darts or wu et altree structured network... or the use of fourier transformations has ben extensively explored in the literature. however the recognition that reusing some of the timeconsuming intermediate results make these methods much more appealing in this context is novel and useful. quality mixed. the methods are wellmotivated and seemingly efficient, but the experimental results lack some more thorough evidence, especially on images of larger size. while the synergy of the applied methods is very high and seems to result in a very efficient methodology, the search space is limited to a very specific type of convolutional deep learning models. the experimental section also lacks more thorough ablation for judging the criticality and clarity medium. the description of the applied methods is extremely formalmathematical and would benefit from a more intuitive explanations and more figures to introduce the later to the most essential ideas in a more gentle way. also, the paper is not structured in a very appealing manner and could have been made more accessible by a better topdown explanation which would explain the synergy of the components and more thorough intuitive comparisons with similar approaches. significance medium. given the fact that transformers are overtaking convolutional networks in many respects, it is hard to judge the future significance of these ideas as transformerbased methods, or even just using transformerbased exploration might render these methods less appealing in the near future. also the lack of experimental results on benchmarks with larger images like imagenet is concerning and makes it harder to judge the expected impact of the methods. could you give more experimental evidence on benchmarks with large images and more classes esp imagenet and other vision tasks. i dont see any concerns regarding the societal impact specific to this work. this paper introduces a nas method called nash. this method can be applied to a variety of tasks and achieve good results. it is meaningful that nas is applied to a variety of tasks, which demonstrates the potential of nas methods.  strengths  the tricks used for acceleration are a bit interesting.  it makes sense to apply the nas method to other tasks.  weaknesses  table3 is mentioned in many places, but i didnt find it.  there are some problems with the experimental part 1. the flops for each model are not reported, and it is not clear to me whether these methods are comparable. 2. is the search space the same for each model? 3. why only use wrn for experiments? different structures should be chosen to verify the effectiveness of the method.  frankly speaking, nash is not a surprising method in terms of method innovation, it is just an application of darts to some extent. please refer to the weakness in the above part. please refer to the weakness in the above part. this paper focuses on the direction of searching architectures for diverse tasks. the designed search space contains convolution layers with large and diverse set of kernel sizes and dilation rates. to address the efficiency challenges encountered in weightsharing, dash is proposed by using mixtureofoperations based on the fourier diagonalization of convolution. extensive experiments on ten tasks suggest the effectiveness of the proposed dash methods. strengths 1. this paper explores a new direction of applying nas in diverse tasks. in their setting, the authors notice the importance of involving convolution layers with large kernel sizes and dilation sizes. to address the efficiency problem in the large search space, mixtureofoperations based on the fourier diagonalization of convolution is used in their proposed dash. 2. extensive experiments validate the importance of involving convolution operations with large kernel sizes and the effectiveness of dash in ten tasks. weaknesses 1. for diverse tasks, one important setting is that whether we can directly search architectures from the supernetwork for different tasks. it seems that the authors did not take this into account. 2. in section 3.3, grid search is used to search hyperparameters in both search and retraining stage. does grid search play a key role in performance? if grid search is also applied to baseline methods, will it improve the performance? 1. i m wondering the performance of dash in the designed search space with convolutions of large kernel sizes and dilation sizes in large data sets, like imagenet. 2. the other two questions are listed in the weakness part. looking forward to authors responses. yes. this paper proposes a significantly improved neural architecture discovery method specialized for convolutional networks. the idea is to train a large supernetwork and cache common computation and select the best performing subnetwork. also, it presents several useful ideas to accelerate to training and exploration of convolutional operators at a large scale. while the work is the combination of somewhat wellknown approaches, the overall approach is novel, interesting and wellmotivated, however its application domain is somewhat limited. still, given the convincing experimental results, good execution and significant gains in terms of training time compared to competing appraoches, i propose this paper to be accepted for neurips 2022.", "accepted": null}
{"paper_id": "nips_2022_u6MpfQPx9ck", "review_text": "this paper introduces an active learning method, probcover, that seeks to maximize probability coverage for the low budget regime. it also provides theoretical analysis and a dual way to view the proposed method with respect to methods better suited to the high budget regime like coreset. the paper received two weak accept and a weak reject rating. after reviewing the paper, reviews, author responses, add additional discussion with reviewers, i believe that on the balance the strengths of the paper outweigh the weaknesses. reviewers overall appreciated the importance of the task, the theoretical analysis, and thorough experiments. most of the questions and clarifications about the approach were sufficiently addressed in the rebuttal, as well as additional requested experiments. reviewers considered the use of a representation space from an auxiliary model to be a weakness, but i agree with the assessment also concurred by reviewer that this is not a limitation that invalidates the contributions of the paper. in practice it is a reasonable approach given the low budget regime. reviewers also questioned the comparison with typiclust, which was originally performed using only the simclr representation space, and not the one based on scan that typiclust also used. the authors added comparison based on the scan representation for cifar10 in their rebuttal, which is appreciated, and showed improvement in this setting as well. however, since an experiment on only one dataset was provided, and the improvement is smaller than for the main reported results using simclr and typiclust outperforms probcover at the smallest budget, the authors are highly encouraged include more comprehensive comparisons using the scan representation as well in the final paper for completeness. overall, acceptance is recommended for this paper.", "accepted": 0}
{"paper_id": "nips_2022_Qr8n979lusV", "review_text": "this paper had reviews ranging from a reject to a weak accept. the key shared concerns among reviewers were concern about how much is really new relative to the cvpr paper li et. al, 2022. the most negative reviewer who is quite expert in this field engaged strongly in the discussion with the authors, highlighting sustained concerns about novelty, substantially slower speed at rendering time, and the loss of the ability to render materials with interesting reflectance properties. i found this review most accurate and detailed. the remaining reviews, while borderline or weakly positive, retained concerns about the limited lighting and reflectance properties. therefore i am deciding to reject this paper", "accepted": 0}
{"paper_id": "nips_2022_KOHC_CYEIuP", "review_text": "the reviewers have reached a consensus of accepting the paper.", "accepted": null}
{"paper_id": "nips_2022_dNXg-h6YX9h", "review_text": "the problem is well introduced and the main results are clearly presented while there are no experimental results, the theoretical contribution seems strong. please add the main open problems to the final version.", "accepted": null}
{"paper_id": "nips_2022_1r1GDXPtuWz", "review_text": "this paper analyses grid worlds, more precisely multiple objects moving in grid worlds, using the mathematical idea of state complexes. the state complex represents all possible configurations as a single space, from which domain properties can be ascertained by grouptheoretic, combinatorial, or geometric analysis. in particular, the paper develops a theory around gromovs link condition to analyze conditions under which collisions can be prevented in such domains. the reviewers had a mixed initial response to this paper. on the positive side, the reviewers appreciated the theoretical development txod and novelty 2ssw. on the negative side, the reviewers struggled to see the significance or relevance of the work to learning or ai 2ssw, 5vs3. the reviewers understood the work as a mechanism for collision checking 2ssw, a means to support learning 5vs3, and a computational mechanism for analyzing gridworld dynamics txod. the author response clarified several aspects of the reviews that were misunderstood. the author response did not sway the reviewers. primarily, the concern is that the paper failed to communicate the relevance of the mathematical analysis of gridworlds to an ai audience. the sole positive reviewer ultimately concurred with the arguments made by the negative reviewers. two reviewers indicate to reject, and one indicates a weak accept. based on the failure of the paper to clearly communicate the relevance of its ideas to any reviewer, the paper is rejected. one suggestion for a future revision would be to present these ideas in the general setting of an mdp instead of a specific domain of a gridworld. the local combinatorial analysis on a generic mdp could potentially be more useful to the mdp community when considering planning for ai safety or problems of mechanism design. the evidence needed to validate the ideas for those communities might again be different from the evidence provided in this paper. as a separate comment, the analysis of the transition dynamics of actions may have related work stemming from predictive state representations. in the papers current form, the reviewers were unable to see a clear contribution.", "accepted": 0}
{"paper_id": "nips_2022_7yHte3tH8Xh", "review_text": "the paper identifies the problem of negative augmentation in graph augmentation methods as it may cause the distribution shift issue. the paper thus proposes a knowledge distillation method that trains a teacher model on the augmented graphs and a student model on the original one. reviewers had concerns on the novelty of the approach and experiments. the discussion between the reviewers and the authors were effective, and two of the reviewers have raised the scores to acceptborderline accept. id recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_L0U7TUWRt_X", "review_text": "this paper had borderline reviews. the reviewers felt that the contributions were significant and that the connection between graph contrastive learning and spectral properties of the graph were valuable. weaknesses included a focus on just node classification and some significant issues with presentation. the authors addressed many concerns presented and agreed to address the presentation issues. we hope that they will indeed do so in the final version of the paper, as the presentation issues would significantly limit the impact of the paper. in our discussion one reviewer strongly advocated rejection, while three pushed for acceptance, putting the paper above bar.", "accepted": 1}
{"paper_id": "nips_2022_wuunqp9KVw", "review_text": "the paper addresses the pluralistic image completion problem. initial reviews were borderline accept 2x , weak accept 2x. the authors provided a rebuttal. both borderline reviewers upgraded to weak accept. the ac agrees and considers the paper a solid contribution to neurips and recommends acceptance.", "accepted": null}
{"paper_id": "nips_2022_ZlCpRiZN7n", "review_text": "this paper proposes a sourcefree domain adaptation method based on unsupervised clustering. the main assumption is that the sourcetrained model could generate target domain features that have smooth predictions in a neighbourhood. the proposed method optimizes the upper bound of the objective of prediction consistency. experimental results show that the proposed method outperforms pseudo label and neighbourhood clustering methods. while the main idea is not significantly novel, the effectiveness of the proposed algorithm is demonstrated by solid experimental studies. this is again a simple and efficient deep learning method designed with intuitions and without strong theoretical evidence. i would recommend acceptance of this paper given its impressive performance and solidness.", "accepted": null}
{"paper_id": "nips_2022_lCGDKJGHoUv", "review_text": "this paper analyzes the behavior of sgd for stochastic convex optimization, showing that there exist problem instances where the sgd solution exhibits both significant empirical risk and generalization gap in the without replacement case. the finding is potentially impactful in the context of sgd generalization bounds. the paper is wellstructured and provides good intuition for the proof technique. however, i encourage the authors to provide a construction of the failure that is more general and less artificial.", "accepted": 0}
{"paper_id": "nips_2022_ATfARCRmM-a", "review_text": "this paper proposes a molecule generation method using frequent subgraphs. there was a positive consensus among reviewers that paper is novel b6na, ektl and wellanalyzed b6na, esyh, and minor suggestions for improved evaluations and presentation ektl, esyh were wellhandled during rebuttals to alleviate reviewer concerns.", "accepted": null}
{"paper_id": "nips_2022__7bphw9JosH", "review_text": "after the author response period, the reviewer ratings were all positive. the reviewers felt that the paper tackles an important problem in assessing the quality of graph generators and proposes a novel, general purpose, and effective approach. the reviewers pointed out several points for clarification in their reviews, which we hope that the authors will address in the final version of the paper. many of these have been addressed during the author response period.", "accepted": null}
{"paper_id": "nips_2022_0vJH6C_h4-", "review_text": "while the scores are borderline, reviewers found the paper interesting and the experiments convincing, so i recommend acceptance. while there were originally concerns about the appropriateness of comparison with qmix and the relationship to related work, i think these were adequately addressed in the rebuttal.", "accepted": null}
{"paper_id": "nips_2022_Magl9CSHB87", "review_text": "the paper under consideration proposes to improve sample quality in gans by performing optimizationbased adversarial mining on the latent space as a preprocessing step, using the fast gradient sign method of goodfellow et al 2015. optimization in the latent space is not a new area, and i was surprised to see e.g. neither bojanowski et al, 2017s optimizing the latent space of generative networks nor azadi et al, 2018s discriminator rejection sampling cited as related ideas. reviewers found the idea interesting and the experiments mostly convincing, the contributions clear. the motivation for this paper hinges upon a proof under certain assumptions that complex distributions require a latent prior with disjointdisconnected support; some reviewers were unclear how this was connected to fgsm but this was cleared up in rebuttal. several reviewers expressed concerns about the farfromstateoftheart gan backbonesmall scale of the experiments, however new experiments involving larger resolution datasets and a stylegan2 base model have quelled these concerns. to the ac, the method is intriguing, appears wellmotivated and wellvalidated, especially in light of new experiments on larger scale data. leveraging an adversarial example discovery procedure for the purpose of improving the prior sampling distribution is a clever and nonobvious innovation, and as the authors note in their rebuttal, while other feedforward generative networks may suffer from the same issues around being too continuous, the training procedure used by gans wherein the discriminator defines a nonstationary objective function, and its gradients are used as the generators learning signal uniquely position them to exploit this trick of performing surgery on the latent prior. for all of their difficulties, gans have a reputation for accomplishing quite a lot in terms of sample fidelity for a fixed model capacity, and ideas like the one presented herein may serve to further alleviate training challenges, as well as elucidate niches in which gans remain useful at a time when the preponderance of attention has shifted to diffusion models. i recommend acceptance.", "accepted": 1}
{"paper_id": "nips_2022_-zlJOVc580", "review_text": "unanimous accept scoring 5675, with confidence, from reviewers who have published in similar areas all four reviewers all agree that the work is novel using such maskedbased representativesauxiliary losses in an rl setting in the _latent_ space opposed to the original rgb space which doesnt sound like an obvious idea apriori, but all reviewers agree the authors back this up with extensive experiments and ablations of their own method in atari and dmc. the authors were very responsive with reviewers, resulting in 3 reviewers increasing their scores each by one. all reviewers agree this paper is easy to follow, and most of their stated weaknessesconfusions have now been addressed or clarified. my view of this work is that the main contribution is a latent reconstruction loss, as an additional objective to whatever the rl task objective is. and this is useful for using data augmentation  selfsupervised learning in rl tasks where representations that can do pixelreconstructions arent whats really required and they show this experimentally e.g. distractions exist in images, so they focus on the lossy latents instead. this seems more novel and distinct to simply pretraining some representation using contrastive  masked methods common in the literature. and then in their extensive dmcatari experiments they augment by removing pixel spatialtime cubes in video sequences given correlations nearby etc and force the latent structure to capture whatever still needs capturing, perhaps a qfunction input for sac, trained jointly. so i agree with the other reviewers that this work seems interesting and novel where the neurips community would benefit from reading  understanding it", "accepted": null}
{"paper_id": "nips_2022_NmUWaaFEDdn", "review_text": "while the reviewers showed some disagreement, the majority of them considered the paper novel and interesting. moreover, during the discussion period the authors improved the clarify of the presentation and of the proved results. overall, the paper makes a good contribution to the conference.", "accepted": 0}
{"paper_id": "nips_2022_3e3IQMLDSLP", "review_text": "strengths the paper introduces a new and interesting idea of double checking with bidirectional models, and thoroughly evaluates the idea on a variety of offline rl datasets and through multiple ablations. weaknesses the main weaknesses seem to be that 1 some of the performance improvements are small, and 2 like prior methods, the method is heavily reliant on tuning a hyperparameter based on the quality of the dataset. it also is somewhat strange that the paper uses v0 datasets from d4rl, since the more recent versions have fixed bugs in the datasets. otherwise, the author response did a good job at discussing and addressing the other reviewer concerns. the reviewers and ac agree that the strengths outweigh the weaknesses, and would make a valuable addition to neurips.", "accepted": null}
{"paper_id": "nips_2022_lCGYC7pXWNQ", "review_text": "the reviewers appreciated that the proposed idea is interesting and is well supported by sufficient empirical evidence. there were some concerns in the initial review and the rebuttal successfully addressed most of them. as a result, two reviewers upgraded their ratings. overall, this paper tackles an important problem of incremental learning and the proposed approach is efficient memorywise and effective performancewise. we are happy to recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_2nJdh_C-UWe", "review_text": "the paper proposes a mechanism for humanai collaborative play in the game honor of kings. the highlevel approach is sensible communicating happens within a small space of metacommands, which can be converted to coherent chunks of agent behaviour. the complexity of the system is high, the experiments are done at large scale, and the empirical findings are impressive in singlehuman collaboration at least. however, the reviewers also point out a large array of concerns to be resolved; of particular concern, and insufficiently addressed, are the ethical issues informed consent raised by multiple reviewers. overall, the reviewers find that the paper is not ready for publication yet, and i concur. i hope the authors will integrate the rich feedback from this reviewing process in a future extensively rewritten iteration.", "accepted": 0}
{"paper_id": "nips_2022_SOqGrmufeRg", "review_text": "spiking neural networks snns have some advantages, especially in terms of power consumption, over standard artificial neural networks anns. however, most trained networks are ann and therefore this work presents a conversion scheme from snns to anns with some desired properties. unfortunately, the reviewers found the contribution of this work insufficient in terms of novelty. the authors argued, in the rebuttal, that there are differences between prior art and the current paper. the reviewers acknowledged these differences but were not convinced that the differences are significant enough. the authors also noted that this work was sent to publication in jan 2021 and was rejected while some of the relevant papers were published after this date. we understand the frustration that this situation is likely to generate. however, when reviewing this work the relevant date is the deadline for neurips submission deadline and therefore these studies should be compared to in the paper.", "accepted": 0}
{"paper_id": "nips_2022_8SilFGuXgmk", "review_text": "this is a borderline paper, and we ended up soliciting two additional reviewers 4f3m and eovh for additional feedback after the main reviewauthor discussion period. the general sentiment shared by the reviewers including the two additional ones is that the paper studies an interestingimportant problem and provides some interesting discussion and results  albeit largely based on natural extensions of existing methods e.g. in the centralized setting. some of these results could also be improved both in terms of better discussion on novelty and just stronger guarantees, but the reviewers generally appreciate that the authors have started discussion and work in this area. multiple reviewers think that the hypothesis relating noniidness with fat tailed distributions is interesting, but are underwhelmed by the supporting analysis and commentary. it would be great if the authors could improve this discussion, either with more concrete analysis or with better empirical evidence. the biggest criticism of this paper that is shared by most if not all of the reviewers is that the current experimental section is severely lacking. the authors mentioned that they would add in more experiments in the final draft of the paper e.g. httpsopenreview.netforum?id8silfguxgmknoteidshubb7e3b. this response seemed satisfactory to the reviewers, so this paper will be accepted based on the premise that the authors will follow up on that promise. in particular, it is expected that the authors will go over the reviews and try to carefully address all comments about the experiments e.g. replicating experiments on at least a few other datasets and models  ideally at larger scale, using experiments to better verify the noniid hypothesis, designing experiments that verify the theoretical guarantees.", "accepted": 0}
{"paper_id": "nips_2022__WHs1ruFKTD", "review_text": "this paper studies the empirical robustness of the general deep equilibrium model deq in the traditional whitebox attackdefense setting. as the topic is underexplored in the literature, the authors first pointed out the challenges of training robust deqs. then, they developed a method to estimate the intermediate gradients of deqs and integrate them into the adversarial attack pipelines. the authors did a good job to address the reviewers concerns in the authorreviewer discussion phase, and at the end, all reviewers unanimously support the acceptance. although ac sees some limitations, e.g., limited advantages of using robust deqs over deep cnns, scalability to largescale datasets and training instability, ac thinks the merits of this paper outweigh them this paper can be a useful guideline when researchers pursue the underexplored problem in the future. hence, ac recommends acceptance.", "accepted": null}
{"paper_id": "nips_2022_q-tTkgjuiv5", "review_text": "reviewers agreed that the model is new and interesting and the theoretical results are solid. the main criticisms are about the model some reviewers felt that it is too specific and there is not enough motivation. some reviewers liked the technical depth while others felt that it is not enough to compensate for the lack of motivation. overall, reviewers felt that the paper in its current form is not ready for publication at neurips.", "accepted": 1}
{"paper_id": "nips_2022_9YQPaqVZKP", "review_text": "this paper measures intraclass neuron response variance, and shows that network performance is better when it is lower. they then use this term as a regularization target, and show that it leads to improved model performance. reviews were high quality. scores were between weak accept and accept, with one reviewer raising their score from weak accept to accept. the most significant concerns were experimental around ablations, around the diversity and scale of models the technique was tested on, and around the tuning of baselines. however, the experiments seemed fairly strong as is, and of course there are always more experimental conditions that can be requested. based upon the reviewer consensus, i also recommend acceptance for this paper.", "accepted": null}
{"paper_id": "nips_2022_AOSIbSmQJr", "review_text": "please add proof outlines to the main body in the final version. also add a discussion on assumption 1 and more insights for the proposed method.", "accepted": 1}
{"paper_id": "nips_2022_qHs3qeaQjgl", "review_text": "this submission studies a somewhat nonstandard version of tolerant closeness testing of distributions over the ndimensional hypercube. instead of only iid samples, it is assumed that the tester is able to efficiently evaluate the probability mass at any point in the domain and to sample from the distribution conditioned on any subset of size two of the domain. the main result is an algorithm with query complexity scaling nearlinearly in the dimension. using only iid samples, one would need exponential dependence on dimension. the algorithm is evaluated on synthetic and realworld datasets. it is experimentally shown that their algorithm outperforms a previous baseline, which in the worst case has complexity scaling exponentially in the dimension. overall, this is an interesting work that appears to meet the bar for acceptance.", "accepted": null}
{"paper_id": "nips_2022_mowt1WNhTC7", "review_text": "all reviewers are positive about this paper, leaning toward accept. the ac does not find sufficient grounds to overrule the consensus.", "accepted": null}
{"paper_id": "nips_2022_lzZstLVGVGW", "review_text": "this work proposes a transformerbased architecture using 3d blocks for spatiotemporal prediction, designed specifically for earth system forecasting applications. the authors show that this method considerably outperforms other approaches both on two climateweather forecasting tasks and on unrelated synthetic tasks. the reviewers agree that this is a strong submission, and it addresses an important area of problems too often neglected by our community. while one reviewer held a conflicting opinion to the other three, this review was completed 20 days after the deadline, and no response was made by this reviewer to the author rebuttal despite my requests to the reviewer. i believe that their concerns have been adequately addressed by the authors. i accordingly recommend acceptance of this paper.", "accepted": 0}
{"paper_id": "nips_2022_Tsy9WCO_fK1", "review_text": "this paper proposes a theory on the lipschitz continuity of the pushforward mapping, which transforms a gaussian distribution to a multimodal distribution. this is a borderline paper. the reviewers are generally positive about the paper and leaning towards acceptance, though there is still some notable gap between the proposed theory and empirical results. the authors are expected to further revise their paper based on the reviewers suggestions.", "accepted": null}
{"paper_id": "nips_2022_rTTh1RIn6E", "review_text": "this paper proposes an outofdistribution ood detection method, where the features of indistribution id samples and those of the ood samples are decorrelated in a classwise manner by using hsic. a theoretical guarantee for decorrelation is provided. the propposed method, conditionali, is evaluated on ood detection benchmarks in image classification and nlp, and shows sota results. reviewers had many major concerns, e.g., novelty from hood, missing baselines with ood exposure, hyperparameter tuning, and insufficient theory to support the proposed method, which the authors addressed well, and most of the reviewers have been convinced. the paper is wellwritten and the mathematical notation seems fine for machine learners. however, the relation between theorem 1 and assumption 1 is weird. any theorem must always hold otherwise its not a theorem, and the sentence theorem 1 holds if assumption 1 holds doesnt make sense. if the claim in theorem 1 requires assumption 1, assumption 1 should appear first, and theorem 1 should state that under assumption1, it holds that ....", "accepted": null}
{"paper_id": "nips_2022_Av8b0vxN7MX", "review_text": "three reviewers gave quite positive ratings and comments on the paper. one another reviewer gave lower ratings. the ac thinks the reviewer with a low rating raised quite reasonable questions but found overall, the pros outweigh the cons which seems to mostly stem from the unclarity in writing. the ac would recommend acceptance but also encourages the reviewers to incorporate the reviews and discussions in the final version.", "accepted": 0}
{"paper_id": "nips_2022_skgJy0CjAO", "review_text": "this paper presents a pretraining approach for integrating logical reasoning into language models. the authors develop an adversarial training method where they minimize the difference between the verifier scores and generator scores for logically inconsistent statements, thereby making the model less likely to generate sequences the verifier identifies as logically inconsistent. all the reviewers were positive to varying degrees on the contributions of this paper, specifically highlighting the clarity of the method, as well as the empirical rigor and results. weaknesses pointed out by reviewers included the underspecification of different type of logical relations and the weakness of the pseudostatement sampling procedure, which doesnt seem to explicitly retrieve logically inconsistent statements as negative examples. despite these questions, the method is well motivated and clear, and the empirical results speak for themselves. this paper represents a solid piece of work and im inclined to see it accepted.", "accepted": null}
{"paper_id": "nips_2022_FRDiimH26Tr", "review_text": "mixtureofexpert moe models have demonstrated a lot of success recently. to further improve upon the existing literature this paper studies moe routing for different network topologies. this is essentially to deal with the communication overhead of moe training. the strategy is to add another layer on top for the topology along with a corresponding objective to optimize. the authors also provide experiments demonstrating improved speed of convergence. the reviewers were in general positive and liked the idea of the paper. the reviewers did however raise issues about lack of clear demonstration that accuracy is not compromised, lack of large data, and a few other more technical concerns. the reviewers concerns seem to be more or less addressed by the authors. my overall assessment of the paper is positive. i think the general premise of the paper is interesting and the paper has interesting ideas. i do agree however that the experiments need to be more thorough. i am recommending acceptance but request that the authors follow the reviewers comments to improve their experimental results", "accepted": null}
{"paper_id": "nips_2022_yoLGaLPEPo_", "review_text": "this paper attempts to improve learning in hyperbolic space under limited data few shot setting. in this regards, the authors propose a hyperbolic feature augmentation method to circumvent overfitting. furthermore, as optimizing using a large number of sampled data can be expensive, the paper proposes an upper bound the classification loss and optimize this tractable upper bound in the tangent space, which is euclidean making the approach much more practical. there was a wide variance among reviewer scores. we thank the authors and reviewers for actively engaging in discussion and taking steps towards improving the paper including for providing additional experiments. finally it would be appropriate to tone down the claim that this is the first paper to perform feature augmentation in hyperbolic space as it might be unsubstantiated, cf weber et al robust largemargin learning in hyperbolic space neurips 2020 which also augments by solving a certification problem.", "accepted": 1}
{"paper_id": "nips_2022_Setj8nJ-YB8", "review_text": "this paper designs new algorithms for finding second order stationary points using only function value queries 0th order information. the main novelty is in designing two approaches for negative curvature finding. the new subroutines can be used in a wide range of algorithms for finding second order stationary points most using first order information and result in new 0th order algorithms with reasonable guarantees. the reviewers had some concerns but most are addressed in the response. in general the reviewers agree that this is a solid contribution to nonconvex optimization.", "accepted": null}
{"paper_id": "nips_2022_L0OKHqYe_FU", "review_text": "this paper proposes an unsupervised learning approach to detect and cluster neural spike sequences in neural spike data. to model the data, the authors propose a hierarchical dirichlet point process model, which employs hawkes processes to model the temporal dynamics of neural activity within sequences, whereas spike rates within a sequence are modelled via a nonhomogeneous poisson process. the authors derive a particle filter based online inference method to detect sequences and infer their types. they use conjugate priors to derive closedform updates for obtaining posterior distributions of model parameters. the authors evaluate the performance of their method on both synthetic and real datasets. strengths  the authors propose a wellgrounded probabilistic generative model for modelling neural activity sequences in spike train data  the model is able to infer the number and types of sequences from the data  the authors have devised a tailored particle filter to infer unobserved latent variables for identifying neural activity sequences and inferring their types, which in turn allows for closedform updates of posterior distributions over model parameters  the results on artificial data indicate that the proposed methodology can detect and correctly identify the types of underlying neural sequence activity in the presence of background noise.  the results on real data are qualitatively interesting and comparable to the results of another approach.  the proposed methodology in principle is scalable to larger datasets with up to 210k spikes. weaknesses  no analysis is done to show how often the inference procedure leads to suboptimal solutions, which requires reruns of measures like merging and pruning as highlighted by the authors.  experiments on synthetic data involve very low number of sequence types up to 3, so it is hard to see how the inference method would perform for a larger number of sequence types  comparison with the alternative approach was only done on one data set.  have you considered or run experiments where you analysed the convergence behaviour of your methodology by assessing how often you manage to recover groundtruth parameters if you optimise on data generated by the process described in section 4, lines 145152?  why have you not compared ppseq across the board? how does ppseqs likelihood compare with the likelihood of your approach?  can you elaborate what do you compute roc curves on in section 6.1? why to you use just one type of sequence for the experiment?  how does your method perform if there is a greater spatial andor temporal overlap between sequences than what we can see in synthetic datasets? i have highlighted technical limitations and weaknesses above. i have nothing further to add here. the authors propose a hierarchical, nonparametric bayesian model to identify neural sequences from neural spike data and then to categorize the neural sequences, all in an online manner. specifically, their model consists of a dirichlet nonhomogeneous poisson process dpp prior for observed spikes and a dirichlet hawkes process dhp prior for the neural sequences generating those observed spikes. they refer to their model as the hierarchical dirichlet point model hdpp. they present a particle filtering method to perform inference under their model as well as a scheme for merging and pruning neural sequence categoriestypes that may have been incorrectly generated early during inference. they compare their method to two other topdown unsupervised methods, convnmf and ppseq, on synthetic data. they demonstrate that their method is comparable in performance to ppseq under increasing background noise rate, while the performance of convnmf steadily drops. they then demonstrate their method on two experimentallycollected neural spike datasets, and present results indicating that their method infers the correct number and types of sequences, converge to similar parameter ranges in independent runs, and a constant expected tune cost with increasing number of observations. strengths  neural recordings for neuroscience are increasingly growing in size and length and under fewer experimental constraints. this direction of neuroscience research begets the need for methods that identify neural sequences in a streaming and nonparametric way. the authors method would provide an important set of contributions to the field.  the authors exposition of their model and inference method is clear. weaknesses  the authors should improve the clarity of the conclusions that they draw from their results.  the authors validate their method against two existing methods, in particularly using ppseq to validate the number and type of sequences detected. i recommend that the authors expand the depth and rigor of their evaluations and their analysis of their method. for example, the authors do not demonstrate their method on a large dataset or provide evaluation of time and memory costs of their method vs. another method such as ppseq. i would also like to see their method perform on a dataset with a larger number of neural sequences, where there may greater uncertainty in sequence types.  how does the time complexity of this method compare to ppseq on a fixedsized dataset such as the songbird or rat datasets used? in other words, what is the baseline time complexity cost imposed by the additions in our method e.g. the nonparametric formulation?  what is the dataset size at which using hdpp faster than ppseq?  figure 5 shows the time cost of the hdpp method, which the authors imply should remain constant as a function of number of detected sequences. what was the number of detected sequences, as a function of the xaxis?  in figure 5, why is there an early higher time cost 50100s before settling in? the authors do not mention any limitations of their method nor do they mention potential negative societal impact of their work. please discuss in greater depth, for example, the tradeoffs that your methods nonparametric formulation and resulting inference algorithm may result in terms of complexity, uncertainty, or power. when, for example, would using ppseq be more desirable? this paper proposed a neural sequence detection method, which is based on a so called hierarchical dirichlet point process. the model can jointly infer an unbounded number of sequences and the event types from data. the paper develops an online algorithm to process the neural spike data, with the particle filtering framework. the proposed method was evaluated on two realworld datasets. strengths 1. the application is very interesting and important 2. the experimental data is interesting. from the test loglikelihood, the proposed algorithm seems working. weakness in summary, there is a big issue in the presentation and the contributions seem weak. 1. the problem formulation is not clear. a good paper should give a clear definition and motivation of the problem you want to solve, but i did not see it in this paper, making me very confused. why do you want to partition the neural spikes into sequences? what is the meaning of doing so? arent the observed events already a sequence by time? for a new sequence inferred by the model, based on what the events are ordered? the notations confuse me further  according to line 118, on the spiketrain data, youve already had k_s to indicate the neural sequence, why do you want to infer them again with the model see eq2? 2. the relationship with the classical hdp is not mentioned. the name is confusing in that i thought you are building a model similar to the hdp of teh et. al. 2005. however, it is not. the classical hdp uses the firstlevel dp to sample an infinite set of bases, and each dp in the second level will share these bases, but generate the data with different mixture weights. it took me a while to find out that this paper is not following the hdp framework. rather, it is more like a hybrid of two dps, one is to partition the data into sequences, and the other to partition the events into types. i do not see a clear hierarchical structure here. the paper should highlight the difference with the classical hdp, to avoid unnecessary misunderstanding. 2. the contributions seem weak. the proposed model is an extension of dirichlet hawkes process by du et. al., 2015. one more dp is added, and it is quite incremental. the inference, based on the particle filtering, is almost the same as the smc used by du et. al., 2015, plus some heuristics to merge and prune. if there are some more significant contributions, the authors should highlight them. otherwise, i view this is a very incremental extension of du et. al., 2015, applied to spiketrain data analysis. 3. the quantitative metric is only based on test loglikelihood. i have a lot of experience of doing point process modeling. although many works use test loglikelihood for evaluation, it is not a reliable metric, and is quite misleading. i look forward to seeing some evaluation that can truly reflect the performance, such as future event time prediction. see above na one hypothesis about neural spike data is that it can be considered as a sequence of neural syllables consisting of the spiking activity of a subset of neurons over a short period of time. understanding these sequences would then enable relating activity recordings to behaviorally or experimentally relevant observables. the present manuscript proposes a method for unsupervised identification of such sequences. recent ml literature, as cited in the manuscript, studied similar problems for point processes in different contexts. in that sense, this manuscript primarily represents an application of existing methodology on hierarchical inference in temporally structured data. briefly, the manuscript proposes a particle filter based samplingbased solution to inferring sequence labels in a hierarchical model where the observables are samples from point processes. the problem is both important and relevant to this community. the manuscript uses a dirichlet process prior, which enables the method to decide on the number of sequence labels without relying on prior information. indeed, such information is not easy to know in practice. briefly, the sequence types labels are modeled as a dirichlet process so that the same type can appear multiple types in a given recording. neuronal spiking activity is then modeled as a hawkes process conditioned on the sequence type. overall, the model is complicated, but it is fitting to the underlying complicated problem. update on aug. 8 in response to the author rebuttal, i decided to increase my overall score from 5 to 6. strengths  unsupervised, adaptive streaming inference of the number of sequence types  computational scalability weaknesses  poor presentation the manuscript is full of typos and grammatical mistakes. while this obscures the underlying meaning very rarely, reading the manuscript becomes timeconsuming and frustrating. similarly, reasoning should be better and more precisely explained. for instance, the argument in lines 266267 on using temporal windows to reduce computational load appears applicable to all methods. please better explain why it would be more beneficial for the proposed method compared to existing methods.  other than unsupervised inference of the number of types, one potential advantage of the proposed method could be realtime detection of sequences in a closedloop experiment if inference can be made faster. according to fig. 5, it doesnt appear fast enough with the hardware used in that experiment. however, this point was never discussed. is this a desirable capability for neuroscience? what kind of desirable experiments could be performed by such a closedloop setup? in my opinion, this point determines whether the proposed contribution is timely and impactful or a technical upgrade without practical consequences. i listed a few questions under weaknesses above. id like to add a few more questions here  could fig. 4 be turned into a comparative study as well? how much time would previous methods require to process that dataset? id be curious to see the results of other methods after temporal windowing as i mentioned under weaknesses.  could you please sort the neurons in fig. 3 identically across the two compared methods to facilitate comparison?  could you please discuss how the different methods would perform when the firing rates are lower?  overall, the demonstrations use only a few sequence types. this may be in line with previous literature. however, one might expect to see a much larger set of sequence types in larger, longer recordings. could you please discuss how the different methods would perform when the number of sequence types is larger? the authors should discuss technical limitations of the work. that is, under which conditions can we expect it to fail? what are the roles of different noise sources, firing rates, etc? this paper describes a hierarchical bayesian latent model to identify neural sequences from spike data. especially in neuroscience, detection of patterns in neural sequences is an important computational problem as the infrared patterns are useful for characterizing brain activity. the key problem is reminiscent of clustering where individual spikes are associated with sequences. the proposed model  hierarchical dirichlet point model hdpp  consists of a dirichlet nonhomogeneous poisson process dpp prior for observed spikes and a dirichlet hawkes process dhp prior for the neural sequences generating those observed spikes. inference is done with sequential monte carlo, including a proposal mechanism for merging and pruning neural sequence categoriestypes that may have been incorrectly generated early during inference. a comparison of the method to two other topdown unsupervised methods convnmf and ppseq on synthetic data is provided. while the description of the hierarchical model seems to be complete, the reviewers asked for clarifications about the motivations. during the rebuttal, the authors were also able to answer various issues about experimental section and regarding the inference procedure, they were able to include results of further experiments. as a results, reviewers decided to raise their grades for the paper. in light of the importance of the problem and the soundness of the methodology, i am inclined to suggest acceptance for this work.", "accepted": 0}
{"paper_id": "nips_2022_Qy1D9JyMBg0", "review_text": "the authors use a mixtureofexperts model in a multimodal setting. the reviewers consider the work technically strong and interesting; the ac concurs.", "accepted": 1}
{"paper_id": "nips_2022_sQiEJLPt1Qh", "review_text": "three reviewers agree that this work meets the bar for acceptance, rating it weak accept, weak accept, and accept. the work provides bounds for approximating continuous piecewise linear functions by relu networks and an algorithm. reviewers praised the novelty and significance, and were positive about clarifications offered during the discussion period, particularly about the time complexity of the algorithm. hence i am recommending accept. i encourage the authors to still work on the items of the discussion and the promised additions such as the open source implementation of their algorithm for the final version of the manuscript.", "accepted": null}
{"paper_id": "nips_2022_Yay6tHq1Nw", "review_text": "this work proposes to learn better representations for language descriptionbased tasks like navigation, via first pretraining a dynamics model on sequences of observations without action labels and using this model to aid rlbased policy learning. good empirical results are presented and the work has been wellreceived. the dynamics policy learnt helps policy learning especially with longer horizons. the authors are encouraged to incorporate the reviewer feedback into account and especially the vae experiments and add discussion.", "accepted": 0}
{"paper_id": "nips_2022_GzESlaXaN04", "review_text": "this work provides lower bounds in the noise free setting for learning two hidden layer networks in the gaussian space. overall it is a fundamental result well within the scope of neurips, continuing a solid line of work and i cannot see any reason for rejection. the authors have engaged with the reviewers, and have committed to make minor revisions and clarifications which i am sure they will do.", "accepted": 1}
{"paper_id": "nips_2022_Q9lm8w6JpXi", "review_text": "in this paper, the authors propose an algorithm bilco for solving graphical time warping, an alignment method for multiple time series data. overall, the proposed approach is interesting, and all reviewers are positive. thus, i also vote for acceptance.", "accepted": null}
{"paper_id": "nips_2022_Q38D6xxrKHe", "review_text": "the paper is quite interesting and rigorous, with intriguing conclusions. the rebuttal also addressed all the major concerns  mostly technical clarity. i congratulate the authors for the nice work and recommend an acceptance for the paper.", "accepted": 1}
{"paper_id": "nips_2022_gQBetxnU4Lk", "review_text": "robot motion planing in dynamic environments remains a significant problem. all reviewers consistently agree that the suggest gnn approach in this paper has useful merits, is of general interest, and that the paper is above the publication threshold. detailed comments of the reviewers provide a good source for some finetuning improvements of the paper.", "accepted": null}
{"paper_id": "nips_2022_AXDNM76T1nc", "review_text": "the authors have introduced video pretraining vpt, a semisupervised learning approach that allows relatively small volumes of labeled data to train an inversedynamics model that is subsequently applied to predict the action labels associated with a far larger, unlabeled dataset. they then train an agent in a supervised regime with respect to these labels to achieve strong performance in minecraft, which requires reasoning over very long time horizons. overall there is clear consensus among the reviewers that this paper is novel, technically sound and of broad interest to the neurips community. the authors have also proactively engaged with reviewer feedback to improve manuscript clarity. i am confident in recommending this paper for acceptance.", "accepted": 1}
{"paper_id": "nips_2022_KnCS9390Va", "review_text": "this paper presents an interesting and novel try at using visionlanguage multimodal models for ood tasks. the experiments are sufficiently validated on diverse ood datasets. besides, the theoretical explanations of softmax scaling are quite insightful. all reviewers give positive scores. during the discussion phase, the authors have accordingly added more ablation studies and comparisons. despite some reviewers raising concerns about the possible limited novelty of using softmax scaling for the cliplike model. the authors give sufficient discussions and provide theoretical justifications, which will inspire the community. the metareviewers thus recommend accepting it.", "accepted": null}
{"paper_id": "nips_2022_hX5Ia-ION8Y", "review_text": "the paper proposes the use of diffusion model for masked video modeling and shows promising results in video generation and completion. all of the reviewers agree that the paper is a good fit for publication at neurips. i appreciate that the authors engaged with the reviewers and improved the paper!", "accepted": null}
{"paper_id": "nips_2022_-vXEN5rIABY", "review_text": "it is very hard to make a final decision on this paper; the scores are 4,6,8, and 5. the research problem raised in this paper is interesting and worth further study. however, reviewers have raised some concerns about the experimental resuls. in the original paper, they did not compare with any external baselines. in the discussion period, the authors presented additional results with betae, which were originally designed for transductive reasoning. not surprisingly, betae delivers poor results. how strong is this comparison? one would expect some experiments with better initialization for unseen entities, e.g. with kb embeddings e.g. transe and conve used in kbc tasks.", "accepted": 1}
{"paper_id": "nips_2022_G2kkDEujOw", "review_text": "this manuscript enjoyed universal recommendation of acceptance from the reviewers after the initial review phase. the reviewers did note several minor issues in these initial reviews, many of which were resolved by insightful responses from the authors. i encourage the authors to edit the manuscript to reflect the insights gained from this interaction when preparing an updated version.", "accepted": null}
{"paper_id": "nips_2022_Ms6QZafNv01", "review_text": "the main criticism raised by the reviewers was the unconvincing experiments. the reviewers generally liked the simplicity of the method presented the paper, but were unconvinced by the impactutility of the results. even though the paper focuses on the convex regime, the paper may benefit from some dl experiments. this is not an uncommon paradigm in research e.g. adam and other convex optimization methods with dl experiments, etc. there were some other comments that are worth addressing in a future version of the paper. e.g. adding the discussion minibatching, improved exposition.", "accepted": 0}
{"paper_id": "nips_2022_U14PKEu18bK", "review_text": "reviewers are generally positive about the submission, and all recommend acceptance post rebuttal. they appreciate the new formulation and the strong results. the ac agrees and recommends acceptance.", "accepted": null}
{"paper_id": "nips_2022_sNcn-E3uPHA", "review_text": "this paper has 2 accepts 7 and 2 borderline accepts 5. the average is 6. the modification of reviewer sqxh does not show in the system, but he stated as follows in our discussion i tend to modify the score of this paper to five. this paper shows an algorithm that delivers outstanding text classification performance despite its extreme simplicity and speed. the quantum explanation for the algorithm is weak. the experiments were limited to somewhat simple text classification problems, but the authors added some convincing results on sentiment analysis in their rebuttal. the paper is clearly written and the provided code is well documented, so it should be reproducible though none of the reviewers did rerun the experiments. the authors provided a revised version that clarified some aspects though it did not include the additional experiments provided in the rebuttal. the consensus after discussion is accept, given that this paper may open a new class of simple and high performance classification algorithms. the explanation for their effectiveness remains an open problem, and this paper could be improved by opening the discussion. we recommend the following modifications in order to mitigate overreaching claims that could make the authors look na\u00efve  quantum interpretation of a sentence as superposition of words. this seems to imply that detecting a single word is sufficient to classify a sentence, and that the best solution is a combination of these weak classifiers based on a single word or ngram. the success of adaboost on text classification testifies of the power of these methods. however such methods do not work so well when sentences are longer, or when there is a stronger compositionality, for instance sentiment analysis imdb, sst. the results on emoint provided in the refutation are a step in the right direction. they show the method can handle some level of context, thought sentences are still short. how could this method handle long sentences? the paper would benefit from a discussion about what the superposition of words hypothesis implies, what are it limitations, and maybe how to overcome them.  table 1 provides preliminary results but some of the claims can be turn off for readers with experience in linear classifiers svm and lr applied to text recognition, as the authors apparently failed to select the correct algorithm sgd classifier. comparisons with svms and lr in table1 suggest bc is 1 million times faster than svms. however, the same difference in speed can be observed within different sklearn svm implementations, depending on the algorithm. the sgd classifier, which supports both svm and lr, has a computational complexity which is even better than the onjk reported in the paper, as it depends on the number of nonzero features rather than the number of features j. furthermore, the accuracy provided for the svm 79.4 also seems far below the sota. for instance, the reference 17 reports an accuracy of 82.27 on 20ng using tfidf svms the most vanilla setting.  as shown in eq.6, the model is fundamentally linear. explainability by taking the feature with the highest weight is as ancient as ml itself. how this method performs better than traditional linear approaches should be better illustrated. the authors just say the words appear semantically correlated to their respective class and do not contain neither noisy words, such as stopwords or punctuation, or words whose meaning is too general to be representative of a specific class.   novelty of this paper in the field of quantuminspired classification. even in the revised version, while they quote work on language modeling, the authors do not seem to have quoted any work on classification, even though it was pointed by the first reviewer httpsojs.aaai.orgindex.phpaaaiarticleview17567. as pointed in their rebuttal, this is very different from their work, but they should mention that work.", "accepted": 0}
{"paper_id": "nips_2022_5yAmUvdXAve", "review_text": "the paper received 4 positive reviewers, and the reviewer increasedremained their scores after the rebuttal. the paper pursues a useful direction of unconstrained face recognition. all reviewers agree that the results are impressive and the method has novelty.", "accepted": null}
{"paper_id": "nips_2022_6yuil2_tn9a", "review_text": "this paper proposes a backdoor injection method that directly manipulates model weights after training. the backdoored method can achieve comparable clean accuracy and a high attack success rate through injecting and compromising handcrafted filters, increasing the separation in activations, and increasing the logit of a target class. the reviewers agree that the proposed backdoor injection method is novel and interesting. the authors are suggested to conduct more experiments on evaluating whether their handcrafted models cannot be detectedremoved by existing defenses.", "accepted": 1}
{"paper_id": "nips_2022_1xqE9fRZch5", "review_text": "the paper proposes a selfsupervised deeplearning framework for imagetoimage translation tasks, such as segmentation, that accommodates and fully exploits longitudinal data. specifically the method provides a mechanism to impose consistency in output across multiple points from the same individual and simple regularisation terms to avoid some problems common with other methods, such as mode collapse. the authors compare the method against baselines in two distinct neuroimaging segmentation tasks, which nicely demonstrate the additional power afforded by imposing longitudinal consistency. the reviews overall reported that the submission tackles an important problem, presents well formulated experiments, and shows a significant advance over the sota. the reviews raised concerns raised included nonspecialist accessibility, adding more related work, and questions w.r.t. the claims, presentation, and relevance of the results, and particularly about the mode collapse problem. the authors have addressed these concerns, in particular by adding a new section section eneed for regularization to the supplementary material which contains several new visualizations and quantifications of the mode collapse problem from ablation experiments. the paper has been updated to reflect some clarifications required by reviewer de4n. some citations suggested by reviewer r9zh are now discussed in the submission. as it is, the paper meets all conditions for acceptance at neurips 2022.", "accepted": null}
{"paper_id": "nips_2022_a3ymtHbL5p5", "review_text": "this paper reveals that the noise added to the pate voting mechanism to attain a differential privacy guarantee enables new forms of leakage of sensitive information. a simple adversary is enough to exploit this noise to extract highfidelity histograms of the votes. as pointed out in this paper, a low consensus vote indeed indicates minoritygroup membership. the main technical contribution is the model to extract pate histograms. strengths this paper is well written. the proposed attack model is simple but interesting. i believe the authors discovery is significant. weaknesses the technical part seems too simple. im not sure such a simple attack model contains enough technical contribution to appear in neurips. this paper might be more appropriate to be published as a report paper at a security conference. weaknesses the technical part seems too simple. im not sure such a simple attack model contains enough technical contribution to appear in neurips. this paper might be more appropriate to be published as a report paper at a security conference. the paper first shows that the prediction histogram from several teachers will leak the membership of minority group for this test data. then in the paper, an attack is proposed to show the possible leakage of histogram under a modern dp approach, pate. finally, it concludes dp doesnt protect the leakage of minority group membership.  strengths 1. this paper takes effort to show what differential privacy cancannot protect. this big topic is meaningful to the differential privacy community and the one who cares about privacy in applications. 2. the paper is written well.  weaknesses 1. the main conclusion of this paper is that the differential privacy doesnt protect the membership of minority group for test data. this conclusion seems not very surprising to me as differential privacy is to protect the privacy of training data. 2. the experiment result also shows that the leakage of true histogram is more severe when the privacy cost is smaller. this result is counterintuitively and the explanation is not very convincing to me. the estimation for q_i from a sequence of i.i.d. samples 01 from bernoulli random variable likely wouldnt be more accurate when q_i is larger, but depends on its variance q_i1q_i. i would suggest authors to further check the difference between qh and barq and see if this difference is smaller when the injected noise is larger. 3. the experiment doesnt include the endtoend evaluation between privacy cost to minority group membership leakage. after rebuttal thank the authors for their reply, which solves w2 and w3 and alleviates my concern in w1. 1. i am not clear why the introduced minority group membership leakage problem is a privacy problem for training data. the motivation is a bit confused. 2. it is better to check the difference between qh and barq to support the explanation for better attack performance with higher noise levels. the authors list some limitations of their proposed attack algorithm. however, i am concerned whether the conclusion is meaningful to the community. pate is a dp algorithm designed for training data and it likely will not protect the test datas information, e.g. the minority group membership discussed in this paper. this paper studies the confidentiality of pate from an adversarial perspective. this paper shows that the attackers can recover the histograms of votes by querying the voting results multitimes. this paper shows an interesting result that adding larger gaussian noise actually increases the risk to the confidentiality of the vote histogram. pros this paper proposed an attack method that can extract the voted histograms by querying the voting results multitimes. this paper uses an example to show that sensitive attributes can be leaked from the voted histograms. cons the experimental settings are not clear. more experiments are needed to support the corresponding claims. 1. does the model update frequently? why cannot the model send the same result to the same query? 2. why do you use the euclidean distance to measure the distance between qhath and barq? why do not use kl or wasserstein metric to measure the distance? 3. in line 247, whats the epsilon and delta for the dp guarantee? in line 250, whats the corresponding budget for sigma  40 etc? 4. in line 248, whats the dp budget for each query? and whats the total budget for 104 queries? 5. in line 282, 8, 105 is the dp budget for one query? 6 in figure 4, whats the corresponding dp budget for sigma40? 7. the results shown in figures 6 and 7 are counterintuitive. whats the corresponding dp budget for each query? to verify the correctness, its better to add a large enough noise i.e. the standard deviation of the gaussian noise is close to inf.. another baseline is that we add no noise to the results. can we reach the same conclusion? 8. is it possible to perform some experiments that the attacker can steal the private attributes from the recovered histogram with some dp guarantee? see the weaknesses and questions part. this work provides an indepth investigation of a particularly relevant failure mode of differential privacy dp when training ml models using the pate framework. in a nutshell, pate takes the prediction of multiple independently trained models and outputs a prediction that is dp by first adding noise to the histogram and then taking the prediction with most votes. this paper describes a privacy attack based in querying the database multiple times with the same question. by building up a distribution of these noisy responses, they show it is possible to back out the underlying histogram that the noise was meant to protect. strengths  the paper is quite enjoyable to read  it is wellwritten, easy to follow, logical, and concise.  it will likely spur muchneeded discussions about the practical usefulness of dp in real applications. weaknesses i dont see any significant weakness. minor suggestions  instead of using m for number of samples and m for number of classes, use different letters.  line 263176 i would prefer if in the appendix there was an explanation of precisely how the privacy cost is computed so that the reader doesnt need to refer to papernot et al. as much. tiny typo line 34 a attacker honestly, i read everything carefully including the appendix and i found it quite clear, thank you! the authors address the limitations of their work. not only i dont have any qualms with this work, but i believe that it is an important contribution that could have a major positive societal impact if the work is taken seriously by the privacy community. i think this paper should definitely be given a spotlight talk. although the fact that dp does not protect against population statistics is a widely known fact, the paper weaves this together with pate which relies on dp statistics to demonstrate the danger of mis interpreting the protection guarantees provided by dp. this is a point worth discussing among the privacy and security community.", "accepted": 0}
{"paper_id": "nips_2022_3vmKQUctNy", "review_text": "the reviewers were split about this paper on one hand they appreciated the motivation and the comprehensive experiments in the paper, on the other they were concerned about the clarity of the paper, even worried about a potential flaw. i have decided to vote to accept given the clear and convincing author response. i urge the authors to take all of the reviewers changes into account if not already done so. once done this paper will be a nice addition to the conference!", "accepted": 0}
{"paper_id": "nips_2022__N4k45mtnuq", "review_text": "all reviews for this paper were positive, albeit with a varying level of enthusiasm. reviewers found the problem, the results both theoretical and experimental and the techniques very interesting. the main concerns were whether the paper is a good fit for the conference given that dimensionality reduction is more of a machine learning tool rather a machine learning problem per se and lack of experiments on real data. but ultimately, the positives significantly outweighed the negatives.", "accepted": null}
{"paper_id": "nips_2022_eF_Mx-3Sm92", "review_text": "the paper studies changepoint detection and localization for functional data, which is an interesting and timely topic. i agree with some reviewers that the paper might be a better fit with the traditional statistical venue. the authors have done a great job in the rebuttal phase in addressing reviewers comments. i believe it is a worthwhile paper to be published in neurips.", "accepted": 1}
{"paper_id": "nips_2022_K4W92FUXSF9", "review_text": "this paper intrdouces the relation between normalizations and adversarial transferability, and proposes a method using random normalization aggregation for enhancing adversarial robustness. three reviewers agreed with the interesing idea, thorough expreiments, theroetical analysis, and the effectivess, so they gave acceptance score. however, one reviewer iobb raised a concern on the results of autoattack aa and lack of indepth discussion on the results. unfortunately, ac and the reviewers failed to make a consensus on decision during the discussion period, . ac carefully read the paper, the rebuttal, and the reviewers discussion. the main remaining issue raised by revieweriobb is it is not clear the reason why the results of aa are highter than pgd. the authors provided more extensive experimental results, focusing on comparing the results of aa and breakdown of four aa and pgd. also, they conjecture these result from handling adversarial transferability under randomnessbased method via normalization aggregation. ac also agrees with the authors and reviewerq4wp that the indepth analysis on the reason of aapgd results is oufofscope of this paper and theses results are consistent to recent works on adversarial transferability. so, this issue might be left as future work. because it seems that the contribution of this paper is enough for machine leanring community except the discussion on the aapgd reason, ac recommends accepting this paper.", "accepted": 0}
{"paper_id": "nips_2022_9cU2iW3bz0", "review_text": "the submission presents a novel and interesting method using recent advances in scorebased diffusion to improve the recently proposed differentiable ais log marginal likelihood estimates. the experiments clearly show the benefit of using monte carlo diffusion. the writing is clear and of high quality. for these reasons, all reviewers were unanimous in recommending acceptance. ac notes 1 out of curiosity, how is the adjusted langevin and hamiltonian versions perform on the static targets?, and 2 further to the question below on the timings, would it be useful to add a comparison where the time is kept similar between ulha and that with mcd, for example, 2k intermediate densities for ulha and k for the mcd variants.", "accepted": null}
{"paper_id": "nips_2022_M_WuaKoaEfQ", "review_text": "this paper proposes a novel method to perform monte carlo integration combining control variates and annealed importance sampling. all reviewers agreed the algorithm was of interest, the theoretical evidence was strong, and the experimental results were sufficiently convincing, so there was a consensus on acceptance. as a minor aside, i would encourage the authors to consider the font sizes in their figures.", "accepted": null}
{"paper_id": "nips_2022_OxHn1Yz_Kl3", "review_text": "the reviewers are all in agreeement that the paper constitutes a fundamental advance in the theory of causal inference. the authors responded to the reviewers remaining questions in a detailed way, and there is no further issue with the paper being accepted.", "accepted": 1}
{"paper_id": "nips_2022_ONB4RdP2GX", "review_text": "the reviewers opinions are quite consistent towards a weak accept. im not confident with the big title hardness in markov decision processes theory and practice. this paper is more like a survey  benchmark review instead of a research article. neither the theory part or the practice part is novel enough as a research article. its a bit thin as a survey paper. i personally tend to weak reject but i respect the reviewers weak accept.", "accepted": null}
{"paper_id": "nips_2022_U_hOegGGglw", "review_text": "it has been shown that linear classifier heads on top of pretrained models can outperform metalearning approaches. however, this is less adaptable than prototypical classifier heads and requires retraining with each new set of classes. this paper theoretically investigates the generalization of prototypical classifiers and uses this to explore several feature transformations to improve their performance. while there were concerns about the novelty over and above cao et al., and some minor clarity issues, the reviewers were all generally in agreement that this is a useful contribution to the community and a good starting point for improving prototype classifiers from a theoretical perspective.", "accepted": null}
{"paper_id": "nips_2022_Ya9lATuQ3gg", "review_text": "this paper uses nearest neighbor methods to retrieve and exploit information from similar games during planning, whilst playing the game of go although the method is extensible to other environments which support muzerostyle agents. the reviewers found this approach interesting and ultimately worth publishing, although there was a range of scores. however, the emerging consensus during the review phase seemed to lean towards acceptance, a recommendation i am happy to support from having read the discussion.", "accepted": 0}
{"paper_id": "nips_2022_iuW96ssPQX", "review_text": "this work proposes a new object detector architector that is based on a cnn stem, combined with a mostly transformerbased architecture, with the addition of a crossfusion module that allows for reconciling coarse and highgrained features for more precise object detection. thee paper is wellwritten, novel and presents a significant gain over the state of the art, using reasonable amount of compute. object detection is a central area of interest and this method shows how to leverage the power of transformers to push the envelope in this domain. therefore, i propose this paper be accepted at neurips 2022.", "accepted": null}
{"paper_id": "nips_2022_nxw9_ny7_H", "review_text": "the decision for this paper was a hard one. i pondered the scores with respect to the engagement of the different reviewers. i believe the initial scores were due to a misunderstanding of the limitations of the baseline model augerino, and how the proposed method solves some of the failures and limitations of augerino e.g. being able to model only affine transformation. i also find the authors expanded their experiments in a convincing manner during the rebuttal period. we encourage the authors to improve the clarity of their contributions in their final version, and to include all additional experiments that were ran during the rebuttal period.", "accepted": 0}
{"paper_id": "nips_2022_7vmyjUHgm9_", "review_text": "the paper proposes a method for finetuning multilingual pretrained language in multiple languages simultaneously. the task is formalized as a constrained optimization problem and the upper bound of the forgetting is given in theory. a method is developed for multilingual finetuning to minimize the upper bound. experiments are conducted in multiple downstream tasks and the model is finetuned in a few highresource languages and the performance is improved in low resource languages as zeroshot settings. the authors responded the reviewers concerns and the reviewers agree the responses addressed their concerns. the paper is recommended to be accepted, and i ask the authors to carefully prepare the final cameraready version based on the reviewers feedback.", "accepted": null}
{"paper_id": "nips_2022_tro0_OqIVde", "review_text": "this paper introduces a new operation gnconv and a computer vision network architecture hornet. motivated by the success philosophy of vision transformers, the key idea of gnconv is to build a recursive form of gated convolution. it make the module inputadaptive and with longrange and highorder spatial interactions. consistent improvement are shown over swin and convnext on wellestablished cv benchmarks such as image classification on imagenet, semantic segmentation on ade20k and object detection on coco. the paper receives receives unanimous accept from all reviewers reviewer 3snz champions the paper with rating score 8, leading to an accept decision.", "accepted": 1}
{"paper_id": "nips_2022_qtQ9thon9fV", "review_text": "this paper received 3 positive reviews 2xba  a. all reviewers acknowledged that this work introduces meaningful and nontrivial contributions, it is well presented, and the claims are supported by strong empirical performance. the remaining questions and concerns were addressed in the authors responses, which seemed convincing to the reviewers. the final recommendation is therefore to accept.", "accepted": null}
{"paper_id": "nips_2022_E9HNxrCFZPV", "review_text": "the paper proposed two testtime adaptation methods a instanceaware batch normalization and b predictionbalanced reservoir sampling and used these to show that the proposed method is better in the noniid setting. the reviewers found this to be an important problem the experiments generally convincing. reviewers objected the choice of dataset not commonly used to evaluate adaptation and baseline models not state of the art models and the effect size. in the end all reviewers found the results strong enough and voted to accept.", "accepted": null}
{"paper_id": "nips_2022_gE_vt-w4LhL", "review_text": "the paper conducts thorough analysis of the conformer architecture and brings insights and techniques from other fields to simplify and improve the model structure, which is also demonstrated to show nice gains. though as pointed by reviewers the novelty is limited, the study is very useful to the field.", "accepted": 1}
{"paper_id": "nips_2022_fpfDusqKZF", "review_text": "the paper proposes an approach, neural basis model nbm, that can be seen as a new subfamily of generalized additive models for interpretability. the proposed model is compared to alternatives, showing competitive performance while being computationally more efficient. the authors successfully addressed questions raised by reviewers. as also noted by the authors, a major limitation of the paper remains to be the requirement of the input features being interpretable. this would limit the applicability and utility of the proposed model, limiting the significance of the contribution.", "accepted": null}
{"paper_id": "nips_2022_2EBn01PJh17", "review_text": "this paper proposes some nice ideas on speeding up gaussian process inference based on approximating the marginal using subsamples. however, several reviewers noted gaps and potentially flaws in the technical details. the reviews as well as detailed replies during the rebuttal period will help the authors prepare a stronger revision, but the work is not airtight and is not ready for publication in its current form", "accepted": 0}
{"paper_id": "nips_2022_QNjyrDBx6tz", "review_text": "this paper proposes a conformal prediction based method for sequential prediction, relaxing the exchangeability assumption. it is robust to distribution shift, and achieves groupconditional coverage guarantees. the method is efficient, novel, and outperforms existing methods. all the reviewers, including myself, find the paper a solid contribution to the methodology and analysis, hence a clear accept.", "accepted": null}
{"paper_id": "nips_2022_Yb3dRKY170h", "review_text": "the paper presents a method for defending deep neural networks against backdoor attacks, i.e., attacks that inject triggered samples into the training set. the method can be seen as an improvement on adversarial neuron pruning anp that uses i soft weight masking swm, ii adversarial trigger recovery atr and iii sparsity regularization sr. the main focus of the paper is in the lowdata regime, especially in the oneshot setting and when the network size is small. the authors have clarified the novelty of the approach wrt to anp and have provided additional experiments addressing some of the reviewers concerns. in view of this, some of the reviewers raised their scores. however, there are still concerns regarding the novelty of the method and the difficulty of setting hyperparameters. the empirical results seem solid, however.", "accepted": null}
{"paper_id": "nips_2022_o4neHaKMlse", "review_text": "this paper proposes a twostage pretrain visual language model which can deal with both high level and region level downstream tasks. experiments show significant improvement sota models. main concerns from reviews are some missing references while the author gave detailed comparisons in the responses. although reviewer gvbus opinion is still somewhat conservative, i think the novelty of the paper is clear and the comparison to the sota is sufficient.", "accepted": null}
{"paper_id": "nips_2022_J0nhRuMkdGf", "review_text": "dear authors, we had a long discussion about this paper. overall, the reviews are positive. several reviewers raised their scores after the rebuttal phase, and they found the response by the authors satisfactory. however, there were some concerns about the novelty of the paper that i summarize here this paper combines some standard techniques and ideas from decentralized optimization and minimax optimization to obtain the presented results. hence, the algorithmic novelty of the paper is limited. perhaps the major contribution of the paper is in the vector that they decide to quantize, but still, the main idea of the paper is very similar to the singleloop variance reduction techniques that were first proposed in stochastic optimization and later used for distributed optimization. the main theoretical challenge that the authors had to face was combining quantization with the extra gradient method as highlighted in the first paragraphs of section 4.1. indeed, similar quantization ideas have been extensively studied in the distributed optimization literature and thus the algorithmic novelty seems to be very limited. similarly, excluding the aforementioned challenge extra gradient  compression the derivation of the theoretical results appears to be tedious but based on standard techniques. considering the above points, the ac and one of the reviewers found the paper below the bar as its novelty is limited. however, four reviewers voted in favor of accepting this paper, as they believe the technical novelty of the paper and its proof techniques are significant enough. i respect the majority vote and hence recommend this paper to be accepted.", "accepted": 1}
{"paper_id": "nips_2022_AQgmyyEWg8", "review_text": "the paper studies decentralized optimization and considers all machines work on the data that follow the same distribution. most of the reviewers think the paper is interesting. i recommend an acceptance.", "accepted": null}
{"paper_id": "nips_2022_xONqm0NUJc", "review_text": "this paper proposes a novel approach for finegrained image recognition, which utilizes the relational information between the global and local views of an object. it is a reasonable and important finding that not only representing local parts but relating them are critical to establishing superior performance. the authors validate their proposals effectiveness with both theoretical explanations and positive empirical results on various benchmarks. the authors also did a great job in rebuttal. they provide more clarifications, extra experiments on large datasets, and newly included error bars. most of the reviewers are satisfied with the rebuttals and discussions, and all reviewers have a consistent recommendation. we think this paper can bring new insights to the visual recognition community and help people understand how the key features and their relations work. please also include the newly added experiments and clarifications in the new revision.", "accepted": null}
{"paper_id": "nips_2022_wcBXsXIf-n9", "review_text": "this paper proposed to use leastsquares loss functions in training deep neural networks. the main idea is to encode class means, whose mutual distances are equivalent. the method is simple but efficient. however, the similar idea has been widely used in multiclass classification svm and fisher discriminant analysis and spectral clustering. more specifically, one reviewer commented that this work encodes class labels as highdimensional vectors similar onehot, and then uses a leastsquares loss. although the authors did not admit this comment, but essecially this comment is indeed right. this idea has been used such as in the following references 1 multicategory support vector machines theory and application to the classification of microarray data and satellite radiance data yoonkyung lee, yi lin  grace wahba 2 prevalence of neural collapse during the terminalphase of deep learning training vardan papyana, x. y. hanb, and david l. donoho", "accepted": 0}
{"paper_id": "nips_2022_vhKaBdOOobB", "review_text": "this paper aims to augment efficient cnns with selfattention. however, since the naive approach to selfattention is computationally expensive and would contradict the point of efficient cnns, the authors introduce a new attention mechanism which captures longrange information without substantially added computation cost. the paper demonstrates that ghostnetv2 exhibits markedly better performance at various compute limits as compared to previously proposed efficient networks. three of the reviewers were quite positive on this paper, noting the novelty of the approach and the strength of the empirical results. one reviewer had several concerns, primarily regarding comparison to nas based approaches and the novelty of the approach. i agree with the other reviewers that it is not reasonable to compare nas approaches to nonnas approaches, and agree that there are marked differences between this work and the previous work cited. i therefore recommend acceptance. i think this will be a valuable contribution to the efficient network community.", "accepted": 1}
{"paper_id": "nips_2022_aPXMGv7aeOn", "review_text": "this paper presents a new nerf method based on tensor decomposition. the method supports both compression and composability, while achieving similar results compared to standard nerf models. the method does not use a neural network. several reviewers found the paper easy to follow, the method novel  sound, and the comparisons comprehensive. two reviewers mentioned the similarity between the proposed work and tensorf. the rebuttal addressed most concerns and highlighted the differences between the two works. as tensorf is a concurrent eccv submission, the existence of tensorf should not be used against the proposed work. the ac agreed with most of the reviewers and recommended accepting the paper.", "accepted": 0}
{"paper_id": "nips_2022_NjeEfP7e3KZ", "review_text": "in this submission, the authors revisit the existing homophily metrics and point out the limitations of existing metrics in analyzing the performance of gnn. then the authors propose a novel homophily metric that specifics harmful heterophily, and further propose adaptive channel mixing acm framework to handle the harmful heterophily. although there exist some concerns about the novelty of the idea as pointed out by 9hm2 and y2du, overall, the proposed metric and framework are wellmotivated, interesting, and effective as pointed out by ichy, y2du, and s2kt, and the experiments are comprehensive and convincing as pointed out by ichy, y2du, and vqe7. due to these, here, i recommend accepting this submission. this submission also can be improved based on the suggestions by reviewers such as writing and typesetting, and hope they find the discussion useful and make this submission a better one.", "accepted": 0}
{"paper_id": "nips_2022_GFgjnk2Q-ju", "review_text": "the paper studies an alignment problem  that of agent seeking powers, and extends previous work turner, 2021  which showed that optimal policies seek power, to demonstrate more generally that parametrically retargetable policies policies whose target can be changed by simple change of hyperparameters of the agent also tend to seek power. the problem is interesting and understudied, and all reviewers agreed that the work was original, nontrivial and significant. most concerns were regarding presentation, which could be at times vague and imprecise in the mathematical parts or unintuitive in the informal parts. the authors presented a plan to significantly address clarity of the paper, which alleviated many of the reviewers concern. please do ensure that the final version include these improvements.", "accepted": 0}
{"paper_id": "nips_2022_pkfpkWU536D", "review_text": "while some of the scores on this paper are mixed, even the negative reviews highlight the quality and interest of the work and have specific and somewhat debatable technical concerns. overall, the ae recommends accept, especially in light of the detailed and thoughtful responses during the rebuttal phase. in the camera ready, the authors are encouraged to see if they can squeeze some of the new results e.g., transfer learning attempt in figure 6 and comparisons to shapeflow in the main body of the paper, where theyre more likely to be noticed.", "accepted": 0}
{"paper_id": "nips_2022_wKd2XtSRsjl", "review_text": "the paper studies the evaluation metric for multimodal generation models. the authors propose a method mid based on estimating mutual information of visual and text embeddings at sample and distribution level. from experiments, the mid correlates with human evaluation on multiple tasks texttoimage and image captioning. the authors provide theoretical intuition and analysis of mid and relation to other divergence scores. experiments are solid and convincing. the reliance on clip is discussed though other multimodal encoders than clip are not evaluated in experiments. author discussion with reviewers are helpful to better understand the paper. overall, it is a solid paper with a clearly described, simple, and effective method.", "accepted": null}
{"paper_id": "nips_2022_GCNIm4cKoRx", "review_text": "the reviewers agree that the theoretical results presented in the paper are solid and advance our understanding of the behavior of temporal difference td methods, which are at the core of most reinforcement learning algorithms. the contributions of the paper can be summarized in two main results  adaptive td combined with a relu neural network converges when the width of the network is sufficiently large;  adaptive td combined with a relu neural network converges faster than its nonadaptive counterpart. both results are important and novel. one consistent complaint among the reviewers was the paper presentation, which was considered slightly sloppy and not very accessible. we strongly encourage the authors to perform a thorough revision of the paper, paying special attention to the definition and consistency of the notation adopted. we also suggest the authors add intuitive explanations wherever possible to make the paper accessible to a wider audience.", "accepted": 0}
{"paper_id": "nips_2022_yewD_qbYifc", "review_text": "while the ideas in this paper are promising, there are issues with the papers presentation and experimental results. the paper needs to be further updated to clarify the proposed method and discuss additional related work. more extensive experimental results are also needed to show the benefits of the proposed approach.", "accepted": 0}
{"paper_id": "nips_2022_GWcdXz0M6a", "review_text": "the paper is motivated by the design of lowregret algorithms for highdimensional sparse linear bandit problems. the challenge is to obtain regret guarantees even in the datapoor regime where the number of samples the learner can gather may be smaller than the dimension. this challenge had been investigated in 12 with a regret scaling as snc_min23 s is the sparsity of the problem, n the number of samples, c_min is the maximum over all possible arm distribution of the resulting average variance. the authors propose a scheme whose regret scales at most as sn h23 where h is a new minimax constant, proven to be smaller than 1c_min. the paper also presents a matching minimax regret lower bound. to achieve this improved regret upper bound, the authors develop a new parameter estimation procedure, based notably on catonis estimator this kind of estimator has been recently advocated in rl with linear function approximation, see rewardfree rl is no harder than rewardaware rl in linear markov decision processes, wagenmaker et al., icml 2022, and the authors could mention this paper and stress the differences in the use of this estimator. the derivation of the lower bound also relies on new techniques as mentioned by one of the reviewers. overall, this is a solid contribution, even though compared to 12, the improvement is not that spectacular.", "accepted": null}
{"paper_id": "nips_2022_vfR3gtIFd8Y", "review_text": "reading the reviews, i think there are ultimately two challenges for the authors to address in this work. first i think ends up being a somewhat simple background for the community problem, as both several reviewers and the authors in their general comments point out significantly more background on kl decomposed kernels may be warranted, and this lack of background along with perhaps some simple notational differences led to what i felt like were some challenges understanding the full paper. with that being said, i dont think the above alone should be sufficient by itself to result in rejection, despite this lack of context likely being a primary contributor to final review scores. however, i do agree with reviewers concerns that there are some concrete comparisons to existing scalable gp literature missing. i think the inclusion of oak  inducing points is a start, but e.g. the use of m40 inducing points in section 3.4 is surprising to me, partly perhaps because its not clear which task this was a problem for you state early that you use m200 for the cascaded tanks task, and partly because none of the dataset sizes involved appear to me to be even remotely beyond the capability of these existing scalable gp approximations in the literature  even up to m512 or m1024 inducing points is fairly standard practice. given the reasonably good performance of oak in some of the new results even with limited inducing point sets, i think that clearly further investigation is warranted there. beyond inducing point methods, there are also nngp  vecchia models which have also been recently made variational via wu et al., 2022, and even exact gps seem readily applicable to some of the tasks considered here with access to even a single moderately powerful gpu.", "accepted": 0}
{"paper_id": "nips_2022_rnJzy8JnaX", "review_text": "after the rebuttal and discussion, two reviewers recommend acceptance, one borderline rejection. most concerns of the raised in the borderline review were addressed at a sufficient detail in the rebuttal. the ac sees no reason the reject this paper.", "accepted": 0}
{"paper_id": "nips_2022_cA8Zor8wFr5", "review_text": "this is an interesting paper with good contribution to the field. most reviews are positive.", "accepted": 1}
{"paper_id": "nips_2022_siG_S8mUWxf", "review_text": "overall this is an interesting paper. it proposed a new formulation of the equivariant graph neural network, subequivariant gnn. reviewers agree that the proposed idea could be useful to the community, albeit with perhaps small application scope. so on the novelty side, this paper is okay. the biggest concern among the reviewers is about the experiments, i.e., mostly the fair comparision. i feel the authors did a reasonable job to explaining why the current baselines were chosen and provided additional experimental evidence. authors could take in account the comments from the reviewers to improve the overall presentation of the paper.", "accepted": null}
{"paper_id": "nips_2022_vK53GLZJes8", "review_text": "this paper presents a counterexampledriven analysis of regularization in td learning with function approximation. despite the papers simplicity, the reviewers unanimously though there was a good contribution being made here, and i agree. highlights include a clarity of presentation and new insights into what is known as the deadly triad. the reviewers generally agreed that these results are relevant to deep rl today, but would have appreciated more forward guidance.", "accepted": null}
{"paper_id": "nips_2022_V88BafmH9Pj", "review_text": "all four reviewers sided to accept the paper, as the proposed contrastive search approach to mitigating text degeneration problem is simple and effective and has applications to a variety of nlg tasks. its evaluation is quite comprehensive and includes competitive baselines, human evaluation, and evaluation of both lmgeneration quality on wikitext103 and effect on a downstream task dialog. two of the reviewers were more hesitant borderline accept, but one of them was quite satisfied with the author response and the other reviewer didnt raise any major issue. the one remaining concern is that experiments with gpt2 were base on the small model, but the rebuttal shows that the findings of the paper mostly hold with bigger language models medium and large but become relatively small with xl. we suggest including these additional experiments in the next version of the paper, along with further discussions of these smaller differences.", "accepted": 1}
{"paper_id": "nips_2022_zuL5OYIBgcV", "review_text": "this work considers the task of training stateoftheart cnns with limited depth. the benefits considered in this work are related to the potential parallelization which is induced by depth reduction. this paper generated a fair bit of discussion with the reviewers about the motivations and the basic thesis. the authors do a good job of representing their viewpoint, and adding a version of this discussion to the final manuscript will undoubtedly be needed. the empirical results look quite promising, but the authors are also encouraged to further discuss adding an additional motivation to reducing depth e.g., a theoretical reason, as proposed by reviewer zbdf, which is currently only one paragraph long in the related section andor performing a deeper study of hyperparameters affecting accuracylatency. with proper framing of the question studied here, the scope of evaluation and the assumptions on the hardware, this will be an interesting contribution to neurips", "accepted": 0}
{"paper_id": "nips_2022_jJwy2kcBYv", "review_text": "reviewers appreciated the papers contribution of a novel method for unsupervised skill learning in marl. while the scores were borderline, reviewers are mostly in favor of acceptance, therefore i recommend acceptance as well. additional baselines and environments added during the rebuttal phase were important considerations in this decision.", "accepted": 0}
{"paper_id": "nips_2022_HIslGib8XD", "review_text": "the paper proposes a method for finding the best anomaly detector among a set of candidate methods that are all based on constructing a score function. the selection method is based on a leaveoneout estimate. some theoretical results are presented and proven in the appendix, and in addition, some experiments are reported. overall, this paper presents a novel and interesting method for an important problem, and the theoretical considerations are certainly a plus. the only major issue of the paper is that only 4 real world data sets were considered, and despite the fact that this problem was raised by the reviewers, the authors did not include more during the rebuttal phase. from my perspective, a strongly theoretical paper does not require extensive experiments, but the paper under review does not fall into this category. and for this reason, more experiments, on say another 15 data sets would have been really helpful. in summary, this is an interesting paper with a sufficiently good theoretical part and some promising experiments. the latter could have been more, but overall this paper should be accepted.", "accepted": 0}
{"paper_id": "nips_2022_Y4vT7m4e3d", "review_text": "the paper studies decentralized local stochastic extragradient for variational inequalities. an extragradient method is developed for this problem. theoretical results are established and complemented by simulations. while there were some concerns about the novelty of the work in the initial review, the authors adequately addressed these comments in their response. while a number of typos were present in the paper, i believe that these can be addressed as a minor revision in the final version. i do encourage the authors to carefully proofread their camera ready submission. the work is of interest to a part of the conference audience and should be accepted.", "accepted": 0}
{"paper_id": "nips_2022_2ktj0977QGO", "review_text": "the paper studies multiple instance learning mil by treating bags as auxiliary information, aiming to identify invariant causal representations using only bag labels available in the mil setting. to achieve identifiability, it is assumed that the prior distribution over the instance latent variables belongs to the nonfactorized exponential family conditioning on the bags. this allows the disentanglement between the causal and noncausal factors and only the causal ones are supposed to contribute to the instance labels while the baglevel labels are used in the proposed objective function in eq. 8 to accommodate the mil setting. experiments are conducted on multiple datasets to demonstrate the instance prediction and outofdistribution generalization performance of the proposed targetedmil algorithm. the perspective of learning invariant causal representations is new in the context of multiple instance learning. reviewers have acknowledged this interesting aspect of the proposed work. authors and reviewers engaged in a detailed discussion and the authors rebuttal helped to address some major confusions, which further improved the qualify of the paper. the authors are encouraged to more clearly highlight the key difference from two important references in the final version of the paper, including identifiable vaes and multiple instance vae, which are relevant to the proposed work. the causal inference related assumption could also be further clarified as suggested by one reviewer.", "accepted": 1}
{"paper_id": "nips_2022_V3kqJWsKRu4", "review_text": "the paper discusses a method for online video instance segmentation. reviewers appreciated the proposed method but raised concerns regarding difference of reported results to other papers, method being similar to prior work, and limited novelty. the rebuttal addressed most of the concerns prompting reviewers to increase their rating to an accept recommendation. ac doesnt see reasons to overturn an unanimous reviewer recommendation.", "accepted": null}
{"paper_id": "nips_2022_a8qX5RG36jd", "review_text": "this paper proposes a latent adaptive structureaware generative language model glm to leverage syntactic knowledge for information extraction tasks. the proposed model incorporates a latent structure induction module that automatically induces treelike structures akin to dependency and constituency trees. experiments in 12 ie benchmarks across 7 tasks showed significant improvements over the baseline. overall, all reviewers feel positively about this paper, even though they mention some aspects which can be improved in the final version. the conversion of information extraction tasks into a problem solvable by a glm problem with three different prediction modules is original and valuable, and the experiments are well designed and generally convincing although additional experiments in more recent and larger scale datasets would make the paper stronger. the author response addressed well all the concerns of the reviewers, including the addition of several missing references. i urge the authors to incorporate these in their paper and to report the runtime of their method, to better understand the tradeoff between performance and speed, as well as examples of induced tree structures produced by their method, as suggested by one of the reviewers.", "accepted": null}
{"paper_id": "nips_2022_p_g2nHlMus", "review_text": "this paper tackles fewshot learning with a transformer architecture and, inspired by the intuition that finegrained information is ignored in existing methods, uses an innerloop token reweighting method to improve results. overall the reviewers appreciated the use of modern architectures vision transformers, the reasonableness of the reweighting intuition, and experimental results. concerns were raised about comparison to existing methods with similar intuitions e.g. a mentioned by ef5w, fairness of the comparison with respect to model capacity and in general ablations demonstrating that its the method not transformers by themselves leading to improved results, and lack of principled explanations for the design choices, and computational complexity. the authors provided strong rebuttals, including new experiments using linear classifiers and prototypical approaches, use of smaller models, and a demonstration of potential pruning methods to address computational complexity. the reviewers were overall receptive to the rebuttal, and all recommended acceptance of this paper after some backandforth. the paper provides both a nice benchmark applying vision transformers to fewshot learning as well as a method that is demonstrably better through ablation studies. therefore, this paper provides several nice contributions to the community, and i recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_tuC6teLFZD", "review_text": "this paper proposes an ensembletype solution for improving the adversarial robustness of a model. the proposed idea is simple, yet novel with theoretical supports. the authors did a good job clarifying reviewers concerns and all reviewers finally recommend acceptance. ac also thinks that this is a good paper in various aspects novel idea, good writeup, solid theoretical supports and has a potential to be a generic ensembletype solution even in nonadversarialstandard setups, e.g., see 1. furthermore, the confidence prediction can be used for other purposes, e.g., outofdistribution detection 2 and active learning 3. it is useful for readers to discuss about these extensions. 1 confident multiple choice learning, kimin lee et al., icml 2017. 2 outofdistribution detection using an ensemble of self supervised leaveout classifiers, apoorv vyas et al., eccv 2018. 3 learning loss for active learning, donggeun yoo and in so kweon, cvpr 2019.", "accepted": null}
{"paper_id": "nips_2022_7KBzV5IL7W", "review_text": "this is a technically good paper, with some flaws. parts of the paper are hard to read. several questions remain, e.g. how to determine the optimal number and location of bouncepoints, and how they depend on room layout and content. the motivation behind some of the comparisons, e.g. to aacs is unclear. regardless, the overall paper presents a welldefined novel idea, and represent a significant contribution. the authors have also addressed most of the reviewers comments satisfactorily. i am recommending that the paper be accepted.", "accepted": 1}
{"paper_id": "nips_2022_UVF3yybAjF", "review_text": "the reviewers and i agree that this result is a solid, if not groundbreaking, result in the theory of robust statistics. it considers a very natural testing problem, and when the fraction of corrupted samples is constant, completely settles the statistical complexity of the problem. there are some concerns that its immediate practical impact is limited, but overall, the consensus is that the paper represents a solid technical contribution, and will be of interest to the robust statistics community, and the learning theory community at large. therefore, we believe this paper is above the bar for acceptance at neurips.", "accepted": 0}
{"paper_id": "nips_2022_Lpla1jmJkW", "review_text": "2 of the 3 reviewers highly appreciated the rebuttal and are now recommending the paper for acceptance without any reservations. the 3rd, most critical reviewer, fmuk did unfortunately not react. the new experiments learning from pixels nicely addresses the reviewers concern about having to carefully choose the system state. also the concern about the number of constants of motion is well addressed. the question about the sensitivity to noise could have been stronger fmuk was talking about physical systems that typically have accelerations as part of their states, but typical sensors only measure positionsangles which typically already produce slightly noisy measurements. applying numerical differentiation twice to get to the accelerations often results in very noisy measurements. hence the question how representative the experiments where the states  that also include dotx  are assumed to be measured perfectly are for real systems. i think this is still an interesting point to discuss  but no dealbreaker for me.", "accepted": 0}
{"paper_id": "nips_2022_2nYz4WZAne4", "review_text": "while the topic of the paper and the reported experimental results appeared to be of interest of the reviewing team, a number of limitations were put to the fore by the reviewers, who graded the paper with scores between 2 and 5, and often emphasized various issues such as a lacunary literature review see in particular comments of reviewer we4h, insufficient comparison with stateoftheart approaches and in particular on realistic testcasses bpmj, as well as on the lack of clarity see for instance in reviever uzbls review the paper is poorly structured and written to the point where it is quite difficult to understand . for all these reasons i recommend rejection.", "accepted": 0}
{"paper_id": "nips_2022_r70ZpWKiCW", "review_text": "the paper was reviewed by four expert reviewers in the field. the initial ratings were three weak accept and one weak reject. in the response to reviewer sfdh who gave weak reject, the authors clarify all the questions from the reviewer, including using labeled data in gta, the data split, training details, advantages of reweighting mechanism. while the reviewer sfdh did not acknowledge the rebuttal, the ac believes that these questions have been sufficiently addressed. given the novel approach, extensive quantitative evaluation, and clear writing, the ac agrees with the three reviewers and recommends to accept.", "accepted": 0}
{"paper_id": "nips_2022_qf12cWVSksq", "review_text": "this paper proposes a novel multibranch style architecture for vision tasks, motivated by a frequency perspective of deep network behaviors. all reviewers are very positive about the motivation, presentation and experimental results. the ac believes this should be a good contribution to the neural architecture design community.", "accepted": 1}
{"paper_id": "nips_2022_owZdBnUiw2", "review_text": "the proposed architecture, afnet, is simple yet effective for endtoend efficient video action recognition. it has good idea, well written paper, and relative solid experimental results to support the claim. the emergency reviewer gives the highest score 6, while the other reviewers do have some concerns with this paper. most of the other reviewers give the rating of borderline accept, after the authours make more efforts in the rebuttal period. in term of these, the initial recommendation would be acceptance.", "accepted": 0}
{"paper_id": "nips_2022_JVoKzM_-lhz", "review_text": "the paper received mixed reviews. after rebuttal, reviewers bq58 and qsj3 decided to raise the rating to weak accept. so all the reviewers give positive ratings and think the authors have addressed their concerns well. taking the comments of the reviewers into account, the ac decided to accept this paper at neurips.", "accepted": null}
{"paper_id": "nips_2022_ZChgD8OoGds", "review_text": "the authors propose an entropy search method for multiobjective bayesian optimization that considers the mutual information gain of the location and value of the optimizer simultaneously while selecting query points. most reviewers found the approach to be interesting. the work is commendable in its attempt to rigorously compare different informationbased acquisition functions via highquality reimplementations of algorithms e.g., pesmo. many reviewers left detailed comments that the authors addressed in part, or agreed to examine in the camera ready. examples include more rigorous examination of the performance with respect to noise and batch size, inferential statistics for the experiments, and transparent reporting on the strengths and weaknesses relative to the existing literature.", "accepted": 0}
{"paper_id": "nips_2022_zfo2LqFEVY", "review_text": "the authors propose an approach for weakly supervised audiovisual parsing of videos. they propose using learnable categorical embedding to do classaware unimodal grouping, combined with crossmodal grouping to timestamp audio, visual and audiovisual events using only video level labels. based on the feedback provided by the reviewers, especially since reviewer knz9 increased their score to borderline accept after the rebuttal period, we recommend this paper for publication at neurips 2022. the reviewers had some concerns about the paper. reviewer knz9 mentioned that the relations of the listed papers to the proposed model were not well explained  they also had some concerns about the experimental results, and the fact that only one dataset was used in the evaluation. reviewer kavj had questions about model performance with event scaling, and time resolution lower bounds. reviewer f8hf commented on the difficulty in following the notation in the paper, and pointed out the results on audio events is not improved. we thank the authors for addressing the comments of the reviewers in their review during the author feedback period. the authors seem to have addressed some of the concernsfeedback from the reviewers with detailed discussions  it would be good to include these discussions, as much as possible, in the updated paper or supplemental materials.", "accepted": 0}
{"paper_id": "nips_2022_MG3YN3z1J4M", "review_text": "the three reviewers all leaned towards rejection for this paper. one reviewer was concerned with the relatively small number of images used in the experiment and how valid the conclusions can be from that for ppp as a better metric. another confusion was over how optimality in padding can be defined. this was important because the mae and snr measures were based off of this.", "accepted": 0}
{"paper_id": "nips_2022_oW4Zz0zlbFF", "review_text": "this paper explores the generalization of minimum norm optima for various metalearning objectives, including basic erm, modelagnostic metalearning maml and implicit maml imaml. the generative model considered is mixed linear regression, in which each of tasks follows a linear  gaussian noise data model a different direction per task. the main conceptual takeaway is to tease out how the task heterogeneity affects the generalization bounds through a cross variance quantity, and to compare how much overfitting happens for the different objectives. the proof techniques largely follow bartlett et al. pnas 2020  due to the linearitygaussianity, it boils down to a series of concentration bounds. since data coming from different tasks has a different svd, this makes the proofs nontrivial to extend.", "accepted": null}
{"paper_id": "nips_2022_CCBJf9xJo2X", "review_text": "this paper joins an interesting area that tackles the use of models in the real world that are accessible publicly via apis. in these cases, there may be adversaries that attempt to steal the model. this can be done by accessing information about the model from particular queries. one of the approaches used to tackle such scenarios is to develop defenses that can detect when such models are being stolen. much of this area is focused on looking at supervised models; the authors introduce a similar approach for encoders. in the encoder case, techniques that involve the decision boundary, suitable for supervised models, no longer apply. the authors provide some technical innovations to get around this issue. essentially they look for evidence of memorization by using a metamodel. overall, the problem is wellmotivated and the tools the authors develop are interesting; the results also appear convincing. the reviewers reached a nearconsensus that the paper is worth accepting, and i agree. nothing here is extremely groundbreaking, but its a wellcrafted approach to handle a case of an important problem that hasnt been addressed yet. the authors generally responded to all of the questions the reviewers had, and i furthermore found the answers convincing. ultimately i think its worth accepting.", "accepted": null}
{"paper_id": "nips_2022_QUyasQGv1Nl", "review_text": "overall, reviewers found that the method is sound but the results are marginal. there are numerous frameworks for selfsupervised learning today. the one introduced here underperforms compared to others, like orl and densecl, as pointed out by the reviewers. the authors in their response then combined their method with orl and densecl. this resulted in 1.1 improvement over orl. this is marginal. i understand that the authors intended to reposition their work as a generalpurpose addon that increases performance. but, far more analysis is required to establish that this 1.1 improvement is real and that is meaningful. there are many tricks for ssl that improve performance by 1 or so, often these are not used because the slowdown they incur is not worth the effort. and the increase in performance is not noticeable. at least, if the authors wish the pivot in this way, then the manuscript requires a rewrite to read as an addon to many methods and to properly evaluate this. as it stands, by not comparing against orl and other methods in the main manuscript, the submission cannot be accepted as is. i encourage the authors to submit to a computer vision venue where such results may be appreciated more; where they may be given more room to thrive into something bigger. and to fully incorporate methods like orl while demonstrating that their method really does produce a meaningful improvement.", "accepted": null}
{"paper_id": "nips_2022_b90lKL1IqcF", "review_text": "it is valuable now to introduce this technical idea, even if the results do not quite match existing methods. future work building on this idea may well do so, and it would impede the progress of the subfield to demand both the new idea and sota results. the rebuttal takes on extra work building on the reviewers suggestions, which gives confidence that the final paper will be of high quality. at the same time, the primary criterion for oral presentation is importance to the community as a whole, so the relatively narrow scope 3d computer vision would really require more worldleading results, and possibly demonstrations of a wider set of applications in order to alert practitioners in adjacent subfields.", "accepted": null}
{"paper_id": "nips_2022_iCxRsZcVVAH", "review_text": "this paper proposes a simple but general way to improve exploration in rl based on the equivalence between reward shifting and the initialization of value function. the paper shows that it is straightforward to implement conservative exploitation and curiositydriven exploration based on this idea. the results on a variety of offlineonline rl settings show that a properly initialized value function i.e., shifted reward can achieve a better explorationexploitation and improve the performance of existing rl algorithms as a result. in general, most of the reviewers found that the proposed method and the results are quite interesting enough to be presented at neurips. while some reviewers had a concern about the lack of challenging tasks, the authors addressed it with updated results in the appendix. the only reviewer with a negative score did not respond during the discussion period. thus, i recommend this paper to be accepted. in the meantime, there are still remaining minor concerns about the presentation of the paper e.g., grammar, lengthy description of a simple idea, etc and the lack of discussion on limitations and related work. i highly recommend the authors to improve them by reorganizing the paper e.g., omit some details and move some important discussion from the appendix to the main text for the cameraready version.", "accepted": 0}
{"paper_id": "nips_2022_t6O08FxvtBY", "review_text": "the reviewers had significantly diverging opinions on this manuscript. the main issue under discussion was whether the framing of this paper as a lottery ticket work was correct, given that the main evaluations use no reinitialization or rewinding. on balance, i think that while one reviewer was very negative about the paper, the disagreement was mostly terminological. the substantial concern is whether the evaluation comparison wherein blo with no rewinding is compared against methods that use rewinding in figure 3 is fair. the authors respond to this by providing comparisons in figures 6a and a11 that evaluate rewinding on some tasks. however, these figures seem to show that the accuracy of blo is completely insensitive to rewindingand even to complete reinitialization in the original lotteryticket sense. this raises the natural question why not just evaluate primarily in the reinitialized case, where theres no need to redefine the term winning ticket? that is, the whole presentation seems to be backwards. the way it should be presentedevaluated is  first, we show that blo outperforms other methods in the classic lottery ticket regime, where we reset all the weights to initialization 100 rewind  this would replace the present figure 3. this would be a fair comparison, comparing classicallotteryticketsetting pruning methods to each other.  next, we show that one advantage of blo is that unlike other winningticketfinding methods, its performance is invariant to rewinding. that is, if what we want is just accuracy of the pruned model and not to do some sort of scientific investigation of the lottery ticket hypothesis then blo outperforms other methods when we dont do any rewinding at all. with this sort of presentation, i think the authors could have avoided the negative reviewers objections. despite these presentationalterminological issues, i think that theres enough technical contribution here with figures 6a and a11 to move forward with acceptance, especially considering the enthusiasm of the other reviewers and the technical novelty of the bilevel approach. the empirical results _are_ there and figures 6a and a11 show a clear connection with the lottery ticket work, theyre just presented strangely. and i think there are not any fundamental technical issues here that forbid acceptance.", "accepted": 1}
{"paper_id": "nips_2022__w-ivKc1cj", "review_text": "all three reviewers have elected to accept the paper, with two weak accepts and one accept. the reviews were thorough and demonstrated an understanding of the paper, and the authors have addressed many of the suggested edits. i find figure 2 of the paper comparison to xirl on xmagical benchmark compelling. recommendation accept.", "accepted": null}
{"paper_id": "nips_2022_BZ92dxDS3tO", "review_text": "this paper originally received slightly positive reviews overall, except for one review, which was plenty of requests of specific clarifications and comments. main issues regarded just the need of clarifying some parts of the method and put better in context of the state of the art and former evaluations. unclear novelty was another raised problem, as well as the need of offtheshelf 2d object detector and real images for training, which might affect the general applicability of the method while weakening the oneshot claim of the work. a lot of concerns were also raised about missing baselines and prior work discussion. authors provided detailed answers to the comments, also engaging in long discussions, especially with the most critical reviewer. in the end, the most positive reviewers seem to be satisfied of the answers to their comments, maintaining the original positive ratings, and also the critical reviewer resulted convinced of the discussion with authors, raising hisher score to weak accept. overall, assuming that the comments and discussions could be included in the final version, this paper can be considered acceptable for neurips 2022 publication.", "accepted": 0}
{"paper_id": "nips_2022_rY2wXCSruO", "review_text": "this work was overall positively technically evaluated with some concerns mainly related to limited experimental validation, the need to some additional justifications and explanation, and the missing computational cost analysis. the provided rebuttal responded sufficiently well to these concerns and the overall evaluation is positive.", "accepted": null}
{"paper_id": "nips_2022_Ag3ycrdh6n", "review_text": "two reviewers consider that the proposed construction is clearly innovative. and all reviewers consider that the contribution is useful to the tensor learning community. the experiments show that the proposed method yields improved performance. the three reviewers who participated in the discussion with the authors andor took into account the rebuttal of the author expressed that they were satisfied with the rebuttal. reviewer bfwm who is the only reviewer assigning a score of 4 or lower, did not consider the rebuttal and did not respond to any message after the initial review. the ac considers that his concerns have been well addressed by the authors and this reviewer states in their initial review that this work might still brings new knowledge to the area. the authors are encouraged to take into account in particular the fruitful discussion with reviewer dt2k to enhance their manuscript with additional discussions and insights, and to further strengthen their experiments if possible consider core tensors in tucker with different ranks in different modes, given that the results tend to be sensitive to the choice of hyperparameters and possibly on the hyperparameter search strategy and to the choice of dataset.", "accepted": 0}
{"paper_id": "nips_2022_Aisi2oEq1sc", "review_text": "after the rebuttal and discussion all reviewers are positive, and recommend acceptance. the ac agrees with this recommendation.", "accepted": null}
{"paper_id": "nips_2022_Iqm6AiHPs_z", "review_text": "this paper studies active labeling, which can be seen as active learning with weak supervision, and proposes an active labeling algorithm based on sgd. the reviewers found that the idea of this paper is innovative. after author response and reviewer discussion, the paper receives generally unanimous support from the reviewers. thus, i recommend acceptance.", "accepted": null}
{"paper_id": "nips_2022_NjImFaBEHl", "review_text": "this paper proposes a relatively complicated method for sourcefree unsupervised domain adaptation, which integrates several techniques into a divide and contrast framework. the idea of dividing the target data into sourcelike subset and targetspecific subset and employing global alignment and feature consistency for each subset is novel when the source data is inaccessible. the contrastive learning and memorybased mmd are novel in the context of sourcefree domain adaptation and introduce theoretical benefits in terms of the expansion theory and domain alignment theory, respectively. reviewers were on the positive side while holding some concerns on the marginal improvement over the sota methods, which were addressed in the author rebuttal. ac generally agreed that the paper has introduced a novel and solid contribution to the field, with a nice connection between algorithmic methods and theoretical insights, and recommended the paper for acceptance. authors are suggested to incorporate all rebuttal material in the revision and if possible, to work out a recipe for easing the adoption of their relatively complicated framework that comes with many modules and loss terms.", "accepted": 1}
{"paper_id": "nips_2022_17KCLTbRymw", "review_text": "the paper explores generation of a volumetric voxel map via hashing, a la kinectfusion et seq from a sequence of 2d images. this is achieved by synthesizing sensor images, and feeding them into a mapping module like kinectfusion. as the reviewers note, this is an interesting goal, and the approach is reasonable, and no novelty of the overall system was questioned. however, the reviewers concur that the synthetic dataset is not sufficient to give confidence that the system is effective in practice. the rather more limited google earth ge examples, do, however, provide evidence that this strategy is effective. as a reader, the most important figure for me is fig 3 in the supmat  a qualitative view of the rerendered ge scene. it is quite clear that the rerendered scenes have learned the characteristic 3d structures of the ge datasets, indicating that the scenespecific training data is a strong contributor to the results. the paper would have been stronger if such qualitative results had been shown for acid scenes, either trained on google maps even if the training needed to be e.g. coastline specific, or trained on depthfromstereo as indicated in the rebuttal.", "accepted": 1}
